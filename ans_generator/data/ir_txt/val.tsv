far computers humans needed information systems assistants	 Because humans can understand natural languages far better than computers can effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans.
Just enables things allows away “big ability “see” useful hidden knowledge improve chosen	 Just as a microscope enables us to see things in the “micro world” and a telescope allows us to see things far away one can imagine a “big data scope” would enable us to extend our perception ability to “see” useful hidden information and knowledge buried in the data which can help make predictions and improve the optimality of a chosen decision.
This general techniques large text users use text data kinds	 This book covers general computational techniques for managing and analyzing large amounts of text data that can help users manage and make use of text data in all kinds of applications.
produce	 Since we communicate using natural languages we produce and consume a large amount of text data every day on all kinds of topics.
This book provide approaches covering useful knowledge required information This materials authors course text	 This book intends to provide a systematic introduction to many of these approaches with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems  This book is primarily based on the materials that the authors have used for teaching a course on the topic of text data management and analysis i e.
relatively shortage IR example Retrieval The Technology RibeiroNeto et 2009 Introduction Information Retrieval Manning al.	 Information Retrieval IR is a relatively mature field and there are no shortage of good textbooks on IR for example the most recent ones include Modern Information Retrieval The Concepts and Technology behind Search by BaezaYates and RibeiroNeto 2011 Information Retrieval Implementing and Evaluating Search Engines by Büttcher et al  2010 Search Engines Information Retrieval in Practice by Croft et al  2009 and Introduction to Information Retrieval by Manning et al.
enable reduction data large nonrelevant text obtain small relevant data problem second data search reach examine data Another apart	 The first is to enable fast reduction of the data size by filtering out a large amount of nonrelevant text data to obtain a small set of most relevant data to a particular application problem  The second is to support an analyst to verify and interpret any patterns discovered from text data where an analyst would need to use search and browsing functions to reach and examine the most relevant support data to the pattern  Another feature that sets this book apart is the availability of a companion toolkit for information retrieval and text mining i.
language text.	 in natural language text.
search engines relatively data e	 In general search engines are useful anywhere there is a relatively large amount of text data e g.
information people intelligent software tools help complete tasks While supporting mining supporting access text mining	 Once again due to the overwhelming amount of information people need intelligent software tools to help discover relevant knowledge to optimize decisions or help them complete their tasks more efficiently  While the technology for supporting text mining is not yet as mature as search engines for supporting text access significant progress has been made in this area in recent years and specialized text mining tools have now been widely used in many application domains.
In structured data schemas relatively handle text explicit structure development intelligent tools requires understand content text.	 In contrast to structured data which conform to welldefined schemas and are thus relatively easy for computers to handle text has less explicit structure so the development of intelligent software tools discussed above requires computer processing to understand the content encoded in text.
Since humans understand text data computers involvement process mining analyzing absolutely necessary applications machines optimize collaboration “combined intelligence” challenge analysis.	 Since humans can understand text data far better than computers can involvement of humans in the process of mining and analyzing text data is absolutely crucial much more necessary than in other big data applications and how to optimally divide the work between humans and machines so as to optimize the collaboration between humans and machines and maximize their “combined intelligence” with minimum human effort is a general challenge in all applications of text data management and analysis.
	 1.
capability connect right time enables access new information	 With this capability a TIS can connect the right information with the right user at the right time  For example a search engine enables a user to access text information through querying whereas a recommender system can push relevant information to a user as new information items become available.
We cover access pull mode complementary user obtain information querying browsing In user information query input return documents relevant query.	 We cover both modes of information access in this book  The pull mode further consists of two complementary ways for a user to obtain relevant information querying and browsing  In the case of querying the user specifies the information need with a keyword query and the system would take the query as input and return documents that are estimated to be relevant to the query.
Nevertheless cover example type knowledge subtopics text data serve concise text	 Nevertheless we can still identify a few common categories which we cover in this book  For example one type of knowledge that a TIS can discover is a set of topics or subtopics buried in text data which can serve as a concise summary of the major content in the text data.
enhanced limited linguistic knowledge depth text robust deeper semantic domains Some capabilities e	 Current NLP techniques mostly rely on statistical machine learning enhanced with limited linguistic knowledge with variable depth of understanding of text data shallow techniques are robust but deeper semantic analysis is only feasible for very limited domains  Some TIS capabilities e g.
.	g  terms sentences documents   .
Visualization.	 Visualization.
Access.	 Part II  Text Data Access.
Analysis techniques analyzing text discover useful topical semantic patterns Chapter gives overview analysis perspective text “subjective world view allows look analysis general analysis general joint analysis text	 Text Data Analysis  This part consists of Chapters 12–19 covering a variety of techniques for analyzing text data to facilitate user digestion of text data and discover useful topical or other semantic patterns in text data  Chapter 12 gives an overview of text analysis from the perspective of data mining where we may view text data as data generated by humans as “subjective sensors” of the world this view allows us to look at the text analysis problem in the more general context of data analysis and mining in general and facilitates the discussion of joint analysis of text and nontext data.
This followed number useful minimum Chapter 13 discusses techniques discovering fundamental lexical units text data	 This is followed by multiple chapters covering a number of the most useful general techniques for analyzing text data without or with only minimum human effort  Specifically Chapter 13 discusses techniques for discovering two fundamental semantic relations between lexical units in text data i.
e clustering text	e  text clustering and text categorization.
e models Chapter 18 discusses opinions expressed data key discovery people analyzing text data them.	e  topic models  Chapter 18 discusses techniques for analyzing sentiment and opinions expressed in text data which are key to discovery of knowledge about preferences opinions and behavior of people based on analyzing the text data produced by them.
Chapter 4 companion META	 Chapter 4 introduces you to the companion toolkit META which is used in exercises in each chapter.
Exercises little background probability	 Exercises assume basic programming experience and a little mathematical background in probability and statistics.
end working powerful—yet easily written questions META Programming selecting	 The exercises at the end of each chapter give students experience working with a powerful—yet easily understandable—text retrieval and mining toolkit in addition to written questions  In a programmingfocused class using the META exercises is strongly encouraged  Programming assignments can be created from selecting a subset of exercises in each chapter.
Programming assignments created selecting subset chapter modular nature toolkit additional programming experiments existing algorithms students exercises programming project.	 Programming assignments can be created from selecting a subset of exercises in each chapter  Due to the modular nature of the toolkit additional programming experiments may be created by extending the existing system or implementing other wellknown algorithms that do not come with META by default  Finally students may use components of META they learned through the exercises to complete a larger final programming project.
You META work uses software license In MIT	 You may choose to include META in your work since it uses a permissive free software license  In fact it is duallicensed under MIT and University of IllinoisNCSA licenses.
et al RibeiroNeto	 2009 Büttcher et al  2010 and BaezaYates and RibeiroNeto 2011.
algorithms robust text data minimum human	 The focus of this book is on covering algorithms that are general and robust which can be readily applied to any text data in any natural language often with no or minimum human effort.
.	   .
zap zirconium text data probability distribution θ .	    zap zirconium zoo and we could model the text data with a probability distribution θ .
Let index represent pcircle respectively gives 1	 Let each index in θS represent psquare pcircle ptriangle respectively  That gives θS 1 3 1 2 1 6 .
draw colors color distribution c1 c2 random pX	 That is we draw two colors from the color distribution  Does the knowledge of c1 inform the probability of c2 No since each draw is done “independently” of the other  In the case where two random variables X and Y are independent pX Y   pXpY .
26 Chapter Background random denoting Y evidence pX “prior” knowledge evidence Y .	 26 Chapter 2 Background Specifically we may view random variable X as denoting our hypothesis and Y as denoting the observed evidence  pX can thus be interpreted as our prior belief about which hypothesis is true it is “prior” because it is our belief before we have any knowledge about evidence Y .
provide update pX Y X independent updating happen case Y pX 2.	 Bayes’ rule is seen to connect the prior belief and posterior belief and provide a way to update the prior belief pX based on the likelihood of the observed evidence Y and obtain the posterior belief pX  Y   It is clear that if X and Y are independent then no updating will happen as in this case pX  Y   pX  2.
	1.
	e.
easier compute derivative.	 This is why we could use the logarithm transformation in the example above which made it easier to compute the derivative.
We’ll use Bayes’ rewrite pθ belief parameters given data D θpθ	 We’ll use Bayes’ rule to rewrite pθ  D or our belief of the parameters given data as pθ  D  pD  θpθ pD 2.
There ways estimate	 There are multiple ways to compute a point estimate based on a posterior distribution.
6 Probabilistic Their statistical previous start apply probabilistic	6 Probabilistic Models and Their Applications With the statistical foundation from the previous sections we can now start to see how we might apply a probabilistic model to text analysis.
	 2  Learn its parameters.
3.	 3.
It A data The important concept information entropy building block measures.	 It is applied in many fields such as electrical engineering computer science mathematics physics and linguistics  A few concepts from information theory are very useful in text data management and analysis which we introduce here briefly  The most important concept of information theory is entropy which is a building block for many other measures.
depicting variable X.	 In the common example of a coin the two values would be 1 or 0 depicting heads or tails and the random variable representing these outcomes is X.
The 1 In HX 1 probabilities symmetric U 1	 The most random is when pX  1  1 2   In that case HX  1 the maximum value  Since the two probabilities are symmetric we get a symmetric inverted U shape as the plot of HX as pX  1 varies.
case 1 value Since symmetric inverted pX varies It’s particular variable maximum	 In that case HX  1 the maximum value  Since the two probabilities are symmetric we get a symmetric inverted U shape as the plot of HX as pX  1 varies  It’s a good exercise to consider when a particular random variable not just the coin example has a maximum or minimal value.
HY For w harder	 You can calculate HY and HZ to confirm this answer  For our applications it may be useful to consider the entropy of a word w in some context  Here highentropy words would be harder to predict.
completely determined HX Y 0 know Y uncertainty X independent HX Y original i.	 Intuitively if X is completely determined by Y  then HX  Y   0 since once we know Y  there would be no uncertainty in X whereas if X and Y are independent then HX  Y  would be the same as the original entropy of X i.
16 I tends large correlated Y Y extreme HX I X 0.	 2 16 It is easy to see that I X Y  tends to be large if X and Y are correlated whereas I X Y  would be small if X and Y are not so related indeed in the extreme case when X and Y are completely independent there would be no reduction of entropy and thus HX  HX  Y  and I X Y   0.
Any topic clearly scope introduce basic machine learning needed better understand Machine techniques learning.	 Any indepth treatment of this topic would clearly be beyond the scope of this book but here we introduce some basic concepts in machine learning that are needed to better understand the content later in the book  Machine learning techniques can often be classified into two types supervised learning and unsupervised learning.
learned able unseen x	 Once the function is learned the computer would be able to take unseen values of x and compute the function f x.
occur use predict nontext stock later classification vector f x.	 The regression problem may occur when we use text data to predict another nontext variable such as sentiment rating or stock prices both cases are also discussed later  In classification as well as regression the input data instance x is often represented as a feature vector where each feature provides a potential clue about which y value is most likely the value of f x.
More let X feature vector A feature attribute objects news article.	 More formally let our collection of objects be X such that xi ∈ X is a feature vector that represents object i  A feature is an attribute of an object that describes it in some way  For example if the objects are news articles one feature could be whether the word good occurred in the article.
sports A classifier ∈	 Thus yi may be sports in our news article classification setup and yj could be politics  A classifier is a function f   that takes a feature vector as input and outputs a predicted label ŷ ∈ Y.
data 36 cases But function	 In applications the training data are generally 36 Chapter 2 Background all the labelled examples that we can generate and the test cases are the data points to which we would like to apply our machine learning program  But what does the function f .
actually simple example sentiment Y positive x’s term good negative	 actually do Consider a very simple example that determines whether a news article has positive or negative sentiment i e  Y  positive negative positive if x’s count for the term good is greater than 1 negative otherwise.
2006 Mitchell text data language	 Bishop 2006 Mitchell 1997  3Text Data Understanding In this chapter we introduce basic concepts in text data understanding through natural language processing NLP.
text TIS helping access analyze understand content data.	 NLP is a foundation of text information systems because how effective a TIS is in helping users access and analyze text data is largely determined by how well the system can understand the content of text data.
Content logically text data analysis management sentence native challenging one.	 Content analysis is thus logically the first step in text data analysis and management  While a human can instantly understand a sentence in their native language it is quite challenging for a computer to make sense of one.
For example mapped boy e.	 For example the noun phrase a boy can be mapped to a semantic entity denoting a boy i e.
reasons difficult 1 “common sense” knowledge communication assume hearer knowledge We lot knows need words clarify	 Specifically there are two reasons why NLP is very difficult  1 We omit a lot of “common sense” knowledge in natural language communication because we assume the hearer or reader possesses such knowledge thus there’s no need to explicitly communicate it  2 We keep a lot of ambiguities which we assume the hearerreader knows how to resolve thus there’s no need to waste words to clarify them.
In “shallow” language arbitrary manner tends scale robust analyzing unrestricted text.	 In summary only “shallow” analysis of natural language processing can be done for arbitrary text and in a robust manner “deep” analysis tends not to scale up well or be robust enough for analyzing unrestricted text.
number different levels At end retrieval relatively cases determine document certain	2 shows a number of TIS tasks that require somewhat different levels of NLP  At one end of the spectrum tasks such as retrieval and classification are relatively easy and in most of the cases they don’t require deep NLP indeed looking at the keywords mentioned in text is often sufficient to determine whether a document is relevant to a query or about a certain topic.
bypass accurately understanding language directly solve application problem.	 When it comes to a specific application task it is often possible to bypass the difficulty in accurately understanding natural language and go directly to solve the application problem.
example Eliza system2 supposed therapist user	 A wellknown example is the Eliza system2 which is supposed to play the role of a therapist and make a dialogue with a human user Weizenbaum 1966.
Person Men	 The following is a sample dialogue  Person Men are all alike.
	 2  httpwww cs.
	cs nott.
cs ac.	cs nott ac.
Even difficult statistical approaches NLP statistical approaches uncertainties associated natural language principled	 Even difficult tasks like machine translation can be done by such statistical approaches  The most useful NLP techniques for building a TIS are statistical approaches which tend to be much more robust than symbolic approaches  Statistical language models are especially useful because they can quantify the uncertainties associated with the use of natural language in a principled way.
3 3 Text	 3 3 Text Representation Techniques from NLP allow us to design many different types of informative features for text objects.
recognizing units meaning	 We’re not even recognizing words which are the basic units of meaning for any language.
easy entities right	 It’s not always easy to identify all the entities with the right types and we might make mistakes.
The useful.	 Relations are even harder to find again we might make mistakes  The level of representation is less robust yet it’s very useful.
can’t significant computation large	 As one would imagine we can’t do that all the time for all kinds of sentences since it may take significant computation time or a large amount of training data.
3 generally sophisticated NLP techniques.	3 shows that if we move downwards we generally see more sophisticated NLP techniques.
generated consumed humans.	 Text data are generated by humans and are meant to be consumed by humans.
In okay completely representation text interpreted humans annotating work	 In that sense it’s okay that computers may not be able to have a completely accurate representation of text data  Patterns that are extracted from text data can be interpreted by humans and then humans can guide the computers to do more accurate analysis by annotating more data guiding machine learning programs to make them work more effectively.
The column shows visualizes generality representation accurately text data general column shows shows representation.	 The table summarizes what we have just seen the first column shows the type of text representation while the second visualizes the generality of such a representation  By generality we mean whether we can do this kind of representation accurately for all the text data very general or only some of them not very general  The third column shows the enabled analysis techniques and the final column shows some examples of applications that can be achieved with a particular level of representation.
inferences hypotheses example gene function reading facts track researchers’ related	 The computer can make inferences about some of the hypotheses that a biologist might be interested in  For example it could determine whether a gene has a certain function by reading literature to extract relevant facts  It could use a logic system to track answers to researchers’ questions about what genes are related to what functions.
questions genes	 It could use a logic system to track answers to researchers’ questions about what genes are related to what functions.
sequence words language following sequences pToday Wednesday 0 Wednesday	 It thus gives any sequence of words a potentially different probability  For example a language model may give the following threeword sequences different probabilities pToday is Wednesday  0 001 pToday Wednesday is  0.
Clearly model	000001 Clearly a language model can be contextdependent.
language model unigram language model assume sequence generating word Thus sequence words equal product word Formally V .	 The simplest language model is the unigram language model in which we assume that a word sequence results from generating each word independently  Thus the probability of a sequence of words would be equal to the product of the probability of each word  Formally let V be the set of words in the vocabulary and w1 .
sequence wi V word.	  wn a word sequence where wi ∈ V is a word.
.	 .
D expect pD D′ diet control θ2.	 Intuitively if D is a text mining paper we would expect pD  θ1  pD  θ2 while if D′ is a blog article discussing diet control we would expect the opposite pD′  θ1  pD′  θ2.
This standard statistics	 This is a standard problem in statistics called parameter estimation and can be solved using many different methods.
	 3.
general English science research database text words functional words	e  a general English text database a computer science research article database and a text mining research paper  In general the words with the highest probabilities in all the three models are those functional words in English because such words are frequently used in any text.
main words tend computer.	 The main idea for doing this is to see what other words tend to cooccur with the word computer.
filter model It language	 To filter out such common words we need a model for such words which can then tell us what words should be filtered  It is easy to see that the general English language model i.
	7.
example documents language	 For example we can represent both documents and queries as being generated from some language model.
background reader sufficient information future dealing powerful	 Given this background however the reader should have sufficient information to understand the future chapters dealing with this powerful statistical tool.
Bibliographic Further textbooks NLP Speech Statistical 1999 Natural Understanding 1995 An statistical Statistical Methods Recognition	 Bibliographic Notes and Further Reading There are many textbooks on NLP including Speech and Language Processing Jurafsky and Martin 2009 Foundations of Statistical NLP Manning and Schütze 1999 and Natural Language Understanding Allen 1995  An indepth coverage of statistical language models can be found in the book Statistical Methods for Speech Recognition Jelinek 1997.
contains detailed discussion retrieval covered later	 Zhai 2008 contains a detailed discussion of the use of statistical language models for information retrieval some of which will be covered in later chapters of this book.
An topic NLP covered information extraction A comprehensive topic Sarawagi 2008 2012.	 An important topic in NLP that we have not covered much in this chapter is information extraction  A comprehensive introduction to this topic can be found in Sarawagi 2008 and a useful survey can be found in Jiang 2012.
requires dataset The META file https metatoolkit.	 Running the default classification experiment requires a dataset to operate on  The 20newsgroups dataset is specified in the default META config file and can be downloaded here https metatoolkit.
metadata 4 META Toolkit Data Management Analysis tokenize dataset particular classification If functionality desired META’s API applications	 Place it in the metadata directory  58 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis to tokenize the dataset and a particular classification algorithm to run for this example  If more advanced functionality is desired programming in C is required to make calls to META’s API applications programming interface.
containing discussion surrounding toolkit.	meta toolkit org containing discussion surrounding the toolkit.
Design goal improve complement current body source information retrieval software software	 4 1 Design Philosophy META’s goal is to improve upon and complement the current body of open source machine learning and information retrieval software  The existing environment of this open source software tends to be quite fragmented.
check version control software installing necessary prerequisites.	 To actually download the toolkit users will check it out with the version control software git in their command line terminal after installing any necessary prerequisites.
0 files	0 The second step is to make sure that any necessary model files are also downloaded.
filecorpus document	 filecorpus  each document is its own file and the name of the file becomes the name of the document.
dat labels gz.	dat labels gz.
orgoverviewtutorial html.	orgoverviewtutorial html.
4 4 step creating index text process.	html  4 4 Tokenization with META The first step in creating an index over any sort of text data is the “tokenization” process.
4 4 The creating sort	 4 4 Tokenization with META The first step in creating an index over any sort of text data is the “tokenization” process.
level simply converting individual vectors counts sparse vectors output structures text process layers way text analyzed An cases chain” generate process filter chains defined zero class’s	 At a high level this simply means converting individual text documents into sparse vectors of counts of terms—these sparse vectors are then typically consumed by an indexer to output an invertedindex over your corpus  META structures this text analysis process into several layers in order to give the user as much power and control over the way the text is analyzed as possible  An analyzer in most cases will take a “filter chain” that is used to generate the final tokens for its tokenization process the filter chains are always defined as a specific tokenizer class followed by a sequence of zero or more filter classes each of which reads from the previous class’s output.
converts documents characters come chained text	 converts documents into streams of single characters  Filters come next and can be chained together  They define ways that text can be modified or transformed.
define transformed examples	 They define ways that text can be modified or transformed  Here are some examples of filters  lengthfilter.
Here examples	 Here are some examples of filters.
	icuproject.
org different ICU different ways httpwww.	org note that different versions of ICU will tokenize text in slightly different ways httpwww.
ntu	ntu edu.
For example unigram words bigram tree features analyzers ngramword analyzers ngrampos icutokenizer type crfprefix pathtocrfmodel method filter type features subtree If di filter like	 This is useful for combining multiple different kinds of features together into your doc ument representation  For example the following configuration would combine unigram words bigram partofspeech tags tree skeleton features and subtree features  analyzers method  ngramword ngram  1 filter  defaultchain analyzers method  ngrampos ngram  2 filter  type  icutokenizer type  ptbnormalizer crfprefix  pathtocrfmodel analyzers method  tree filter  type  icutokenizer type  ptbnormalizer features  skel subtree tagger  pathtogreedytaggermodel parser  pathtosrparsermodel If an application requires specific text analysis operations one can specify di rectly what the filter chain should look like by modifying the configuration file.
chain filter	filter blocks each of which defines a step in the filter chain  All filter chains must start with a tokenizer.
	apache.
	ac.
cs.	 httpcogcomp cs.
html httpterrier	html httpterrier org httpwww.
illinois.	shtml httpcogcomp cs illinois.
edupagesoftwareview l httpscikitlearn.	cs illinois edupagesoftwareview l Curator httpscikitlearn.
illinois edupagesoftwareview httpscikitlearn.	illinois edupagesoftwareview l Curator httpscikitlearn.
	edupagesoftwareview l Curator httpscikitlearn.
META	 META can be used together with existing toolkits in multiple ways.
In text data document format This familiar techniques	 Exercises In its simplest form text data could be a single document in  txt format  This exercise will get you familiar with various techniques that are used to analyze text.
txt stop Now profile program remove stop	toml twocities txt stop Now use the profile program to remove stop words from the document twocities.
txt profile program stop	txt stop Now use the profile program to remove stop words from the document twocities.
improved version Porter 1980 Some examples runs run argues → lies lying lie lie stemmer read	 It is a slightly improved version from the original Porter Stemmer from 1980  Some examples are run runs running → run argue argued argues arguing → argu lies lying  lie → lie META uses the Porter2 stemmer by default  You can read more about the Porter2 stemmer here httpsnowball.
tartarus.	tartarus.
	illinois edumassung1p2sdemo.
The parse rooted sentence noun NP verb VP period.	1  The parse tree is rooted with S denoting Sentence the sentence is composed of a noun phrase NP followed by a verb phrase VP and period.
technique frequency counting This	 Frequency Analysis  Perhaps the most common textprocessing technique is frequency counting  This simply counts how many times each unique word appears in a document or corpus.
including words words words I took	 not including stop words  Instead of single words we can also look at strings of n words called ngrams  Consider this sentence I took a vacation to go to a beach.
Consider sentence	 Consider this sentence I took a vacation to go to a beach.
vector counts downside order This word context This document.	 This vector of counts representation does have a downside though we lost the order of the words  This representation is also known as “bagofwords” since we only know the counts of each word and no longer know the context or position  This unigram counting scheme can be used with POS tags or any other type of token derived from a document.
Use ngram frequency n 1	 Use the following three commands to do an ngram frequency analysis on a document for n ∈ 1 3.
frequent majority given	 These are the most frequent words and make up a majority of text  At the same time many words may only appear once in a given document.
We plot rank word axis count axis graph word given collection.	 We can plot the rank of a word on the x axis and the frequency count on the y axis  Such a graph can give us an idea of the word distribution in a given document or collection.
plays important applications.	 Text access technology plays two important roles in text management and analysis applications.
The goal data information right connection users fetch takes	 The general goal of text data access is to connect users with the right information at the right time  This connection can be done in two ways pull where the users take the initiative to fetch relevant information out from the system and push where the system takes the initiative to offer relevant information to users.
highlevel overview problem text retrieval In techniques supporting access	 In this chapter we will give a highlevel overview of these two modes of text data access  Then we will formalize and motivate the problem of text retrieval  In the following chapters we will cover specific techniques for supporting text access in both push and pull modes.
1	1 describes how these modes fit together along with querying and browsing.
user initiates typically engine.	 In pull mode the user initiates the access process to find the relevant text data typically by using a search engine.
information need disappear satisfied.	 a temporary information need that might disappear once the need is satisfied.
social understand decide explore information entity related event e	 Another example is that during the process of analyzing social media data to understand opinions about an emerging event the analyst may also decide to explore information about a particular entity related to the event e g  a person which can also trigger a search activity.
relevant However exact want taxi attraction user knowledge target pages use query related pages information.	 Push 75 “right keywords” which would bring to the user relevant pages directly  However if a tourist doesn’t know the exact address of an attraction the tourist may want to take a taxi to an approximate location and then walk around to find the attraction  Similarly if a user does not have a good knowledge about the target pages he or she can also use an approximate query to reach some related pages and then browse into truly relevant information.
In longterm analytics mode monitor data relevant social media related Another scenario push mode appropriately In producer information disseminating	 In some longterm analytics applications it would also be desirable to use the push mode to monitor any relevant text data such as relevant social media about a topic related to the application  Another scenario of push mode is producerinitiated recommendation which can be more appropriately called selective dissemination of information SDI  In such a scenario the producer of information has an interest in disseminating the information among relevant users and would push an information item to such users.
product search pages example.	 Advertising of product information on search result pages is such an example.
Ad extremely important ad information far informa needs.	 Ad hoc retrieval is extremely important because ad hoc information needs show up far more frequently than longterm informa tion needs.
In Figure 5 snapshot httptiman.	 In Figure 5 2 we show a snapshot of a prototype system httptiman.
constructed based search engine regular search dining 5 2 Sample topic querying	eduprojsosurf where a topic map automatically constructed based on a set of queries collected in a commercial search engine has been added to a regular search ClickThrough Search Result for dining table Figure 5 2 Sample interface of browsing with a topic map where browsing and querying are naturally integrated.
When user submits query search results search engine shown pane.	 When a user submits a new query through the search box the search results from a search engine will be shown in the right pane.
The search region.	 The search result pane would be updated with new results corresponding to the documents in the selected topic region.
This needed ad hoc like immediately TR search engine.	 This is a frequently needed task because users often have temporary ad hoc information needs for various tasks 5 3 Text Retrieval 79 and would like to find the relevant information immediately  The system to support TR is a text retrieval system or a search engine.
like relevant information immediately The support TR text search engine.	3 Text Retrieval 79 and would like to find the relevant information immediately  The system to support TR is a text retrieval system or a search engine.
U.	 U.
structured data easier schema meaning field	 Therefore structured data are often easier to manage and analyze since they conform to a clearly defined schema where the meaning of each field is well defined.
In databases determining satisfy user’s major maining quickly especially queries challenge exists important figure documents worrying quickly database maintain data power	 In databases since what items should be returned is clearly specified there is no challenge in determining which data elements satisfy the user’s query and thus should be returned a major re maining challenge is how to find the answers as quickly as possible especially when there are many queries being issued at the same time  While the efficiency challenge also exists in a search engine a more important challenge there is to first figure out which documents should be returned for a query before worrying about how to return the answers quickly  In database applications—at least tradi tional database applications—it is also very important to maintain the integrity of the data that is to ensure no inconsistency occurs due to power failure.
way prove method better rely evaluation test collections In database research prove algorithm complexity simulation	 Thus there is no mathematical way to prove that one answer is better than another or prove one method is better than another  Instead we always have to rely on empirical evaluation using some test collections and users  In contrast in database research since the main issue is efficiency one can prove one algorithm is better than another by analyzing the computational complexity or do some simulation study.
retrieval—the accurately	 Note that however when doing simulation study to determine which algorithm is faster we also face the same problem as in text retrieval—the simulation may not accurately reflect the real applications.
Overview Text Data Because difference communities basis Databases widespread applications domain strong studies retrieval community early	 82 Chapter 5 Overview of Text Data Access Because of the difference the two fields have been traditionally studied in differ ent communities with a different application basis  Databases have had widespread applications in virtually every domain with a wellestablished strong industry  The IR community that studies text retrieval has been an interdisciplinary community involving library and information science and computer science but had not had a strong industry base until the Web was born in the early 1990s.
database important online fields structures closer closer questions right “How rank “How interactive search” important prob	 Furthermore because of the inherent similarity between database search and TR because both efficiency and effectiveness accuracy are important and because most online data has text fields as well as some kind of structures the two fields are now moving closer and closer to each other leading to some common fundamental questions such as “What should be the right query language” “How can we rank items accurately” “How do we find answers quickly” and “How do we support interactive search” Perhaps the most important conclusion from this comparison is that the prob lem of text retrieval is an empirically defined problem.
This works answered reasoning mathematical	 This means that which method works better cannot be answered by pure analytical reasoning or mathematical proofs.
	 .
user’s q .	 A user’s query q  q1 q2 .
Similarly di1	 Similarly a document di  di1     .
means expect return set Thus return R′q alterna tive	 This means that it is unrealistic to expect a computer to return exactly the set Rq unlike the case in database search where this is feasible  Thus the best a computer can do is to return an approximation of Rq which we will denote by R′q  Now how can a computer compute R′q At a high level there are two alterna tive strategies document selection vs.
That binary clas 1 1 d relevant d 0 nonrelevant.	 That is we will design a binary clas sification function or an indicator function f q  d ∈ 0 1  If f q  d  1 d would be assumed to be relevant whereas if f q  d  0 it would be nonrelevant.
Often 84 5 In query relevant documents forcing binary decision delivery query	 Often the query is 84 Chapter 5 Overview of Text Data Access either overconstrained or underconstrained  In the case of an overconstrained query there may be no relevant documents matching all the query words so forcing a binary decision may result in no delivery of any search result  If the query is underconstrained too general there may be too many documents matching the query resulting in overdelivery.
designing retrieval	 In particular in order to implement the program to do that we have to have a computational definition of relevance and we achieve this goal by designing a retrieval model which gives us a formalization of relevance.
Robertson oretical foundation framing retrieval problem ranking More related papers IR Information Willett survey	 The probability ranking principle Robertson 1997 is generally regarded as the the oretical foundation for framing the retrieval problem as a ranking problem  More historical work related to this as well as a set of important research papers in IR up to 1997 can be found in Readings in Information Retrieval Sparck Jones and Willett 1997  A brief survey of IR history can be found in Sanderson and Croft 2012.
models different thinking end similar The proven diagnosing model developing	 Interestingly although all these models are based on different thinking in the end the retrieval functions tend to be very similar and involve similar variables  The axiomatic retrieval framework has proven effective for diagnosing deficiencies of a retrieval model and developing improved retrieval models accordingly e.
g Zhai 2011 Although proposed perimentation effective	g  BM25 Lv and Zhai 2011  Although many models have been proposed very few have survived extensive ex perimentation to prove effective and robustness.
This TF.	 This is called a term frequency TF.
general occurs frequently document value larger Another	 In general if the word occurs more frequently in the document the value of this function would be larger  Another factor is the document length.
Here similar query	 Here we assume that if a document is more similar to a query than another document then the first document would be assumed to be more relevant than the second one.
representation document It	 What does this mean in terms of representation of the document It means that we will rely solely on this vector to represent the original document and thus ignore everything else including e g  the order of the words which may sometimes be important to keep.
probably programming library vector documents programming.	 As you can probably guess the topic is likely about programming language and the library is actually a software library  By using this vector space representation we can intuitively capture the dif ferences between topics of documents  Next d3 is pointing in a direction that might be about presidential and programming.
That’s refined order actually suggest implemented First concepts	 That’s why this is called the vector space retrieval framework  It has to be refined in order to actually suggest a particular function that can be implemented on a computer  First it did not say how to define or select the basic concepts terms.
3 Space Retrieval 6 Instantiation Vector In section instantiate vector specific ranking	3 Vector Space Retrieval Models 93 6 3 1 Instantiation of the Vector Space Model In this section we will discuss how to instantiate a vector space model so that we can get a very specific ranking function.
vector space.	 For example it did not say how we should define the dimensions of the vectors  It also did not say how we place a document vector or query vector into this space.
strategy document yi zero means corresponding present document When it’s zero absent.	 Here the simplest strategy is to use a bit vector to represent both a query and a document and that means each element xi and yi would take a value of either zero or one  When it’s one it means the corresponding word is present in the document or query  When it’s zero it’s absent.
vocabulary occur	 Many words in the vocabulary don’t occur in a single document many words will only occasionally occur in a given document.
Most words vocabulary particular document Now documents query vector compute similarity product dot vectors products	 Most words in the vocabulary will be absent in any particular document  Now that we have placed the documents and the query in the vector space let’s look at how we compute the similarity between them  A commonly used similarity measure is the dot product the dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors.
place vectors space similarity.	 Then we made assumptions about how we place vectors in the vector space and how we define the similarity.
You probably relevant relevant.	 You may realize that some documents are probably relevant and others probably not relevant.
If ask user R′q.	 If we ask you to rank these documents how would you rank them Your answer as the user is the ideal ranking R′q.
Most d4 probably match	 Most users would agree that d4 and d3 are probably better than the others since these two really cover the query well  They match news presidential and campaign so they should be ranked on top.
vector space model close think model documents.	 Let’s see if our vector space model could do the same or could do something close to our ideal ranking  First think about how we actually use this model to score documents.
In Figure 5	 In Figure 6 5 we show two documents d1 and d3 and we have the query here also.
news	 In d3 it matched news presidential and campaign.
relevant	 Intuitively d3 is more relevant and should be scored higher than d2.
dimensions 2 queries vectors 3.	 We need to do three things 1  define the dimensions the concept of what a document is 2  decide how to place documents and queries as vectors in the vector space and 3.
decide place queries define similarity	 decide how to place documents and queries as vectors in the vector space and 3  define the similarity between two vectors.
scoring scores number query words showed space work improve topic section.	3 Vector Space Retrieval Models 97 and showed that this scoring function scores a document based on the number of distinct query words matched in it  We also showed that such a simple vector space model still doesn’t work well and we need to improve it  This is the topic for the next section.
	3.
6 d3 d2 problem capture .	6 we would like d4 to be ranked above d3 and d2 is really not relevant  The problem here is that this function couldn’t capture the following characteristics  .
particular vectors vector Naturally revisit	 In particular it has to do with how we place the vectors in the vector space  Naturally in order to fix these problems we have to revisit those assumptions.
3 Space 99 d3 scores.	3 Vector Space Retrieval Models 99 Unfortunately d2 and d3 still have identical scores.
count collection M documents M 5 higher presidential suggests global statistics information decrease dimension rep d2.	 If you count the occurrence of the word in the whole collection of M documents where M  5 then we would see that about has a much higher count than presidential  This idea suggests that we could somehow use the global statistics of terms or some other information to try to decrease the weight of the about dimension in the vector rep resentation of d2.
increase weight presidential vector score three.	 At the same time we hope to somehow increase the weight of presidential in the d3 vector  If we can do that then we can expect that d2 will get an overall score of less than three while d3 will get a score of about three.
Let dfw plot IDF varying illustrated Figure 6 10.	 Let k represent dfw if you plot the IDF function by varying k then you will see a curve like the one illustrated in Figure 6 10.
form	 Whether there is a better form of 6.
evaluation skills 9 test different If diagonal figure reasonable IDF IDF “these terms essentially	 With the evaluation skills you will learn in Chapter 9 you can test your different instantiations  If we use a linear function like the diagonal line as shown in the figure it may not be as reasonable as the IDF function we just defined  In the standard IDF we have a dropping off point where we say “these terms are essentially not very useful.
dropping “these This sense occurs it’s term common.	 In the standard IDF we have a dropping off point where we say “these terms are essentially not very useful ” This makes sense when the term occurs so frequently that it’s unlikely to differentiate two documents’ relevance since the term is so common.
course works better validated running data	 Of course which one works better still has to be validated by running experiments on a data set.
	11.
	 6.
3.	3.
The dfw query term query count w document cw cw reason received count	 The other is the document frequency dfw which is the number of documents that contain w  The other variables involved in the formula include the count of the query term w in the query and the count of w in the document represented as cw q and cw d respectively  Looking at d5 again it’s not hard to realize that the reason why it has received a high score is because it has a very high count of the term campaign.
Intriguingly order lower contribution matching term	 Intriguingly in order to lower the score for this document we need to somehow restrict the contribution of matching this term in the document.
That idea 6 14.	 That is exactly the idea of TF transformation illustrated in Figure 6 14.
formula upper bound effective.	 The BM25 TF formula we discussed has an upper bound while being robust and effective.
6 3.	 6 3.
Document Length In section issue	3 5 Document Length Normalization In this section we will discuss the issue of document length normalization.
In case	 In such a case we don’t want to penalize this long document.
Here use average document pivot reference	 1996  Here the idea is to use the average document length as a pivot or reference point.
interesting property set b indicating length normalization	 This normalizer has an interesting property first we see that if we set the parameter b to zero then the normalizer value would be one indicating no length normalization at all.
normalized sublinear length TF formula denominator smaller	 There is also a query TF component and in the middle there is normalized TF  For this we have the double logarithm as we discussed before this is to achieve a sublinear transformation  We also put a document length normalizer in the denominator of the TF formula which causes a penalty for long documents since the larger the denominator is the smaller the TF weight is.
b The formula Okapi BM25 length formula IDF component query TF component.	 The document length normalization is controlled by the parameter b  The next formula is called Okapi BM25 or just BM25  It’s similar to the pivoted length normalization formula in that it has an IDF component and a query TF component.
It’s easier English languages natural language processing boundaries There improve similarity function we’ve	 It’s easier in English when we have a space to separate the words but in some other languages we may need to do some natural language processing to determine word boundaries  There is also possibility to improve the similarity function  So far we’ve used the dot product but there are other measures.
The product If consider different regarded vectors means normalize vector	 The dot product still seems the best and one of the reasons is because it’s very general in fact it’s sufficiently general  If you consider the possibilities of doing weighting in different ways cosine measure can be regarded as the dot product of two normal ized vectors  That means we first normalize each vector and then we take the dot product.
structured documents.	 This method has worked very well for scoring structured documents.
researchers addressed problem long To add constant normalization formula.	 Here researchers have addressed the problem of overpenalization of long documents by BM25  To address this problem the fix is actually quite simple  We can simply add a small constant to the TF normalization formula.
what’s small fix documents BM25.	 But what’s interesting is that we can analytically prove that by doing such a small modification we will fix the problem of overpenalization of long documents by the original BM25.
tics ranking heuristics include TF term transformation	 We generally need to use multiple heuris tics to design a ranking function we gave some examples which show the need for several heuristics which include   TF term frequency weighting and sublinear transformation .
query likelihood assumption query pq d Intuitively user order retrieve d condition document R 1 interpreted user likes document let’s retrieval models.	 In query likelihood our assumption is that this probability of relevance can be approximated by the probability of a query given a document and relevance pq  d  R  1  Intuitively this probability just captures the following probability if a user likes document d how likely would the user enter query q in order to retrieve document d The condition part contains document d and R  1 which can be interpreted as the condition that the user likes document d  To understand this idea let’s first take a look at the basic idea of probabilistic retrieval models.
case clicked	 In this case let’s say the user clicked on document d1 so there’s a one associated with the pair q1 d1.
large search data ask question relevance” look entries particular d particular likely We q d count column ratio Clearly 1 d pR 0 d q 1.	 We can imagine that we have a large amount of search data and are able to ask the question “how can we estimate the probability of relevance” Simply if we look at all the entries where we see a particular d and a particular q we can calculate how likely we will see a one in the third column  We can first count how many times we see q and d as a pair in this table and then count how many times we actually have also seen a one in the third column and compute the ratio  Clearly pR  1  d  q  pR  0  d  q  1.
5 What d3 d2 R cases.	5  What about d2 and d3 For d2 R is equal to 1 in both cases.
What For equal	 What about d2 and d3 For d2 R is equal to 1 in both cases.
In queries queries	 In fact there are even more unseen queries because you cannot predict what queries will be typed in by users.
relevant If look conditional obvious estimate big 6.	 Note that we have made an interesting assumption here we assume that a user formulates the query based on an imaginary relevant document  If you just look at this as a conditional probability it’s not obvious we are making this assumption  We have to somehow be able to estimate this conditional probability without relying on the big table from Figure 6.
model ideal document compose retrieve assume sampling words	” Under this model the user would use this ideal document as a basis to compose a query to try and retrieve a desired document  More concretely we assume that the query is generated by sampling words from the document.
concretely query generated words document user word imaginary query word.	 More concretely we assume that the query is generated by sampling words from the document  For example a user might pick a word like presidential from this imaginary document and then use this as a query word.
Let’s	 Let’s see how this works exactly.
	e.
presidential	 Suppose now the query is presidential campaign.
d3 d2.	 Similarly we can calculate probabilities for the other two documents d3 and d2.
In distinguish that’s	 In fact we can’t even distinguish them from d2  Clearly that’s not desirable.
The process word	 The user can generate the query using a similar process  They may pick a word such as presidential and another word such as campaign.
zero.	 Essentially we are only considering the words in the query because if a word is not in the query its contribution to the sum would be zero.
method observed document different form	 In our smoothing method we’re assuming the words that are not observed in the document have a somewhat different form of probability.
We rewrite sum query d difference minus This sum w ∈ V q log αd matched terms observe query.	 We can then rewrite the second sum of query words not matched in d as a difference between the scores of all words in the vocabulary minus all the query words matched in d  This is actually quite useful since part of the sum over all w ∈ V can now be written as q log αd   Additionally the sum of query words matched in d is in terms of words that we observe in the query.
sum words Just like vector space intersection vector If look	 Additionally the sum of query words matched in d is in terms of words that we observe in the query  Just like in the vector space model we are now able to take a sum of terms in the intersection of the query vector and the document vector  If we look at this rewriting further as shown in Figure 6.
IDF pw popularity term Because denominator weight entire	 In the denominator we achieve the IDF effect through pw  C or the popularity of the term in the collection  Because it’s in the denominator a larger collection probability actually makes the weight of the entire term smaller.
Because denominator larger collection smaller.	 Because it’s in the denominator a larger collection probability actually makes the weight of the entire term smaller.
document frequency different effect VS interpretation related length	 Remember IDF has a logarithm of document frequency but here we have something different  Intuitively however it achieves a similar effect to the VS interpretation  We also have something related to the length normalization.
nice logarithm achieves sublinear scaling term	 But what’s nice with probabilistic modeling is that we are automatically given a logarithm function which achieves sublinear scaling of our term “weights.
interpolation mixing coefficient JelinekMercer smoothing.	 The first is a lin ear interpolation with a fixed mixing coefficient  This is also called JelinekMercer smoothing.
6 26 document language MLE number words	 Figure 6 26 shows how we estimate the document language model by using MLE  That gives us word counts normal ized by the total number of words in the document.
text times 100 10 C probability pseenw d 1 λ	 Since text appears ten times in d and d  100 our MLE estimate is 10 100   In the background we have ptext  C  0 001 giving our smoothed probability of pseenw  d  1 − λ .
001 6.	 0 001  In Figure 6.
fixed λ takes μ Based Figure	 Instead however αd is not simply a fixed λ but a dynamic coefficient which takes μ  0 as a parameter  Based on Figure 6.
This smoothing derstood dynamic interpolation Another smoothing d μ .	 This smoothing can be un derstood as a dynamic coefficient interpolation  Another way to understand this formula—which is even easier to remember—is to rewrite this smoothing method in this form pw  d  cw d  μ .
8 Document d Total words 100 pwd μ2 0.	8 Document d Total words  100 p“network”d   0 001 pwd    μ2 0 ∞pwC cw d  μpwC — d  μ p“text”d 10  μ  0.
counts	 3000 counts to that word.
Let’s complete	 Let’s look at the complete function for Dirichlet prior smoothing now.
pw C d μ d C	 pw  C d  μ d d  μ   cw d d  μ d  μ   pw  C 6.
having probabilistic model concludes recall assumptions order	 Some advantages here are having assumptions clearly stated and a final form dictated by a probabilistic model  This section also concludes our discussion of the query likelihood probabilistic retrieval models  Let’s recall what assumptions we have made in order to derive the functions that we have seen the following.
recall assumptions order seen	 Let’s recall what assumptions we have made in order to derive the functions that we have seen the following  1.
pR d q pq	e  pR  d  q ≈ pq  d.
word seen probability background	 If a word is not seen in the document its probability is proportional to its probability in the collection smoothing with the background collection.
2004 diagnostic retrieval proposed Fang al multiple improved In chapter feedback system.	 2004 and a diagnostic eval uation method for assessing deficiencies of a retrieval model is proposed in Fang et al  2011 where multiple improved basic retrieval functions are also derived  7Feedback In this chapter we will discuss feedback in a TR system.
7.	 This is illustrated in Figure 7.
implemented updates query list user sent search returns discussed depth Chapter These shown user.	 As shown feedback is often implemented as updates to a query which alters the list of returned documents  We can see the user would type in a query and then the query would be sent to a standard search engine which returns a ranked list of results we discussed this in depth in Chapter 6  These search results would be shown to the user.
In case users simply assume ranked documents relevant k 10	 In this case we don’t have to involve users since we simply assume that the top k ranked documents are relevant  Let’s say we assume the top k  10 documents are relevant.
… Judgments d2 dk – … Document collection Query Feedback	5 … Judgments d1 d2 – d3 … dk – … Document collection Query Updated query Feedback Retrieval engine User Figure 7.
retrieval You language models analyze word learning	1 How feedback is part of an information retrieval system  You may recall that we talked about using language models to analyze word associations by learning related words to the word computer see Chapter 3.
related words original query expand documents words program query pseudo relevance reliable hope useful method called	 These related words can then be added to the original query to expand the query which helps find documents that don’t necessarily match computer but match other words like program and software that may not have been in the original query  Unfortunately pseudo relevance feedback is not completely reliable we have to arbitrarily set a cutoff and hope that the ranking function is good enough to get at least some useful documents  There is also another feedback method called implicit feedback.
We assume displayed text probably user idea feedback update query This important user activity search results.	 We can assume this displayed text is probably relevant or interesting to the user since they clicked on it  This is the idea behind implicit feedback and we can again use this information to update the query  This is a very important technique used in modern search engines—think about how Google and Bing can collect user activity to improve their search results.
relevance feedback user effort method talked pseudo feedback assumed k document relevant user	 In relevance feedback we use explicit relevance judgements which require some user effort but this method is the most reliable  We talked about pseudo feedback where we simply assumed the top k document are relevant without involving the user at all.
While method involve effort judgements results Next query retrieval models future obtained	 While this method does involve users the user doesn’t have to make explicit effort to make judgements on the results  Next we will discuss how to apply feedback techniques to both the vector space and query likelihood retrieval models  The future sections do not make any note of how the feedback documents are obtained since no matter how they are obtained they would be dealt with the same way by each of the following two feedback methods.
1 Space Model section retrieval based previous improve retrieval accuracy queries We examples query nonrelevant specific	1 Feedback in the Vector Space Model This section is about feedback in the vector space retrieval model  As we have discussed feedback in a TR system is based on learning from previous queries to improve retrieval accuracy in future queries  We will have positive examples which are the documents that we assume to be relevant to a particular query and we have negative examples which are nonrelevant to a specific query.
way depends particular strategy general space model feedback modify high space documents.	 The way the system gets these judged documents depends on the particular feedback strategy that is employed which was discussed in the previous section  The general method in the vector space model for feedback is to modify our query vector  We want to place the query vector in a better position in the high dimensional term space plotting it closer to relevant documents.
basic idea Rocchio Geometrically closer vectors qm .	 This is the basic idea behind Rocchio feedback  Geometrically we’re talking about moving a vector closer to some vectors and away from other vectors  Algebraically it means we have the following formula using the arrow vector notation for clarity qm  α .
Another interpretation second term sum documents centroid negative feedback shift original relevant away negative centroid terms weight centroid	 Another interpretation of the second term the sum over positive documents is the centroid vector of relevant feedback documents while the third term is the centroid vector of the negative feedback documents  In this sense we shift the original query towards the relevant centroid and away from the negative centroid  Thus the average over these two terms computes one dimension’s weight in the centroid of these vectors.
Let’s detailed Imagine small presidential text query q 0.	 Let’s take a look at a detailed example depicted below  Imagine we have a small vocabulary V  news about presidential  campaign food  text and a query q  1 1 1 1 0 0.
small V presidential text q 1 1 Recall vocabulary V	 Imagine we have a small vocabulary V  news about presidential  campaign food  text and a query q  1 1 1 1 0 0  Recall from Chapter 6 that our vocabulary V is a fixedlength term vector.
Recall Chapter V	 Recall from Chapter 6 that our vocabulary V is a fixedlength term vector.
	0 d4  1.
0	0 2 0 0.
0 0 For feedback compute centroid tive	0 2 0 0 0 For Rocchio feedback we first compute the centroid of the positive and nega tive feedback documents.
	02.
0 067γ α	5β − 1 5γ  α − 0 067γ  α  3.
new better nonrelevant documents—this precisely feedback method practice potential somewhat large computation	 We rerun the search with this new query  Due to the movement of the query vector we should match the relevant documents much better since we moved q closer to them and away from the nonrelevant documents—this is precisely what we want from feedback  If we apply this method in practice we will see one potential problem we would be performing a somewhat large computation to calculate the centroids and modify all the weights in the new query.
clustered use negative examples small avoid means relatively high α original terms.	 On the other hand positive documents tend to be clustered together and they are often in a consistent direction with respect to the query  Because of this effect we sometimes don’t use the negative examples or set the parameter γ to be small  It’s also important to avoid overfitting which means we have to keep relatively high weight α on the original query terms.
don’t want sample reformulate original meaning Those typed user important	 We don’t want to overly trust a small sample of documents and completely reformulate the query without regard to its original meaning  Those original terms are typed in by the user because the user decided that those terms were important Thus we bias the modified vector towards the original query direction.
2 Models section feedback language derive query making assumptions term	2 Feedback in Language Models This section is about feedback for language modeling in the query likelihood model of information retrieval  Recall that we derive the query likelihood ranking function by making various assumptions such as term independence.
think corporating feedback immediately information assumed query generated assembling ideal model information.	 However if we think about in corporating feedback information it is not immediately obvious how to modify query likelihood to perform feedback  Many times the feedback information is additional information about the query but since we assumed that the query is generated by assembling words from an ideal document language model we don’t have an easy way to add this additional information.
This called difference model pw θ̂Q before.	 This method is called KLdivergence because this can be interpreted as measuring the divergence i e  difference between two distributions one is the query model pw  θ̂Q and the other is the document language model from before.
	e.
This based 5 observing documents collected judgements documents logs means.	 This approach is based on a generative model shown in Figure 7 5  Let’s say we are observing the posi tive documents which are collected by users’ judgements the top k documents from a search clickthrough logs or some other means.
depends background distribution addition parameter it’s distribution.	 Which word will show up depends on both the topic distribution and background distribution  In addition it would also depend on the mixing parameter λ if λ is high it’s going to prefer the background distribution.
addition mixing high going prefer background distribution.	 In addition it would also depend on the mixing parameter λ if λ is high it’s going to prefer the background distribution.
2 θF maximize log likelihood documents model likelihood	2 We choose this probability distribution θF to maximize the log likelihood of the feedback documents under our model  This is the same idea as the maximum likelihood estimator.
	 For example the words airport and security still show up as high probabilities in each case naturally because they occur frequently in the topranked documents.
topic combined query help	 Clearly these are relevant to this topic and if combined with the original query can help us match other documents in the index more accurately.
language model manipulated include documents We method parameters feedback model discriminates words.	 This generalization allows us to use a language model for the query which can be manipulated to include feedback documents  We described a method for estimating the parameters in this feedback model that discriminates between topic words relevant to the query and background words useless stop words.
An empirical Buckley Pseudorelevance feedback	 Bibliographic Notes and Further Reading An early empirical comparison of various relevance feedback techniques can be found in Salton and Buckley 1990  Pseudorelevance feedback has become popu lar after positive results being observed in TREC experiments e.
Pseudorelevance feedback popu results	 Pseudorelevance feedback has become popu lar after positive results being observed in TREC experiments e.
proposed appears methods estimating language model pseudo feedback.	 The positional relevance model proposed in Lv and Zhai 2010 appears to be one of the most effective methods for estimating a query language model for pseudo feedback.
13 large kind solutions provide tradeoff How majority queries Engine This focuses IR search	13  In a real search system storing modified query vectors for all observed queries will take up a large amount of space  How could you optimize the amount of space required What kind of solutions provide a tradeoff between space and query time How about an online system that benefits the majority of users or the majority of queries 8Search Engine Implementation This chapter focuses on how to implement an information retrieval IR system or a search engine.
search query queries optimize required What kind tradeoff space query majority This focuses implement information search IR	 In a real search system storing modified query vectors for all observed queries will take up a large amount of space  How could you optimize the amount of space required What kind of solutions provide a tradeoff between space and query time How about an online system that benefits the majority of users or the majority of queries 8Search Engine Implementation This chapter focuses on how to implement an information retrieval IR system or a search engine  In general an IR system consists of four components.
documents raw strings separate tokens indexer This poor affect end	 This component takes in documents as raw strings and determines how to separate the large document string into separate tokens or features  These token streams are then passed on to the indexer  This is perhaps the most vital part of the system as a whole since a poor tokenization method will affect all other parts of the indexing and propagate downstream to the end user.
vital method parts propagate downstream user.	 These token streams are then passed on to the indexer  This is perhaps the most vital part of the system as a whole since a poor tokenization method will affect all other parts of the indexing and propagate downstream to the end user.
poor affect end user	 This is perhaps the most vital part of the system as a whole since a poor tokenization method will affect all other parts of the indexing and propagate downstream to the end user  Indexer.
discussed previous added existing items fairly current search	 It was discussed in detail in the previous chapter so in this chapter we will just outline how it may be added to an existing system  For the first three items there are fairly standard techniques that are essentially used in all current search engines.
common practice add cache disk.	 Thus it is common practice to add a cache between the front facing API and the document index on disk.
determines	 This determines how we represent a document.
In search engine leave scorer	 In real search engine systems we often leave the term scoring up to the index scorer module.
1 1 long	 1 Quill′s 1 book 1 is 1 very 2 long  1.
aren’t words representation exercises ways represent bigram POStags grammatical tree	 Of course we aren’t restricted to using a unigram words representation  Look back to the exercises from Chapter 4 to see some different ways in which we can represent text  We could use bigram words POStags grammatical parse tree features or any combination.
TREC large real world Google index	 TREC research datasets may even be as large as several terabytes  This doesn’t even take into account real world production systems such as Google that index the entire Web.
This portions raw time Furthermore running queries files want necessary statistics	 This requires us to design indexing systems that only load portions of the raw corpus in memory at one time  Furthermore when running queries on our indexed files we want to ensure that we can return the necessary term statistics fast enough to ensure a usable search engine.
“proximity heuristics” matching terms common store position term occurrence terms text e.	 In order to support “proximity heuristics” rewarding matching terms that are together it is also common to store the position of each term occurrence  Such position information can be used to check whether all the query terms are matched within a certain window of text e.
This information stored file document 8.	 This information is stored in the postings file since it is document specific  Figure 8.
example term ID 56 look 56 The ID 78 Total file actual 56 → assigned IDs lexicon indexed	 For example if we want to score the term computer which is term ID 56 we look up 56 in the lexicon  The information we receive could be Term ID 56 Document frequency 78 Total number of occurrences 443 Offset into postings file 8923754 Of course the actual lexicon would just store 56 → 78 443 8923754  Since the tokenizer assigned term IDs sequentially we could represent the lexicon as a large array indexed by term ID.
	 .
note large size lexicon postings file.	 Also make note of the large difference in size of the lexicon and postings file.
A popular approach approach Scan In tokenization assign	 A popular approach for indexing is the following sortingbased approach  Scan the raw document stream sequentially  In tokenization assign each document an ID.
2 shows produce The documents fit flushed	 Figure 8 2 shows how documents produce terms originally in document ID order  The terms from multiple documents are then sorted by term ID in small postings chunks that fit in memory before they are flushed to the disk.
ID small postings fit flushed created similar inverted index.	 The terms from multiple documents are then sorted by term ID in small postings chunks that fit in memory before they are flushed to the disk  A forward index may be created in a very similar way to the inverted index.
termtodocument query time There efficiency relevant	 In the next section we’ll see how using the inverted termtodocument mapping can greatly decrease query scoring time  There are other efficiency aspects that are relevant to the forward index as well such as compression and caching.
This leads algorithm index.	 This leads us to our first scoring algorithm using the inverted index.
In reality fetchdocs return contains formation background probability scoreterm function Once we’ve iterated score accumulators	 In reality the fetchdocs function would return some object that contains in formation about the current term in the document such as count background probability or any other necessary information that the scoreterm function would need to operate  Once we’ve iterated through all the query terms the score accumulators have been finalized.
We documents accumulated scores save sorting documents query opposed index zero	 We just need to sort these documents by their accumulated scores and return usually the top k  Again we save time in this sorting operation by only sorting documents that contained a query term as opposed to sorting every single document in the index even if its score is zero  8.
sorting sorting documents query term single score	 Again we save time in this sorting operation by only sorting documents that contained a query term as opposed to sorting every single document in the index even if its score is zero.
ranking size score accumula tors	3 2 Documentatatime Ranking One disadvantage to termatatime ranking is that the size of the score accumula tors scores will be the size of the number of documents matching at least one term.
Otherwise scoring surpass hold best completely priority queue Using index list	 Otherwise as with termatatime scoring a document may start out with a lower score than another only to surpass it as more terms are scored  We can hold the k best completely scored documents with a priority queue  Using the inverted index we can get a list of document IDs and postings data that need to be scored.
We hold completely queue document IDs	 We can hold the k best completely scored documents with a priority queue  Using the inverted index we can get a list of document IDs and postings data that need to be scored.
Algorithm	 See Algorithm 8.
optional scoring database filter documents terms 8	 The filtering function can then be an optional parameter to the scoring function which has access to the document metadata store usually a database and a forward index in order to filter documents that contain certain terms  8 3.
stopping postings chunk shards All data final chunk pieces multiple inverted Consider shards search	 This is easily achieved by stopping the postings chunk merging process when the number of chunks is equal to the number of desired shards  All the same data is stored as one final chunk but it’s just broken down into several pieces  But why would we want multiple inverted index chunks Consider when we have the number of shards equal to the number of threads or cluster nodes in our search system.
don’t create storage query time computationally expensive especially engine	 Thus we don’t need to create any additional storage structures for the index  The downside is that all the processing is done at query time which could be quite computationally expensive especially when using a search engine with many users.
In	 In practice we can have some compromise between these two extremes e g.
pseudofeedback method judgements stored	 Of course this only touches on the pseudofeedback method  There is also clickthrough data which can be stored in a database and relevance judgements which can be stored the same way.
δencoding bitwise vByte block .	 γ encoding bitwise   δencoding bitwise   vByte block and .
Using bitwise compression means bit operations integer Unary.	 Using bitwise compression means performing some bit operations for every bit that is encoded in order to “build” the compressed integer back into its original form  Unary.
	5.
Every needs read Block reading bytes time bits In block compression schemes bitwise usually pre schemes	 Every single bit needs to be read in order to read one integer  Block compression attempts to alleviate this issue by reading bytes at a time instead of bits  In block compression schemes only one bitwise operation per byte is usually required as opposed to at least one operation per bit in the pre vious three schemes e.
length.	 fixed binary length.
head list O1 time need LRU look tail end constant	 Once an element is inserted or accessed it is moved to the head front of the list in O1 time  When we need to remove the LRU item we look at the element at the tail end of the linked list and delete it also in constant time.
cache barrel cache originally It cache.	6 2 DBLRU cache The double barrel LRU cache was originally used in the popular Lucene search engine 2 It is a simplified approximation of the LRU cache.
simplified approximation cache.	2 It is a simplified approximation of the LRU cache.
The retrieve postings term x.	 The algorithm is as follows assuming we want to retrieve the postings list for term ID x.
This test idea talking This different improving general.	 This is usu ally done by using a test collection which is a main idea that we’ll be talking about in this chapter  This has been very important for comparing different algorithms and for improving search engines systems in general.
How search results measuring relevant ones	 How accurate are the search results In this case we’re measuring a system’s capability of ranking relevant documents on top of nonrelevant ones  Efficiency.
useful real user things important	 Usability  How useful is the system for real user tasks Here interfaces and many other things are also important and we typically have to do user studies.
formulated search results Finally matches ranked list This algorithms set reused times.	 Ideally they have to be made by users who formulated the queries because those are the people that know exactly what the documents search results would be used for  Finally we have to have measures to quantify how well a system’s result matches the ideal ranked list that would be constructed based on users’ relevance judgements  This methodology is very useful for evaluating retrieval algorithms because the test set can be reused many times.
results feel maybe better sense documents returned returned	 But if you look at the results you will feel that maybe system A is better in the sense that we don’t have that many documents returned and among the three documents returned two of them are relevant.
You category B In case we’ll define measures quantify need define measures perspectives results.	 You might be in the second category and then you might find that system B is better  In either case we’ll have to also define measures that would quantify the information need of a user  We will need to define multiple measures because users have different perspectives when looking at the results.
This retrieval documents relevant 3	 This can be captured by the measure of precision where we simply compute to what extent all the retrieval results are relevant  100 precision would mean all the retrieved documents are relevant  Thus in this case system A has a precision of 2 3  0.
retrieved The So ideal result precision recall 1.	 Precision and recall are focused on looking at a the number of retrieved relevant documents  The two measures differ based on the denominator of the formula  So what would be an ideal result If precision and recall are both 1.
it’s important solution think multiple ideas it’s analyze differences think scenario.	 the arithmetic mean but it’s important not to settle on this solution rather think whether there are other ways to approach it  Once you have multiple ideas it’s important to analyze their differences and then think about which one makes more sense in a real scenario.
saw basic measure performance result 5 problem quality ranked list results.	 We saw that precision and recall are the two basic ways to quantitatively measure the performance of a search result  But as we talked about in depth in Chapter 5 the text retrieval problem is a ranking problem not a classification one  Thus we need to evaluate the quality of a ranked list as opposed to whether a relevant document was returned anywhere in the results.
user D1 relevant two.	 What if the user stops at the second position The precision is the same since both D1 and D2 are relevant 100 or two out of two.
The user stops interesting additional documents	 The recall is two out of ten or 20  If the user stops at the third position we have an interesting case because we don’t have any additional relevant documents so the recall does not change  However the precision is lower because we have two out of three relevant documents.
lower relevant documents.	 However the precision is lower because we have two out of three relevant documents.
In precision situation	 There the recall is four out of ten  When can we get a recall of five out of ten In this list we don’t have it  For convenience we often assume that the precision is zero in a situation like this.
numbers different plot curve what’s shown 9.	 Since we can get a lot of precisionrecall numbers at different positions we can plot a curve this is what’s shown on the right side of Figure 9.
On xaxis values values.	 On the xaxis are the recall values  On the yaxis are the precision values.
determine thought want possible Therefore users But brings original better users	 On the other hand if a user wants to determine whether an idea has been thought of before they will want to emphasize high recall so that they see as many relevant documents as possible and don’t miss the chance to find the idea  Therefore those users would favor system A  But this brings us back to the original question which one is better Again this actually depends on the users or more precisely the users’ task.
area different consider The add correspond document second	 One way is to look at the area underneath the curve—the average precision  Basically we’re going to take a look at every different recall point and consider the precision  The precisions we add up correspond to retrieving the first relevant document the second and so on.
precision meaningful tells user perspective.	 That precision is very meaningful because it tells us what a user would see from their perspective.
Evaluating Ranked Lists Queries Average computed query experiment queries capture variance For ex ample happens perform ment systems’	1 Evaluating Ranked Lists from Multiple Queries Average precision is computed for just one query  Generally though we experiment with many different queries in order to capture the variance across them  For ex ample one system may perform very well with one query on which another system happens to perform poorly using only this query would not give an accurate assess ment of each systems’ capability.
Generally experiment queries query systems’ Using queries researcher precision	 Generally though we experiment with many different queries in order to capture the variance across them  For ex ample one system may perform very well with one query on which another system happens to perform poorly using only this query would not give an accurate assess ment of each systems’ capability  Using more queries then requires the researcher to take an average of the average precision over all these queries.
Lm ranked m different Then avpLi	  Lm be the ranked lists returned from running m different queries  Then we have MAPL  1 m m∑ i1 avpLi  9.
testing You’ve queries average precision topic.	4 Imagine you are again testing a new algorithm  You’ve tested multiple topics queries and have the average precision for each topic.
wish engine If want MAP preferred depends users’	 If you wish to improve the search engine for those difficult queries then gMAP would be preferred  If you just want to improve over all kinds of queries then perhaps MAP would be preferred  So again the answer depends on your users’ tasks and preferences.
measure represent users’ case think document entire collection.	 Which measure is most likely going to represent your users’ needs As a special case of the mean average precision we can also think about the case where there is precisely one relevant document in the entire collection.
This actually happens example what’s known item know target Amazon Or	 This actually happens quite often for example in what’s called a known item search where you know a target page such as Amazon or Facebook  Or in another application such as question answering there is only one answer.
single 1 r difference.	 For one single topic using r or using 1 r wouldn’t make any difference.
difference topics.	 The difference appears when there are many topics.
like large r values Those large lower relevant rank	 Just like MAP this sum will be dominated by large values of r   So what are those values Those are basically large values that indicate lower ranked results  That means the relevant items rank very low down on the list.
So discussed binary relevant nonrelevant degree highly documents useful lower	 So far we discussed about binary judgements—that means a documents is judged as being relevant or nonrelevant  Earlier we made the point that relevance is a matter of degree  We often are able to distinguish very highly relevant documents from documents that are still useful but with a lower relevance.
In 9 example relevance levels level marginally	 In Figure 9 7 we show an example of three relevance levels level three for highly relevant two for marginally relevant and one for nonrelevant.
nonrelevant point gain usually utility If cumulatively gain returned	 Looking at the first document the user can gain three points looking at the nonrelevant documents the user would only gain one point  This gain usually matches the utility of a document from a user’s perspective  If we assume the user stops at ten documents we can cumulatively sum the information gain from traversing the list of returned documents.
considering position document know document marginally relevant document non know list.	 There is one deficiency which is not considering the rank or position of each document  Looking at the CG sum of the top four documents we know there is only one highly relevant document one marginally relevant document two non relevant documents we don’t know where they are ranked in the list.
But capture intuition The good top.	 But how can we capture that intuition The second three is not as good as the first three at the top.
We divide gain weight based order capture	 We divide this gain by a weight based on the position in order to capture this positionbased penalty.
discounted measuring ranked multiple levels judgment We little different queries normalized discounted NDCG NDCGL	 At this point we have a discounted cumulative gain for measuring the utility of a ranked list with multiple levels of judgment  We still need to do a little bit more in order to make this measure comparable across different queries  The idea here is normalized discounted cumulative gain NDCG NDCGL  DCGL IDCG .
need little different queries.	 We still need to do a little bit more in order to make this measure comparable across different queries.
It DCG ideal particular query.	7 It is simply DCG normalized by the ideal DCG IDCG for a particular query.
In basically measure applied task scale dependant application The main utility k cutoff	 In a more general way this is basically a measure that can be applied through any ranked task with a large range of judgments  Furthermore the scale of the judgments can be dependant on the application at hand  The main idea of this measure is to summarize the total utility of the top k documents you always choose a cutoff and then you measure the total utility.
impossible considering correlate We	 As a result it’s usually impossible to actually label all of the documents for all the queries especially considering a data set like the Web  It’s also challenging to correlate the evaluation measures with the perceived utility of users  We have to consider carefully what the users care about and then design measures to capture their preferences.
8 displays A experiment.	8 displays some sample average precision results from system A and system B in two different experiments  As you can see in the bottom of the figure we have the MAP for each system in each experiment.
As MAP experiment experiment Yet exact average precisions different queries	 As you can see in the bottom of the figure we have the MAP for each system in each experiment  They happen to be identi cal in experiment one and two  Yet if you look at the exact average precisions for different queries you will realize that in one case you might feel that you can trust the conclusion here given by the average.
there’s variance unreliable look	 If there’s a big variance that means that the results could fluctuate according to different queries which makes the result unreliable  So let’s look at these results again in the second case.
look second case.	 So let’s look at these results again in the second case.
coins plus denote heads minus denote randomly flipping larger doesn’t quantified concept p value.	 If you flip seven coins using plus to denote heads and minus to denote tails then these could easily be the results of just randomly flipping the seven coins  The fact that the average is larger doesn’t tell us anything This intuition can be quantified by the concept of a p value.
assumption systems depending queries observe difference left right observe values deviating zero B’s	 Say we started with the assumption that there’s no difference between the two systems  But we assume that because of random fluctuations depending on the queries we might observe a difference thus the actual difference might be on the left side or right side  This curve shows the probability that we would observe values that are deviating from zero here when we subtract system A’s MAP from system B’s MAP or even vice versa.
strategies testing evaluating mix showing results	 What we haven’t covered are some other evaluation strategies such as AB testing this is where an evaluating system would mix the results of two methods randomly showing the mix of results to users.
NDCG definitely know research papers small number perspective number likely real application Further Reading Evaluation information trieval empirical problems general.	 MAP and NDCG are the two main measures that you should definitely know about since you will see them often in research papers  Finally retrieving up to ten documents or some small number is easier to interpret from a user’s perspective since this is the number of documents they would likely see in a real application  Bibliographic Notes and Further Reading Evaluation has always been an important research problem in information re trieval and in empirical AI problems in general.
meth established information retrieval early topic Jones Willett 1997.	 The Cranfield evaluation meth odology was established in 1960s by early pioneers of information retrieval re searchers important early papers on the topic can be found in Sparck Jones and Willett 1997.
address challenges	 Naturally there had to be some further extensions of the classical search algorithms to fully address new challenges encountered in web search.
problem information known heighten particular page’s rank advantage scored e	 The second problem is that there is much low quality information known as spam  Search engine optimization is the attempt to heighten a particular page’s rank by taking advantage of how pages are scored e g.
adding necessarily actual content creating fake particular popular different designed detect 2012.	 adding many words that are not necessarily relevant to the actual content or creating many fake links to a particular page to make it seem more popular than it really is  Many different approaches have been designed to detect and prevent such spamming practices Spirin and Han 2012.
improve The space search On hand don’t special pages documents applica web search.	 For example we can imagine that using links between pages can improve scoring  The algorithms that we talked about such as the vector space model are general— they can be applied to any search application  On the other hand they also don’t take advantage of special characteristics of pages or documents in specific applica tions such as web search.
addresses particular MapReduce There We’ll ranked high.	 This addresses the issue of scalability in particular Google’s MapReduce framework is very influential  There are also techniques that have been developed for addressing the spam problem  We’ll have to prevent those spam pages from being ranked high.
Then pages analysis The discuss way combine different ranking 10.	 Then we discuss how we can take advantage of links between pages in link analysis  The last technique we discuss is learning to rank which is a way to combine many different features for ranking  10.
Those cause service sites believe creating requests respect exclusion	 Those may cause the site to experience a denial of service some sites will also block IP addresses that they believe to be crawling them or creating too many requests  In a similar vein a crawler should respect the robot exclusion protocol.
different PDFs formats web information recognize duplicate loop.	 You also need to handle different types of files such as images PDFs or any other kinds of formats on the web that contain useful information for your search application  Ideally the crawler should recognize duplicate pages so it doesn’t repeat itself or get stuck in a loop.
crawler loop Finally discover hidden contain you’d like index.	 Ideally the crawler should recognize duplicate pages so it doesn’t repeat itself or get stuck in a loop  Finally it may be useful to discover hidden URLs these are URLs that may not be linked from any page yet still contain content that you’d like to index.
Finally URLs URLs linked page contain content	 Finally it may be useful to discover hidden URLs these are URLs that may not be linked from any page yet still contain content that you’d like to index.
natural task variation	 Parallel crawling is also very natural because this task is very easy to parallelize  One interesting variation is called focused crawling.
changing posts index easily scenario it’s especially important requests server overwhelmed.	 By changing the id parameter we can iterate through all forum posts and index them quite easily  In this scenario it’s especially important to add a delay between requests so that the server is not overwhelmed.
repeated create search	 Finally we might face the scenario of incremental crawling or repeated crawling  Let’s say you want to be able to create a web search engine.
shows Google System	1 shows the architecture of the Google File System GFS.
perspective programmer normal file doesn’t know operators Another feature directly chunk efficient well.	 From the application perspective the programmer would see a normal file  The program doesn’t have to know where exactly it’s stored and can just invoke high level operators to process the file  Another feature is that the data transfer is directly between application and chunk servers so it’s efficient in this sense as well.
GFS framework hides low level programmer.	 Like GFS this framework hides low level features from the programmer.
This data sent	 This is the data that is sent to the reduce function.
It partial counts We iterate array shown figure.	 It already has these partial counts so all it needs to do is simply add them up  We have a counter and then iterate over all the words that we see in this array shown in pseudocode at the bottom of the figure.
As similar index output	 As we can see this is already very similar to building an inverted index the output here is indexed by a word and we have a dictionary of the vocabulary.
Figure illustrates example map function key ID string content document.	 Figure 10 5 illustrates this example  Now we assume the input to map function is a key  value pair where the key is a document ID and the value denotes the string content of all the words in that document.
input like index entry word documents documents All concatenate continuous chunk stored filesystem.	 We see the reduce function’s input looks like an inverted index entry  It’s just the word and all the documents that contain the word and the frequency of the word in those documents  All we need to do is simply to concatenate them into a continuous chunk of data and this can be then stored on the filesystem.
search tradi people collecting literature information These types	 For example people might search for a web page or entry page—this is different from the tradi tional library search where people are primarily interested in collecting literature information  These types of queries are often called navigational queries where the purpose is to navigate into a particular targeted page.
result concerns researchers number major extensions exploit improve scoring topic section exploit information form	 As a result of all these concerns researchers have made a number of major extensions to the standard ranking algorithms  One is to exploit links to improve scoring which is the main topic of this section  There are also algorithms to exploit large scale implicit feedback information in the form of clickthroughs.
links link that’s document	 Figure 10 6 shows a snapshot of a part of the web  We can see there are many links that connect different pages and in the center there is a description of a link that’s pointing to the document on the right side.
description text text.	 This description text is called anchor text.
On pointed This concept citations cascading citations.	 On the other hand if those pages that are pointing to you are not pointed to by many other pages then you don’t get that much credit  This is the concept of indirect citations or cascading citations.
Let’s assume surfer decides different randomly follow link current randomly choose jump	 Let’s assume that a random surfer or random walker can be on any of these pages  When the random surfer decides to move to a different page they can either randomly follow a link from the current page or randomly choose a document to jump to from the entire collection.
The PageRank score average probability surfer di.	 The PageRank score of a document di is the average probability that the surfer visits di.
probability reaching random α allows smoothing You 1 N comes transition N	 This part captures the probability of reaching this page through random jumping where α is the probability of random jumping  This also allows us to see why PageRank captures a smoothing of the transition matrix  You can think this 1 N comes from another transition matrix that has all the elements as 1 N .
At point Interestingly update	 At that point we will have the PageRank scores for all the pages  Interestingly this update formula can be interpreted as propagating scores across the graph.
know specialized results.	 Therefore if you know the current query is about sports we can use this specialized PageRank score to rank the results.
	 In the beginning of this section we also mentioned that hub pages are useful.
pages.	 Authority pages capture the intuition of widelycited pages.
HITS general	 Like PageRank HITS is also quite general and has many applications in graph and network analysis.
Briefly we’ll	 Briefly we’ll describe how it works.
talked major	 We also talked about the PageRank and HITS algorithms as two major link analysis algorithms in web search.
We include tilde indicate page single score approach ment relevant query hypothesize related features func parameters.	 We can even include a feature such as whether the URL has a tilde because this might indicate a home page  The question is of course how can we combine these features into a single score In this approach we simply hypothesize that the probability that this docu ment is relevant to this query is a function of all these features  We hypothesize that the probability of relevance is related to these features through a particular func tion that has some parameters.
Here denote ith	 Here we have Xi to denote the ith feature value and we can have as many features as we would like.
combined The feature controlled larger βi function.	 We assume that these features can be combined in a linear manner  The weight of feature Xi is controlled by a parameter βi  A larger βi would mean the feature would have a higher weight and it would contribute more to the scoring function.
The truly estimate Let’s look simple example Figure 10 9.	 The next task is to see how we estimate the parameters so that the function can truly be applied that is we need to estimate the β values  Let’s take a look at a simple example shown in Figure 10 9.
Let’s simple 10 example	 Let’s take a look at a simple example shown in Figure 10 9  In this example we have three features.
course challenge solve probabilities	 Of course then the challenge is that the optimization problem will be harder to solve  In contrast we might have another case where we predicted probabilities of relevance around 0.
Even high long 0 ranking acceptable user.	9 for nonrelevant documents  Even though the predicted score is very high as long as the truly relevant documents receive scores that are greater than 0 9 the ranking will still be acceptable to a user.
trend user tasks After want decision perform	 Another trend is that we might see systems that try to go beyond search to support user tasks  After all the reason why people want to search is to solve a problem or to make a decision to perform a task.
For look review directly product simply click button site purchase.	 For example you can sometimes look at the review displayed directly in search results if the user decides to buy the product they can simply click a button to go to the shopping site directly and make the purchase.
Researchers want suggested citations Currently support We think information	 Researchers might want to find related work or suggested citations  Currently there’s not much support for a task such as writing a paper  We can think about any intelligent system—especially intelligent information systems—specified by three nodes.
nodes we’ll able DataUser ask	 If we connect these nodes into a triangle then we’ll able to specify an information system  We can call this triangle the DataUser Service Triangle  The three questions you ask are as follows.
connect different ways example pages support search search.	 Now imagine you can connect all these in different ways  For example if you connect everyone with web pages and support search and browsing you get web search.
If positive complaint attach information com plaint automatically generic tell response All people	 If they detect a positive message not a complaint then they might take this opportunity to attach some promotion information  If it’s a com plaint then you might be able to automatically generate some generic response first and tell the customer that he or she can expect a detailed response later  All of these aim to help people to improve their productivity.
initiated Graph manual human engine	 Google has initiated some of this work via its Knowledge Graph  Once we can get to that level without much manual human effort the search engine can provide a much better service.
web search users filtering documents dynamic source pushed include filter litera ture recommender.	 Examples are li brary or web search  Conversely most users also have longterm information needs such as filtering or recommending documents or any other item type from a dynamic information source here the user is pushed information by a system  Examples include a news filter email filter moviebook recommender or litera ture recommender.
users needs recommending item type dynamic source system.	 Conversely most users also have longterm information needs such as filtering or recommending documents or any other item type from a dynamic information source here the user is pushed information by a system.
filtering characterize Contentbased filtering learn kind user likes current believe describes likes Collaborative filtering 11 Systems users collaborative item.	 Collaborative filtering look at who likes x and characterize u Contentbased filtering is to learn what kind of content a user likes and then match the content of a current article with a “content prototype” that we believe describes well what the user likes  Collaborative filtering is to look at what other 222 Chapter 11 Recommender Systems similar users like and assume that if those other users who are similar to you like an item you may also like it  Note that if we can get user ratings of items collaborative filtering can be applied to recommend any item.
user rating e.	 Each user can be represented by a rating vector i e.
userbased dation	 We continue this chapter with contentbased recommendation followed by a section on userbased recommen dation collaborative filtering.
initialization module categories This profile.	 This information is set in an initialization module that would take a user’s input perhaps from the user’s specified keywords or chosen categories  This is fed into the system as the initial user profile.
applications users’ stable Due user user recommended item user indicate item relevant not.	 Typically in information filtering applications the users’ information need is stable  Due to this the system would have many opportunities to observe the user if the user views a recommended item since the user can indicate whether the recommended item was relevant or not.
If bad	 If you deliver a bad document you would lose 2.
All optimized .	 All these modules would have to be optimized to maximize the utility function U .
document scoring exists engine vectorspace treated query.	 The document vector could be fed into a scoring module which already exists in a search engine that implements the vectorspace model where the profile will be treated as a query.
11 4 data collect filtering scores	 Figure 11 4 depicts the type of data that you can collect in the filtering system  We have the scores and the status of relevance.
depicts collect filtering We status relevance.	4 depicts the type of data that you can collect in the filtering system  We have the scores and the status of relevance.
So threshold little bit deliver misses user extra is.	 So how do we do that We could lower the threshold a little bit and deliver some near misses to the user to see what their response to this extra document is.
advantage user.	 Exploitation means you would take advantage of the information learned about the user.
In bound threshold interesting problem lower	 In general we can only get an upper bound for the true optimal threshold because the threshold might be lower than we found it’s possible that some of the discarded items might actually be interesting to the user  So how do we solve this problem We can lower the threshold to explore a little bit.
Collaborative makes decisions user based “collaborative” basic based solely similar user collaborative users.	2 Collaborative Filtering In collaborative filtering a system makes decisions for an individual user based on judgements of other users hence it is “collaborative”  The basic idea is to infer individual interests or preferences based solely on similar users  Given a user collaborative filtering finds a set of similar users.
doesn’t matter This contrast previous looked item similarity filtering infer individual’s based similar users general idea Figure 11.	 The text content of items doesn’t matter This is in sharp contrast to the previous section where we looked at item similarity through contentbased filtering  Here we will infer an individual’s interest based on other similar users  The general idea is displayed in Figure 11.
	  on.
	g.
If don’t sufficient sparsity that’s	 If we don’t have sufficient data there will be a data sparsity problem and that’s often called the cold start problem.
particular going retrieve relevant current	 When we consider a particular user we’re going to try to retrieve the relevant i e  similar users to the current user.
similar users object matically predicted user object combination ratings different users.	 The idea here is to look at whether similar users to this user have liked this object or not  Mathe matically the predicted rating of this user on this object is a combination of the normalized ratings of different users.
related ua user ui.	 Naturally the weight is related to the similarity between ua and a particular user ui.
2 m∑ i1 wua	2 we can write the predicted normalized rating V̂aj  k   m∑ i1 wua  ui .
wua ui 0 Once predicted trans range	 is the similarity function and k is the normalizer k  1∑m i1 wua  ui 11 4 that ensures V̂aj ∈ 0 1  Once we have the predicted normalized rating we trans form it into the rating range that ua uses X̂aj  V̂aj  na.
0 1 Once predicted form X̂aj V̂aj na	4 that ensures V̂aj ∈ 0 1  Once we have the predicted normalized rating we trans form it into the rating range that ua uses X̂aj  V̂aj  na  11.
Another measure cosine measure vectors space cosine angle discussed book space model We’ll 14.	 Another measure is the cosine measure which treats the rating vectors as vectors in the vector space and measures the cosine of the angle between the two vectors As we’ve discussed previously in this book this measure has been used in the vector space model for retrieval  We’ll also see how it is used in clustering in Chapter 14.
Let’s news news news If wait relevant interesting Another reason data	 Let’s think about news filtering as soon as the system detects the news articles you have to decide whether the news would be interesting to a user  If you wait for a few days even an accurate recommendation of the most relevant news is not interesting  Another reason why it’s hard is due to data sparseness.
measures quantify values r measure	 Both these measures quantify the difference in values between r̂ and the true rating r   Using such a measure is natural when we have ratings on some ordinal scale.
evaluation As final types valuable users affecting evaluation most.	 Of course this learningovertime evaluation can also be generalized to multiple users in the same way as previously discussed  As a final note for both recommender system types it is valuable to find those users affecting the evaluation metric the most.
2000.	 2000.
sensors” data sensors text similar generated machine sensors jointly analyze text nontext together.	 Specifically we will view humans as “subjective sensors” of our world and text data as data generated by such subjective sensors making text data more similar to other kinds of data generated by objective machine sensors and enabling us to naturally discuss how to jointly analyze text and nontext data together.
In com process text data example search frequently relevant documents	 In most cases however the com puters only play a minor role in the entire process of making use of text data for example we use search engines frequently but once we find relevant documents the further processing of the found documents is generally done manually.
Such process text small digest	 Such a manual process is acceptable when the amount of text data to be processed is small the application task does not demand a fast response and when we have the time to digest text data.
The discover knowledge “sufficient” data For example biomedical reveal genegene potential opportunity disease.	 The other kind is those that can discover knowledge that we humans may not be able to do even if we have “sufficient” time to read all the text data  For example an intelligent biomedical literature anlayzer may reveal a chain of associations of genes and diseases by synthesizing genegene relations and genedisease relations scattered in many different research articles thus suggesting a potential opportunity to design drugs targeting some of the genes for treatment of a disease.
integrate communities different speeds scientific	 How can we integrate the knowledge that is covered in different communities using different vocabularies to help study a particular problem Answering such a question speeds up scientific discovery.
For perform ing opinions society’s policy needed Disaster disaster real	 For example perform ing sentiment analysis on people’s opinions about policies can help better un derstand society’s response to a policy and thus potentially improve the policy if needed  Disaster response and management would benefit early discovery of any warning signs of a natural disaster which is possible through analyzing tweets in real time.
allows things “micro world” allows far era “datascope” large amounts kind text unique opportunities help thoughts easy data	 Just as a microscope allows us to see things in the “micro world” and a telescope allows us to see things far away in the era of big data we may envision a “datascope” would allow us to “see” useful hidden knowledge buried in large amounts of data  As a special kind of data text data presents unique opportunities to help us “see” virtually all kinds of knowledge we encode in text especially knowledge about people’s opinions and thoughts which may not be easy to see in other kinds of data  12.
data advantage able problems.	 Looking at text data in this way has an advantage of being able to integrate all types of data together which is instrumental in almost all data mining problems.
data module kinds mining data need different	 Inside of the data mining module you can also see we have a number of different kinds of mining algorithms  Of course for different kinds of data we generally need different algorithms each suitable for mining a particular kind of data.
For reason problem definition look landscape topics text mining analytics.	 For this reason we might be concerned with joint mining of text and nontext data  With this problem definition we can now look at the landscape of the topics in text mining and analytics.
basically mind world As humangenerated know world author	 This is basically what a person has in mind about the world  As the users of humangenerated data we will never exactly know what the real world actually looked like at the moment when the author made the observation.
The main mining process uncover real human sensor.	 The main goal of text mining is to reverse this process of generating text data and uncover various knowledge about the real world as it was observed by the human sensor.
text context	 However text data generally also has context associated with it.
Now analyze period comparison Similarly partition data that’s associated form comparisons In sense data text	 Now we can analyze text data in each time period and make a comparison  Similarly we can partition text data based on location or any other metadata that’s associated with it to form interesting comparisons in those areas  In this sense nontext data can provide interesting angles or perspectives for text data analysis.
regarded mining ered 18.	 This can be regarded as one example of mining knowledge about the observer and will be cov ered in Chapter 18.
We affecting understanding Chapter 14 ideas similar terms	 We can replace one with another without affecting the understanding of the sen tence  Chapter 14 gives some additional ideas not discussed in this chapter about how to group similar terms together.
discussed chapter group As words cat word class animal.	 Chapter 14 gives some additional ideas not discussed in this chapter about how to group similar terms together  As an example the words cat and dog have a paradigmatic relation because they are in the same word class animal.
These relations generalized capture basic They language wa don’t entities.	 These two relations are in fact so fundamental that they can be generalized to capture basic relations between units in arbitrary sequences  They can be gener alized to describe relations of any items in a language that is wa and wb don’t have to be words  They could be phrases or entities.
relations elements sequence.	 Syntagmatic relations capture cooccurring elements that tend to show up in the same sequence.
Such syntagmatic relations opinions	 Such syntagmatic relations would help us show the detailed opinions about the product.
look left That Clearly words	 For example we can look at what words occur in the left part of this context  That is what words occur before we see cat or dog Clearly these two words have a similar left context.
knowing words verb eats right we’ve taken away Then question left tend right eats Therefore question tend	 Here however we’re interested in knowing what other words are correlated with the verb eats  On the right side of the figure we’ve taken away the two words around eats  Then we ask the question what words tend to occur to the left of eats What words tend to occur to the right of eats Therefore the question here has to do with whether there are some other words that tend to cooccur with eats.
Note relations tend word.	 Note that paradigmatic relations and syntagmatic relations are closely related in that paradigmatically related words tend to have a syntagmatic relation with the same word.
applicationspecific main pardigmatically related words pute similarity functions.	 Naturally this would be applicationspecific but again the main idea for discovering pardigmatically related words is to com pute the similarity of their contexts  Let’s see how we exactly compute these similarity functions.
exactly	 Let’s see how we exactly compute these similarity functions.
13.	 Figure 13.
timately sense higher	 Ul timately we have to test the approach with real data and see if it gives us really semantically related words  Analytically we can also analyze this formula  Initially it does make sense because this formula will give a higher score if there is more over lap between the two contexts.
case intuitively case match terms confidence saying If rely robust.	 In our case we should intuitively prefer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context  If you only rely on one highscoring term it may not be robust.
vector represent terms weights Depending assign weights cover strongly associated candidate The converted representation	 When we rep resent a term vector to represent a context with a term vector we would likely see some terms have higher weights and other terms have lower weights  Depending on how we assign weights to these terms we might be able to use these weights to dis cover the words that are strongly associated with a candidate word in the context  The idea is to use the converted representation of the context to see which terms are scored high.
simply frequent term correlated candidate common words like frequently weighting terms based means	 We can’t simply say a frequent term in the context would be correlated with the candidate word because many common words like the will occur frequently in the context  However if we apply IDF weighting we can then reweight these terms based on IDF  That means the words that are common like the will get penalized.
ways approaches Specifically help similarity function compute paradigmatic More BM25	 There are many different ways to implement this general idea but we just talked about a few of the approaches  Specifically we talked about using text retrieval models to help us design an effective similarity function to compute the paradigmatic relations  More specifically we used BM25 TF and IDF weighting to discover paradigmatic relations.
look know occurred	 We look at the pres ence or absence of meat given that we know eats occurred in the context.
replace we’ll conditional eats.	 If we replace these probabilities with their corresponding conditional probabilities in the entropy function we’ll get the conditional entropy conditioned on the presence of eats.
The information makes help prediction case This way measure words tells extent predict know presence absence word.	 The inequality states that we can only reduce uncertainty by adding more information which makes sense  As we know more information it should always help us make the prediction and can’t hurt the prediction in any case  This conditional entropy gives us one way to measure the association of two words because it tells us to what extent we can predict one word given that we know the presence or absence of another word.
	 3.
However different w1 different bound entropy	 However if we try to predict a different word other than w1 we will get a different upper bound for the entropy calculation.
shows mutual problem 13.	 The next section shows how we can use mutual information to solve this problem  13.
Specifically mutual defined X HX HY	 Specifically the question we are interested in here is how much of a reduction in entropy of X can we obtain by knowing Y   Mathematically mutual information can be defined as I X Y   HX − HX  Y   HY − HY  X.
Mathematically information defined X Y HX − HX Y HY X.	 Mathematically mutual information can be defined as I X Y   HX − HX  Y   HY − HY  X.
That mutual information mutual syntagmatic mining Figure 13	 That is why mutual information is more general and more useful  Let’s examine the intuition of using mutual information for syntagmatic relation mining in Figure 13 10.
KLdivergence C.	4 which is in the context of KLdivergence see Appendix C.
The numerator distribution	 The numerator of the fraction is the observed joint distribution and the denominator is the expected joint distribution if they are independent.
different mean variables independent The sum random	 If the numerator is different from the denominator that would mean the two variables are not independent and their difference can measure the strength of their association  The sum is simply to take all of the combinations of the values of these two random variables into consideration.
variable values zero If mutual shows mutual divergence joint independence information	 In our case each random variable can choose one of the two values zero or one so we have four combinations  If we look at this form of mutual information it shows that the mutual information measures the divergence of the actual joint distribution from the expected distribution under the independence assumption  The larger this divergence is the higher the mutual information would be.
look form information mutual measures divergence actual expected distribution divergence information Let’s look involved mutual infor displayed	 If we look at this form of mutual information it shows that the mutual information measures the divergence of the actual joint distribution from the expected distribution under the independence assumption  The larger this divergence is the higher the mutual information would be  Let’s further look at exactly what probabilities are involved in the mutual infor mation formula displayed in Figure 13.
know calculate probabilities easily calculate mutual information.	 Once we know how to calculate these probabilities we can easily calculate the mutual information.
12 The means probabilities words probabilities occurs second probability	12  The first new constraint means if we add up the probabilities of two words cooccurring and the probabilities when the first word occurs and the second word does not occur we get exactly the probability that the first word is observed.
The shows need know compute presence word words segment All based In general empirical count events observed Figure	 The figure shows that we only need to know how to compute the three boxed probabilities namely the presence of each word and the cooccurrence of both words in a segment  All others can be computed based on them  In general we can use the empirical count of events in the observed data to estimate the probabilities as shown in Figure 13.
analysis	 And they can be combined to perform joint analysis as well.
The convince improve application If question Does query association statisticallysignificant We type evaluation 9 perform not.	 The best way to convince an application developer that they should use word as sociation mining is to show how it can improve their application  If the application is search the question becomes Does adding query expansion via word association mining improve MAP at a statisticallysignificant level We know how to perform this type of evaluation from Chapter 9  The variable we control here between the two experiments is whether we perform query expansion or not.
scenario 0 89 dataset.	 Imagine the scenario where we have a baseline system with a MAP of 0 89 on some dataset.
documents sentences words.	 These objects could be documents sentences or words.
data The clustering results	 groups of data objects  The clustering results can sometimes be regarded as knowledge directly useful in an application.
objects similar jects appear However strictly defined exactly measure	 imagine objects inside the same cluster are similar in some way—more so than ob jects that appear in two different clusters  However such a definition of clustering is strictly speaking not well defined as we did not make it clear how exactly we should measure similarity.
However based size object different results right define clustering state desired measuring refer bias.	 However if we define the similarity based on the size of an object then we would have very different results as shown in the figure on the right side  Thus when we define a clustering task it is important to state the desired perspective of measuring similarity which we refer to as a “clustering bias.
“right” clustering bias clearly different clustering injected different	 The “right” clustering bias clearly has to be determined by the specific application  In different algorithms the clustering bias is injected in different ways.
different clustering injected	 In different algorithms the clustering bias is injected in different ways.
clustering topic chapter 17 example clustering ments sentences case clustering small documents.	 This particular chapter focuses on similaritybased clustering and the topic analysis chapter Chapter 17 is a fine example of model based clustering  In this chapter we examine clustering techniques for both words and docu ments  Clustering sentences can be viewed as a case of clustering small documents.
sentences clustering start overview clustering techniques categorize approaches.	 Clustering sentences can be viewed as a case of clustering small documents  We first start with an overview of clustering techniques where we categorize the different approaches.
Later discuss term clustering relatedness topic models mentioned Chapter Then technique called	 Later we will first discuss term clustering using semantic relatedness via topic language models mentioned in Chapter 2  Then we explore a simple probabilistic technique called pointwise mutual information.
Finally implementation clustering As rest information techniques discussed algorithms text clustering.	 Finally a brief note on the implementation of clustering algorithms  As with the rest of the chapters in this part of the book we will see that the information retrieval techniques that we discussed in Part II are often also very useful for implementing many other algorithms for text analysis including clustering.
The discuss general different types words syntactic features.	 The clustering techniques we discuss are general so they can be potentially used for clustering many different types of objects including e g  unigram words bigram words trigram POStags or syntactic tree features.
document vectors plotted larger	 The cosine similar ity captures the cosine of the angle between the two document vectors plotted in their highdimensional space the larger the angle the more dissimilar the docu ments are.
issue need cluster cluster clus document defined To clusters	 This does present an issue though when we need to compare the similarity of a cluster with a cluster or a clus ter with a single document  Until now we have only defined similarity measures that take two documents as input  To simplify this problem we will treat individ ual documents as clusters thus we only need to compare clusters for similarity.
	 Algorithm 14.
algorithm Initialize randomly Assign cluster centroid	1 Kmeans clustering algorithm Initialize K randomly selected centroids while not converged do Assign each document to the cluster whose centroid is closest to it using sim.
sim similarity steps marked expectation step Ex.	 Let sim  be the chosen document document similarity measure  The two steps in Kmeans are marked as the expectation step Ex.
lead centroid represent new locations algorithm continues iterate	 The assignments lead to three tentative clusters each of which can then be used to compute a new centroid to better represent the cluster shown as three stars in new locations in d  The algorithm continues to iterate with the new centroids as the starting centroids to reassign all the data points.
docode cluster determined frame d improve positions reassignment step marked	 in the pseu docode  Then once the cluster assignments are determined frame d shows how the centroids are recomputed to improve the centroids’ positions  This centroid reassignment step is marked as Max.
14 3 goal clustering wish related terms By “related” usually semantic	 14 3 Term Clustering The goal of term clustering is quite similar to document clustering we wish to find related terms  By “related” we usually mean words that have a similar semantic meaning.
14.	 Figure 14.
reproduced Section 3 unigram language pw language documents	5 is reproduced here from Section 3 4  Also recall that we used the maximum likelihood estimate of a unigram language model to find pw  θ̂  where θ̂ in our case is the topic language model associated with documents containing the term computer.
estimated topic models vocabulary pw	3 After we estimated the topic and background language models we used the following formula to assign scores to words in our vocabulary scorew  pw  computer pw  C   14.
D 14 5 set documents containing C words appear context numerator denominator score.	 D  14 5 where D is the set of documents containing the term computer and C is the entire collection of documents  We see that words that are more likely to appear in the context of computer will have a greater numerator than denominator thus increasing the score.
We context numerator	 We see that words that are more likely to appear in the context of computer will have a greater numerator than denominator thus increasing the score.
formula smoothed language C cw V 6 C count words V size vocabulary set variable V number	 Thus the formula for computing a smoothed background language model would be pw  C  cw C  1 C  V   14 6 where C is the total count of all the words in collection C and V  is the size of the complete vocabulary set  Note that the variable V  in the denominator is the total number of pseudo counts we have added to all the words in the vocabulary.
Pointwise Mutual Information treats word random cooccurrence context window words words wi window n look words wi−n	3 2 Pointwise Mutual Information Pointwise Mutual Information PMI treats word occurrences as random variables and quantifies the probability of their cooccurrence within some context of a window of words  For example to find words that cooccur with wi using a window of size n we look at the words wi−n .
e larger independent	e  pwi  wj is substantially larger than their expected probability of cooccurrence if there were independent i e.
Indeed low entropy mutual information Despite useful building blocks methods allowing understand information word book.	 Indeed since mutual information is bounded by the entropy of one of the two variables and a rare word has very low entropy it generally wouldn’t have a high mutual information with any other word  Despite their drawbacks however PMI and nPMI are often used in practice and are also useful building blocks for more advanced methods as well as allowing us to understand the basic idea behind information capture in word cooccurrence  We thus included a discussion in this book.
.	 wn−1 .
	  w1 as pwn  wn−1     .
w1 cn−1	      w1  pwn  cnpcn  cn−1 .
cn−1	 cn−1 .
	e.
Computationwise simply derived based likelihood function mutual complexity model bigrams 2 Brown et	 Computationwise we simply do agglomerative hierarchi cal clustering and measure the “distance” of two words based on a derived function based on the likelihood function that can capture the loss of mutual information due to merging the two words  Due to the complexity of the model only bigrams n  2 were originally investigated Brown et al.
7 sample results phrases meanings phrases e g.	7 for sample results which are noncompositional phrases whose meaning is not a direct composition of the meanings of individual words  Such noncompositional phrases can also be discovered using some other statistical methods see e g.
3 3.	 14 3 3.
However representation discussed entire space increasing processing vectors As neural language model et al.	 However the heuristic way to obtain vector representation discussed in Chap ter 13 has the disadvantage that we need to make many ad hoc choices especially in how to obtain the term weights  Another deficiency is that the vector spans the entire space of words in the vocabulary increasing the complexity of any further processing applied to the vectors  As an alternative we can use a neural language model Mikolov et al.
word vector representation The idea methods corresponds latent define language model solely based vector representations words parameters	 Such an approach is also called word embedding which refers to the mapping of a word into a vector representation in a lowdimensional space  The general idea of these methods is to assume that each word corresponds to a vector in an unknown latent lowdimensional space and define a language model solely based on the vector representations of the involved words so that the parameters for such a language model would be the vector representations of words.
general idea methods assume space model based representations parameters language model representations fitting data set models represented	 The general idea of these methods is to assume that each word corresponds to a vector in an unknown latent lowdimensional space and define a language model solely based on the vector representations of the involved words so that the parameters for such a language model would be the vector representations of words  As a result by fitting the model to a specific data set we can learn the vector representations for all the words  These language models are called neural language models because they can be represented as a neural network.
methods described class promising open new text fact vector systematically objective	 using the methods we described in Chapter 13 and the ngram class language model word embedding provides a very promising new alternative that can poten tially open up many interesting new applications of text mining due to its flexibility in formulating the objective functions to be optimized and the fact that the vector representation is systematically learned through optimizing an explicitly defined objective function.
high similarity	 allowing words in the same cluster or with high similarity to “match” with each other.
algorithm performs spectacularly example intracluster similarity acceptable adequate representation able capture concept needs reexamined.	 Even if your clustering algorithm performs spectacularly in terms of for example intracluster similarity the clusters may not be acceptable from a human viewpoint unless an adequate feature representation was used it’s possible that the feature representation is not able to capture a crucial concept and needs to be reexamined.
cluster calculate predefined effective category dominates appears clustering recreated assignments origi nal dataset supervision.	 In other words take each cluster Ci and calculate the percentage of each predefined class in it  The clustering algorithm would be effective if for each Ci one predefined category dominates and scarcely appears in other clusters  Effectively the clustering algorithm recreated the class assignments in the origi nal dataset without any supervision.
Categorization 15.	 15Text Categorization 15.
solve problem text categorization applications.	 To solve such a problem we can use text categorization techniques which have widespread applications.
result categorization different	 As a result the categorization results may depend on the order of application of different rules.
creating automatically rules categorization rules rules combined	e  creating training examples and the machine will learn from these examples to somewhat automatically construct rules for categorization only that the rules are somewhat “soft” and weighted and how the rules should be combined is also learned based on the training data.
examples somewhat categorization rules somewhat weighted supervised learning cateorization pears require effort data data available	 creating training examples and the machine will learn from these examples to somewhat automatically construct rules for categorization only that the rules are somewhat “soft” and weighted and how the rules should be combined is also learned based on the training data  Note that although in such a supervised machine learning approach cateorization ap pears to be “automatic” it does require human effort in creating the training data unless the training data is naturally available to us which sometimes does happen.
The humancreated features based approach manner training data automatically construct rules fea easily interpreted root leaves i.	 The humancreated rules if any can also be used as features in such a learning based approach and they will be combined in a weighted manner to minimize the classification errors on the training data with the weights automatically learned  The machine may also automatically construct soft rules based on primitive fea tures provided by humans as in the case of decision trees Quinlan 1986 which can be easily interpreted as a “rulebased” classifier but the paths from the root to the leaves i.
Once classifier categorizer text data combine tiple e.	 Once a classifier categorizer is trained it can be used to categorize any unseen text data  In general all these learningbased categorization methods rely on discrimina tive features of text objects to distinguish categories and they would combine mul tiple features in a weighted manner where the weights are automatically learned i e.
adjusted minimize data methods tend	 adjusted to minimize errors of categorization on the training data  Different methods tend to vary in their way of measuring the errors on the training data i e.
category classify object according distribution classifiers compute object category object control	 unigram language model for each category  They classify an object based on the likelihood that the object would be observed according to each distribution  Discriminative classifiers compute features of a text object that can provide a clue about which category the object should be in and combine them with parameters to control their weights.
xi xi feature assigned tokenizer 8 V	 As with our re trieval setup each xi has xi  V  one dimension for each feature as assigned by the tokenizer  Our vector from Chapter 8 is an example of such an xi with a very small vocabulary of size V   8  mr.
classifier support distinguishing Multiclass classification arbitrary number labels.	 In binary classification there are only two categories  Depending on the type of classifier it may only support distinguishing between two different classes  Multiclass classification can support an arbitrary number of labels.
multiple classifiers Regression realvalued opposed For example particular day	 As we will see it’s possible to combine multiple binary classifiers to create a multiclass classifier  Regression is a very related problem to classification it assigns realvalued scores on some range as opposed to discrete labels  For example a regression problem could be to predict the amount of rainfall for a particular day given rainfall data for previous years.
best able distinguish based sentence hand Y pass.	 Even the best learning algorithm would not be able to distinguish between positive and negative documents based only on sentence lengths 1 On the other hand suppose our task is basic essay scoring where Y  fail  pass.
1 On scoring Y fail	1 On the other hand suppose our task is basic essay scoring where Y  fail  pass.
sentence length standard words	 Instead of using sentence length we decide to use the standard unigram words representation.
overhyped If rarer informative context—such term can’t polarity.	 This term is now captured in bigrams such as overhyped period and very overhyped  If we see the same rarer informative word in a different context—such as was overhyped—this is now an outofvocabulary term and can’t be used in determining the sentence polarity.
roots captures abstract view produc structure square abstracted set syntactic all.	2 omits all node labels except the roots of all subtrees  This captures a more abstract view of the produc tion rules focused more on structure  Lastly the right square is a fully abstracted structural feature set with no syntactic category labels at all.
given sentence.	 These edits are used to transform a given sentence.
sentence applied appear come R nario Zhai 2016 writers native	 With a source sentence S and a reference text collection R it applied edits that make S appear to come from R  In the nonnative text analysis sce nario Massung and Zhai 2016 we operate on text from writers who are not native English speakers.
removeof vector particular	    removeof   2 could be a feature vector for a particular sentence.
Usually words default order improve accuracy.	 Usually unigram words will be the default method and more advanced techniques are added as necessary in order to improve accuracy.
When features parse trees run data magnitude simple unigram Running sentence partofspeech running	 When using features from grammatical parse trees a parser must first be run across the data set which is often at least an order of magnitude slower than simple whitespace delimited unigram words processing  Running a parser requires the sentence to be partofspeech tagged and running coreference resolution requires grammat ical parse trees.
actually able distinguish class	 is actually able to distinguish between class labels.
The documents build	 The training documents will be used to build f .
different classes	 Here there are three different classes represented as different colors plotted in the vector space.
finding nearest performing	 For one finding the nearest neighbors requires performing a search engine query for each testing instance.
receive n search engine	 We also receive a performance benefit since we only need to do n similarity comparisons as opposed to a full search engine query over all the training documents.
ory forward required Y	 Mem ory aside from the forward index is required to store the parameters which can be represented as OY  V    Y floating point numbers.
including stateoftheart 2011 group learning linear classifiers based feature weights w x	 There are many such algorithms including the stateoftheart support vector machines SVM classifier Campbell and Ying 2011  We call this group of learning algorithms linear classifiers because their decision is based on a linear combination of feature weights w and x  Figure 15.
advanced trick change linear classifiers classifiers refer text learning tails linear specify training	 Some more advanced meth ods such as the kernel trick may change linear classifiers into nonlinear classifiers but we refer the reader to a text more focused on machine learning for the de tails Bishop 2006  In this book we choose to examine the relatively simple perceptron classifier on which many other linear classifiers are based  We need to specify several parameters which are used in the training algorithm.
We terminate training small measured norm previous There discuss book familiarize general algorithm refer 2006 details implementation	 We may terminate training early if the change in w is small this is usually measured by comparing the norm of the current iteration’s weights to the norm of the previous iteration’s weights  There are many discussions about the choice of learning rate convergence criteria and more but we do not discuss these in this book  Instead we hope to familiarize the reader with the general spirit of the algorithm and again refer the reader to Bishop 2006 for many more details on algorithm implementation and theoretical properties.
Eventually w iterations signifying best accuracy final classify need classification prob nicely categories.	 Eventually the change in w will be small after some number of iterations signifying that the algorithm has found the best accuracy it could  The final w vector is saved as the model and it can be used to classify unseen documents  What if we need to support multiclass classification Not all classification prob lems fit nicely into two categories.
The w saved model classification prob	 The final w vector is saved as the model and it can be used to classify unseen documents  What if we need to support multiclass classification Not all classification prob lems fit nicely into two categories.
We accuracy number correct predictions number total predictions.	 We are also usually more concerned about accuracy the number of correct predictions divided by the number of total predictions.
variance fold high accuracy algorithm stage algorithm separate corpus likely features split i.	 If the variance is high it means that the accuracies are not very similar between folds  Having one fold with a very high accuracy suggests that your learning algorithm may have overfit during that training stage when using that trained algorithm on a separate corpus it’s likely that the accuracy would be very low since it modeled noise or other uninformative features from that particular split i.
This represents minimum “beat” 3000 documents consisting classes 1000	 This represents the minimum score to “beat” when using your classifier  Say there are 3000 documents consisting of three classes each with 1000 documents.
sum The positive rate probability mass matrix predicting	 Therefore the rows all sum to one  The diagonal represents the true positive rate and hopefully most of the probability mass lies here indicating a good classifier  Based on the matrix we see that predicting Chinese was 80.
This Chinese relatively Japanese easy classifier distinguish classifier prediction true chose Japanese Based label	 This shows that while English and Chinese had relatively the same difficulty Japanese was very easy for the classifier to distinguish  We also see that if the classifier was wrong in a prediction on a Chinese or English true label it almost always chose Japanese as the answer  Based on the matrix the classifier seems to default to the label “Japanese”.
entire list headlines For different angle news summarization input paragraph format.	 Of course this wouldn’t be the entire list of headlines but only those headlines that would interest a user  For a different angle consider a news summarization task where the input is one text news article and the output should be one paragraph explaining what the article talks about in a readable format.
consider news text paragraph format.	 For a different angle consider a news summarization task where the input is one text news article and the output should be one paragraph explaining what the article talks about in a readable format.
product lets business satisfied why.	 Summariz ing all reviews of a product lets the business know whether the buyers are satisfied and why.
pattern previous evaluation The metrics possible use	 Following the pattern of previous chapters we then move on to evaluation of text summarization  The two methods each have evaluation metrics that are particularly focused towards their respective implementation but it is possible to use e.
Summarization use vectors order create	2 Extractive Text Summarization Information retrievalbased techniques use the notion of sentence vectors and similarity functions in order to create a summarization text.
sentence vector structure document based number words retrievalbased	 A sentence vector is equivalent in structure to a document vector albeit based on a smaller number of words  Below we will outline a basic information retrievalbased summarization system  1.
This retains sentences original portrayed	 This strategy retains coherency since the sentences in the summary are mostly in the same order as they were in the original document  Step one is portrayed in Figure 16.
segment similarity low shift topic	 We can inspect these changes to segment the document into passages when the similarity is low i e  a shift in topic occurs.
alternative approach use operated	 a shift in topic occurs  An alternative approach to this segmentation is to simply use paragraphs if the document being operated on contains that information although most of the time this is not the case.
sentence shows sentence	 Essentially this algorithm greedily reranks each sentence in the current passage outputting only the top few as a summary  Figure 16 2 shows the output of the algorithm when we only select one sentence from each passage.
” formula applied documents returned retrieval term based relevance query alreadyselected Since task sentence document formulated	” Originally the MMR formula was applied to documents returned from an information retrieval system hence the term reranking  Documents were selected based on their marginal relevance to a query which is our variable p in addition to nonredundancy to alreadyselected documents  Since our task deals with sentence retrieval p can be a user profile text about the user the entire document itself or it could even be a query formulated by the user.
marginal relevance query documents.	 Documents were selected based on their marginal relevance to a query which is our variable p in addition to nonredundancy to alreadyselected documents.
	7.
3 creates sentences ment document use language gives principled text.	3 Abstractive Text Summarization An abstractive summary creates sentences that did not exist in the original docu ment or documents  Instead of a document vector we will use a language model to represent the original text  Unlike the document vector our language model gives us a principled way in which to generate text.
accomplish create parameters sum allow number 0 1 choose	3 depicts how we can accomplish this task  First we create a list of all our parameters and incrementally sum their probabilities this will allow us to use a random number on 0 1 to choose a word wi.
.	.
	   .
	 We go to the cumulative point 0.
summary certain length s.	 We can repeat this process until our summary is of a certain length or until we generate an endofsentence token s.
major	 There is one major disadvantage to this abstractive summarization method.
longrange text.	 That is there will be no longrange dependencies in our generated text.
Named entity extract people syntactic relation entities perform.	 Named entity recognition can be used to extract people places or businesses from the text  Dependency parsers and other syntactic techniques can be used to find the relation between the entities 324 Chapter 16 Text Summarization and the actions they perform.
This control summarization topical In environment possible merge context summary sound entity	 This control over text summarization and layout enables an easilyreadable summary since it has a natural topical flow  In this environment it would be possible to merge similar sen tences with conjunctions such as and or but depending on the context  To make the summary sound even more natural pronouns can be used instead of entity names if the entity name has already been mentioned.
For natural generation implementation.	 For more on advanced natural language generation we suggest Reiter and Dale 2000 which has a focus on practicality and implementation.
Evaluation Text In extractive passages output summary This solution retrieval evaluate	 16 4 Evaluation of Text Summarization In extractive summarization representative sentences were selected from passages in the text and output as a summary  This solution is modeled as an information retrieval problem and we can evaluate it as such.
This modeled information retrieval critical technique attempts	 This solution is modeled as an information retrieval problem and we can evaluate it as such  Redundancy is a critical issue and the MMR technique we discussed attempts to alleviate it.
MMR technique evaluation consider redundant user want read twice 9.	 Redundancy is a critical issue and the MMR technique we discussed attempts to alleviate it  When doing our evaluation we should consider redundant sentences to be irrelevant since the user does not want to read the same information twice  For a more detailed explanation of IR evaluation measures please consult Chapter 9.
When evaluation user want information For detailed explanation IR measures Chapter 9.	 When doing our evaluation we should consider redundant sentences to be irrelevant since the user does not want to read the same information twice  For a more detailed explanation of IR evaluation measures please consult Chapter 9.
precision F1 16 possible passage function dependent metrics average precision output	 Therefore we can use precision recall and F1 score  16 5 Applications of Text Summarization 325 It is possible to rank the passage scoring retrieval function using position dependent metrics such as average precision or NDCG but with the final output this is not feasible.
ROUGE difference comparison measure possibilities—any measure compare groups text potentially	 ROUGE would be used to quantify the difference  For the comparison measure we have many possibilities—any measure that can compare two groups of text would be potentially applicable.
Was summary information needs If entire chapter provided abstractive	 Was the summary able to capture the important information that the evaluator needs If the original text was an entire textbook chapter could the user read a threeparagraph summary and obtain sufficient information to answer the provided exercises This is the only metric that can be used for both extractive and abstractive measures.
Applications beginning we’ve mentioned news articles retrieval opinion saves time manually reading enhancing summary	 16 5 Applications of Text Summarization At the beginning of the chapter we’ve already touched on a few summarization applications we mentioned news articles retrieval results and opinion summa rization  Summarization saves users time from manually reading the entire corpus while simultaneously enhancing preexisting data with summary “annotations.
We topic passages aspect describing unigram summarizer topic readable	 We can use this topic analysis to collect passages of text into a large group of comments on one aspect  Instead of describing this aspect with sorted unigram words we could run a summarizer on each topic generating readable text as output.
Being text huge trading unaware discovery electronic finding relevant information litigation court cases rely	 Being able to summarize in text a huge amount of structured trading data could reveal patterns that humans would otherwise be unaware of—this is an example of knowledge discovery  Ediscovery electronic discovery is the process of finding relevant information in litigation lawsuits and court cases  Lawyers rely on ediscovery to sift through vast amounts of textual information to build their case.
In way coupled search allows data selected relevant query search results quickly explain	 In this way summarization and search are coupled search allows a subset of data to be selected that is relevant to a query and the summarization can take the search results and quickly explain them to the user.
For latent aspect rating analysis Wang et	 For applications latent aspect rating analysis Wang et al.
A topic granularities.	 A topic can have different granularities.
time produced authors	 We might know the time associated with the text data or locations where the text data were produced or the authors of the text or the sources of the text.
looking able trending	 For example looking at topics over time we would be able to discover whether there’s a trending topic or some topics might be fading away.
case topics topics extent For example shows Topic 1 Topic covered small portion.	 In this case there are k topics  We also would like to know which topics are covered in which documents and to what extent  For example in Doc 1 the visualization shows that Topic 1 is well covered while Topic 2 and Topic k are covered with a small portion.
example visualization Topic covered Topic 2 Topic k 2 cover Topic covers Topic k	 For example in Doc 1 the visualization shows that Topic 1 is well covered while Topic 2 and Topic k are covered with a small portion  Doc 2 on the other hand covered Topic 2 very well but it did not cover Topic 1 at all  It also covers Topic k to some extent.
problem shown 3 documents.	 More formally we can define the problem as shown in Figure 17 3  First we have as input a collection of N text documents.
Here collection C denote text article di.	 Here we can denote the text collection as C and denote a text article as di.
techniques chapter specify topics The output k topics like θ1 .	 However in the techniques that we will discuss in this chapter we need to specify a number of topics  The output includes the k topics that we would like to discover denoted by θ1   .
document πij probability document covering topic	    θk and the coverage of topics in each document of di which is denoted by πij   πij is the probability of document di covering topic θj .
like what’s shown 17.	 A possible scenario may look like what’s shown in Figure 17.
These topics quantify topic There consider statistics terms.	 These words then become candidate topics  Next we will need to design a scoring function to quantify how good each term is as a topic  There are many things that we can consider when designing such a function with a main basis being the statistics of terms.
statistical applied	 An advantage of using such a statistical approach to define a scoring function is that the scoring function would be very general and can be applied to any natural language any text.
look picked try avoid picking that’s considering candidate	 The first term of course will be picked  When we pick the next term we will look at what terms have already been picked and try to avoid picking a term that’s too similar  So while we are considering the ranking of a term in the list we are also considering the redundancy of the candidate term with respect to the terms that we already picked.
normalize estimate probability The ensure topic document forming topics coverage.	 We can then just normalize these counts as our estimate of the coverage probability for each topic  The normalization is to ensure that the coverage of each topic in the document would add to one thus forming a distribution over the topics for each document to characterize coverage.
	 Here we have a text document that’s about an NBA basketball game.
sports occur On topic like game counted coverage	 sports which may not occur at all in a document about the topic  On the other hand there are many words related to the topic like basketball and game which should presumably also be counted when estimating the coverage of a topic.
hand related basketball	 On the other hand there are many words related to the topic like basketball and game which should presumably also be counted when estimating the coverage of a topic.
In section improved	 In the next section we will discuss an improved representation of a topic as a distribution over words that can address these problems  17.
Finally solve need ambiguous words potentially topics elegantly probability	 Finally to solve the problem of word ambiguity we need to “split” ambiguous words to allow them to be used to potentially describe multiple topics  It turns out that all these can be elegantly achieved by using a probability distri bution over words i e.
words topic sports game basketball football play sportsrelated terms likelihood words nonzero chance word article	 For example the high probability words for the topic “sports” are sports game basketball football play and star  These are all intuitively sportsrelated terms whose occurrences should contribute to the likelihood of covering the topic “sports” in an article  Note that in general the distribution may give all the words a nonzero probability since there is always a very very small chance that even a word not so related to the topic would be mentioned in an article about the topic.
It words topics.	 It now uses multiple words to describe a topic allowing us to describe fairly complicated topics.
When distribution denote topic topic analysis refined formal Figure 3 making topic θi word As know topics topics data.	 When using a word distribution to denote a topic our task of topic analysis can be further refined based on the formal definition in Figure 17 3 by making each topic a word distribution  That is each θi is now a word distribution and we have As a computation problem our input is text data a collection of documents C and we assume that we know the number of topics k or hypothesize that there are k topics in the text data.
know vocabulary determines treated basic units	 As part of our input we also know the vocabulary V which determines what units would be treated as the basic units i.
In model controlling kind high low parameters model differently data high probabilities parameters	 In general our model will have some parameters which can be denoted by they control the behavior of the model by controlling what kind of data would have high or low probabilities  If you set these parameters to different values the model would behave differently that is it would tend to give different data points high or low probabilities  We design the model in such a way that its parameters would encode the knowl edge we would like to discover.
Then estimate parameters based infer desired output parameter knowledge discover.	 Then we attempt to estimate these parameters based on the data or infer the values of parameters based on the observed data so as to generate the desired output in the form of parameter values which we have designed to denote the knowledge we would like to discover.
obviously varies change ’s settings maximize observed data yields model parameters hoped text output data mining	 This probability obviously depends on this setting of so that’s why it varies as you change ’s value in order to find ∗ the parameter settings that maximize the probability of the observed data  Such a search yields our estimate of the model parameters  These parameters are precisely what we hoped to discover from the text data so we view them as the output of our data mining or topic analysis algorithm.
Mining One Text discuss model text single goal discover topic Figure 17.	 17 3 Mining One Topic from Text In this section we discuss the simplest instantiation of a generative model for modeling text data where we assume that there is just one single topic covered in the text and our goal is to discover this topic  More specifically as illustrated in Figure 17.
The problem function	 The next line shows an equivalent optimization problem with the loglikelihood  The equivalence is due to the fact that the log arithm function results in a monotonic transformation of the original likelihood function and thus does not affect the solution of the optimization problem.
	e.
construct Lagrange original ob function encodes parameter.	11  We will first construct a Lagrange function which combines our original ob jective function with another term that encodes our constraint with the Lagrange multiplier denoted by λ introducing an additional parameter.
1 M linear equations parameters Note Lagrange multiplier constraint linear equations unigram cwi j1 d .	1 We thus have in total M  1 linear equations corresponding to the M word probability parameters and λ  Note that the equation for the Lagrange multiplier λ is precisely our original constraint  We can easily solve this system of linear equations to obtain the Maximum Likelihood estimate of the unigram language model as pwi  θ̂   cwi  d∑M j1 cwj  d cwi  d d .
	 17.
reach sufficient However optimum condition imagine document paper.	 Zero derivatives are a necessary condition for the function to reach an optimum but not sufficient  However in this case we have only one local optimum thus the condition is also sufficient  What would the topic discovered from a document look like Let’s imagine the document is a text mining paper.
document like Let’s imagine text	 What would the topic discovered from a document look like Let’s imagine the document is a text mining paper.
representation ideal prob characterize topic Giving high direct assumed model distribution generate How common es distribution answer word model topic distribution need	 As a topic representation such a distribution is not ideal because the high prob ability words are function words which do not characterize the topic  Giving com mon words high probabilities is a direct consequence of the assumed generative model which uses one distribution to generate all the words in the text  How can we improve our generative model to downweight such common words in the es timated word distribution for our topic The answer is that we can introduce a second background word distribution into the generative model so that the com mon words can be generated from this background model and thus the topic word distribution would only need to generate the contentcarrying topical words.
unigram topic like model fixed words In Figure 17 13 distributions generate data common words language generate document.	 We thus have a mixture model with two component unigram language models one being the unknown topic that we would like to discover and one being a background language model that is fixed to assign high probabilities to common words  In Figure 17 13 we see that the two distributions can be mixed together to generate the text data with the background model generates common words while the topic language model to generate contentbearing words in the document.
First flip pθd pθB If shows heads θB chosen word generate	 First we flip a biased coin which would show up as heads with probability pθd and thus as tails with probability pθB  1 − pθd to decide which word distribution to use  If the coin shows up as heads we would use θd  otherwise θB  We will use the chosen word distribution to generate a word.
This mixture	 This is also the scenario that we used to motivate the use of the mixture model.
unknown	 Figure 17 18 illustrates such a scenario  In this scenario the only parameters unknown would be the topic word distribution pw  θd.
If mixture model 17 18 black box notice actually exactly number parameters unigram language	 If we view the mixture model in Figure 17 18 as a black box we would notice that it actually now has exactly the same number of parameters indeed the same parameters as the simplest single unigram language model.
It obvious constraint together” words require understanding behavior parameter estimation mixture explain section	 It might not be obvious why the constraint of “working together” with the given background model would have the effect of factoring out the com mon words from θd as it would require understanding the behavior of parameter estimation in the case of a mixture model which we explain in the next section  17 3.
3 Behavior mixture illustrated Figure 17	3 4 Behavior of a Mixture Model In order to understand some interesting behaviors of mixture models we take a look at a very simple case as illustrated in Figure 17 19.
1 text write case 17 19.	1 to text  We can write down the likelihood function in such a case as shown in Fig ure 17 19.
20 constraint able 1.	20  Note that the two probabilities must sum to one so we have to respect this constraint  If there were no constraint we would have been able to set both probabilities to their maximum value which would be 1.
	 0 1  0.
.	5 .
	5   0.
solution reason higher probability given smaller text solution probability order ensure overall estimate word higher probability background smaller given w1 higher distribution higher word combined w1 tend probabilities different try giving high probability	 Looking into the process of reaching this solution we see that the reason why text has a higher probability than the is because its corresponding probability by the background model ptext  θB is smaller than that of the had the background model given the a smaller probability than text our solution would give the a higher probability than text in order to ensure that the overall probability given by the two models working together is the same for text and the  Thus the ML estimate tends to give a word a higher probability if the background model gives it a smaller probability or more generally if one distribution has given word w1 a higher probability than w2 then the other distribution would give word w2 a higher probability than word w1 so that the combined probability of w1 given by the two distributions working together would be the same as that of w2  In other words the two distributions tend to give high probabilities to different words as if they try to avoid giving the high probability to the same word.
21 added words ically document pw adding the’s document multiply likelihood occurrences.	21 we have shown a scenario where we’ve added more words to the document specif ically more the’s to the document  What would happen to the estimated pw  θ if we keep adding more and more the’s to the document As we add more words to the document we would need to multiply the likelihood function by additional terms to account for the additional occurrences.
likelihood function increase choosing model hard 0.	21 and try to picture what would happen to the likelihood function if we increase the probability of choosing the background model  It is not hard to notice that if pθB  0.
	5 would be even larger.
summarize estimation problem model general maximum likelihood estimator First component attempts assign words likelihood.	 To summarize we discussed the mixture model the estimation problem of the mixture model and some general behaviors of the maximum likelihood estimator  First every component model attempts to assign high probabilities to high fre quent words in the data so as to collaboratively maximize the likelihood.
Unlike estimate analyti cal solution analytical ML problem model number parameters single language fix model	 Unlike the simplest unigram language model whose ML estimate has an analyti cal solution there is no analytical solution to the ML estimation problem for the twocomponent mixture model even though we have exactly the same number of parameters to estimate as a single unigram language model after we fix the back ground model and the choice probability of the component models i e.
	e.
particular distribution heavily biased likelihood dictate imagine 0.	 In general our prior may be biased toward a particular distribution  Indeed a heavily biased prior can even dominate over the data likelihood to essen tially dictate the decision  For example imagine our prior says pθB  0.
3 Mining 363 With Estep basis EM	3 Mining One Topic from Text 363 With the Estep and Mstep as the basis the EM algorithm works as follows.
usually repeat multiple times initializations practice highest value estimated The algorithm	 Due to this we usually repeat the algorithm multiple times with different initializations in practice using the run that gives the highest likelihood value to obtain the estimated parameter values  The EM algorithm is illustrated in Figure 17.
share count cw dpz 1 cw pz cw dpz 0 cw dpz cw 17 showing Thus simply discounted words distribution estimate pw	 Similarly θB has its own share of the count which is cw dpz 1  w  cw d1 − pz  0  w and we have cw dpz  0  w  cw dpz  1  w  cw d 17 5 showing that all the counts of word w have been split between the two distributions  Thus the Mstep is simply to normalize these discounted counts for all the words to obtain a probability distribution over all the words which can then be regarded as our improved estimate of pw  θd.
Second assume models equal assume model word distribution known fixed	 Second we assume the two component models θd and θB have equal probabilities we also assume that the background model word distribution is known fixed as shown in the third column of the table.
33 count 4 0.	33 we would obtain a discounted count of the 4 × 0.
worth main word distribution topic hope	 For now it is worth pointing out that while the main goal of our EM algorithm is to obtain a more discriminative word distribution to represent the topic that we hope to discover i.
probabilities believed topic add obtain extent document cov vs.	 Specifically these are the probabilities that a word is believed to have come from the topic distribution and we can add them up to obtain an estimate of to what extent the document has cov ered background vs.
e.	e.
That oscillate Only satisfied parameters guaranteed converge	 That is the parameters may oscillate even though the likelihood is increasing  Only if some conditions are satisfied would the parameters be guaranteed to converge see Wu 1983.
case model process consists steps.	 As in the case of the simple mixture model the process of generating a word still consists of two steps.
constraint consequence having Despite point	 we have more constraint equations which is a consequence of having more parameters  Despite the third point the kinds of constraints are essentially the same as before namely there are two.
background Estep Figure	  k  B corresponding to the k topics and the extra background topic  The Estep uses Bayes’ Rule to infer the probability of each value for z as shown in Figure 17.
counts based generated .	 The amount of split counts of w that θj can get is determined based on the inferred likelihood that w is generated by topic θj .
Once distribution shown Figure To j document covers topic collect split words d θj normalize topics.	 Once we have such a split count of each word for each distribution we can easily pool together these split counts to reestimate both π and pw  θ as shown in Figure 17 33  To reestimate πd j  the probability that document d covers topic θj we would simply collect all the split counts of words in document d that belong to each θj  and then normalize these counts among all the k topics.
split word collection normalize cases related reestimation π π values chosen ensure reestimated π sum document.	 Similarly to reestimate pw  θj we would collect the split counts of a word toward θj from all the documents in the collection and then normalize these counts among all the words  Note that the normalizers are very different in these two cases which are directly related to the constraints we have on these parameters  In the case of reestimation of π  the constraint is that the π values must sum to one for each document thus our normalizer has been chosen to ensure that the reestimated values of π indeed sum to one for each document.
need compute values documents iterate unique pzd	34 we see that in the Estep we need to compute the probability of z values for every unique word in each document  Thus we can iterate over all the documents and for each document iterate over all the unique words in the document to compute the corresponding pzd w.
probability w document topics Note word generally Once counts way aggregate counts normalize them.	 We would use the estimated probability distribution pzd w to split the count of word w in document d among all the topics  Note that the same word would generally be split in different ways in different documents  Once we split the counts for all the words in this way we can aggregate the split counts and normalize them.
parameters p values encode preferences.	 Specifi cally we denote all the parameters by  and introduce a prior distribution p over all the possible values of  to encode our preferences.
Adding prior p compromise ML estimate p.	 Adding the prior p would encourage the model to seek a compromise of the ML estimate which maximizes pData and the mode of the prior which maximizes p.
form like lihood distribution function data ing data” influence prior entirely pseudo data original data.	 Since the posterior distribution is of the same form as the like lihood function of the original data we can interpret the posterior distribution as the likelihood function for an imagined pseudo data set that is formed by augment ing the original data with additional “pseudo data” such that the influence of the prior is entirely captured by the addition of such pseudo data to the original data.
We difference adding additional pseudo word Mstep probability	 We see that the difference is adding an additional pseudo count for word w in the Mstep which is proportional to the probability of the word in the prior.
Intuitively means prior infinitely strong collect able In general increase data dominate eventually overriding completely collect data.	 Intuitively in Bayesian inference this means that if the prior is infinitely strong then no matter how much data we collect we will not be able to override the prior  In general however as we increase the amount of data we will be able to let the data dominate the estimate eventually overriding the prior completely as we collect infinitely more data.
generative words given documents probability new unseen document distribution documents like generative document generative model category assigning generative gives document.	 PLSA is a generative model for modeling the words in a given document but it is not a generative model for documents since it cannot give a probability of a new unseen document it cannot give a distribution over all the possible documents  However we sometimes would like to have a generative model for documents  For example if we can estimate such a model for documents in each topic category then we would be able to use the model for text categorization by comparing the probability of observing a document from the generative model of each category and assigning the document to the category whose generative model gives the high est probability to the document.
representing coverage .	      θk representing k topics with a topic coverage distribution πd j .
Although LDA use MLE parameters required solve problem parameters fewer PLSA output topic i.	 Although the likelihood function of LDA is more complicated than PLSA we can still use the MLE to estimate its parameters Naturally the computation required to solve such an optimization problem is more complicated than LDA  It is now easy to see that LDA has only k  M parameters far fewer than PLSA  However the cost is that the interesting output that we would like to generate in topic analysis i.
If it’s hard discern topics adequate process different Directly et al.	 If it’s hard to discern θu then the top topics must not be an adequate representation of d  Of course this process is repeated for many different documents in the collection  Directly from Chang et al.
For example analysis produce features we’d wish heldout data accuracy improves—although topics	 For example if the topic analysis is meant to produce new features for text categorization then classification accuracy is the metric we’d wish to improve  In such a case loglikelihood of heldout data and even topic coherency is not a concern if the classification accuracy improves—although model interpretability may be compromised if topics are not humandistinguishable.
Further PLSA paper LDA et 2003.	 2003  Bibliographic Notes and Further Reading We’ve mentioned the original PLSA paper Hofmann 1999 and its successor LDA Blei et al  2003.
2009 compares inference methods topic concludes	 Asuncion et al  2009 compares various inference methods for topic models and concludes that they are all very similar.
evaluation referenced Chang	 For evaluation we’ve referenced Chang et al.
This unique text data compared great observers—we data concept	 This is actually a unique advantage of text data as compared to other data because this offers us a great opportunity to understand the observers—we can mine text data to understand their opinions  Let’s start with the concept of an opinion.
example product know opinion holder target.	 Let’s take a simple example of a product review  In this case we already know the opinion holder and the target.
18.	 Figure 18.
	3 shows a sentence extracted from a news article.
Opinion vary identify opin onephrase	 Opinion content can also vary on the surface we can identify a onesentence opin ion or a onephrase opinion.
For all”	 For example “I don’t like this phone at all” is clearly an opinion by the speaker about a phone.
Each holder content context.	4  Each representation should identify the opinion holder target content and context.
analysis numerical reviews Web positive example analysis categories.	 In the case of polarity analysis we sometimes also have numerical ratings as you often see in some reviews on the Web  A rating of five might denote the most positive and one may be the most negative for example  In emotion analysis there are also different ways to design the categories.
A denote emotion analysis ways categories typical categories happy fearful surprised	 A rating of five might denote the most positive and one may be the most negative for example  In emotion analysis there are also different ways to design the categories  Some typical categories are happy sad fearful angry surprised and disgusted.
discuss	 We’ll discuss this idea in the next section.
We mix different effective it’s words unit information humans communication words cient task sentiment	 We can have a mix of these with different nvalues  Unigrams are often very effective for text processing tasks it’s mostly because words are the basic unit of information used by humans for communication  However unigram words may not be suffi cient for a task like sentiment analysis.
sentence else.	 For example we might see a sentence “It’s not good” or “It’s not as good as something else.
feature allow search complex natural help features enrich representations we’ve mentioned 15 feature design affects machine	 In general pattern discovery algorithms are very useful for feature construction because they allow us to search a large space of possible features that are more complex than words and natural language processing is very important to help us derive complex features that can enrich text representations  As we’ve mentioned in Chapter 15 feature design greatly affects categorization accuracy and is arguably the most important part of any machine learning applica tion.
we’ve accuracy arguably important machine applica tion It machine error knowledge	 As we’ve mentioned in Chapter 15 feature design greatly affects categorization accuracy and is arguably the most important part of any machine learning applica tion  It would be most effective if you can combine machine learning error analysis and specific domain knowledge when designing features.
sense want features frequent Specificity features tend Clearly causes versus	 In that sense we want the features to be frequent  Specificity requires the features to be discriminative so naturally the features tend to be less frequent  Clearly this causes a tradeoff between frequent versus infrequent features.
infrequent analysis engineering task.	 Clearly this causes a tradeoff between frequent versus infrequent features  Particularly in our case of sentiment analysis feature engineering is a critical task.
Suppose distinguish positive The output log There M features xi	 Suppose we just wanted to distinguish positive from negative  The predictors features are represented as X and we can output a score based on the log probability ratio There are M features all together and each feature value xi is a real number.
You assume 1 1 X transformed form	 You may recall from Chapter 10 that in logistic regression we assume the log probability that Y  1 is a linear function of the features  This would allow us to also write pY  1  X as a transformed form of the linear function of the features.
If multiple multiple adapt binary logistic solve multilevel illustrated ure 18.	 If we have multiple categories or multiple levels we will adapt the binary logistic regression problem to solve this multilevel rating prediction as illustrated in Fig ure 18.
The idea binary ask predict j above.	 The idea is that we can introduce multiple binary classifiers in each case we ask the classifier to predict whether the rating is j or above  So when Yj  1 it means the rating is j or above.
This So k regular regression	 This is to make the notation more consistent with what we show in the ordinal logistic regression  So we now have k − 1 regular logistic regression classifiers each with its own set of parameters.
With ratings 6.	 With this approach we can now predict ratings as shown in Figure 18 6.
we’re going rating	5 we’re going to say yes the rating is k.
problem k − classifiers	 The second problem is that these k − 1 classifiers are not really independent.
fact benefits number parameters	 In fact this would allow us to have two benefits  One is to reduce the number of parameters significantly.
classifier distinct	 However each classifier still has a distinct predicted rating value.
score compared in.	 This score will then be compared with a set of trained α values to see which range the score is in.
decompose overall rating room location service overall ratings different aspects obtain reviewers’ hotel.	 What we want to do is to decompose this overall rating into ratings on different aspects such as value room location and service  If we can decompose the overall ratings into ratings on these different aspects we can obtain a much more detailed understanding of the reviewers’ opinions about the hotel.
In interpret different aspects accurately aspect	 In order to interpret the ratings on different aspects accurately we also need to know these aspect weights.
opinionbased opinion	 For example we can do opinionbased entity ranking or we can generate an aspectlevel opinion summary.
All enables personalized	 All this enables personalized product recommendation.
Later use unsupervised models segmentation we’re words frequencies different aspects	 Later we will see that we can also use unsupervised models to do the segmentation  In the second stage Latent Rating Regression we’re going to use these words and their frequencies in different aspects to predict the overall rating.
example cussion like high weight figure given 3.	 For example if in the dis cussion of location you see a word like amazing mentioned many times it will have a high weight in the figure it’s given a weight of 3.
10 observed overall d On This case generative embed variables.	10 is all the observed information rd the overall rating and ciw d  On the right side is all the latent hidden information that we hope to discover  This is a typical case of a generative model where we embed the interesting latent variables.
right latent information discover interesting latent variables Then set given	 On the right side is all the latent hidden information that we hope to discover  This is a typical case of a generative model where we embed the interesting latent variables  Then we set up a generative probability for the overall rating given the observed words.
document overall use denote w going d.	 Each review document is denoted as d and the overall ratings denoted by rd   We use ciw d to denote the count of word w in aspect segment i  The model is going to predict the rating based on d so we’re interested in the rating regression problem of prd  d.
generate overall we’re going α ues multivariate distribution we’re going compute average aspect mean overall rating Note	 This means when we generate our overall rating we’re going to first draw a set of α val ues from this multivariate Gaussian prior distribution  Once we get these α values we’re going to compute the weighted average of aspect ratings as the mean of the normal distribution to generate the overall rating rd   Note that β is indexed by both i and w.
Once we’re normal overall rd .	 Once we get these α values we’re going to compute the weighted average of aspect ratings as the mean of the normal distribution to generate the overall rating rd .
The sum zero sum words	 The sum would be zero for words that do not occur so we can simply take the sum of all the words in the vocabulary.
likelihood rating overall rating given particular α	3 The likelihood rating is the probability of generating this observed overall rating given this particular α value and some other parameters.
2010 solve LARA problem First segmentation different learn ratings	 2010  Earlier we talked about how to solve the LARA problem in two stages  First we did segmentation of different aspects and then used a latent regression model to learn the aspect ratings and weights.
To generated predict aspect rating combine overall unified model rating conditioned	 To predict the overall rating based on the generated text we first predict the aspect rating and then combine them with aspect weights to predict the overall rating  This gives us a unified generative model where we model both the generation of text and the overall rating conditioned on text.
Another compare reviews	 Another application is that you can compare different reviews on the same hotel.
reviewers The higher weights aspects people dimension don’t care compared weights model.	 This means these top ten reviewers tend to put a lot of weight on value as compared with other dimensions  The bottom ten refers to reviewers that have put higher weights on other aspects than value these are people who care about another dimension and don’t care so much about value at least compared to the top ten group  These ratios are computed based on the inferred weights from the model.
We hotels provides Looking average half aspects like room hotels	 We can see the average prices of hotels favored by the top ten reviewers are indeed much cheaper than those that are favored by the bottom ten  This provides some indirect way of validating the inferred weights  Looking at the average price in these three cities you can actually see the top ten group tends to have below average prices whereas the bottom half that cares about aspects like service or room condition tend to have hotels that have higher prices than average.
What average different reviewers.	 What you see is average weights along different dimensions by different groups of reviewers.
gave hotels aspect weight service.	 They gave the expensive hotels five stars with heavy aspect weight on service.
Text readily standard techniques need feature representation.	 Text sentiment analysis can be readily done by using just text categorization but standard techniques tend to be insufficient so we need to have an enriched feature representation.
multigrain topic model McDonald sentiment Jo Rating Analysis KDD al.	 topicsentiment mixture model Mei et al  2007a multigrain topic model Titov and McDonald 2008 and aspect and sentiment unification model Jo and Oh 2011  Techniques for Latent Aspect Rating Analysis are mainly covered in two KDD papers Wang et al.
model McDonald model 2011 Latent Aspect KDD papers al.	 2007a multigrain topic model Titov and McDonald 2008 and aspect and sentiment unification model Jo and Oh 2011  Techniques for Latent Aspect Rating Analysis are mainly covered in two KDD papers Wang et al.
chapter Introduction In big data structured data available	 Details of these techniques can be found in the references provided at the end of this chapter  19 1 Introduction In realworld big data applications we would have both structured data and un structured text data available to help us make predictions and support decision making.
seen The include	 In this figure we see that there are multiple sensors—including human sen sors—to report what we have seen in the real world in the form of data  The data include both nontext data and text data.
This general model applications focus special	 This big picture is actually very general and can serve as a model for many important applications of big data  Since the focus of the book is on text data it is useful to consider the special case of the loop shown in Figure 19.
	g.
A terms paradigmatic relations able kind useful feature problems discussed text discover interesting interaction chapter.	 A set of terms with paradigmatic relations may be a better indicator than any single term and sentiment tags that we may be able to generate based on text data are yet another kind of useful feature for some prediction problems  What has not been discussed is how we can jointly mine text and nontext data together to discover interesting knowledge that could not be otherwise discovered by using either one alone  This interaction is the topic of the chapter.
discussed jointly knowledge discovered alone.	 What has not been discussed is how we can jointly mine text and nontext data together to discover interesting knowledge that could not be otherwise discovered by using either one alone.
2 Text In discuss use nontext context enrich text techniques extension topics associated context g.	2 Contextual Text Mining In this section we discuss how to use nontext data as context to enrich topic analysis of text data  Such analysis techniques can be regarded as an extension of topic analysis to further reveal the correlation of topics and any associated context e g.
data based venues group published SIGIR published KDD	 Similarly we can partition the data based on the venues we can group all the papers published in SIGIR and compare them with those published in KDD or ACL.
tend cover authors	 For example the authors in Texas may tend to cover one particular aspect more than another while the authors in other locations may be different.
Once topics coverage rest That distribution topic corresponding word i.	 Once a view of topics and a topic coverage distribution have been chosen the rest of the generation process is exactly the same as in PLSA  That is we will use the topic coverage and selection distribution to sample a topic and then use the corresponding word distribution of the topic i.
In potentially different view different coverage Note deter mines view different topic This illustrated	 In such a generation process each word can be potentially generated using a different view and a different topic coverage distribution depending on the contexts chosen to direct the generation process  Note that the context that deter mines the choice of view of a topic can be different from the context chosen to decide the topic coverage  This is illustrated in Figure 19.
The seen CPLA topics context context deciding result discover single set topics reveal different contexts standard PLSA coverage document coverage associated	 The standard PLSA can easily be seen as a special case of CPLA when we have used just one single view of topics by using the whole collection as context and used each document ID as a context for deciding topic coverage  As a result what we can discover using PLSA is just one single set of topics characterizing the content in the text data with no way to reveal the difference of topics covered in different contexts  The standard PLSA can only reveal the coverage of topics in each document but cannot discover the topic coverage associated with a particular context.
table variations sets serve topics text cell enable discussion news	 This table is not only immediately useful for understanding the major topics and their variations in these two sets of news articles but can also serve as entry points to facilitate browsing into very specific topics in the text collection e g  each cell can be made clickable to enable a user to examine the relevant discussion in the news articles in detail.
trends topics time.	7 we show a visualization of the trends of topics over time.
topical trends useful revealing dynamics time enable topics contexts help discover potentially associated patterns Figure 8 spatiotemporal set articles	 Thus these topical trends not only are themselves useful for revealing the topics and their dynamics over time but also enable comparative analysis of topics across different contexts to help discover interesting patterns and potentially interesting events associated with the patterns  In Figure 19 8 we show spatiotemporal patterns of the coverage of the topic of government response in the same data set of blog articles about Hurricane Katrina.
coverage different event S victim	 These visualizations show the distribution of the coverage of the topic in different weeks of the event over the states in the U S  We see that initially the coverage is concentrated mostly in the victim states in the south but the topic gradually spread to other locations over time.
focusing	 The topic we are focusing on is about the retrieval models shown on the left  The goal is to analyze the impact of two events.
publication paper language approach retrieval To events different contexts CPLSA.	 The other event is the publication of a seminal paper in 1998 by Ponte and Croft 1998 in which the language modeling approach to information retrieval was introduced  The paper is also known to have made a high impact on information retrieval research  To understand the impact of these two events we can use time periods before and after an event as different contexts and apply CPLSA.
The paper known high	 The paper is also known to have made a high impact on information retrieval research.
subtopic retrieval tasks TREC years language modeling paper study probabilistic logic models 1998 modeling approaches parameter integral studies language IR.	 XML retrieval email retrieval and subtopic retrieval which are connected to some tasks introduced in TREC over the years  The results on the bottom show that before the language modeling paper was published in 1998 the study of retrieval models focused on probabilistic logic and Boolean models but after 1998 there was a clear focus on language modeling approaches and parameter estimation which is an integral part of studies of language models for IR.
Thus results help potentially reflected 4 Networks discuss text context.	 Thus these results can help potentially reveal the impact of an event as reflected in the text data  19 4 Topic Analysis with Social Networks as Context In this section we discuss how to mine text data with a social network as context.
new objective likelihood parameter 0 regularizer.	 As shown in Figure 19 11 the new modified objective function is a weighted sum of the standard PLSA likelihood function and a regularizer where the parameter λ ∈ 0 1 controls the weight on the regularizer.
generate communities correspond	12 show that PLSA is unable to generate the four communities that correspond to our intuition.
Indeed table.	 Indeed it is very easy to label them with the four communities as shown in the table.
However network paths subnetworks discover topics comparative 19.	 However text data can also be associated with edges in a network paths or even subnetworks to help discover topics or perform comparative analysis  19.
Context applications interested mining text understand As case mining understand	5 Topic Analysis with Time Series Context In many applications we may be interested in mining text data to understand events that happened in the real world  As a special case we may be interested in using text mining to understand a time series.
time topic appeared stream crash Similarly topics reported news presidential stream Prediction	 If the crashing time of the stocks corresponds to a time when a particular news topic suddenly appeared in the news stream there might be a potential relationship between the topic and the stock crash  Similarly one might also be interested in understanding what topics reported in the news stream were relevant for a presidential election and thus interested in finding topics in news stream that are correlated with the fluctuation of the Presidential Prediction Market which measures people’s opinions toward each presidential candidate.
That topic frequently text variable tends values.	 That is whenever the topic is mentioned frequently in the text stream the time series variable tends to have higher or lower values.
The iterative discovered topic time series induce	15  The idea of this approach is to do an iterative adjustment of topics discovered by topic models using time series to induce a prior.
15 topic generate time assess causally related correlated series For 4 correlated	15 we first take the text stream as input and apply regular topic modeling to generate a number of topics four shown here  Next we use the external time series to assess which topic is more causally related correlated with the external time series by using a causality measure such as Granger Test  For example in this figure topic 1 and topic 4 may be more correlated than topic 2 and topic 3.
word strongly series.	 However here we go further to improve them by zooming into the word level to further identify the words that are most strongly correlated with the time series.
figure shows potential potentially correlated subtopics negative.	 The figure shows a potential split of topic 1 into two such potentially more correlated subtopics one with w1 and w3 positive and one with w2 and w4 negative.
pure models maximizing topic coherence xaxis topics	16  Here we see that the pure topic models will be very good at maximizing topic coherence thus scoring high on the xaxis meaning the discovered topics will all be meaningful.
19 We F retaining terms statistically significant difference data.	  bpxt−p  19 1 We then perform an F test to evaluate if retaining or removing the lagged x terms would make a statistically significant difference in fitting the data.
results generated support.	 We now show some sample results generated by this approach to illustrate the applications that it can potentially support.
18 analyzing presidential series.	 In Figure 19 18 we see some additional results from analyzing presidential election time series.
	 19.
textbased data knowledge world 440 prediction optimizing decision making especially data widespread data context mining text data help annota	 We specifically focused on the discussion of textbased prediction which is generally very useful for big data applications that involve text data  Since textbased prediction can help us infer new knowledge about the world 440 Chapter 19 Joint Analysis of Text and Structured Data some of which can even go beyond what’s discussed in the content of text text based prediction is often very useful for optimizing our decision making especially when combined with other nontext data that are often also available and it has widespread applications  Nontext data can provide a context for mining text data while text data can also help interpret patterns discovered from nontext data such as pattern annota tion.
Interactive joint text traditional line enable “text dimension” hierarchical topic Zhang al.	 Interactive joint analysis of text and structured data can also be supported by combining the traditional On line Analytical Processing OLAP techniques with topic modeling to enable users to drilldown and rollup in the “text dimension” using a hierarchical topic structure as described in Zhang et al.
extreme attempt support 2.	 At the other extreme the system can attempt to provide task support directly to the user so as to minimize the user’s effort as illustrated in Figure 20 2.
challenges building goal 1 Operators databases support applications common defined language SQL.	 Although there are still many challenges in designing and building such a system it is an important goal to work on  20 1 Text Analysis Operators Relational databases are able to support many different applications via a set of common operators as defined in a query language such as SQL.
1 Text Relational databases able different common defined query Similarly identify common supporting analysis.	1 Text Analysis Operators Relational databases are able to support many different applications via a set of common operators as defined in a query language such as SQL  Similarly we may identify a common set of operators for supporting text analysis.
First need define pro cessing type sequence set V	 First we would need to define the data types that we may be interested in pro cessing  Naturally the most important data type is a TEXTOBJECT which can be defined as a sequence of words in a vocabulary set V .
order text capture interesting data structures ranked list	 Another possibility is TEXTOBJECTSEQUENCE where we care about order of the text objects TEXTOBJECTSEQUENCE can capture interesting data structures such as a ranked list of articles or sentences.
3 operators.	3 we show the following operators.
browsing tion Select.	 Querying browsing and recommenda tion can all be regarded as an instance of Select.
WEIGHTED TEXTOBJECTSET → Ranking text ranking produces sequence text order special case language	 WEIGHTED TEXTOBJECTSET → TEXTOBJECTSEQUENCE  The Ranking operator takes as input a weighted set of text objects that specifies the per spective of ranking and a set of text objects and it produces a sequence of text objects sorted in order as output  As a special case the WEIGHTED TEXTOBJECTSET can be a word distribution representing a query language model.
→ TOPICSET	  TEXTOBJECTSET → TOPICSET .
TOPICSET.	    TOPICSET.
	 As an example consider a news mining application.
extend search topic models enable text clustering mechanism text META.	8 where we extend a search engine to support various topic models which can further enable improved text categorization text summarization and text clustering  Indexes forward and inverted are a common storage mechanism for most text mining applications in META.
objects analysis analysis.	 If we make higherlevel functions that pass these objects to different analysis components we enable this unified view of text analysis.
3 META Unified 455 facilitates mentioned advanced text mining techniques run output searching run e g.	3 META as a Unified System 455 The index structure also facilitates the idea mentioned earlier advanced text mining techniques are run on a smaller relevant subset of the entire dataset  For this we can take the output from searching an inverted index and run e g.
analysis Split META’s algorithms run search	 Thus the analysis operators such as Select and Split take META’s index objects as input and return objects such as a sequence  The algorithms that run for Select could be a filter or search engine and Split could be metatopicsldamodel or some other clustering algorithm.
Bayesian Here statistics Chapter A Distribution From	 AAPPENDIX Bayesian Statistics Here we examine Bayesian statistics in more depth as a continuation of Chapter 2 and Chapter 17  A 1 Binomial Estimation and the Beta Distribution From section 2.
2 probability density function pdf But .	2 This is the probability density function pdf of the Beta distribution  But what is α  β αβ A 3 The .
4 Note sum support x reciprocal If 1 0 αβ xα−11 xβ−1dx	 A 4 Note that the sum over the support of x is the reciprocal of that constant  If we divide by it multiply by reciprocal we will get one as desired∫ 1 0 α  β αβ xα−11 − xβ−1dx  1.
prior Beta ended Beta distribution—this prior In reasoning prior	 We picked our prior to be the Beta distribution and our posterior distribution ended up also being a Beta distribution—this is because we picked the conjugate prior In any event the whole reasoning behind having a prior is so we can include some reasonable guess for the parameters before we even see any data.
1 set α β peak 0 8	1 the higher we set α and β the sharper the peak at 0 8 will be.
H T prior plays stronger θ small true estimate “large prior smooths estimation.	 Initially though when H  T is relatively low our prior plays a stronger role in the estimation of θ   As we all know a small number of flips will not give an accurate estimate of the true θ—we’d like to see what our estimate becomes as our number of flips approaches infinity or some “large enough” value  In this sense our prior also smooths our estimation.
In prior	 In this sense our prior also smooths our estimation.
prior turns α held A.	 If our prior turns out to be incorrect eventually the observed data will over shadow the pseudo counts from the hyperparameters anyway since α and β are held constant  A.
probabilities sum	 The word at index i would have a probability pi of occurring and the sum of all words’ probabilities would sum to one.
Like Dirichlet distribution vectors	 Like the multinomial the Dirichlet is a distribution over positive vectors that sum to one.
Here’s pdf In notation α binomial.	 Here’s the pdf In this notation we have pθ  α  θ is what we draw from the Dirichlet in the Beta it was the parameter to be used in the binomial.
multinomial.	 θ is what we draw from the Dirichlet in the Beta it was the parameter to be used in the binomial  Here it is the vector of parameters to be used in the multinomial.
	 .
equally likely mixture uneven mixture	 It is equally likely to get an even mixture uneven mixture or anywhere in between.
likely document discusses	 It’s most likely that a document mainly discusses a handful of topics while the rest are A.
.	 .
EM augment data assuming guessed values true algorithm	 Intuitively what EM does is to iteratively augment the data by “guessing” the values of the hidden variables and to reestimate the parameters by assuming that the guessed values are the true values  The EM algorithm is a hillclimbing approach thus it can only be guaranteed to reach a local maxima.
Data easily pw θF	 Complete Data 467 The right side of this equation is easily seen to be a function with pw  θF  as variables.
In example binary hidden z word indicate word “generated” model pw topic Let dij th word document We zij defined zij 0 otherwise.	 In our example we introduce a binary hidden variable z for each occurrence of a word w to indicate whether the word has been “generated” from the background model pw  C or the topic model pw  θF   Let dij be the j th word in document di  We have a corresponding variable zij defined as follows zij 1 if word dij is from background 0 otherwise.
pH θn Note left equation remains variable occur	 pH  X θn  Note that the left side of the equation remains the same as the variable H does not occur there.
B.	 B.
However certain general smoothing method sum according pw θ̂Q occur document d efficiently	 However when θ̂D is based on certain general smoothing method the computation would only involve a sum over those that both have a nonzero probability according to pw  θ̂Q and occur in document d  Such a sum can be computed much more efficiently with an inverted index.
2 α parameter note α smoothing formula.	2 where α is a parameter that needs to be set empirically  Please note that this α is different from αd in the smoothing formula.
.	 where F  d1     .
set eter indicates feedback needs set	      dk is the set of feedback documents and λ is yet another param eter that indicates the amount of “background noise” in the feedback documents and that needs to be set empirically.
