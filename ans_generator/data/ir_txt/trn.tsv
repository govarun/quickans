ChengXiang Zhai Massung Analysis A Practical Retrieval Recent language text web pages news scientific emails enterprise media blog reviews increasing tools vast efficiently data generated generated rich content.	ChengXiang Zhai Sean Massung Text Data Management and Analysis A Practical Introduction to Information Retrieval Recent years have seen a dramatic growth of natural language text data including web pages news articles scientific literature emails enterprise documents and social media such as blog articles forum posts product reviews and tweets  This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently  Unlike data generated by a computer system or sensors text data are usually generated directly by humans and capture semantically rich content.
increasing demand manage vast amounts data effectively efficiently Unlike generated usually humans semantically content.	 This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently  Unlike data generated by a computer system or sensors text data are usually generated directly by humans and capture semantically rich content.
data directly humans rich content As text data valuable opinions preferences addition knowledge text In contrast conform easy handle text explicit structure	 Unlike data generated by a computer system or sensors text data are usually generated directly by humans and capture semantically rich content  As such text data are especially valuable for discovering knowledge about human opinions and preferences in addition to many other kinds of knowledge that we encode in text  In contrast to structured data which conform to welldefined schemas thus are relatively easy for computers to handle text has less explicit structure requiring computer processing toward understanding of the content encoded in text.
especially knowledge encode contrast conform easy handle text structure current natural language point language text range statistical heuristic management past decades.	 As such text data are especially valuable for discovering knowledge about human opinions and preferences in addition to many other kinds of knowledge that we encode in text  In contrast to structured data which conform to welldefined schemas thus are relatively easy for computers to handle text has less explicit structure requiring computer processing toward understanding of the content encoded in text  The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades.
In contrast conform welldefined schemas relatively easy text explicit structure requiring content text The current technology natural point enable understand natural statistical analysis data past decades.	 In contrast to structured data which conform to welldefined schemas thus are relatively easy for computers to handle text has less explicit structure requiring computer processing toward understanding of the content encoded in text  The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades.
technology natural natural language text management data usually topic.	 The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades  They are usually very robust and can be applied to analyze and manage text data in any natural language and about any topic.
usually robust applied natural introduction emphasis covering knowledge required build	 They are usually very robust and can be applied to analyze and manage text data in any natural language and about any topic  This book provides a systematic introduction to many of these approaches with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems.
book provides introduction approaches covering knowledge skills required build Because better effective information text systems	 This book provides a systematic introduction to many of these approaches with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems  Because humans can understand natural languages far better than computers can effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans.
Depending information distinguish kinds information The retrieval assist relevant needed solving specific application problem effectively turning text data relevant data	 Depending on how a text information system collaborates with humans we distinguish two kinds of text information systems  The first is information retrieval systems which include search engines and recommender systems they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem thus effectively turning big raw text data into much smaller relevant text data that can be more easily processed by humans.
text data knowledge directly decision providing direct support users.	 The second is text mining application systems they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making thus providing more direct task support for users.
Preface growth leverage computational raw knowledge support especially true decision application domains medicine security safety learning discovery intelligence “micro telescope allows far away “big data enable perception useful knowledge data help improve chosen	 Preface The growth of “big data” created unprecedented opportunities to leverage computational and statistical approaches to turn raw data into actionable knowledge that can support various application tasks  This is especially true for the optimization of decision making in virtually all application domains such as health and medicine security and safety learning and education scientific discovery and business intelligence  Just as a microscope enables us to see things in the “micro world” and a telescope allows us to see things far away one can imagine a “big data scope” would enable us to extend our perception ability to “see” useful hidden information and knowledge buried in the data which can help make predictions and improve the optimality of a chosen decision.
This virtually health medicine education business	 This is especially true for the optimization of decision making in virtually all application domains such as health and medicine security and safety learning and education scientific discovery and business intelligence.
data include data form natural text	 Text data include all data in the form of natural language text e g.
English text pages news scientific emails government kinds data role	g  English text or Chinese text all the web pages social media data such as tweets news scientific literature emails government documents and many other kinds of enterprise data  Text data play an essential role in our lives.
Text	 Text data play an essential role in our lives.
explosive text data makes impossible difficult people consume manner Thus intelligent help data quickly accurately time need growth	 The explosive growth of text data makes it impossible or at least very difficult for people to consume all the relevant text data in a timely manner  Thus there is an urgent need for developing intelligent information retrieval systems to help people manage the text data and get access to the needed relevant information quickly and accurately at any time  This need is a major reason behind the recent growth of the web search engine industry.
This need recent web engine fact humans communication purposes rich valuable information	 This need is a major reason behind the recent growth of the web search engine industry  Due to the fact that text data are produced by humans for communication purposes they are generally rich in semantic content and often contain valuable knowledge information opinions and preferences of people.
fact data produced communication generally rich contain opinions	 Due to the fact that text data are produced by humans for communication purposes they are generally rich in semantic content and often contain valuable knowledge information opinions and preferences of people.
“big great discover knowledge applications especially knowledge human text data.	 Thus as a special kind of “big data” text data offer a great opportunity to discover various kinds of knowledge useful for many applications especially knowledge about human opinions and preferences which is often directly expressed in text data.
For tap opinionated data product reviews discussions text	 For example it is now the norm for people to tap into opinionated text data such as product reviews forum discussions and social media text to obtain opinions.
intelligent software discover knowledge optimizing complete efficiently While supporting text mining search supporting significant progress area recent specialized text mining The subtitle major topics text	 Once again due to the overwhelming amount of information people need intelligent software tools to help discover relevant knowledge for optimizing decisions or helping them complete their tasks more efficiently  While the technology for supporting text mining is not yet as mature as search engines for supporting text access significant progress has been made in this area in recent years and specialized text mining tools have now been widely used in many application domains  The subtitle of this book suggests that we cover two major topics information retrieval and text mining.
text mining mature text access area text tools widely The book suggests major text mining roughly correspond needed build application systems i.	 While the technology for supporting text mining is not yet as mature as search engines for supporting text access significant progress has been made in this area in recent years and specialized text mining tools have now been widely used in many application domains  The subtitle of this book suggests that we cover two major topics information retrieval and text mining  These two topics roughly correspond to the techniques needed to build the two types of application systems discussed above i.
book major retrieval	 The subtitle of this book suggests that we cover two major topics information retrieval and text mining.
These topics roughly correspond techniques build application systems i.	 These two topics roughly correspond to the techniques needed to build the two types of application systems discussed above i.
e analytics artificial structure application use techniques topic areas contrast conform welldefined schemas easy computers structure development intelligent discussed requires processing content encoded text.	e  search engines and text analytics systems although the separation of the two is mostly artificial and only meant to help provide a highlevel structure for the book and a sophisticated application system likely would use many techniques from both topic areas  In contrast to structured data which conform to welldefined schemas and are thus relatively easy for computers to handle text has less explicit structure so the development of intelligent software tools discussed above requires computer processing to understand the content encoded in text.
search engines text systems meant provide highlevel structure sophisticated In contrast welldefined schemas relatively text explicit intelligent software requires understand content	 search engines and text analytics systems although the separation of the two is mostly artificial and only meant to help provide a highlevel structure for the book and a sophisticated application system likely would use many techniques from both topic areas  In contrast to structured data which conform to welldefined schemas and are thus relatively easy for computers to handle text has less explicit structure so the development of intelligent software tools discussed above requires computer processing to understand the content encoded in text.
This materials course e Text Information Massive Online Courses Retrieval Search Engines” Mining taught author Coursera	 This book is primarily based on the materials that the authors have used for teaching a course on the topic of text data management and analysis i e  CS410 Text Information Systems at the University of Illinois at Urbana–Champaign as well as the two Massive Open Online Courses MOOCs on “Text Retrieval and Search Engines” and “Text Mining and Analytics” taught by the first author on Coursera in 2015.
	e.
directly match similar	 Most of the materials in the book directly match those of these two MOOCs with also similar structures of topics.
As book main reference MOOCs.	 As such the book can be used as a main reference book for any of these two MOOCs.
2010 et al.	 2010 Search Engines Information Retrieval in Practice by Croft et al.
	 2008.
contrast Mining far mature actually question.	 In contrast with IR Text Mining TM is far from mature and is actually still in its infancy  Indeed how to define TM precisely remains an open question.
open question.	 Indeed how to define TM precisely remains an open question.
As textbook major representative	 As a textbook on TM our book provides a basic introduction to the major representative techniques for TM.
	 the META toolkit available at httpsmetatoolkit.
Many exercises book toolkit readers practical skills book solve This consists Part overview content background later.	 Many exercises in the book are also designed based on this toolkit to help readers acquire practical skills of experimenting with the learned techniques from the book and applying them to solve realworld application problems  This book consists of four parts  Part I provides an overview of the content covered in the book and some background knowledge needed to understand the chapters later.
parts Part I needed understand chapters II contain Text Access techniques called	 This book consists of four parts  Part I provides an overview of the content covered in the book and some background knowledge needed to understand the chapters later  Parts II and III contain the major content of the book and cover a wide range of techniques in IR called Text Data Access techniques and techniques in TM called Text Data Analysis techniques respectively.
Part provides overview background later Parts II major called Data Access techniques called Text Data techniques	 Part I provides an overview of the content covered in the book and some background knowledge needed to understand the chapters later  Parts II and III contain the major content of the book and cover a wide range of techniques in IR called Text Data Access techniques and techniques in TM called Text Data Analysis techniques respectively.
contain content range Data techniques techniques called respectively.	 Parts II and III contain the major content of the book and cover a wide range of techniques in IR called Text Data Access techniques and techniques in TM called Text Data Analysis techniques respectively.
Part summarizes techniques combined provide support text	 Part IV summarizes the book with a unified framework for text management and analysis where many techniques of IR and TM can be combined to provide more advanced support for text data access and analysis with humans in the loop to control the workflow.
experiment algorithms textbook undergraduate course reference course aspects practitioners range practical managing analyzing build interesting	 META can be used by anyone to easily experiment with algorithms and build applications but modifying it or extending it would require at least some basic knowledge of C programming  The book can be used as a textbook for an upperlevel undergraduate course on information retrieval and text mining or a reference book for a graduate course to cover practical aspects of information retrieval and text mining  It should also be useful to practitioners in industry to help them acquire a wide range of practical techniques for managing and analyzing text data that they can use immediately to build various interesting realworld applications.
textbook upperlevel undergraduate text book course cover information text	 The book can be used as a textbook for an upperlevel undergraduate course on information retrieval and text mining or a reference book for a graduate course to cover practical aspects of information retrieval and text mining.
useful practitioners acquire wide range techniques managing analyzing text data build interesting decades online information According California “.	 It should also be useful to practitioners in industry to help them acquire a wide range of practical techniques for managing and analyzing text data that they can use immediately to build various interesting realworld applications  Introduction In the last two decades we have experienced an explosive growth of online information  According to a study done at University of California Berkeley back in 2003 “.
decades information.	 Introduction In the last two decades we have experienced an explosive growth of online information.
According University Berkeley	 According to a study done at University of California Berkeley back in 2003 “.
	.
.	.
world produces 1 unique 250 child kinds comprise	the world produces between 1 and 2 exabytes 1018 petabytes of unique information per year which is roughly 250 megabytes for every man woman and child on earth  Printed documents of all kinds comprise only .
Printed documents comprise 03 et al.	 Printed documents of all kinds comprise only  03 of the total ” Lyman et al.
” et 2003 textual	03 of the total ” Lyman et al  2003 A large amount of online information is textual information i.
A large online information	 2003 A large amount of online information is textual information i e  in natural language text.
e text For example Berkeley cited .	e  in natural language text  For example according to the Berkeley study cited above “Newspapers represent 25 terabytes annually magazines represent 10 terabytes .
For example Berkeley study terabytes annually	 For example according to the Berkeley study cited above “Newspapers represent 25 terabytes annually magazines represent 10 terabytes .
office represent terabytes.	     office documents represent 195 terabytes.
office documents	   office documents represent 195 terabytes.
billion sent year terabytes ” Of articles tweets literature documents etc.	 It is estimated that 610 billion emails are sent each year representing 11000 terabytes ” Of course there are also blog articles forum posts tweets scientific literature government documents etc.
Roe 2012 trillion sent IDC report digital 300 trillion While general information especially important arguably following	 Roe 2012 updates the email count from 610 billion emails in 2003 to 107 trillion emails sent in 2010  According to a recent IDC report report Gantz  Reinsel 2012 from 2005 to 2020 the digital universe will grow by a factor of 300 from 130 exabytes to 40000 exabytes or 40 trillion gigabytes  While in general all kinds of online information are useful textual information plays an especially important role and is arguably the most useful kind of information for the following reasons.
recent IDC report 2012 gigabytes general online information information especially role arguably kind information natural way encoding	 According to a recent IDC report report Gantz  Reinsel 2012 from 2005 to 2020 the digital universe will grow by a factor of 300 from 130 exabytes to 40000 exabytes or 40 trillion gigabytes  While in general all kinds of online information are useful textual information plays an especially important role and is arguably the most useful kind of information for the following reasons  Text natural language is the most natural way of encoding human knowledge.
plays especially information following	 While in general all kinds of online information are useful textual information plays an especially important role and is arguably the most useful kind of information for the following reasons.
result form example exclusively manuals detailed explanations operate	 As a result most human knowledge is encoded in the form of text data  For example scientific knowledge almost exclusively exists in scientific literature while technical manuals contain detailed explanations of how to operate devices.
knowledge exclusively Text common type	 For example scientific knowledge almost exclusively exists in scientific literature while technical manuals contain detailed explanations of how to operate devices  Text is by far the most common type of information encountered by people.
Text far information people.	 Text is by far the most common type of information encountered by people.
produces text form expressive sense images Indeed image engines supported matching companion “matching”	 Indeed most of the information a person produces and consumes daily is in text form  4 Chapter 1 Introduction Text is the most expressive form of information in the sense that it can be used to describe other media such as video or images  Indeed image search engines such as those supported by Google and Bing often rely on matching companion text of images to retrieve “matching” images to a user’s keyword query.
Chapter expressive media images Indeed image search engines Google Bing matching companion text retrieve keyword query.	 4 Chapter 1 Introduction Text is the most expressive form of information in the sense that it can be used to describe other media such as video or images  Indeed image search engines such as those supported by Google and Bing often rely on matching companion text of images to retrieve “matching” images to a user’s keyword query.
matching companion images user’s text created software related people exploit text	 Indeed image search engines such as those supported by Google and Bing often rely on matching companion text of images to retrieve “matching” images to a user’s keyword query  The explosive growth of online text information has created a strong demand for intelligent software tools to provide the following two related services to help people manage and exploit big text data  Text Retrieval.
Retrieval.	 Text Retrieval.
The growth data people	 The growth of text data makes it impossible for people to consume the data in a timely manner.
encode generally leading	 Since text data encode much of our accumulated knowledge they generally cannot be discarded leading to e g.
large individual’s capacity skim	g  the accumulation of a large amount of literature data which is now beyond any individual’s capacity to even skim over.
accumulation large over.	 the accumulation of a large amount of literature data which is now beyond any individual’s capacity to even skim over.
growth new created	 The rapid growth of online text information also means that no one can possibly digest all the new information created on a daily basis.
text access quickly leading web Indeed search Bing essential daily life	 Thus there is an urgent need for developing intelligent text retrieval systems to help people get access to the needed relevant information quickly and accurately leading to the recent growth of the web search industry  Indeed web search engines like Google and Bing are now an essential part of our daily life serving millions of queries daily.
Indeed search Bing daily life millions In general search useful relatively e.	 Indeed web search engines like Google and Bing are now an essential part of our daily life serving millions of queries daily  In general search engines are useful anywhere there is a relatively large amount of text data e.
g enterprise specific Text	g  desktop search enterprise search or literature search in a specific domain such as PubMed  Text Mining.
search literature specific	 desktop search enterprise search or literature search in a specific domain such as PubMed.
communication purposes generally rich people discovering knowledge especially knowledge human opin directly expressed text	 Text Mining  Due to the fact that text data are produced by humans for communication purposes they are generally rich in semantic content and often contain valuable knowledge information opinions and preferences of people  As such they offer great opportunity for discovering various kinds of knowledge useful for many applications especially knowledge about human opin ions and preferences which is often directly expressed in text data.
offer great knowledge useful applications especially human preferences	 As such they offer great opportunity for discovering various kinds of knowledge useful for many applications especially knowledge about human opin ions and preferences which is often directly expressed in text data.
opinionated data forum media interesting optimize decisionmaking product choosing overwhelming people need software knowledge decisions help efficiently mining mature search access significant recent text mining tools widely application	 For example it is now the norm for people to tap into opinionated text data such as product reviews forum discussions and social media text to obtain opinions about topics interesting to them and optimize various decisionmaking tasks such as purchasing a product or choosing a service  Once again due to the overwhelming amount of information people need intelligent software tools to help discover relevant knowledge to optimize decisions or help them complete their tasks more efficiently  While the technology for supporting text mining is not yet as mature as search engines for supporting text access significant progress has been made in this area in recent years and specialized text mining tools have now been widely used in many application domains.
While supporting text mature engines progress area recent years mining In contrast structured schemas easy handle text software content encoded text The current processing enable understand involved wide range analysis data developed past	 While the technology for supporting text mining is not yet as mature as search engines for supporting text access significant progress has been made in this area in recent years and specialized text mining tools have now been widely used in many application domains  In contrast to structured data which conform to welldefined schemas and are thus relatively easy for computers to handle text has less explicit structure so the development of intelligent software tools discussed above requires computer processing to understand the content encoded in text  The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text a main reason why humans often should be involved in the loop but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades.
technology enable precisely understand main reason involved loop wide approaches text data developed past	 The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text a main reason why humans often should be involved in the loop but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades.
They analyze natural language book introduction skills useful information systems.	 They are usually very robust and can be applied to analyze and manage text data in any natural language and about any topic  This book intends to provide a systematic introduction to many of these approaches with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems.
This provide introduction emphasis knowledge build useful The e.	 This book intends to provide a systematic introduction to many of these approaches with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems  The two services discussed above i e.
text retrieval natural analyzing shown 1.	 text retrieval and text mining conceptually correspond to the two natural steps in the process of analyzing any “big text data” as shown in Figure 1.
	1.
raw data large application small data relevant data particular application decisionmaking problem unnecessary large amounts text data This raw big smaller relevant text help	 While the raw text data may be large a specific application often requires only a small amount of most relevant text data thus conceptually the very first step in any application should be to identify the relevant text data to a particular application or decisionmaking problem and avoid the unnecessary processing of large amounts of nonrelevant text data  This first step of converting the raw big text data into much smaller but highly relevant text data is often accomplished by techniques of text retrieval with help from users e.
step smaller techniques help e.	 This first step of converting the raw big text data into much smaller but highly relevant text data is often accomplished by techniques of text retrieval with help from users e.
g collect data decision	g  users may use multiple queries to collect all the relevant text data for a decision problem.
users decision problem.	 users may use multiple queries to collect all the relevant text data for a decision problem.
Once small set analyze data users knowledge data.	 6 Chapter 1 Introduction Once we obtain a small set of most relevant text data we would need to further analyze the text data to help users digest the content and knowledge in the text data.
text discover knowledge patterns text data support task assessing knowledge original raw data discovered knowledge verify useful access decisionsupport knowledge	 This is the text mining step where the goal is to further discover knowledge and patterns from text data so as to support a user’s task  Furthermore due to the need for assessing trustworthiness of any discovered knowledge users generally have a need to go back to the original raw text data to obtain appropriate context for interpreting the discovered knowledge and verify the trustworthiness of the knowledge hence a search engine system which is primarily useful for text access also has to be available in any textbased decisionsupport system for supporting knowledge provenance.
need assessing discovered users generally raw data interpreting discovered knowledge engine supporting steps conceptually interleaved fullfledged intelligent	 Furthermore due to the need for assessing trustworthiness of any discovered knowledge users generally have a need to go back to the original raw text data to obtain appropriate context for interpreting the discovered knowledge and verify the trustworthiness of the knowledge hence a search engine system which is primarily useful for text access also has to be available in any textbased decisionsupport system for supporting knowledge provenance  The two steps are thus conceptually interleaved and a fullfledged intelligent text information system must integrate both in a unified framework.
steps interleaved fullfledged text unified	 The two steps are thus conceptually interleaved and a fullfledged intelligent text information system must integrate both in a unified framework.
pointing “big data” text data different kinds produced directly data g.	 It is worth pointing out that put in the context of “big data” text data is very different from other kinds of data because it is generally produced directly by humans and often also meant to be consumed by humans as well  In contrast other data tend to be machinegenerated data e g.
In g data collected sensors.	 In contrast other data tend to be machinegenerated data e g  data collected by using all kinds of physical sensors.
g collected kinds sensors Since humans understand data far computers involvement process text absolutely crucial big applications optimally divide machines collaboration “combined intelligence” general data analysis.	g  data collected by using all kinds of physical sensors  Since humans can understand text data far better than computers can involvement of humans in the process of mining and analyzing text data is absolutely crucial much more necessary than in other big data applications and how to optimally divide the work between humans and machines so as to optimize the collaboration between humans and machines and maximize their “combined intelligence” with minimum human effort is a general challenge in all applications of text data management and analysis.
sensors far involvement process mining text necessary applications optimally divide machines machines maximize intelligence” minimum general challenge applications text analysis.	 data collected by using all kinds of physical sensors  Since humans can understand text data far better than computers can involvement of humans in the process of mining and analyzing text data is absolutely crucial much more necessary than in other big data applications and how to optimally divide the work between humans and machines so as to optimize the collaboration between humans and machines and maximize their “combined intelligence” with minimum human effort is a general challenge in all applications of text data management and analysis.
The regarded assist humans information retrieval systems large application problem effectively raw text data smaller text processed text data useful task completion support With steps Figure 1 followed unified information system.	 The two steps discussed above can be regarded as two different ways for a text information system to assist humans information retrieval systems assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem thus effectively turning big raw text data into much smaller relevant text data that can be more easily processed by humans while text mining application systems can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making thus providing more direct task support for users  With this view we partition the techniques covered in the book into two parts to match the two steps shown in Figure 1 1 which are then followed by one chapter to discuss how all the techniques may be integrated in a unified text information system.
With view techniques covered match shown	 With this view we partition the techniques covered in the book into two parts to match the two steps shown in Figure 1.
1 integrated	1 which are then followed by one chapter to discuss how all the techniques may be integrated in a unified text information system.
book provide major concepts information text mining practical viewpoint includes companion toolkit META help readers learn	 The book attempts to provide a complete coverage of all the major concepts techniques and ideas in information retrieval and text data mining from a practical viewpoint  It includes many handson exercises designed with a companion software toolkit META to help readers learn how to apply techniques of information 1.
companion software toolkit help readers information 1 text mining realworld text data experiment improve interesting This book textbook graduates library scientists reference book practitioners working relevant managing data.	 It includes many handson exercises designed with a companion software toolkit META to help readers learn how to apply techniques of information 1 1 Functions of Text Information Systems 7 retrieval and text mining to realworld text data and learn how to experiment with and improve some of the algorithms for interesting application tasks  This book can be used as a textbook for computer science undergraduates and graduates library and information scientists or as a reference book for practitioners working on relevant application problems in analyzing and managing text data.
1 Information 7 text mining learn experiment tasks.	1 Functions of Text Information Systems 7 retrieval and text mining to realworld text data and learn how to experiment with and improve some of the algorithms for interesting application tasks.
This textbook scientists reference relevant problems data	 This book can be used as a textbook for computer science undergraduates and graduates library and information scientists or as a reference book for practitioners working on relevant application problems in analyzing and managing text data  1.
perspective text TIS capabilities illustrated 1 2	1 Functions of Text Information Systems From a user’s perspective a text information system TIS can offer three distinct but related capabilities as illustrated in Figure 1 2  Information Access.
	2.
Information This capability user useful information	 Information Access  This capability gives a user access to the useful information when the user needs it.
This capability gives access needs	 This capability gives a user access to the useful information when the user needs it.
Since main purpose relevant minimum sufficient relevant items	 Since the main purpose of Information Access is to connect a user with relevant information a TIS offering this capability generally only does minimum analysis of text data sufficient for matching relevant information with a user’s information need and the original inforation items e g.
	g.
web delivered form delivered text generally need information items information Text	 web pages are often delivered to the user in their original form though summaries of the delivered items are often provided  From the perspective of text analysis a user would generally need to read the information items to further digest and exploit the delivered information  Knowledge Acquisition Text Analysis.
This enables text obtain large TIS data patterns referred	 This capability enables a user to acquire useful knowledge encoded in the text data that is not easy for a user to obtain without synthesizing and analyzing a relatively large portion of the data  In this case a TIS can analyze a large amount of text data to discover interesting patterns buried in text  A TIS with the capability of knowledge acquisition can be referred to as an analysis engine.
case TIS A TIS acquisition For example search engine reviews user major positive product compare multiple similar	 In this case a TIS can analyze a large amount of text data to discover interesting patterns buried in text  A TIS with the capability of knowledge acquisition can be referred to as an analysis engine  For example while a search engine can return relevant reviews of a product to a user an analysis engine would enable a user to obtain directly the major positive or negative opinions about the product and to compare opinions about multiple similar products.
A referred engine.	 A TIS with the capability of knowledge acquisition can be referred to as an analysis engine.
search return relevant analysis enable directly product opinions multiple TIS acquisition generally text synthesize information text new information knowledge	 For example while a search engine can return relevant reviews of a product to a user an analysis engine would enable a user to obtain directly the major positive or negative opinions about the product and to compare opinions about multiple similar products  A TIS offering the capability of knowledge acquisition generally would have to analyze text data in more detail and synthesize information from multiple text documents discover interesting patterns and create new information or knowledge  Text Organization.
data multiple text documents discover interesting patterns new knowledge.	 A TIS offering the capability of knowledge acquisition generally would have to analyze text data in more detail and synthesize information from multiple text documents discover interesting patterns and create new information or knowledge.
annotate text documents topical structures connected space following structures.	 This capability enables a TIS to annotate a collection of text documents with meaningful topical structures so that scattered information can be connected and a user can navigate in the information space by following the structures.
text directly users general facilitating access knowledge capability plays role TIS access example added allow browse following	 While such structures may be regarded as “knowledge” acquired from the text data and thus can be directly useful to users in general they are often only useful for facilitating either information access or knowledge acquisition or both  In this sense the capability of text organization plays a supporting role in a TIS to make information access and knowledge acquisition more effective  For example the added structures can allow a user to search with constraints on structures or browse by following structures.
sense capability text supporting information knowledge For added allow constraints structures.	 In this sense the capability of text organization plays a supporting role in a TIS to make information access and knowledge acquisition more effective  For example the added structures can allow a user to search with constraints on structures or browse by following structures.
For user search constraints structures leveraged perform detailed consideration	 For example the added structures can allow a user to search with constraints on structures or browse by following structures  The structures can also be leveraged to perform detailed analysis with consideration of constraints on structures.
leveraged perform consideration constraints structures pull mode user initiative “pull” useful waits user respond information.	 The structures can also be leveraged to perform detailed analysis with consideration of constraints on structures  Information access can be further classified into two modes pull and push  In the pull mode the user takes initiative to “pull” the useful information out from the system in this case the system plays a passive role and waits for a user to make a request to which the system would then respond with relevant information.
In mode takes initiative useful case user respond	 In the pull mode the user takes initiative to “pull” the useful information out from the system in this case the system plays a passive role and waits for a user to make a request to which the system would then respond with relevant information.
e	e  a temporary information need e g.
need	 a temporary information need e g.
opinions example search engine Google serves user mode.	 an immediate need for opinions about a product  For example a search engine like Google generally serves a user in pull mode.
example search pull	 For example a search engine like Google generally serves a user in pull mode.
In takes initiative “push” user information useful user relatively information	 In the push mode the system takes initiative to “push” recommend to the user an information item that the system believes is useful to the user  The push mode often works well when the user has a relatively stable information need e g.
The works information need e know “in interests user	 The push mode often works well when the user has a relatively stable information need e g  hobby of a person in such a case a system can know “in advance” a user’s preferences and interests making it feasible to recommend information to a user without having the user to take the initiative.
g case advance” user’s making feasible user book.	g  hobby of a person in such a case a system can know “in advance” a user’s preferences and interests making it feasible to recommend information to a user without having the user to take the initiative  We cover both modes of information access in this book.
person case making feasible recommend user having	 hobby of a person in such a case a system can know “in advance” a user’s preferences and interests making it feasible to recommend information to a user without having the user to take the initiative.
The obtain relevant querying querying user information query query In link information	 The pull mode further consists of two complementary ways for a user to obtain relevant information querying and browsing  In the case of querying the user specifies the information need with a keyword query and the system would take the query as input and return documents that are estimated to be relevant to the query  In the case of browsing the user simply navigates along structures that link information items together and progressively reaches relevant information.
In querying information query estimated relevant case browsing simply link progressively reaches information.	 In the case of querying the user specifies the information need with a keyword query and the system would take the query as input and return documents that are estimated to be relevant to the query  In the case of browsing the user simply navigates along structures that link information items together and progressively reaches relevant information.
simply information progressively navigate relevant documents it’s clear browsing querying interleaved naturally.	 In the case of browsing the user simply navigates along structures that link information items together and progressively reaches relevant information  Since querying can also be regarded as a way to navigate in one step into a set of relevant documents it’s clear that browsing and querying can be interleaved naturally.
regarded way navigate step relevant documents browsing naturally user search querying	 Since querying can also be regarded as a way to navigate in one step into a set of relevant documents it’s clear that browsing and querying can be interleaved naturally  Indeed a user of a web search engine often interleaves querying and browsing.
web search engine interleaves	 Indeed a user of a web search engine often interleaves querying and browsing.
data achieved process text mining defined mining useful community natural developed communities adopt different perspective problem.	 Knowledge acquisition from text data is often achieved through the process of text mining which can be defined as mining text data to discover useful knowledge  Both the data mining community and the natural language processing NLP community have developed methods for text mining although the two communities tend to adopt slightly different perspective on the problem.
mining text kind i.	 From a data mining perspective we may view text mining as mining a special kind of data i.
	e.
text goals data goal naturally regarded discover extract latent topics topical trends	 text  Following the general goals of data mining the goal of text mining would naturally be regarded as to discover and extract interesting patterns in text data which can include latent topics topical trends or outliers.
From NLP perspective text convert text form knowledge represen tation inferences based	 From an NLP perspective text mining can be regarded as to partially understand natural language text convert text into some form of knowledge represen tation and make limited inferences based on the extracted knowledge.
g In course text mining applications pattern i.	g  who met with whom  In practice of course any text mining applications would likely involve both pattern discovery i.
In course involve pattern discovery	 who met with whom  In practice of course any text mining applications would likely involve both pattern discovery i.
applications pattern i.	 In practice of course any text mining applications would likely involve both pattern discovery i.
e view extraction e.	e  data mining view and information extraction i e.
data	 data mining view and information extraction i.
e NLP view representation enables 10 1 semantically meaningful patterns stringlevel representations	e  NLP view with information extraction serving as enriching the semantic representation of text which enables pattern 10 Chapter 1 Introduction finding algorithms to generate semantically more meaningful patterns than directly working on word or stringlevel representations of text.
serving text enables pattern 10 meaningful word representations text.	 NLP view with information extraction serving as enriching the semantic representation of text which enables pattern 10 Chapter 1 Introduction finding algorithms to generate semantically more meaningful patterns than directly working on word or stringlevel representations of text.
emphasis kinds text data mining view book information	 Due to our emphasis on covering general and robust techniques that can work for all kinds of text data without much manual effort we mostly adopt the data mining view in this book since information extraction techniques tend to be more languagespecific and generally require much manual effort.
stress information extraction essential text information attempts support deeper Applications text mining classified direct applications directly necessarily indirectly help information access.	 However it is important to stress that information extraction is an essential component in any text information system that attempts to support deeper knowledge discovery or semantic analysis  Applications of text mining can be classified as either direct applications where the discovered knowledge would be directly consumed by users or indirect applications where the discovered knowledge isn’t necessarily directly useful to a user but can indirectly help a user through better support of information access.
variations cover identify type knowledge TIS discover set subtopics buried serve data.	 However due to the wide range of variations of the “knowledge” it is impossible to use a small number of categories to cover all the variations  Nevertheless we can still identify a few common categories which we cover in this book  For example one type of knowledge that a TIS can discover is a set of topics or subtopics buried in text data which can serve as a concise summary of the major content in the text data.
example type discover set buried text data serve summary data Another type knowledge opinions topic	 For example one type of knowledge that a TIS can discover is a set of topics or subtopics buried in text data which can serve as a concise summary of the major content in the text data  Another type of knowledge that can be acquired from opinionated text is the overall sentiment polarity of opinions about a topic  1.
Another type	 Another type of knowledge that can be acquired from opinionated text is the overall sentiment polarity of opinions about a topic.
2 text consist 1 3.	 1 2 Conceptual Framework for Text Information Systems Conceptually a text information system may consist of several modules as illustrated in Figure 1 3.
3 module processing techniques raw effectively matched user’s query engine processed general	3  First there is a need for a module of content analysis based on natural language processing techniques  This module allows a TIS to transform raw text data into more meaningful representations that can be more effectively matched with a user’s query in the case of a search engine and more effectively processed in general in text analysis.
First need analysis based	 First there is a need for a module of content analysis based on natural language processing techniques.
This transform text meaningful representations user’s query general text analysis techniques rely statistical machine learning linguistic knowledge depth understanding robust limited	 This module allows a TIS to transform raw text data into more meaningful representations that can be more effectively matched with a user’s query in the case of a search engine and more effectively processed in general in text analysis  Current NLP techniques mostly rely on statistical machine learning enhanced with limited linguistic knowledge with variable depth of understanding of text data shallow techniques are robust but deeper semantic analysis is only feasible for very limited domains  Some TIS capabilities e.
Some e.	 Some TIS capabilities e.
summarization deeper	g  summarization tend to require deeper NLP than others e.
tend require search.	 summarization tend to require deeper NLP than others e g  search.
Most text text simply units order words retained.	g  search  Most text information systems use very shallow NLP where text would simply be represented as a “bag of words” where words are basic units for representation and the order of words is ignored although the counts of words are retained.
Most text use simply represented words basic units order ignored counts words representation possible entities techniques text.	 search  Most text information systems use very shallow NLP where text would simply be represented as a “bag of words” where words are basic units for representation and the order of words is ignored although the counts of words are retained  However a more sophisticated representation is also possible which may be based on recognized entities and relations or other techniques for more indepth understanding of text.
text systems shallow text simply represented basic units representation words words However based techniques text.	 Most text information systems use very shallow NLP where text would simply be represented as a “bag of words” where words are basic units for representation and the order of words is ignored although the counts of words are retained  However a more sophisticated representation is also possible which may be based on recognized entities and relations or other techniques for more indepth understanding of text.
However possible recognized indepth text.	 However a more sophisticated representation is also possible which may be based on recognized entities and relations or other techniques for more indepth understanding of text.
components users The following seen functions managing text information.	 With content analysis as the basis there are multiple components in a TIS that are useful for users in different ways  The following are some commonly seen functions for managing and analyzing text information.
analyzing Take user’s query return documents.	 The following are some commonly seen functions for managing and analyzing text information  Search  Take a user’s query and return relevant documents.
Take user’s documents.	 Take a user’s query and return relevant documents.
search engines useful search users effectively text	 Web search engines are among the most useful search engines that enable users to effectively and efficiently deal with a huge amount of text data.
FilteringRecommendation stream decide user’s relevant filter Depending TIS recommender relevant items users goal filter nonrelevant items.	 FilteringRecommendation  Monitor an incoming stream decide which items are relevant or nonrelevant to a user’s interest and then recommend relevant items to the user or filter out nonrelevant items  Depending on whether the system focuses on recognizing relevant items or nonrelevant items this component in a TIS may be called a recommender system whose goal is to recommend relevant items to users or a filtering system whose goal is to filter out nonrelevant items to allow a user to keep only the relevant items.
Monitor stream relevant nonrelevant relevant items recognizing relevant nonrelevant items TIS called recommend items filter allow relevant items email filter	 Monitor an incoming stream decide which items are relevant or nonrelevant to a user’s interest and then recommend relevant items to the user or filter out nonrelevant items  Depending on whether the system focuses on recognizing relevant items or nonrelevant items this component in a TIS may be called a recommender system whose goal is to recommend relevant items to users or a filtering system whose goal is to filter out nonrelevant items to allow a user to keep only the relevant items  Literature recommender and spam email filter are examples of a recommender system and a filtering system respectively.
recommender spam recommender 12 Categorization.	 Literature recommender and spam email filter are examples of a recommender system and a filtering system respectively  12 Chapter 1 Introduction Categorization.
12 Chapter Introduction categories depending applications.	 12 Chapter 1 Introduction Categorization  Classify a text object into one or several of the predefined categories where the categories can vary depending on applications.
vary	 Classify a text object into one or several of the predefined categories where the categories can vary depending on applications.
categories organizing text facilitating access multiple subject classify positive neutral sentiment polarity examples text	 The categories can also be used for organizing text data and facilitating text access  Subject categorizers that classify a text article into one or multiple subject categories and sentiment taggers that classify a sentence into positive negative or neutral in sentiment polarity are both specific examples of a text categorization system.
Summarization text essential content effort text	 Summarization  Take one or multiple text documents and generate a concise summary of the essential content  A summary reduces human effort in digesting text information and may also improve the efficiency in text mining.
Take text human effort information text	 Take one or multiple text documents and generate a concise summary of the essential content  A summary reduces human effort in digesting text information and may also improve the efficiency in text mining.
A digesting efficiency mining summarization called News opinion	 A summary reduces human effort in digesting text information and may also improve the efficiency in text mining  The summarization component of a TIS is called a summarizer  News summarizer and opinion summarizer are both examples of a summarizer.
component TIS summarizer Analysis.	 The summarization component of a TIS is called a summarizer  News summarizer and opinion summarizer are both examples of a summarizer  Topic Analysis.
News opinion	 News summarizer and opinion summarizer are both examples of a summarizer.
extract Topics directly facilitate data users browsing text data companion data authors data analysis trends profiles authors.	 Take a set of documents and extract and analyze topics in them  Topics directly facilitate digestion of text data by users and support browsing of text data  When combined with the companion nontextual data such as time location authors and other meta data topic analysis can generate many interesting patterns such as temporal trends of topics spatiotemporal distributions of topics and topic profiles of authors.
Topics directly users browsing data location authors meta topic trends topics spatiotemporal topics	 Topics directly facilitate digestion of text data by users and support browsing of text data  When combined with the companion nontextual data such as time location authors and other meta data topic analysis can generate many interesting patterns such as temporal trends of topics spatiotemporal distributions of topics and topic profiles of authors.
nuggets” text.	 Information Extraction  Extract entities relations of entities or other “knowledge nuggets” from text.
“knowledge	 Extract entities relations of entities or other “knowledge nuggets” from text.
graph support edges paths graph application mining algorithms discover entityrelation patterns.	 The information extraction component of a TIS enables construction of entityrelation graphs  Such a knowledge graph is useful in multiple ways including support of navigation along edges and paths of the graph and further application of graph mining algorithms to discover interesting entityrelation patterns.
Such graph including support navigation algorithms discover entityrelation patterns Clustering.	 Such a knowledge graph is useful in multiple ways including support of navigation along edges and paths of the graph and further application of graph mining algorithms to discover interesting entityrelation patterns  Clustering.
similar objects	 Clustering  Discover groups of similar text objects e.
Discover similar	 Discover groups of similar text objects e.
.	 terms sentences documents   .
.	   .
clustering plays helping space.	 The clustering component of a TIS plays an important role in helping users explore an information space.
discovering clusters items	 It is also useful for discovering outliers by identifying the items that do not form natural clusters with other items  Visualization.
visualization component engaging interesting	 The visualization component is important for engaging humans in the process of discovering interesting patterns.
visual patterns 1 Organization visualization results generated algorithms This serves covered later book.	 Since humans are very good at recognizing visual patterns 1 3 Organization of the Book 13 visualization of the results generated from various text mining algorithms is generally desirable  This list also serves as an outline of the major topics to be covered later in this book.
This outline major topics later book Specifically search filtering Part II text data clustering topic analysis summarization III analysis covered book want focus general data extraction requires techniques.	 This list also serves as an outline of the major topics to be covered later in this book  Specifically search and filtering are covered first in Part II about text data access whereas categorization clustering topic analysis and summarization are covered later in Part III about text data analysis  Information extraction is not covered in this book since we want to focus on general approaches that can be readily applied to text data in any natural language but information extraction often requires languagespecific techniques.
Specifically search covered Part II text access summarization data Information covered approaches natural information extraction languagespecific	 Specifically search and filtering are covered first in Part II about text data access whereas categorization clustering topic analysis and summarization are covered later in Part III about text data analysis  Information extraction is not covered in this book since we want to focus on general approaches that can be readily applied to text data in any natural language but information extraction often requires languagespecific techniques.
intended focus algorithms	 Visualization is also not covered due to the intended focus on algorithms in this book.
information extraction important relevant analysis management.	 However it must be stressed that both information extraction and visualization are very important topics relevant to text data analysis and management.
Part I This consists chapters concepts needed understanding book book.	 Part I  Overview and Background  This part consists of the first four chapters and provides an overview of the book and background knowledge including basic concepts needed for understanding the content of the book that some readers may not be familiar with and an introduction to the META toolkit used for exercises in the book.
This consists overview book basic needed book familiar META exercises processing understanding obtaining informative text text applications.	 This part consists of the first four chapters and provides an overview of the book and background knowledge including basic concepts needed for understanding the content of the book that some readers may not be familiar with and an introduction to the META toolkit used for exercises in the book  This part also gives a brief overview of natural language processing techniques needed for understanding text data and obtaining informative representation of text needed in all text data analysis applications.
Access consists 5–11 covering text data	 Text Data Access  This part consists of Chapters 5–11 covering the major techniques for supporting text data access.
Chapters major This provides basic techniques retrieval 5 models functions search Chapter systems Chapter 8 evaluation retrieval 9.	 This part consists of Chapters 5–11 covering the major techniques for supporting text data access  This part provides a systematic discussion of the basic information retrieval techniques including the formulation of retrieval tasks as a problem of ranking documents for a query Chapter 5 retrieval models that form the foundation of the design of ranking functions in a search engine Chapter 6 feedback techniques Chapter 7 implementation of retrieval systems Chapter 8 and evaluation of retrieval systems Chapter 9.
This systematic retrieval techniques documents 5 retrieval models form foundation functions engine Chapter feedback 7 evaluation covers important retrieval 10 techniques analyzing links text improving ranking supervised learning briefly discussed.	 This part provides a systematic discussion of the basic information retrieval techniques including the formulation of retrieval tasks as a problem of ranking documents for a query Chapter 5 retrieval models that form the foundation of the design of ranking functions in a search engine Chapter 6 feedback techniques Chapter 7 implementation of retrieval systems Chapter 8 and evaluation of retrieval systems Chapter 9  It then covers web search engines the most important application of information retrieval so far Chapter 10 where techniques for analyzing links in text data for improving ranking of text objects are introduced and application of supervised machine learning to combine multiple features for ranking is briefly discussed.
web engines important application information techniques links application machine learning features covers systems “push” access opposed mode information access search i.	 It then covers web search engines the most important application of information retrieval so far Chapter 10 where techniques for analyzing links in text data for improving ranking of text objects are introduced and application of supervised machine learning to combine multiple features for ranking is briefly discussed  The last chapter in this part Chapter 11 covers recommender systems which provide a “push” mode of information access as opposed to the “pull” mode of information access supported by a typical search engine i.
Chapter covers systems access opposed “pull” mode information	 The last chapter in this part Chapter 11 covers recommender systems which provide a “push” mode of information access as opposed to the “pull” mode of information access supported by a typical search engine i.
	e.
users III Text	 querying by users  Part III  Text Data Analysis.
Part III.	 Part III.
This 12–19 techniques text data text data useful topical data.	 This part consists of Chapters 12–19 covering a variety of techniques for analyzing text data to facilitate user digestion of text data and discover useful topical or other semantic patterns in text data.
gives text perspective data data data humans “subjective sensors” analysis problem general analysis mining analysis text data.	 Chapter 12 gives an overview of text analysis from the perspective of data mining where we may view text data as data generated by humans as “subjective sensors” of the world this view allows us to look at the text analysis problem in the more general context of data analysis and mining in general and facilitates the discussion of joint analysis of text and nontext data.
13 techniques fundamental lexical units	 Specifically Chapter 13 discusses techniques for discovering two fundamental semantic relations between lexical units in text data i e.
e.	e.
paradigmatic syntagmatic relations generate	 paradigmatic relations and syntagmatic relations which can be regarded as an example of discovering knowledge about the natural language used to generate the text data i e.
linguistic knowledge.	e  linguistic knowledge.
linguistic knowledge 14 closely generate associate annotations unorganized text data	 linguistic knowledge  Chapter 14 and Chapter 15 cover respectively two closely related techniques to generate and associate meaningful structures or annotations with otherwise unorganized text data i.
Chapter 14 Chapter 15 cover respectively closely generate associate meaningful unorganized	 Chapter 14 and Chapter 15 cover respectively two closely related techniques to generate and associate meaningful structures or annotations with otherwise unorganized text data i.
categorization Chapter 16 discusses text summarization facilitating human digestion text	 text clustering and text categorization  Chapter 16 discusses text summarization useful for facilitating human digestion of text information.
Chapter text summarization useful information.	 Chapter 16 discusses text summarization useful for facilitating human digestion of text information.
Chapter 17 detailed discussion patterns models.	 Chapter 17 provides a detailed discussion of an important family of probabilistic approaches to discovery and analysis of topical patterns in text data i e  topic models.
topic	 topic models.
discusses analyzing sentiment opinions expressed key discovery knowledge opinions people text	 Chapter 18 discusses techniques for analyzing sentiment and opinions expressed in text data which are key to discovery of knowledge about preferences opinions and behavior of people based on analyzing the text data produced by them.
Finally Chapter discusses analysis text nontext applications use possible analysis IV Text	 Finally Chapter 19 discusses joint analysis of text and nontext data which is often needed in many applications since it is in general beneficial to use as much data as possible for gaining knowledge and intelligence through big data analysis  Part IV  Unified Text Management and Analysis System.
Management System This consists Chapter 20 attempt discuss discussed integrated unified potentially unified management analysis	 Unified Text Management and Analysis System  This last part consists of Chapter 20 where we attempt to discuss how all the techniques discussed in this book can be conceptually integrated in an operatorbased unified framework and thus potentially implemented in a general unified system for text management and analysis that can be useful for supporting a wide range of different applications.
This 20 conceptually integrated unified framework potentially text range applications.	 This last part consists of Chapter 20 where we attempt to discuss how all the techniques discussed in this book can be conceptually integrated in an operatorbased unified framework and thus potentially implemented in a general unified system for text management and analysis that can be useful for supporting a wide range of different applications.
META general support provides META related existing including natural processing learning data Due discussions intuitive.	 This part also serves as a roadmap for further extension of META to provide effective and general highlevel support for various applications and provides guidance on how META may be integrated with many other related existing toolkits including particularly search engine systems database systems natural language processing toolkits machine learning toolkits and data mining toolkits  Due to our attempt to treat all the topics from a practical perspective most of the discussions of the concepts and techniques in the book are informal and intuitive.
Due attempt topics perspective concepts techniques book intuitive.	 Due to our attempt to treat all the topics from a practical perspective most of the discussions of the concepts and techniques in the book are informal and intuitive.
satisfy readers interested book provide rigorous topics Use Book broad cover breadth	 To satisfy the needs of some readers that might be interested in deeper understanding of some topics the book also includes an appendix with notes to provide a more detailed and rigorous explanation of a few important topics  1 4 How to Use this Book Due to the extremely broad scope of the topics that we would like to cover we have to make many tradeoffs between breadth and depth in coverage.
	 1.
4 Due extremely broad scope cover coverage tradeoff concepts practical techniques cover advanced techniques learn	4 How to Use this Book Due to the extremely broad scope of the topics that we would like to cover we have to make many tradeoffs between breadth and depth in coverage  When making 16 Chapter 1 Introduction such a tradeoff we have chosen to emphasize the coverage of the basic concepts and practical techniques of text data mining at the cost of not being able to cover many advanced techniques in detail and provide some references at the end of many chapters to help readers learn more about those advanced techniques if they wish to.
16 Chapter chosen emphasize concepts practical techniques cost able techniques end advanced to.	 When making 16 Chapter 1 Introduction such a tradeoff we have chosen to emphasize the coverage of the basic concepts and practical techniques of text data mining at the cost of not being able to cover many advanced techniques in detail and provide some references at the end of many chapters to help readers learn more about those advanced techniques if they wish to.
Our hope foundation able resource We techniques techniques applicable discuss implemented effort requiring analysis text language	 Our hope is that with the foundation received from reading this book you will be able to learn about more advanced techniques by yourself or via another resource  We have also chosen to cover more general techniques for text management and analysis and favor techniques that can be applicable to any text in any natural language  Most techniques we discuss can be implemented without any human effort or only requiring minimal human effort this is in contrast to some more detailed analysis of text data particularly using natural language processing techniques.
We cover management techniques applicable language.	 We have also chosen to cover more general techniques for text management and analysis and favor techniques that can be applicable to any text in any natural language.
important understand detail.	 Such “deep analysis” techniques are obviously very important and are indeed necessary for some applications where we would like to go indepth to understand text in detail.
techniques envision readers.	 In practice it would be beneficial to combine both kinds of techniques  We envision three main and potentially overlapping categories of readers.
main overlapping readers Students.	 We envision three main and potentially overlapping categories of readers  Students.
Students handson experience real	 Students  This book is specifically designed to give you handson experience in working with real text mining tools and applications.
book applications individually reading Chapters good prerequisite 1 2 3 familiarize future chapters.	 This book is specifically designed to give you handson experience in working with real text mining tools and applications  If used individually we suggest first reading through Chapters 1–4 in order to get a good understanding of the prerequisite knowledge in this book  Chapters 1 2 and 3 will familiarize you with the concepts and vocabulary necessary to understand the future chapters.
individually suggest order book Chapters 2 3 vocabulary understand future Chapter 4 chapter.	 If used individually we suggest first reading through Chapters 1–4 in order to get a good understanding of the prerequisite knowledge in this book  Chapters 1 2 and 3 will familiarize you with the concepts and vocabulary necessary to understand the future chapters  Chapter 4 introduces you to the companion toolkit META which is used in exercises in each chapter.
Chapters concepts understand future	 Chapters 1 2 and 3 will familiarize you with the concepts and vocabulary necessary to understand the future chapters.
hope chapter provide work mining code large head focus If instructor	 We hope the exercises and chapter descriptions provide inspiration to work on your own text mining project  The provided code in META should give a large head start and allow you to focus more on your contribution  If used in class there are several logical flows that an instructor may choose to take.
C exercises files Instructors.	 META is written in modern C although some exercises may be accomplished only by modifying config files  Instructors.
We topics combined curricula.	 Instructors  We have gathered a logical and cohesive collection of topics that may be combined together for various course curricula.
logical collection course curricula Part Part 2 book undergraduate Information focus	 We have gathered a logical and cohesive collection of topics that may be combined together for various course curricula  For example Part 1 and Part 2 of the book may be used as an undergraduate introduction to Information Retrieval with a focus on how search engines work.
different undergraduate course choose 1 book Text skipping Part 2 search implementation choice supplemental emphasis chapter.	 A different undergraduate course may choose to survey 1 4 How to Use this Book 17 the entire book as an Introduction to Text Data Mining while skipping some chapters in Part 2 that are more specific to search engine implementation and applications specific to the Web  Another choice would be using all parts as a supplemental graduate textbook where there is still some emphasis on practical programming knowledge that can be combined with reading referenced papers in each chapter.
How Use 17 Data Mining skipping chapters 2 implementation specific parts textbook emphasis practical reading referenced students implementing references	4 How to Use this Book 17 the entire book as an Introduction to Text Data Mining while skipping some chapters in Part 2 that are more specific to search engine implementation and applications specific to the Web  Another choice would be using all parts as a supplemental graduate textbook where there is still some emphasis on practical programming knowledge that can be combined with reading referenced papers in each chapter  Exercises for graduate students could be implementing some methods they read in the references into META.
choice parts graduate textbook practical combined reading referenced chapter Exercises graduate methods references META exercises end students easily understandable—text retrieval mining	 Another choice would be using all parts as a supplemental graduate textbook where there is still some emphasis on practical programming knowledge that can be combined with reading referenced papers in each chapter  Exercises for graduate students could be implementing some methods they read in the references into META  The exercises at the end of each chapter give students experience working with a powerful—yet easily understandable—text retrieval and mining toolkit in addition to written questions.
graduate students methods exercises students experience written class exercises	 Exercises for graduate students could be implementing some methods they read in the references into META  The exercises at the end of each chapter give students experience working with a powerful—yet easily understandable—text retrieval and mining toolkit in addition to written questions  In a programmingfocused class using the META exercises is strongly encouraged.
programmingfocused class strongly Programming assignments selecting subset chapter.	 In a programmingfocused class using the META exercises is strongly encouraged  Programming assignments can be created from selecting a subset of exercises in each chapter.
Using corpora toolkit yield different e g.	 Using different corpora with the toolkit can yield different project challenges e g.
review	g  review summary vs.
sentiment analysis.	 sentiment analysis.
Most industry serve inspiration	 Practitioners  Most readers in industry would most likely use this book as a reference although we also hope that it may serve as some inspiration in your own work.
Most readers likely use book inspiration As student user suggestion reading current delve	 Most readers in industry would most likely use this book as a reference although we also hope that it may serve as some inspiration in your own work  As with the student user suggestion we think you would get the most of this book by first reading the initial three chapters  Then you may choose a chapter relevant to your current interests and delve deeper or refresh your knowledge.
As user book chapters Then chapter relevant deeper refresh Since simply files anticipate way handle baseline programming	 As with the student user suggestion we think you would get the most of this book by first reading the initial three chapters  Then you may choose a chapter relevant to your current interests and delve deeper or refresh your knowledge  Since many applications in META can be used simply via config files we anticipate it as a quick way to get a handle on your dataset and provide some baseline results without any programming required.
simply config files anticipate quick dataset programming end thought implementations task include META free	 Since many applications in META can be used simply via config files we anticipate it as a quick way to get a handle on your dataset and provide some baseline results without any programming required  The exercises at the end of each chapter can be thought of as default implementations for a particular task at hand  You may choose to include META in your work since it uses a permissive free software license.
fact University licenses.	 In fact it is duallicensed under MIT and University of IllinoisNCSA licenses.
share modifications META proprietary	 Of course we still encourage and invite you to share any modifications extensions and improvements with META that are not proprietary for the benefit of all the readers.
No matter hope comments suggestions improvement book Thanks Chapter Further Reading books	 No matter what your goal we hope that you find this book useful and educational  We also appreciate your comments and suggestions for improvement of the book  Thanks for reading 18 Chapter 1 Introduction Bibliographic Notes and Further Reading There are already multiple excellent text books in information retrieval IR.
We book.	 We also appreciate your comments and suggestions for improvement of the book.
18 Chapter Introduction Bibliographic Reading information retrieval IR retrieval foudational work 1979 Salton McGill 1983 Salton 1989 Another BaezaYates	 Thanks for reading 18 Chapter 1 Introduction Bibliographic Notes and Further Reading There are already multiple excellent text books in information retrieval IR  Due to the long history of research in information retrieval and the fact that much foudational work has been done in 1960s even some very old books such as van Rijsbergen 1979 and Salton and McGill 1983 and Salton 1989 remain very useful today  Another useful early book is Frakes and BaezaYates 1992.
long fact foudational old van Rijsbergen 1979 McGill 1989 useful	 Due to the long history of research in information retrieval and the fact that much foudational work has been done in 1960s even some very old books such as van Rijsbergen 1979 and Salton and McGill 1983 and Salton 1989 remain very useful today.
useful Frakes 1992.	 Another useful early book is Frakes and BaezaYates 1992.
More recent ones include 2004 Witten et	 More recent ones include Grossman and Frieder 2004 Witten et al  1999 and Belew 2008.
recent ones et 2008 al al.	 The most recent ones are Manning et al  2008 Croft et al  2009 Büttcher et al.
al 2009	 2008 Croft et al  2009 Büttcher et al.
BaezaYates view topic retrieval attempts	 2010 and BaezaYates and RibeiroNeto 2011  Compared with these books this book has a broader view of the topic of information retrieval and attempts to cover both text retrieval and text mining.
Compared view topic information text retrieval text mining existing books touched categorization topics previous included important techniques Recommender missing books way	 Compared with these books this book has a broader view of the topic of information retrieval and attempts to cover both text retrieval and text mining  While some existing books on IR have also touched some topics such as text categorization and text clustering which we classify as text mining topics no previous book has included an indepth discussion of topic mining and analysis an important family of techniques very useful for text mining  Recommender systems also seem to be missing in the existing books on IR which we include as an alternative way to support users for text access complementary with search engines.
existing books text clustering classify previous book indepth discussion family mining.	 While some existing books on IR have also touched some topics such as text categorization and text clustering which we classify as text mining topics no previous book has included an indepth discussion of topic mining and analysis an important family of techniques very useful for text mining.
existing IR include access	 Recommender systems also seem to be missing in the existing books on IR which we include as an alternative way to support users for text access complementary with search engines.
Readers want milestones look readings Sparck Willett	 Readers who want to know more about the history of IR research and the major early milestones should take a look at the collection of readings in Sparck Jones and Willett 1997.
The mining multiple e	 The topic of text mining has also been covered in multiple books e g.
A book emphasis mining belief application loop search engines essential text functions 1 raw text data set efficiently mining i.	g  Feldman and Sanger 2007  A major difference between this book and those is our emphasis on the integration of text mining and information retrieval with a belief that any text data application system must involve humans in the loop and search engines are essential components of any text mining systems to support two essential functions 1 help convert a large raw text data set into a much smaller but more relevant text data set which can be efficiently anlayzed by using a text mining algorithm i.
	 Feldman and Sanger 2007.
A text retrieval involve engines essential systems support 1 relevant data set anlayzed text mining algorithm	 A major difference between this book and those is our emphasis on the integration of text mining and information retrieval with a belief that any text data application system must involve humans in the loop and search engines are essential components of any text mining systems to support two essential functions 1 help convert a large raw text data set into a much smaller but more relevant text data set which can be efficiently anlayzed by using a text mining algorithm i.
result coverage techniques applications.	e  knowledge provenance  As a result this book provides a more complete coverage of techniques required for developing big text data applications.
result complete developing applications The focus robust data natural	 As a result this book provides a more complete coverage of techniques required for developing big text data applications  The focus of this book is on covering algorithms that are general and robust which can be readily applied to any text data in any natural language often with no or minimum human effort.
An lack coverage Notes 19 techniques text information IE tend languagespecific require Another reason approaches	 An evitable cost of this focus is its lack of coverage Bibliographic Notes and Further Reading 19 of some key techniques important for text mining notably the information extraction IE techniques which are essential for text mining  We decided not to cover IE because the IE techniques tend to be languagespecific and require nontrivial manual work by humans  Another reason is that many IE techniques rely on supervised machine learning approaches which are well covered in many existing machine learning books see e.
IE techniques	 We decided not to cover IE because the IE techniques tend to be languagespecific and require nontrivial manual work by humans.
reason IE supervised learning approaches covered machine learning books	 Another reason is that many IE techniques rely on supervised machine learning approaches which are well covered in many existing machine learning books see e.
g Bishop Mitchell 1997 start survey Sarawagi 2008 articles	g  Bishop 2006 Mitchell 1997  Readers who are interested in knowing more about IE can start with the survey book Sarawagi 2008 and review articles Jiang 2012.
2006 1997.	 Bishop 2006 Mitchell 1997.
topic book interact developing data application interface design excellent detailed information	 From an application perspective another important topic missing in this book is information visualization which is due to our focus on the coverage of models and algorithms  However since every application system must have a userfriendly interface to allow users to optimally interact with a system those readers who are interested in developing text data application systems will surely find it useful to learn more about user interface design  An excellent reference to start with is Hearst 2009 which also has a detailed coverage of information visualization.
interface users readers text data learn	 However since every application system must have a userfriendly interface to allow users to optimally interact with a system those readers who are interested in developing text data application systems will surely find it useful to learn more about user interface design.
start Hearst detailed coverage Finally book read language processing e.	 An excellent reference to start with is Hearst 2009 which also has a detailed coverage of information visualization  Finally due to our emphasis on breadth the book does not cover any component algorithm in depth  To know more about some of the topics readers can further read books in natural language processing e.
breadth cover depth To know topics readers natural	 Finally due to our emphasis on breadth the book does not cover any component algorithm in depth  To know more about some of the topics readers can further read books in natural language processing e.
read books processing e	 To know more about some of the topics readers can further read books in natural language processing e g.
g Martin 1999 books IR e.	g  Jurafsky and Martin 2009 Manning and Schütze 1999 advanced books on IR e.
Schütze g.	 Jurafsky and Martin 2009 Manning and Schütze 1999 advanced books on IR e g.
BaezaYates learning g.	g  BaezaYates and RibeiroNeto 2011 and books on machine learning e g.
	g.
Bishop	 Bishop 2006.
You specific recommendations Bibliographic Notes chapter corresponding 2Background This chapter contains background information know sections.	 You may find more specific recommendations of readings relevant to a particular topic in the Bibliographic Notes at the end of each chapter that covers the corresponding topic  2Background This chapter contains background information that is necessary to know in order to get the most out of the rest of this book readers who are already familiar with these basic concepts may safely skip the entire chapter or some of the sections.
information order rest book safely skip sections.	 2Background This chapter contains background information that is necessary to know in order to get the most out of the rest of this book readers who are already familiar with these basic concepts may safely skip the entire chapter or some of the sections.
statistics concepts algorithms book continue mathematical information basic machine useful categorization	 We first focus on some basic probability and statistics concepts required for most algorithms and models in this book  Next we continue our mathematical background with an overview of some concepts in information theory that are often used in many text mining applications  The last section introduces the basic idea and problem setup of machine learning particularly supervised machine learning which is useful for classification categorization or textbased prediction in the text domain.
background information mining	 Next we continue our mathematical background with an overview of some concepts in information theory that are often used in many text mining applications.
basic setup learning machine learning useful textbased general learning data mining tasks.	 The last section introduces the basic idea and problem setup of machine learning particularly supervised machine learning which is useful for classification categorization or textbased prediction in the text domain  In general machine learning is very useful for many information retrieval and data mining tasks.
general learning useful information data Basics Probability As models play	 In general machine learning is very useful for many information retrieval and data mining tasks  2 1 Basics of Probability and Statistics As we will see later in this chapter and in many other chapters probabilistic or statistical models play a very important role in text mining algorithms.
As later chapters probabilistic play role text algorithms.	 2 1 Basics of Probability and Statistics As we will see later in this chapter and in many other chapters probabilistic or statistical models play a very important role in text mining algorithms.
reader vocabulary probabilistic approaches book.	 This section gives every reader a sufficient background and vocabulary to understand these probabilistic and statistical approaches covered in the later chapters of the book.
probability distribution way assign likelihood probability .	 A probability distribution is a way to assign likelihood to an event in some probability space .
probability sixsided die Each different Thus orange purple act observing	 As an example let our probability space be a sixsided die  Each side has a different color  Thus   red orange yellow green blue purple and an event is the act of rolling the die and observing a color.
Each	 Each side has a different color.
red yellow blue purple event rolling die observing uncertainty die distribution	 Thus   red orange yellow green blue purple and an event is the act of rolling the die and observing a color  We can quantify the uncertainty of rolling the die by declaring a probability distribution over all possible events.
quantify rolling die probability distribution	 We can quantify the uncertainty of rolling the die by declaring a probability distribution over all possible events.
fair rolling We probability collection probabilities corresponds 16 use model concerning	 Assuming we have a fair die the probability of rolling any specific color is 16  or about 16  We can represent our probability distribution as a collection of probabilities where the first index corresponds to pred  16  the second index corresponds to porange  16  and so on  But what if we had an unfair die We could use a differentprobability distribution θ ′ to model events concerning it.
But use distribution ′ concerning it.	 But what if we had an unfair die We could use a differentprobability distribution θ ′ to model events concerning it.
case orange rolled colors.	 In this case red and orange are assumed to be rolled more often than the other colors.
note θ text try about.	 Be careful to note the difference between the sample space and the defined probability model θ used to quantify its uncertainty  In our text mining tasks we usually try to estimate θ given some knowledge about.
methods estimate θ useful Consider x ∼	 The different methods to estimate θ will determine how accurate or useful the probabilistic model is  Consider the following notation x ∼ θ .
Consider ∼ θ theta random distribution The variable value certain probability defined θ	 Consider the following notation x ∼ θ   We read this as x is drawn from theta or the random variable x is drawn from the probability distribution θ   The random variable x takes on each value from with a certain probability defined by θ .
x takes defined For θ ′ 3 orange In text application tasks usually	 The random variable x takes on each value from with a certain probability defined by θ   For example if we had x ∼ θ ′ then there is a 2 3 chance that x is either red or orange  In our text application tasks we usually have as V  the vocabulary of some text corpus.
x θ chance x tasks text corpus.	 For example if we had x ∼ θ ′ then there is a 2 3 chance that x is either red or orange  In our text application tasks we usually have as V  the vocabulary of some text corpus.
application V corpus For	 In our text application tasks we usually have as V  the vocabulary of some text corpus  For example the vocabulary could be V  a and apple .
.	 For example the vocabulary could be V  a and apple   .
zirconium text data probability distribution θ Thus w write pw θ θ word pw data θ	  zap zirconium zoo and we could model the text data with a probability distribution θ   Thus if we have some word w we can write pw  θ read as the probability of w given θ   If w is the word data we might have pw  data  θ  0.
003 distributions models finite outcomes.	003  In our examples we have only considered discrete probability distributions  That is our models only assign probabilities for a finite discrete set of outcomes.
models discrete outcomes distributions infinite	 That is our models only assign probabilities for a finite discrete set of outcomes  In reality there are also continuous probability distributions where there are an infinite number of “events” that are not countable.
In infinite number countable.	 In reality there are also continuous probability distributions where there are an infinite number of “events” that are not countable.
normal real according parameters necessary chapters discrete probability distribution probability	 For example the normal or Gaussian distribution is a continuous probability distribution that assigns real valued probabilities according to some parameters  We will discuss continuous distributions as necessary in later chapters  For now though it’s sufficient to understand that we can use a discrete probability distribution to model the probability of observing a single word in a vocabulary V .
continuous For it’s understand discrete model probability vocabulary .	 We will discuss continuous distributions as necessary in later chapters  For now though it’s sufficient to understand that we can use a discrete probability distribution to model the probability of observing a single word in a vocabulary V .
use discrete distribution single word	 For now though it’s sufficient to understand that we can use a discrete probability distribution to model the probability of observing a single word in a vocabulary V   2.
2.	 2.
23 axioms probability discrete We check	1 Basics of Probability and Statistics 23 The Kolmogorov axioms describe facts about probability distributions in general both discrete and continuous  We discuss them now since they are a good sanity check when designing your own models.
A valid θ space	 A valid probability distribution θ with probability space  must satisfy the following three axioms.
≤ pθω ∈	   Each event has a probability between zero and one 0 ≤ pθω ∈  ≤ 1  2.
event probability zero	 Each event has a probability between zero and one 0 ≤ pθω ∈  ≤ 1  2.
An zero probability occurring ω′	 2 1   An event not in  has probability zero and the probability of any event occurring from  is one pθω ′  0 ω′ ∈  and pθ  1.
An probability event ′ 0 pθ 1 2.	 An event not in  has probability zero and the probability of any event occurring from  is one pθω ′  0 ω′ ∈  and pθ  1  2.
2	 2 2 .
2 probability	2   The probability of all disjoint events sums to one∑ ω∈ pθω  1.
speaking subset space outcome corresponding	3 Note that strictly speaking an event is defined as a subset of the probability space  and we say that an event happens if and only if the outcome from a random experiment i e  randomly drawing an outcome from  is in the corresponding subset of outcomes defined by the event.
outcomes Thus easy understand subset event probability special happens probability	 randomly drawing an outcome from  is in the corresponding subset of outcomes defined by the event  Thus it is easy to understand that the special event corresponding to the empty subset is an impossible event with a probability of zero whereas the special event corresponding to the complete set itself always happens and so has a probability of 1 0.
Thus corresponding impossible event event probability 0 As case consider precisely exactly distribution	 Thus it is easy to understand that the special event corresponding to the empty subset is an impossible event with a probability of zero whereas the special event corresponding to the complete set itself always happens and so has a probability of 1 0  As a special case we can consider an event space with only those events that each have precisely one element of outcome which is exactly what we assumed when talking about a distribution over all the words.
As case space element exactly talking Here corresponds event defined subset element happens corresponding word.	0  As a special case we can consider an event space with only those events that each have precisely one element of outcome which is exactly what we assumed when talking about a distribution over all the words  Here each word corresponds to the event defined by the subset with the word as the only element clearly such an event happens if and only if the outcome is the corresponding word.
space precisely distribution	 As a special case we can consider an event space with only those events that each have precisely one element of outcome which is exactly what we assumed when talking about a distribution over all the words.
Here corresponds element event happens	 Here each word corresponds to the event defined by the subset with the word as the only element clearly such an event happens if and only if the outcome is the corresponding word  2.
	 2 1.
1 Conditional die rolling	1 1 Joint and Conditional Probabilities For this section let’s modify our original die rolling example.
We distribution probabilities 1 6 1 6 6 1 1 6 Let’s assume look like 24 Chapter 2 Background yellow green respectively.	 We will keep the original distribution as θC indicating the color probabilities θC 1 6 1 6 1 6 1 6 1 6 1 6   Let’s also assume that each color is represented by a particular shape  This makes our die look like 24 Chapter 2 Background  where the colors of the shapes are red orange yellow blue green and purple respectively.
This makes die like shapes red orange blue respectively.	 This makes our die look like 24 Chapter 2 Background  where the colors of the shapes are red orange yellow blue green and purple respectively.
create	 We can now create another distribution for the shape θS.
That 1 3 1 2 1	 That gives θS 1 3 1 2 1 6 .
xC represent variable ∼ θS random	 Then we can let xC ∼ θC represent the color random variable and let xS ∼ θS represent the shape random variable  We now have two variables to work with.
We A joint example probability xC red probability	 We now have two variables to work with  A joint probability measures the likelihood that two events occur simultaneously  For example what is the probability that xC  red and xS  circle Since there are no red circles this has probability zero.
A probability events simultaneously xC red probability	 A joint probability measures the likelihood that two events occur simultaneously  For example what is the probability that xC  red and xS  circle Since there are no red circles this has probability zero.
joint probability Consider modified blue green Thus circles instead green	 In this case the joint probability is 1 6 because there is only one green circle  Consider a modified die  where we changed the color of the blue circle the fourth element in the set to green  Thus we now have two green circles instead of one green and one blue.
green	 Thus we now have two green circles instead of one green and one blue.
As example fair circles combinations green circle 12 .	 As another example if we had a 12sided fair die with 5 green circles and 7 other combinations of shape and color then pxC  green xS  circle  5 12 .
Let’s die	 Let’s use the original die with six unique colors.
Say rolled color How red square.	 Say we know that a square was rolled  With that information what is the probability that the color is red How about purple We can write this as pxC red  xS  square.
red pxC	 With that information what is the probability that the color is red How about purple We can write this as pxC red  xS  square.
square 1 2	 Since we know there are two squares of which one is red pred square  1 2 .
	 2 4 2.
1 Probability 25 numerator Y looking x normalized pY i.	1 Basics of Probability and Statistics 25 The numerator pX  x  Y  y is the probability of exactly the configuration we’re looking for i e  both x and y have been observed which is normalized by pY  y the probability that the condition is true i.
x y condition	e  both x and y have been observed which is normalized by pY  y the probability that the condition is true i e.
observed Using knowledge circle xS circle pxS One mention independence.	 y has been observed  Using this knowledge we can calculate pxC  green  xS  circle pxC  green  xS  circle  pxC  green xS  circle pxS  circle 16 12 1 3   One other important concept to mention is independence.
knowledge calculate green green circle pxC xS circle 1	 Using this knowledge we can calculate pxC  green  xS  circle pxC  green  xS  circle  pxC  green xS  circle pxS  circle 16 12 1 3   One other important concept to mention is independence.
One important concept	 One other important concept to mention is independence.
variables dependent value situation c2 θC.	 In the previous examples the two random variables were dependent meaning the value of one will influence the value of the other  Consider another situation where we have c1 c2 ∼ θC.
inform probability “independently” In case variables X Y pX	 Does the knowledge of c1 inform the probability of c2 No since each draw is done “independently” of the other  In the case where two random variables X and Y are independent pX Y   pXpY .
In variables X Y pX case Both joint questions text.	 In the case where two random variables X and Y are independent pX Y   pXpY   Can you see why this is the case Both conditional and joint probabilities can be used to answer interesting questions about text.
Both conditional probabilities answer interesting document probability observing probability observing know	 Can you see why this is the case Both conditional and joint probabilities can be used to answer interesting questions about text  For example given a document what is the probability of observing the word information and retrieval in the same sentence What is the probability of observing retrieval if we know information has occurred 2 1.
rule derived pX pY pX	1 2 Bayes’ Rule Bayes’ rule may be derived using the definition of conditional probability pX  Y   pX Y pY and pY  X  pY  X pX .
probabilities equal pX pY XpX simplify pY	 Therefore setting the two joint probabilities equal pX  Y pY   pX Y   pY  XpX  We can simplify them as pX  Y   pY  XpX pY .
We The known Bayes’ Reverend Thomas 1701–1761.	 We can simplify them as pX  Y   pY  XpX pY   2 5 The above formula is known as Bayes’ rule named after the Reverend Thomas Bayes 1701–1761.
2.	 2.
5 known Bayes’ rule Reverend Thomas 1701–1761 applications book formula text chapter topic analysis	5 The above formula is known as Bayes’ rule named after the Reverend Thomas Bayes 1701–1761  This rule has widespread applications  In this book you will see heavy use of this formula in the text categorization chapter as well as the topic analysis chapter among others.
widespread applications In heavy categorization analysis chapter	 This rule has widespread applications  In this book you will see heavy use of this formula in the text categorization chapter as well as the topic analysis chapter among others.
book heavy text categorization chapter leave implementation Bayes’ rule inference related	 In this book you will see heavy use of this formula in the text categorization chapter as well as the topic analysis chapter among others  We will leave it up to the individual chapters to explain their use of this rule in their implementation  Essentially though Bayes’ rule can be used to make inference about a hypothesis based on the observed evidence related to the hypothesis.
use implementation.	 We will leave it up to the individual chapters to explain their use of this rule in their implementation.
Essentially based evidence related	 Essentially though Bayes’ rule can be used to make inference about a hypothesis based on the observed evidence related to the hypothesis.
pX interpreted true “prior” knowledge Y Y posterior knowing Y	 pX can thus be interpreted as our prior belief about which hypothesis is true it is “prior” because it is our belief before we have any knowledge about evidence Y   In contrast pX  Y  encodes our posterior belief about the hypothesis since it is our belief after knowing evidence Y .
contrast encodes posterior knowing evidence Bayes’ seen belief provide prior belief likelihood evidence obtain posterior pX	 In contrast pX  Y  encodes our posterior belief about the hypothesis since it is our belief after knowing evidence Y   Bayes’ rule is seen to connect the prior belief and posterior belief and provide a way to update the prior belief pX based on the likelihood of the observed evidence Y and obtain the posterior belief pX  Y .
Y independent happen Y pX.	 It is clear that if X and Y are independent then no updating will happen as in this case pX  Y   pX.
	 2.
3 Flips In probability investigate flipping coin example presence word document easily problem.	3 Coin Flips and the Binomial Distribution In most discussions on probability a good example to investigate is flipping a coin  For example we may be interested in modeling the presence or absence of a particular word in a text document which can be easily mapped to a coin flipping problem.
denoted θ means tails θ probability use distribution.	 The probability of heads is denoted as θ  which means the probability of tails is 1 − θ   To model the probability of success in our case “heads” we can use the Bernoulli distribution.
want k successes instead use binomial	 If we want to model n throws and find the probability of k successes we instead use the binomial distribution.
write k θk1 −	 We can write it as pk heads n k θk1 − θn−k.
2 − θn−k	 2 6 We can also write it as follows pk heads  n kn − k θk1 − θn−k  2.
6 kn − k θk1	6 We can also write it as follows pk heads  n kn − k θk1 − θn−k  2.
formula Well let’s apart n trials k heads tails.	 2 7 But why is it this formula Well let’s break it apart  If we have n total binary trials and want to see k heads that means we must have flipped k heads and n − k tails.
If total trials k heads − k	 If we have n total binary trials and want to see k heads that means we must have flipped k heads and n − k tails.
The observing k θ probability observing n k − .	 The probability of observing each of the k heads is θ  while the probability of observing each of the remaining n − k tails is 1 − θ .
order outcomes For particular sequence outcomes h h tails probability 2 1 sequence simply product event e.	 What if we do care about the order of the outcomes For example what is the probability of observing the particular sequence of outcomes h t  h h t where h and t denote heads and tails respectively Well it is easy to see that the probability 2 1 Basics of Probability and Statistics 27 of observing this sequence is simply the product of observing each event i e.
× − θ θ θ θ θ31 θ2 tails commonly multinomial distribution text models particular	 θ × 1 − θ × θ × θ × 1 − θ  θ31 − θ2 with no adjustment for different orders of observing three heads and two tails  The more commonly used multinomial distribution in text analysis which models the probability of seeing a word in a particular scenario e g.
The multinomial analysis models probability seeing word scenario e g Bernoulli distribution	 The more commonly used multinomial distribution in text analysis which models the probability of seeing a word in a particular scenario e g  in a document is very similar to this Bernoulli distribution just with more than two outcomes.
	g.
similar Bernoulli distribution	 in a document is very similar to this Bernoulli distribution just with more than two outcomes.
1 Estimation data maybe D n 5 D h t Now like based observed data.	1 4 Maximum Likelihood Parameter Estimation Now that we have a model for our coin flipping how can we estimate its parameters given some observed data For example maybe we observe the data D that we discussed above where n  5 D  h t  h h t  Now we would like to figure out what θ is based on the observed data.
4 Maximum Parameter estimate For example maybe data 5 D h h h like figure based observed Using maximum θ data	4 Maximum Likelihood Parameter Estimation Now that we have a model for our coin flipping how can we estimate its parameters given some observed data For example maybe we observe the data D that we discussed above where n  5 D  h t  h h t  Now we would like to figure out what θ is based on the observed data  Using maximum likelihood estimation MLE we choose the θ that has the highest likelihood given our data i.
like based data.	 Now we would like to figure out what θ is based on the observed data.
likelihood MLE θ highest	 Using maximum likelihood estimation MLE we choose the θ that has the highest likelihood given our data i.
probability data maximized likelihood i.	e  choose the θ such that the probability of observed data is maximized  To compute the MLE we would first write down the likelihood function i.
choose probability observed data compute	 choose the θ such that the probability of observed data is maximized  To compute the MLE we would first write down the likelihood function i e.
pD θ θ31	 To compute the MLE we would first write down the likelihood function i e  pD  θ which is θ31 − θ2 as we explained earlier.
pD θ31 − earlier θ maximizes function θ −	e  pD  θ which is θ31 − θ2 as we explained earlier  The problem is thus reduced to find the θ that maximizes the function f θ  θ31 − θ2.
θ The problem θ − θ2.	 pD  θ which is θ31 − θ2 as we explained earlier  The problem is thus reduced to find the θ that maximizes the function f θ  θ31 − θ2.
problem θ maximizes f θ − log f θ log − preserves order Using condition value derivative θ	 The problem is thus reduced to find the θ that maximizes the function f θ  θ31 − θ2  Equivalently we can attempt to maximize the loglikelihood log f θ  3 log θ  2 log1 − θ since logarithm transformation preserves the order of values  Using knowledge of calculus we know that a necessary condition for a function to achieve a maximum value at a θ value is that the derivative at the same θ value is zero.
attempt maximize log θ 3 log logarithm values necessary function derivative θ log f θ dθ θ 2 − θ 0 θ 35.	 Equivalently we can attempt to maximize the loglikelihood log f θ  3 log θ  2 log1 − θ since logarithm transformation preserves the order of values  Using knowledge of calculus we know that a necessary condition for a function to achieve a maximum value at a θ value is that the derivative at the same θ value is zero  Thus we just need to solve the following equation d log f θ dθ 3 θ − 2 1 − θ 0 and we easily find that the solution is θ  35.
Using knowledge necessary value solve following equation θ dθ 2 1 − 0 easily solution	 Using knowledge of calculus we know that a necessary condition for a function to achieve a maximum value at a θ value is that the derivative at the same θ value is zero  Thus we just need to solve the following equation d log f θ dθ 3 θ − 2 1 − θ 0 and we easily find that the solution is θ  35.
More H heads T tails arg max max θ θH1 H T .	 More generally let H be the number of heads and T be the number of tails  The MLE of the probability of heads is given by θMLE  arg max θ pD  θ arg max θ θH1 − θT H H  T .
MLE probability θMLE θ θ − H T 2 notation max represents argument	 The MLE of the probability of heads is given by θMLE  arg max θ pD  θ arg max θ θH1 − θT H H  T   28 Chapter 2 Background The notation arg max represents the argument i.
28 2 max	 28 Chapter 2 Background The notation arg max represents the argument i e.
e.	e.
case function	 θ in this case that makes the likelihood function i.
e pD θ reach	e  pD  θ reach its maximum.
The solution MLE shown ratio It characteristic MLE estimated probability counts denoted probability.	 The solution to MLE shown above should be intuitive the θ that maximizes our data likelihood is just the ratio of heads  It is a general characteristic of the MLE that the estimated probability is the normalized counts of the corresponding events denoted by the probability.
MLE estimated probability normalized multinomial gives possible probability consequence outcomes probability	 It is a general characteristic of the MLE that the estimated probability is the normalized counts of the corresponding events denoted by the probability  As an example the MLE of a multinomial distribution which will be further discussed in detail later in the book gives each possible outcome a probability proportional to the observed counts of the outcome  Note that a consequence of this is that all unobserved outcomes would have a zero probability according to MLE.
As example MLE distribution gives possible probability proportional Note This especially data sample problem motivates estimation	 As an example the MLE of a multinomial distribution which will be further discussed in detail later in the book gives each possible outcome a probability proportional to the observed counts of the outcome  Note that a consequence of this is that all unobserved outcomes would have a zero probability according to MLE  This is often not reasonable especially when the data sample is small a problem that motivates Bayesian parameter estimation which we discuss below.
This especially sample small problem discuss 2.	 This is often not reasonable especially when the data sample is small a problem that motivates Bayesian parameter estimation which we discuss below  2.
1.	 2 1.
5 potential sample small attempts fit data	1 5 Bayesian Parameter Estimation One potential problem of MLE is that it is often inaccurate when the size of the data sample is small since it always attempts to fit the data as well as possible.
Bayesian Parameter Estimation potential problem size data small Consider data heads The MLE probability heads	5 Bayesian Parameter Estimation One potential problem of MLE is that it is often inaccurate when the size of the data sample is small since it always attempts to fit the data as well as possible  Consider an extreme example of observing just two data points of flipping a coin which happen to be all heads  The MLE would say that the probability of heads is 1.
extreme example observing data points flipping happen heads The MLE probability	 Consider an extreme example of observing just two data points of flipping a coin which happen to be all heads  The MLE would say that the probability of heads is 1 0 while the probability of tails is 0.
MLE probability 1 0 probability 0.	 The MLE would say that the probability of heads is 1 0 while the probability of tails is 0.
intuitively inaccurate maximizes observed data points.	 Such an estimate is intuitively inaccurate even though it maximizes the probability of the observed two data points.
addressed alleviated uncertainty Bayesian Bayesian parameter possible parameter variable.	 This problem of “overfitting” can be addressed and alleviated by considering the uncertainty on the parameter and using Bayesian parameter estimation instead of MLE  In Bayesian parameter estimation we consider a distribution over all the possible values for the parameter that is we treat the parameter itself as a random variable.
Specifically represent values value θ data D evidence belief based	 Specifically we may use pθ to represent a distribution over all possible values for θ  which encodes our prior belief about what value is the true value of θ  while the data D provide evidence for or against that belief  The prior belief pθ can then be updated based on the observed evidence.
observed Bayes’ D belief given data pD calculated	 The prior belief pθ can then be updated based on the observed evidence  We’ll use Bayes’ rule to rewrite pθ  D or our belief of the parameters given data as pθ  D  pD  θpθ pD 2 8 where pD can be calculated by summing over all configurations of θ .
8 summing continuous 2.	8 where pD can be calculated by summing over all configurations of θ   For a continuous distribution that would be 2.
	 For a continuous distribution that would be 2.
1 Probability 29 pD θ ′pD θ ′dθ 2 probability θ pθ D θ	1 Basics of Probability and Statistics 29 pD ∫ θ ′ pθ ′pD  θ ′dθ ′ 2 9 which means the probability for a particular θ is pθ  D  pD  θpθ∫ θ ′ pθ ′pD  θ ′dθ ′   2.
9 θ θpθ∫ pθ ′pD θ ′dθ 2.	9 which means the probability for a particular θ is pθ  D  pD  θpθ∫ θ ′ pθ ′pD  θ ′dθ ′   2.
10 quantities	 2 10 We have special names for these quantities .
We special .	10 We have special names for these quantities .
prior probability likelihood D .	 pθ  D the posterior probability of θ   pθ the prior probability of θ   pD  θ the likelihood of D .
prior probability θ ′ ′pD θ ′ likelihood The	 pθ the prior probability of θ   pD  θ the likelihood of D   ∫ θ ′ pθ ′pD  θ ′dθ ′ the marginal likelihood of D The last one is called the marginal likelihood because the integration “marginalizes out” removes the parameter θ from the equation.
θ ∫ θ ′dθ likelihood θ likelihood data remains constant observing sum possible values θ usually pθ D pθpD	 pD  θ the likelihood of D   ∫ θ ′ pθ ′pD  θ ′dθ ′ the marginal likelihood of D The last one is called the marginal likelihood because the integration “marginalizes out” removes the parameter θ from the equation  Since the likelihood of the data remains constant observing the constraint that pθ  D must sum to one over all possible values of θ  we usually just say pθ  D ∝ pθpD  θ.
′ θ ′dθ marginal D marginal out” parameter Since likelihood data remains constant observing D	 ∫ θ ′ pθ ′pD  θ ′dθ ′ the marginal likelihood of D The last one is called the marginal likelihood because the integration “marginalizes out” removes the parameter θ from the equation  Since the likelihood of the data remains constant observing the constraint that pθ  D must sum to one over all possible values of θ  we usually just say pθ  D ∝ pθpD  θ.
distribution characterizes uncertainty value infer computing estimate e.	 That is the posterior is proportional to the prior times the likelihood  The posterior distribution of the parameter θ fully characterizes the uncertainty of the parameter value and can be used to infer any quantity that depends on the parameter value including computing a point estimate of the parameter i e.
posterior distribution parameter θ uncertainty value parameter value including computing estimate e.	 The posterior distribution of the parameter θ fully characterizes the uncertainty of the parameter value and can be used to infer any quantity that depends on the parameter value including computing a point estimate of the parameter i e.
	e.
single value ways based	 a single value of the parameter  There are multiple ways to compute a point estimate based on a posterior distribution.
One distribution weighted For discrete 2.	 One possibility is to compute the mean of the posterior distribution which is given by the weighted sum of probabilities and the parameter values  For a discrete distribution EX ∑ x xpx 2.
x 11 continuous distribution x	 For a discrete distribution EX ∑ x xpx 2 11 while in a continuous distribution EX ∫ x xf xdx 2.
posterior Maximum MAP pD	12 Sometimes we are interested in using the mode of the posterior distribution as our estimate of the parameter which is called Maximum a Posteriori MAP estimate given by θMAP  arg max θ pθ  D  arg max θ pD  θpθ.
30 Chapter 2 deviate consideration probability parameter belief pθ.	 2 13 30 Chapter 2 Background Here it is easy to see that the MAP estimate would deviate from the MLE with consideration of maximizing the probability of the parameter according to our prior belief encoded as pθ.
It problem prefer heads For continuation discussion consult 2.	 It is through the use of appropriate prior that we can address the overfitting problem of MLE since our prior can strongly prefer an estimate where neither heads nor tails should have a zero probability  For a continuation and more indepth discussion of this material consult Appendix A  2.
2 1 Probabilistic Models Their Applications statistical foundation previous probabilistic	 2 1 6 Probabilistic Models and Their Applications With the statistical foundation from the previous sections we can now start to see how we might apply a probabilistic model to text analysis.
	1.
In processing interested defines distributions sequences Such called statistical language text data i.	 In general in text processing we would be interested in a probabilistic model for text data which defines distributions over sequences of words  Such a model is often called statistical language model or a generative model for text data i.
sequences words started observed corpus words multinomial consider order words.	 a probabilistic model that can be used for sampling sequences of words  As we started to explain previously we usually treat the sample space  as V  the set of all observed words in our corpus  That is we define probability distributions over words from our dataset which are essentially multinomial distributions if we do not consider the order of words.
started sample V set observed corpus distributions multinomial distributions consider order models data	 As we started to explain previously we usually treat the sample space  as V  the set of all observed words in our corpus  That is we define probability distributions over words from our dataset which are essentially multinomial distributions if we do not consider the order of words  While there are many more sophisticated models for text data see e.
words multinomial order	 That is we define probability distributions over words from our dataset which are essentially multinomial distributions if we do not consider the order of words.
e g 1997 model language useful number text fact	 While there are many more sophisticated models for text data see e g  Jelinek 1997 this simplest model often called unigram language model is already very useful for a number of tasks in text data management and analysis due to the fact that the words in our vocabulary are very well designed meaningful basic units for human communications.
1997 called unigram language model number tasks data management	 Jelinek 1997 this simplest model often called unigram language model is already very useful for a number of tasks in text data management and analysis due to the fact that the words in our vocabulary are very well designed meaningful basic units for human communications.
statistical ” model means estimating	 For now we can discuss the general framework in which statistical models are “learned ” Learning a model means estimating its parameters.
words parameter element V The workflow	 In the case of a distribution over words we have one parameter for each element in V   The workflow looks like the following.
workflow	 The workflow looks like the following  1.
	 1  Define the model.
model 2 parameters.	 Define the model  2  Learn its parameters.
	 Learn its parameters.
model step addressed.	 Apply the model  The first step has already been addressed.
The step example occurring corpus In set probabilities	 The first step has already been addressed  In our example we wish to capture the probabilities of individual words occurring in our corpus  In the second step we need to figure out actually how to set the probabilities for each word.
That count number 2.	 That is the count of a unique word wi divided by the total number of words 2.
2 corpus value pwi θ This shown More sophisticated models parameter estimation discussed	2 Information Theory 31 in the corpus could be the value of pwi  θ  This can be shown to be the solution of the MLE of the model  More sophisticated models and their parameter estimation will be discussed later in the book.
shown MLE	 This can be shown to be the solution of the MLE of the model.
models estimation book use case analyzing specific corpus observing unseen data new possible model parameters knowledge hope discover	 More sophisticated models and their parameter estimation will be discussed later in the book  Finally once we have θ defined what can we actually do with it One use case would be analyzing the probability of a specific subset of words in the corpus and another could be observing unseen data and calculating the probability of seeing the words in the new text  It is often possible to design the model such that the model parameters would encode the knowledge we hope to discover from text data.
θ use probability specific subset corpus unseen calculating new possible model encode text data.	 Finally once we have θ defined what can we actually do with it One use case would be analyzing the probability of a specific subset of words in the corpus and another could be observing unseen data and calculating the probability of seeing the words in the new text  It is often possible to design the model such that the model parameters would encode the knowledge we hope to discover from text data.
design model parameters In case estimated parameters directly Please probabilistic tool analysis—that’s main 2.	 It is often possible to design the model such that the model parameters would encode the knowledge we hope to discover from text data  In such a case the estimated model parameters can be directly used as the output result of text mining  Please keep in mind that probabilistic models are a general tool and don’t only have to be used for text analysis—that’s just our main application 2.
estimated directly output result	 In such a case the estimated model parameters can be directly used as the output result of text mining.
mind probabilistic text analysis—that’s main Information uncertainty transfer storage quantified information	 Please keep in mind that probabilistic models are a general tool and don’t only have to be used for text analysis—that’s just our main application 2 2 Information Theory Information theory deals with uncertainty and the transfer or storage of quantified information in the form of bits.
Information deals storage quantified	2 Information Theory Information theory deals with uncertainty and the transfer or storage of quantified information in the form of bits.
A text analysis introduce concept information theory entropy measures problem uncertainty variable.	 A few concepts from information theory are very useful in text data management and analysis which we introduce here briefly  The most important concept of information theory is entropy which is a building block for many other measures  The problem can be formally defined as the quantified uncertainty in predicting the value of a random variable.
The concept information entropy building block measures The problem formally quantified uncertainty random	 The most important concept of information theory is entropy which is a building block for many other measures  The problem can be formally defined as the quantified uncertainty in predicting the value of a random variable.
variable heads How measure randomness This	 The more random this random variable is the more difficult the prediction of heads or tails will be  How does one quantitatively measure the randomness of a random variable like X This is precisely what entropy does.
How quantitatively randomness random variable X random variable HX measure number If completely certain need information HX	 How does one quantitatively measure the randomness of a random variable like X This is precisely what entropy does  Roughly the entropy of a random variable X HX is a measure of expected number of bits needed to represent the outcome of an event x ∼ X  If the outcome is known completely certain we don’t need to represent any information and HX  0.
random measure needed outcome ∼	 Roughly the entropy of a random variable X HX is a measure of expected number of bits needed to represent the outcome of an event x ∼ X.
If completely don’t represent HX If outcome unknown represent outcome efficiently possible.	 If the outcome is known completely certain we don’t need to represent any information and HX  0  If the outcome is unknown we would like to represent the outcome in bits as efficiently as possible.
outcome unknown fewer Entropy 32 Chapter bits x ∼ formula HX x∈X px	 If the outcome is unknown we would like to represent the outcome in bits as efficiently as possible  That means using fewer bits for common occurrences and more bits when the event is less likely  Entropy gives us the expected number 32 Chapter 2 Background of bits for any x ∼ X using the formula HX  − ∑ x∈X px log2 px.
fewer common occurrences event gives expected number Chapter 2 Background bits x formula − px log2	 That means using fewer bits for common occurrences and more bits when the event is less likely  Entropy gives us the expected number 32 Chapter 2 Background of bits for any x ∼ X using the formula HX  − ∑ x∈X px log2 px.
Entropy gives 32 2 formula ∑ x∈X px 2.	 Entropy gives us the expected number 32 Chapter 2 Background of bits for any x ∼ X using the formula HX  − ∑ x∈X px log2 px  2.
2 14 In cases log2 0 generally 0 log2 0	 2 14 In the cases where we have log2 0 we generally just define this to be 0 since log2 0 is undefined.
cases 0 define 0 undefined We different HX different random variables X The formula scope suffices HX means randomness complete events equally likely.	14 In the cases where we have log2 0 we generally just define this to be 0 since log2 0 is undefined  We will get different HX for different random variables X  The exact theory and reasoning behind this formula are beyond the scope of this book but it suffices to say that HX  0 means there is no randomness HX  1 means there is complete randomness in that all events are equally likely.
HX variables X.	 We will get different HX for different random variables X.
reasoning formula scope book HX HX means events likely.	 The exact theory and reasoning behind this formula are beyond the scope of this book but it suffices to say that HX  0 means there is no randomness HX  1 means there is complete randomness in that all events are equally likely.
For coin events looks HX log2 − pX pX calculate calculation HX 2 1 1 coin 1 0 HX −0 log2 0 − 1 1 0.	 For our coin example where the sample space is two events heads or tails the entropy function looks like HX  −pX  0 log2 pX  0 − pX  1 log2 pX  1  For a fair coin we would have pX  1  pX  0  1 2   To calculate HX we’d have the calculation HX  − 1 2 log2 1 2 − 1 2 log2 1 2 1 whereas for a completely biased coin with pX  1  1 pX  0  0 we would have HX  −0 log2 0 − 1 log2 1  0.
pX 0 2 we’d 1 − 1 1 biased pX pX 0 −0 log2	 For a fair coin we would have pX  1  pX  0  1 2   To calculate HX we’d have the calculation HX  − 1 2 log2 1 2 − 1 2 log2 1 2 1 whereas for a completely biased coin with pX  1  1 pX  0  0 we would have HX  −0 log2 0 − 1 log2 1  0.
HX HX 1 2 log2 1 2 log2 2 1 completely pX log2 0 − 1	 To calculate HX we’d have the calculation HX  − 1 2 log2 1 2 − 1 2 log2 1 2 1 whereas for a completely biased coin with pX  1  1 pX  0  0 we would have HX  −0 log2 0 − 1 log2 1  0  For this example we had only two possible outcomes i e.
For random	 For this example we had only two possible outcomes i e  a binary random variable.
binary variable As formula entropy easily	e  a binary random variable  As we can see from the formula this idea of entropy easily generalizes to random variables with more than two outcomes in those cases the sum is over more than two elements.
binary entropy easily variables cases elements plot HX probability heads plot shown Figure	 a binary random variable  As we can see from the formula this idea of entropy easily generalizes to random variables with more than two outcomes in those cases the sum is over more than two elements  If we plot HX for our coin example against the probability of heads pX  1 we receive a plot like the one shown in Figure 2.
As entropy random variables outcomes cases sum elements If coin example plot 2 1.	 As we can see from the formula this idea of entropy easily generalizes to random variables with more than two outcomes in those cases the sum is over more than two elements  If we plot HX for our coin example against the probability of heads pX  1 we receive a plot like the one shown in Figure 2 1.
ends xaxis 1 small large function random random	 At the two ends of the xaxis the probability of X  1 is either very small or very large  In both these cases the entropy function has a low value because the outcome is not very random  The most random is when pX  1  1 2 .
In function value outcome	 In both these cases the entropy function has a low value because the outcome is not very random.
consider particular minimal In particular let’s cases example random Y takes value	 It’s a good exercise to consider when a particular random variable not just the coin example has a maximum or minimal value  In particular let’s think about some special cases  For example we might have a random variable Y that always takes a value of 1.
In let’s think special example random Y takes value	 In particular let’s think about some special cases  For example we might have a random variable Y that always takes a value of 1.
For value	 For example we might have a random variable Y that always takes a value of 1.
random Z equally value In cases HY outcome outcome	 Or there’s a random variable Z that is equally likely to take a value of 1 2 or 3  In these cases HY  HZ since the outcome of Y is much easier to predict than the outcome of Z.
In outcome outcome This captures.	 In these cases HY  HZ since the outcome of Y is much easier to predict than the outcome of Z  This is precisely what entropy captures.
precisely HY confirm	 This is precisely what entropy captures  You can calculate HY and HZ to confirm this answer.
For consider entropy predict.	 For our applications it may be useful to consider the entropy of a word w in some context  Here highentropy words would be harder to predict.
Here words harder Let W random word occurs corpus.	 Here highentropy words would be harder to predict  Let W be the random variable that denotes whether a word occurs in a document in our corpus.
W word document corpus Say W 1 occurs	 Let W be the random variable that denotes whether a word occurs in a document in our corpus  Say W  1 if the word occurs and W  0 otherwise.
Say 1 0 HWthe compares HWcomputer word close word document	 Say W  1 if the word occurs and W  0 otherwise  How do you think HWthe compares to HWcomputer The entropy of the word the is close to zero since it occurs everywhere  It’s not surprising to see this word in a document thus it is easy to predict that Wthe  1.
HWthe The close zero occurs It’s word Wthe This case like way.	 How do you think HWthe compares to HWcomputer The entropy of the word the is close to zero since it occurs everywhere  It’s not surprising to see this word in a document thus it is easy to predict that Wthe  1  This case is just like the biased coin that always lands one way.
lands way The word harder occurs entropy	 This case is just like the biased coin that always lands one way  The word computer on the other hand is a less common word and is harder to predict whether it occurs or not so the entropy will be higher.
harder occurs	 The word computer on the other hand is a less common word and is harder to predict whether it occurs or not so the entropy will be higher.
HX HX knowing help resolving	e  HX  Y   HX since knowing Y does not help at all in resolving the uncertainty of X.
Y Another concept mutual information random X Y entropy X random Y	 HX  Y   HX since knowing Y does not help at all in resolving the uncertainty of X  34 Chapter 2 Background Another useful concept is mutual information defined on two random variables I X Y  which is defined as the reduction of entropy of X due to knowledge about another random variable Y  i e.
Background useful concept information defined random Y Y e X HX .	 34 Chapter 2 Background Another useful concept is mutual information defined on two random variables I X Y  which is defined as the reduction of entropy of X due to knowledge about another random variable Y  i e  I X Y   HX − HX  Y .
I X − HX .	e  I X Y   HX − HX  Y .
shown mutual information equivalently written X Y HY − HY	15 It can be shown that mutual information can be equivalently written as I X Y   HY − HY  X.
Intuitively information measure correlation Clearly measure X Y mutual including entropy later book.	 Intuitively mutual information can measure the correlation of two random variables  Clearly as a correlation measure on X and Y  mutual information is symmetric  Applications of these basic concepts including entropy conditional entropy and mutual information will be further discussed later in this book.
2 3 Machine Learning learning important technique problems	 2 3 Machine Learning Machine learning is a very important technique for solving many problems and has very broad applications.
In management analysis uses treatment scope book introduce concepts machine better	 In text data management and analysis it has also many uses  Any indepth treatment of this topic would clearly be beyond the scope of this book but here we introduce some basic concepts in machine learning that are needed to better understand the content later in the book.
typically humans receives form supervision	 It is called “supervised” because typically the y values must be provided by humans for each x and thus the computer receives a form of supervision from the humans.
y takes finite set labels function serve classifier “right” label labels problem problem.	 When y takes a value from a finite set of values which can be called labels a function f   can serve as a classifier in that it can be used to map an instance x to the “right” label or multiple correct labels when appropriate  Thus the problem can be called a classification problem.
Thus case problem binary classification When y	 Thus the problem can be called a classification problem  The simplest case of the classification problem is when we have just two labels known as binary classification  When y 2.
case known binary When y	 The simplest case of the classification problem is when we have just two labels known as binary classification  When y 2.
y 3 35 real value called regression called prediction goal mainly y given x term meaningful	 When y 2 3 Machine Learning 35 takes a real value the problem is often called a regression problem  Both forms of the problem can also be called prediction when our goal is mainly to infer the unknown y for a given x the term “prediction” is especially meaningful when y is some property of a future event.
forms mainly infer unknown term meaningful y event In occur common case text categorization We chapter later	 Both forms of the problem can also be called prediction when our goal is mainly to infer the unknown y for a given x the term “prediction” is especially meaningful when y is some property of a future event  In textbased applications both forms may occur although the classification problem is far more common in which case the problem is also called text categorization or text classification  We dedicate a chapter to this topic later in the book Chapter 15.
classification regression input data instance feature vector feature clue value f learns data combine weights indicate final function y simply error minimum	 In classification as well as regression the input data instance x is often represented as a feature vector where each feature provides a potential clue about which y value is most likely the value of f x  What the computer learns from the training data is an optimal way to combine these features with weights on them to indicate their importance and their influence on the final function value y  “Optimal” here simply means that the prediction error on the training data is minimum i.
error training minimum	 “Optimal” here simply means that the prediction error on the training data is minimum i.
e.	e.
predicted ŷ values y data.	 the predicted ŷ values are maximally consistent with the true y values in the training data.
A attribute way objects news good features document’s feature vector	 A feature is an attribute of an object that describes it in some way  For example if the objects are news articles one feature could be whether the word good occurred in the article  All these different features are part of a document’s feature vector which is used to represent the document.
For example objects feature word occurred	 For example if the objects are news articles one feature could be whether the word good occurred in the article.
document’s represent In cases	 All these different features are part of a document’s feature vector which is used to represent the document  In our cases the feature vector will usually have to do with the words that appear in the document.
document.	 In our cases the feature vector will usually have to do with the words that appear in the document.
We set yi setup classifier function f	 We also have Y which is the set of possible labels for each object  Thus yi may be sports in our news article classification setup and yj could be politics  A classifier is a function f .
A f .	 A classifier is a function f .
vector outputs	 that takes a feature vector as input and outputs a predicted label ŷ ∈ Y.
Thus f meaning ŷ If correct	 Thus we could have f xi  sports meaning ŷ  sports  If the true y is also sports the classifier was correct in its prediction.
If sports classifier prediction evaluate algorithm use labels learn	 If the true y is also sports the classifier was correct in its prediction  Notice how we can only evaluate a classification algorithm if we know the true labels of the data  In fact we will have to use the true labels in order to learn a good function f .
evaluate algorithm know In learn f unseen classify	 Notice how we can only evaluate a classification algorithm if we know the true labels of the data  In fact we will have to use the true labels in order to learn a good function f   to take unseen feature vectors and classify them.
fact labels order learn f feature vectors classify For machine learning split corpus X training data data.	 In fact we will have to use the true labels in order to learn a good function f   to take unseen feature vectors and classify them  For this reason when studying machine learning algorithms we often split our corpus X into two parts training data and testing data.
feature vectors machine corpus	 to take unseen feature vectors and classify them  For this reason when studying machine learning algorithms we often split our corpus X into two parts training data and testing data.
reason studying split corpus X parts data data.	 For this reason when studying machine learning algorithms we often split our corpus X into two parts training data and testing data.
The classifier e g.	 The training portion is used to build the classifier and the testing portion is used to evaluate the performance e g.
g correct labels In Background labelled generate cases program.	g  determine how many correct labels were predicted  In applications the training data are generally 36 Chapter 2 Background all the labelled examples that we can generate and the test cases are the data points to which we would like to apply our machine learning program.
function news article	 But what does the function f   actually do Consider a very simple example that determines whether a news article has positive or negative sentiment i.
x’s count term negative	e  Y  positive negative positive if x’s count for the term good is greater than 1 negative otherwise.
Y positive positive term greater course example classifier outputs label.	 Y  positive negative positive if x’s count for the term good is greater than 1 negative otherwise  Of course this example is overly simplified but it does demonstrate the basic idea of a classifier it takes a feature vector as input and outputs a class label.
Of overly simplified basic takes vector	 Of course this example is overly simplified but it does demonstrate the basic idea of a classifier it takes a feature vector as input and outputs a class label.
training articles contain term investigate specific creating data.	 Based on the training data the classifier may have determined that positive sentiment articles contain the term good more than once therefore this knowledge is encoded in the function  In Chapter 15 we will investigate some specific algorithms for creating the function f   based on the training data.
In investigate algorithms creating function	 In Chapter 15 we will investigate some specific algorithms for creating the function f .
based data retrieval 7 Chapter classifiers know machine	 based on the training data  Other topics such as feedback for information retrieval Chapter 7 and sentiment analysis Chapter 18 make use of classifiers or resemble them  For this reason it’s good to know what machine learning is and what kinds of problems it can solve.
information sentiment Chapter 18 use classifiers	 Other topics such as feedback for information retrieval Chapter 7 and sentiment analysis Chapter 18 make use of classifiers or resemble them.
reason it’s learning solve.	 For this reason it’s good to know what machine learning is and what kinds of problems it can solve.
structures X Since approach unsupervised.	 However we may still learn latent properties or structures of X  Since there is no human effort involved such an approach is called unsupervised.
human effort similar dataset data data instances	 Since there is no human effort involved such an approach is called unsupervised  For example the computer can learn that some data instances are very similar and the whole dataset can be represented by three major clusters of data instances such that in each cluster the data instances are all very similar.
For example similar dataset major This essentially technique Another learning model called “generative models” parameters data.	 For example the computer can learn that some data instances are very similar and the whole dataset can be represented by three major clusters of data instances such that in each cluster the data instances are all very similar  This is essentially the clustering technique that we will discuss in Chapter 14  Another form of unsupervised learning is to design probabilistic models to model the data called “generative models” where we can embed interesting parameters that denote knowledge that we would like to discover from the data.
This form unsupervised probabilistic model data “generative models” parameters knowledge like discover	 This is essentially the clustering technique that we will discuss in Chapter 14  Another form of unsupervised learning is to design probabilistic models to model the data called “generative models” where we can embed interesting parameters that denote knowledge that we would like to discover from the data.
Another model called interesting parameters denote knowledge data fitting model estimate values explain treat knowledge approach topics discussed 17.	 Another form of unsupervised learning is to design probabilistic models to model the data called “generative models” where we can embed interesting parameters that denote knowledge that we would like to discover from the data  By fitting the model to our data we can estimate the parameter values that can best explain the data and treat the obtained parameter values as the knowledge discovered from the data  Applications of such an approach in analyzing latent topics in text are discussed in detail in Chapter 17.
Notes Further basic statistics Lehmann	 Bibliographic Notes and Further Reading Detailed discussion of the basic concepts in probability and statistics can be found in many textbooks such as Hodges and Lehmann 1970.
excellent Myung	 An excellent introduction to the maximum likelihood estimation can be found in Myung 2003.
introduction given Analysis et introduction	 An accessible comprehensive introduction to Bayesian statistics is given in the book Bayesian Data Analysis Gelman et al  1995  Cover and Thomas 1991 provide a comprehensive introduction to information theory.
introduction learning concepts machine learning e.	 1995  Cover and Thomas 1991 provide a comprehensive introduction to information theory  There are many books on machine learning where a more rigorous introduction to the basic concepts in machine learning as well as many specific machine learning approaches can be found e.
information rigorous concepts learning	 Cover and Thomas 1991 provide a comprehensive introduction to information theory  There are many books on machine learning where a more rigorous introduction to the basic concepts in machine learning as well as many specific machine learning approaches can be found e.
machine learning introduction machine learning specific learning approaches	 There are many books on machine learning where a more rigorous introduction to the basic concepts in machine learning as well as many specific machine learning approaches can be found e.
g 2006 In data understanding NLP.	g  Bishop 2006 Mitchell 1997  3Text Data Understanding In this chapter we introduce basic concepts in text data understanding through natural language processing NLP.
concerned techniques understand meaning natural text.	 NLP is concerned with developing computational techniques to enable a computer to understand the meaning of natural language text.
involve following tasks	 In general this may involve the following tasks  Lexical analysis.
Lexical analysis The purpose analysis figure basic language	 Lexical analysis  The purpose of lexical analysis is to figure out what the basic meaningful units in a language are e.
units g.	 The purpose of lexical analysis is to figure out what the basic meaningful units in a language are e g.
	g.
words determine meaning word In separated word Chinese delimiter	 words in English and determine the meaning of each word  In English it is rather easy to determine the boundaries of words since they are separated by spaces but it is nontrivial to find word boundaries in some other languages such as Chinese where there is no clear delimiter to separate words.
determine boundaries spaces nontrivial boundaries Chinese delimiter Syntactic analysis.	 In English it is rather easy to determine the boundaries of words since they are separated by spaces but it is nontrivial to find word boundaries in some other languages such as Chinese where there is no clear delimiter to separate words  Syntactic analysis.
syntactic related structure sentence analysis.	 Syntactic analysis  The purpose of syntactic analysis is to determine how words are related with each other in a sentence thus revealing the syntactic structure of a sentence  Semantic analysis.
Semantic semantic determine meaning sentence.	 Semantic analysis  The purpose of semantic analysis is to determine the meaning of a sentence.
unit Pragmatic The determine meaning context e.	 This typically involves the computation of meaning of a whole sentence or a larger unit based on the meanings of words and their syntactic structure  Pragmatic analysis  The purpose of pragmatic analysis is to determine meaning in context e.
pragmatic meaning g acts	 The purpose of pragmatic analysis is to determine meaning in context e g  to infer the speech acts of language.
infer language humans A Chapter analysis purpose communication.	 to infer the speech acts of language  Natural language is used by humans to communicate with each other  A deeper understanding 40 Chapter 3 Text Data Understanding of natural language than semantic analysis is thus to further understand the purpose in communication.
humans A 40 natural language purpose communication.	 Natural language is used by humans to communicate with each other  A deeper understanding 40 Chapter 3 Text Data Understanding of natural language than semantic analysis is thus to further understand the purpose in communication.
Text Understanding language Discourse analysis.	 A deeper understanding 40 Chapter 3 Text Data Understanding of natural language than semantic analysis is thus to further understand the purpose in communication  Discourse analysis.
	 Discourse analysis.
Discourse chunk text sentences placed sentences Figure	 Discourse analysis is needed when a large chunk of text with multiple sentences is to be analyzed in such a case the connections between these sentences must be considered and the analysis of an individual sentence must be placed in the appropriate context involving other sentences  In Figure 3.
1 simple English “A chasing	1 we show what is involved in understanding a very simple English sentence “A dog is chasing a boy on the playground.
” case involves words example dog	” The lexical analysis in this case involves determining the syntactic categories parts of speech of all the words for example dog is a noun and chasing is a verb.
Syntactic noun phrase.	 Syntactic analysis is to determine that a and boy form a noun phrase.
entities relations obtain representation	 Semantic analysis is to map noun phrases to entities and verb phrases to relations so as to obtain a formal representation of the meaning of the sentence.
e dog	e  b1 and a dog to an entity denoting a dog i.
i.	 b1 and a dog to an entity denoting a dog i.
e.	e.
level relevant For assume chased infer b1 scared.	 Note that with this level of understanding one may also infer additional information based on any relevant common sense knowledge  For example if we assume that if someone is being chased he or she may be scared we could infer that the boy being chased b1 may be scared.
pragmatic reveal person intend action owner dog	 Finally pragmatic analysis might further reveal that the person who said this sentence might intend to request an action such as reminding the owner of the dog to take the dog back.
While clear simple general challenging natural text.	 While it is possible to derive a clear semantic representation for a simple sentence like the one shown in Figure 3 1 it is in general very challenging to do this kind of analysis for unrestricted natural language text.
main reason natural language designed efficient contrast language designed understanding.	 The main reason for this difficulty is because natural language is designed to make human communication efficient this is in contrast with a programming language which is designed to facilitate computer understanding.
lot “common knowledge natural language communication reader there’s communicate	 1 We omit a lot of “common sense” knowledge in natural language communication because we assume the hearer or reader possesses such knowledge thus there’s no need to explicitly communicate it.
ambiguities assume hearerreader resolve need waste ambiguity ambiguity involve reasoning large commonsense knowledge general artificial In	 2 We keep a lot of ambiguities which we assume the hearerreader knows how to resolve thus there’s no need to waste words to clarify them  As a result natural language text is full of ambiguity and resolving ambiguity would generally involve reasoning with a large amount of commonsense knowledge which is a general difficult challenge in artificial intelligence  In this sense NLP is “AI complete” i.
language text general difficult challenge intelligence.	 As a result natural language text is full of ambiguity and resolving ambiguity would generally involve reasoning with a large amount of commonsense knowledge which is a general difficult challenge in artificial intelligence.
“AI complete” problems intelligence.	 In this sense NLP is “AI complete” i e  as difficult as any other difficult problems in artificial intelligence.
difficult following natural	 as difficult as any other difficult problems in artificial intelligence  The following are a few examples of specific challenges in natural language understanding  Wordlevel ambiguity.
The examples specific understanding Wordlevel A word multiple multiple	 The following are a few examples of specific challenges in natural language understanding  Wordlevel ambiguity  A word may have multiple syntactic categories and multiple senses.
ambiguity A multiple multiple	 Wordlevel ambiguity  A word may have multiple syntactic categories and multiple senses.
word senses design noun POS multiple meanings noun ambiguous	 A word may have multiple syntactic categories and multiple senses  For example design can be a noun or a verb ambiguous POS root has multiple meanings even as a noun ambiguous sense  Syntactic ambiguity.
For noun multiple sense.	 For example design can be a noun or a verb ambiguous POS root has multiple meanings even as a noun ambiguous sense.
Syntactic phrase sentence syntactic structures.	 Syntactic ambiguity  A phrase or a sentence may have multiple syntactic structures.
A phrase sentence multiple syntactic structures natural different interpretations natural vs.	 A phrase or a sentence may have multiple syntactic structures  For example natural language processing can have two different interpretations “processing of natural language” vs.
“natural ambiguous	 “natural processing of language” ambiguous modification.
example telescope leading different result telescope prepositional What exactly unclear.	 Another example A man saw a boy with a telescope has two distinct syntactic structures leading to a different result regarding who had the telescope ambiguous prepositional phrase PP attachment  Anaphora resolution  What exactly a pronoun refers to may be unclear.
Anaphora resolution exactly refers For example Bill buy refer Bill Data	 Anaphora resolution  What exactly a pronoun refers to may be unclear  For example in John persuaded Bill to buy a TV for himself  does himself refer to John or Bill 42 Chapter 3 Text Data Understanding Presupposition.
What exactly	 What exactly a pronoun refers to may be unclear.
John persuaded Bill Data Presupposition smoking implies making inferences	 For example in John persuaded Bill to buy a TV for himself  does himself refer to John or Bill 42 Chapter 3 Text Data Understanding Presupposition  He has quit smoking implies that he smoked before making such inferences in a general way is difficult  3.
	 3.
1 History NLP Research NLP researchers understood human language purpose machine translation.	1 History and State of the Art in NLP Research in NLP dated back to at least the 1950s when researchers were very optimistic about having computers that understood human language particularly for the purpose of machine translation.
That insufficient	 That is a dictionary is insufficient instead we would need an encyclopedia.
machine translation ambitious researchers tackled ambitious NLP late 1960s success techniques limited application For speech recognition applications transcribe Such figuring structure crucial speech	 Realizing that machine translation may be too ambitious researchers tackled less ambitious applications of NLP in the late 1960s and 1970s with some success though the techniques developed failed to scale up thus only having limited application impact  For example people looked at speech recognition applications where the goal is to transcribe a speech  Such a task requires only limited understanding of natural language thus more realistic for example figuring out the exact syntactic structure is probably not very crucial for speech recognition.
example people recognition goal speech limited natural example figuring exact syntactic structure crucial interesting projects demonstrated clear natural	 For example people looked at speech recognition applications where the goal is to transcribe a speech  Such a task requires only limited understanding of natural language thus more realistic for example figuring out the exact syntactic structure is probably not very crucial for speech recognition  Two interesting projects that demonstrated clear ability of computer understanding of natural language are worth mentioning.
task language figuring exact probably crucial speech	 Such a task requires only limited understanding of natural language thus more realistic for example figuring out the exact syntactic structure is probably not very crucial for speech recognition.
clear language One Eliza role therapist dialogue	 Two interesting projects that demonstrated clear ability of computer understanding of natural language are worth mentioning  One is the Eliza project where shallow rules are used to enable a computer to play the role of a therapist to engage a natural language dialogue with a human.
The block world demonstrated deep understanding paid realworld inference	 The other is the block world project which demonstrated feasibility of deep semantic understanding of natural language when the language is limited to a toy domain with only blocks as objects  In the 1970s–1980s attention was paid to process realworld naturallanguage text data particularly story understanding  Many formalisms for knowledge representation and heuristic inference rules were developed.
process Many formalisms representation developed.	 In the 1970s–1980s attention was paid to process realworld naturallanguage text data particularly story understanding  Many formalisms for knowledge representation and heuristic inference rules were developed.
conclusion stories confirming largescale knowledge uncertainty After researchers started natural language processing robust real statistical success	 However the general conclusion was that even simple stories are quite challenging to understand by a computer confirming the need for largescale knowledge representation and inferences under uncertainty  After the 1980s researchers started moving away from the traditional symbolic logicbased approaches to natural language processing which mostly had proven to be not robust for real applications and paying more attention to statistical approaches which enjoyed more success initially in speech recognition but later also in virtually all other NLP tasks.
After 1980s moving away symbolic logicbased approaches natural processing robust approaches enjoyed speech later virtually symbolic tend rules advantage patterns 3.	 After the 1980s researchers started moving away from the traditional symbolic logicbased approaches to natural language processing which mostly had proven to be not robust for real applications and paying more attention to statistical approaches which enjoyed more success initially in speech recognition but later also in virtually all other NLP tasks  In contrast to symbolic approaches statistical approaches tend to be more robust because they have less reliance on humangenerated rules instead they often take advantage of regularities and patterns in 3.
2 Systems empirical solely training	2 NLP and Text Information Systems 43 empirical uses of language and rely solely on labeled training data by humans and application of machine learning techniques.
knowledge useful rely statistical machine techniques linguistic These statistical techniques NLP	 While linguistic knowledge is always useful today the most advanced natural language processing techniques tend to rely on heavy use of statistical machine learning techniques with linguistic knowledge only playing a somewhat secondary role  These statistical NLP techniques are successful for some of the NLP tasks.
These NLP successful NLP tasks speech relatively stateoftheart taggers Parsing parsing reasonably high	 These statistical NLP techniques are successful for some of the NLP tasks  Part of speech tagging is a relatively easy task and stateoftheart POS taggers may have a very high accuracy above 97 on news data  Parsing is more difficult though partial parsing can probably be done with reasonably high accuracy e.
tagging task accuracy	 Part of speech tagging is a relatively easy task and stateoftheart POS taggers may have a very high accuracy above 97 on news data.
difficult probably reasonably g 90 recognizing	 Parsing is more difficult though partial parsing can probably be done with reasonably high accuracy e g  above 90 for recognizing noun phrases1.
recognizing However structure remains difficult mainly	g  above 90 for recognizing noun phrases1  However full structure parsing remains very difficult mainly because of ambiguities.
However structure parsing remains Semantic difficult aspects analysis information named entities people organization entities word sense recognizing product	 above 90 for recognizing noun phrases1  However full structure parsing remains very difficult mainly because of ambiguities  Semantic analysis is even more difficult only successful for some aspects of analysis notably information extraction recognizing named entities such as names of people and organization and relations between entities such as who works in which organization word sense disambiguation distinguishing different senses of a word in different contexts of usage and sentiment analysis recognizing positive opinions about a product in a product review.
parsing remains difficult successful notably extraction recognizing named works organization disambiguation distinguishing different word analysis review Inferences speech analysis domains.	 However full structure parsing remains very difficult mainly because of ambiguities  Semantic analysis is even more difficult only successful for some aspects of analysis notably information extraction recognizing named entities such as names of people and organization and relations between entities such as who works in which organization word sense disambiguation distinguishing different senses of a word in different contexts of usage and sentiment analysis recognizing positive opinions about a product in a product review  Inferences and speech act analysis are generally only feasible in very limited domains.
Semantic analysis aspects analysis information extraction recognizing organization relations entities organization word sense disambiguation word contexts recognizing positive Inferences speech act generally limited domains.	 Semantic analysis is even more difficult only successful for some aspects of analysis notably information extraction recognizing named entities such as names of people and organization and relations between entities such as who works in which organization word sense disambiguation distinguishing different senses of a word in different contexts of usage and sentiment analysis recognizing positive opinions about a product in a product review  Inferences and speech act analysis are generally only feasible in very limited domains.
Inferences speech generally feasible domains robust text.	 Inferences and speech act analysis are generally only feasible in very limited domains  In summary only “shallow” analysis of natural language processing can be done for arbitrary text and in a robust manner “deep” analysis tends not to scale up well or be robust enough for analyzing unrestricted text.
cases training data human achieve reasonable 2 Text Systems required robustness tend useful inevitable errors caused general difficulty	 In many cases a significant amount of training data created by human labeling must be available in order to achieve reasonable accuracy  3 2 NLP and Text Information Systems Because of the required robustness and efficiency in TIS applications in general robust shallow NLP techniques tend to be more useful than fragile deep analysis techniques which may hurt application performance due to inevitable analysis errors caused by the general difficulty of NLP.
3 2 robustness efficiency general NLP techniques deep hurt inevitable analysis errors caused difficulty value difficult task natural language	 3 2 NLP and Text Information Systems Because of the required robustness and efficiency in TIS applications in general robust shallow NLP techniques tend to be more useful than fragile deep analysis techniques which may hurt application performance due to inevitable analysis errors caused by the general difficulty of NLP  The limited value of deep NLP for some TIS tasks is further due to various ways to bypass the more difficult task of precisely understanding the meaning of natural language text and directly optimize the task performance.
NLP Information Systems TIS general robust shallow NLP performance errors caused	2 NLP and Text Information Systems Because of the required robustness and efficiency in TIS applications in general robust shallow NLP techniques tend to be more useful than fragile deep analysis techniques which may hurt application performance due to inevitable analysis errors caused by the general difficulty of NLP.
The limited TIS natural text directly improved NLP techniques general TIS performance NLP necessarily notably relatively compared difficult machine translation deep clearly	 The limited value of deep NLP for some TIS tasks is further due to various ways to bypass the more difficult task of precisely understanding the meaning of natural language text and directly optimize the task performance  Thus while improved NLP techniques should in general enable improved TIS task performance lack of NLP capability isn’t necessarily a major barrier for some application tasks notably text retrieval which is a relatively easy task as compared with a more difficult task such as machine translation where deep understanding of natural language is clearly required.
NLP enable task NLP capability barrier notably relatively task translation deep natural language required 3.	 Thus while improved NLP techniques should in general enable improved TIS task performance lack of NLP capability isn’t necessarily a major barrier for some application tasks notably text retrieval which is a relatively easy task as compared with a more difficult task such as machine translation where deep understanding of natural language is clearly required  Figure 3.
easy cases don’t require looking keywords sufficient determine query end machine translation question require precise understanding parse sentence target language structure similarly wrong	 At one end of the spectrum tasks such as retrieval and classification are relatively easy and in most of the cases they don’t require deep NLP indeed looking at the keywords mentioned in text is often sufficient to determine whether a document is relevant to a query or about a certain topic  At the other end however tasks such as machine translation and question answering would require much more precise understanding for example a wrong parse of a sentence generally would lead to a wrong translation unless the target language has a similar ambiguity structure and similarly a wrong understanding of the question would lead to wrong answers.
Men	 Person Men are all alike.
httpwww.	 httpwww.
nott	nott ac.
	ac.
	ukpszgxkcoursesg5aiai002historyeliza.
htm 3 Systems Person	htm 3 2 NLP and Text Information Systems 45 ELIZA  In what way Person They’re always bugging us about something or other.
NLP 45 ELIZA way Person Can specific example Person here.	2 NLP and Text Information Systems 45 ELIZA  In what way Person They’re always bugging us about something or other  ELIZA Can you think of a specific example Person Well my boyfriend made me come here.
ELIZA come He says I’m depressed	 ELIZA Your boyfriend made you come here Person He says I’m depressed much of the time.
Person It’s true.	 Person It’s true.
ELIZA Do think coming help unhappy	 ELIZA Do you think coming here will help you not to be unhappy Person I need some help that much seems certain.
What mean help Perhaps learn	 ELIZA What would it mean to you if you got some help Person Perhaps I could learn to get along with my mother.
On useful engage conversation language rules following going I remember X Do rules directly	 ELIZA Tell me more about your family On the surface the dialogue appears to be quite natural and indeed such a dialogue might be useful to engage a depressed patient in a conversation  However the system does not really understand the language and solely relies on heuristic rules like the following to keep the dialogue going I remember X → Do you often think of X always → Can you think of a specific example Such rules enable the system to directly perform the task i.
heuristic like X → X Such rules directly	 However the system does not really understand the language and solely relies on heuristic rules like the following to keep the dialogue going I remember X → Do you often think of X always → Can you think of a specific example Such rules enable the system to directly perform the task i.
making conversation necessarily meaning words determining sentence.	 making a conversation without necessarily trying to understand the real meaning of words and determining the meaning of the entire sentence.
turned	 Such a patternbased way of solving a problem has turned out to be quite powerful.
Indeed modern natural understanding essentially similar	 Indeed modern machine learning approaches to natural language understanding are essentially based on this and in many ways are similar to the Eliza system but with two important differences.
The rules learning rule training data function instead having supply rules “soft” rules Chapter Text Data automatically data minimum users e.	 The first is that the rules in a machine learning system would not be exact or strict instead they tend to be stochastic and the probabilities of choosing which rule would be empirically set based on a training data set where the expected behavior of a function to be computed is known  Second instead of having human to supply rules the “soft” rules may be learned 46 Chapter 3 Text Data Understanding automatically from the training data with only minimum help from users who can e.
Second instead human learned Text Understanding e	 Second instead of having human to supply rules the “soft” rules may be learned 46 Chapter 3 Text Data Understanding automatically from the training data with only minimum help from users who can e g.
elements tasks like approaches statistical approaches tend robust symbolic	 specify the elements in a rule  Even difficult tasks like machine translation can be done by such statistical approaches  The most useful NLP techniques for building a TIS are statistical approaches which tend to be much more robust than symbolic approaches.
NLP techniques tend robust Statistical language models quantify principled way.	 The most useful NLP techniques for building a TIS are statistical approaches which tend to be much more robust than symbolic approaches  Statistical language models are especially useful because they can quantify the uncertainties associated with the use of natural language in a principled way.
Statistical language especially quantify uncertainties associated use natural principled way.	 Statistical language models are especially useful because they can quantify the uncertainties associated with the use of natural language in a principled way.
Text Representation allow design types features Figure	3 Text Representation Techniques from NLP allow us to design many different types of informative features for text objects  Let’s take a look at the example sentence A dog is chasing a boy on the playground in Figure 3.
This true language.	 This is true for every language.
way representing represent data Unfortunately downside representation semantic text	 This is perhaps the most general way of representing text since we can always use this approach to represent any text data  Unfortunately the downside to this representation is that it can’t allow us to perform semantic analysis which is often needed for many applications of text mining.
can’t allow applications basic units course characters useful general	 Unfortunately the downside to this representation is that it can’t allow us to perform semantic analysis which is often needed for many applications of text mining  We’re not even recognizing words which are the basic units of meaning for any language  Of course there are some situations where characters are useful but that is not the general case.
Of course useful general	 Of course there are some situations where characters are useful but that is not the general case.
representation performing obtain sentence like chasing.	 The next version of text representation is performing word segmentation to obtain a sequence of words  In the example sentence we get features like dog and chasing.
dog	 In the example sentence we get features like dog and chasing.
With level suddenly freedom example easily	 With this level of representation we suddenly have much more freedom  By identifying words we can for example easily discover the most frequent words in this document or the whole collection.
By example document collection words form topics representing text sequence words opens possibilities.	 By identifying words we can for example easily discover the most frequent words in this document or the whole collection  These words can then be used to form topics  Therefore representing text data as a sequence of words opens up a lot of interesting analysis possibilities.
These	 These words can then be used to form topics.
characters Chinese it’s sequence spaces	 However this level of representation is slightly less general than a string of characters  In some languages such as Chinese it’s actually not that easy to identify all the word boundaries since in such a language text is a sequence of characters with no spaces in between words.
In languages word sequence spaces words solve rely perform advanced isn’t words representation string	 In some languages such as Chinese it’s actually not that easy to identify all the word boundaries since in such a language text is a sequence of characters with no spaces in between words  To solve this problem we have to rely on some special techniques to identify words and perform more advanced segmentation that isn’t only based on whitespace which isn’t always 100 accurate  So the sequence of words representation is not as robust as the string of characters representation.
To special identify perform advanced based accurate.	 To solve this problem we have to rely on some special techniques to identify words and perform more advanced segmentation that isn’t only based on whitespace which isn’t always 100 accurate.
So robust string characters In English obtain level	 So the sequence of words representation is not as robust as the string of characters representation  In English it’s very easy to obtain this level of representation so we can use this all the time.
obtain use natural language partofspeech This allows example nouns associated verbs.	 In English it’s very easy to obtain this level of representation so we can use this all the time  If we go further in natural language processing we can add partofspeech POS tags to the words  This allows us to count for example the most frequent nouns or we could determine what kind of nouns are associated with what kind of verbs.
allows count nouns determine kind	 This allows us to count for example the most frequent nouns or we could determine what kind of nouns are associated with what kind of verbs.
opportunities analysis Note Figure 3 use additional features representing sequence tags don’t necessarily replace	 This opens up more interesting opportunities for further analysis  Note in Figure 3 3 that we use a plus sign on the additional features because by representing text as a sequence of part of speech tags we don’t necessarily replace the original word sequence.
way data words tags enriches text data enabling principled analysis.	 Instead we add this as an additional way of representing text data  Representing text as both words and POS tags enriches the representation of text data enabling a deeper more principled analysis.
text enriches representation text deeper analysis If structure opens example styles error	 Representing text as both words and POS tags enriches the representation of text data enabling a deeper more principled analysis  If we go further then we’ll be parsing the sentence to obtain a syntactic structure  Again this further opens up more interesting analysis of for example the writing styles or grammatical error correction.
If sentence obtain syntactic opens interesting writing grammatical correction.	 If we go further then we’ll be parsing the sentence to obtain a syntactic structure  Again this further opens up more interesting analysis of for example the writing styles or grammatical error correction.
If analysis recognize animal We person location deduction chasing	 If we go further still into semantic analysis then we might be able to recognize dog as an animal  We also can recognize boy as a person and playground as a location and analyze their relations  One deduction could be that the dog was chasing the boy and the boy is on the playground.
We recognize location analyze relations deduction playground.	 We also can recognize boy as a person and playground as a location and analyze their relations  One deduction could be that the dog was chasing the boy and the boy is on the playground.
deduction dog chasing boy playground add entityrelation	 One deduction could be that the dog was chasing the boy and the boy is on the playground  This will add more entities and relations through entityrelation recognition.
This entities relations entityrelation recognition Now appears collection person object.	 This will add more entities and relations through entityrelation recognition  Now we can count the most frequent person that appears in this whole collection of news articles  Or whenever you see a mention of this person you also tend to see mentions of another person or object.
Now count frequent appears collection articles Or tend object potentially good	 Now we can count the most frequent person that appears in this whole collection of news articles  Or whenever you see a mention of this person you also tend to see mentions of another person or object  These types of repeated pattens can potentially make very good features.
person mentions person These types good features	 Or whenever you see a mention of this person you also tend to see mentions of another person or object  These types of repeated pattens can potentially make very good features  Such a highlevel representation is even less robust than the sequence of words or POS tags.
types potentially features highlevel robust POS tags identify entities mistakes.	 These types of repeated pattens can potentially make very good features  Such a highlevel representation is even less robust than the sequence of words or POS tags  It’s not always easy to identify all the entities with the right types and we might make mistakes.
representation it’s useful logic predicates inference rules rules interesting derived	 The level of representation is less robust yet it’s very useful  If we move further to a logic representation then we have predicates and inference rules  With inference rules we can infer interesting derived facts from the text.
predicates inference With infer facts text As imagine can’t sentences time large	 If we move further to a logic representation then we have predicates and inference rules  With inference rules we can infer interesting derived facts from the text  As one would imagine we can’t do that all the time for all kinds of sentences since it may take significant computation time or a large amount of training data.
inference infer derived facts imagine kinds computation time training	 With inference rules we can infer interesting derived facts from the text  As one would imagine we can’t do that all the time for all kinds of sentences since it may take significant computation time or a large amount of training data.
speech acts level sentence.	 Finally speech acts would add yet another level of representation of the intent of this sentence.
In example request.	 In this example it might be a request.
allow interesting observer	 Knowing that would allow us to analyze even more interesting things about the observer or the author of this sentence.
intention actions 3 shows generally sophisticated	 What’s the intention of saying that What scenarios or what kinds of actions will occur Figure 3 3 shows that if we move downwards we generally see more sophisticated NLP techniques.
techniques require attempt solve If levels represent deeper analysis language potential errors.	 Unfortunately such techniques would require more human effort as well and they are generally less robust since they attempt to solve a much more difficult problem  If we analyze our text at levels that represent deeper analysis of language then we have to tolerate potential errors.
levels language errors That necessary words right arrow representation closer knowledge mind.	 If we analyze our text at levels that represent deeper analysis of language then we have to tolerate potential errors  That also means it’s still necessary to combine such deep analysis with shallow analysis based on for example sequences of words  On the right side there is an arrow that points down to indicate that as we go down our representation of text is closer to the knowledge representation in our mind.
it’s necessary shallow analysis example sequences words indicate representation text mind Clearly errors direct shadow analysis robust deeper	 That also means it’s still necessary to combine such deep analysis with shallow analysis based on for example sequences of words  On the right side there is an arrow that points down to indicate that as we go down our representation of text is closer to the knowledge representation in our mind  That’s the purpose of text mining Clearly there is a tradeoff here between doing deeper analysis that might have errors but would give us direct knowledge that can be extracted from text and doing shadow analysis that is more robust but wouldn’t give us the necessary deeper representation of knowledge.
points representation closer purpose text tradeoff deeper text shadow wouldn’t	 On the right side there is an arrow that points down to indicate that as we go down our representation of text is closer to the knowledge representation in our mind  That’s the purpose of text mining Clearly there is a tradeoff here between doing deeper analysis that might have errors but would give us direct knowledge that can be extracted from text and doing shadow analysis that is more robust but wouldn’t give us the necessary deeper representation of knowledge.
direct extracted text wouldn’t necessary deeper knowledge data humans consumed humans result mining humans	 That’s the purpose of text mining Clearly there is a tradeoff here between doing deeper analysis that might have errors but would give us direct knowledge that can be extracted from text and doing shadow analysis that is more robust but wouldn’t give us the necessary deeper representation of knowledge  Text data are generated by humans and are meant to be consumed by humans  As a result in text data analysis and text mining humans play a very important role.
As result analysis mining humans important role They meaning it’s	 As a result in text data analysis and text mining humans play a very important role  They are always in the loop meaning that we should optimize for a collaboration between humans and computers  In that sense it’s okay that computers may not be able to have a completely accurate representation of text data.
loop meaning collaboration humans computers okay completely accurate Patterns accurate analysis data learning effectively.	 They are always in the loop meaning that we should optimize for a collaboration between humans and computers  In that sense it’s okay that computers may not be able to have a completely accurate representation of text data  Patterns that are extracted from text data can be interpreted by humans and then humans can guide the computers to do more accurate analysis by annotating more data guiding machine learning programs to make them work more effectively.
extracted text data humans accurate annotating guiding machine learning Different different analyses shown	 Patterns that are extracted from text data can be interpreted by humans and then humans can guide the computers to do more accurate analysis by annotating more data guiding machine learning programs to make them work more effectively  Different text representation tends to enable different analyses as shown in Figure 3.
text representation tends analyses	 Different text representation tends to enable different analyses as shown in Figure 3 4.
In add deeper results represent	4  In particular we can gradually add more and more deeper analysis results to represent text data that would open up more interesting representation opportunities and analysis capabilities.
add represent text representation opportunities The summarizes type text second generality representation By representation accurately data general	 In particular we can gradually add more and more deeper analysis results to represent text data that would open up more interesting representation opportunities and analysis capabilities  The table summarizes what we have just seen the first column shows the type of text representation while the second visualizes the generality of such a representation  By generality we mean whether we can do this kind of representation accurately for all the text data very general or only some of them not very general.
generality accurately text data general enabled shows applications level representation.	 By generality we mean whether we can do this kind of representation accurately for all the text data very general or only some of them not very general  The third column shows the enabled analysis techniques and the final column shows some examples of applications that can be achieved with a particular level of representation.
column shows final column applications achieved representation As text processed string processing algorithms.	 The third column shows the enabled analysis techniques and the final column shows some examples of applications that can be achieved with a particular level of representation  As a sequence of characters text can only be processed by string processing algorithms.
robust general In compression application don’t need word boundaries	 They are very robust and general  In a compression application we don’t need to know word boundaries although knowing word boundaries might actually help.
expect enabled kinds	 As you may expect many applications can be enabled by these kinds of analysis.
thesaurus discovering related topic	 For example thesaurus discovery has to do with discovering related words and topic and opinionrelated applications can also be based on wordlevel representation.
People topics covered collection topic distribution we’ll gradually add additional	 People might be interested in knowing major topics covered in the collection of text where a topic is represented as a distribution over words  Moving down we’ll see we can gradually add additional representations.
syntactic graph analyze	 By adding syntactic structures we can enable syntactic graph analysis we can use graph mining algorithms to analyze these syntactic graphs.
structurebased help objects categories syntactic structures If classify articles look structures add relations information	 We can also generate structurebased features that might help us classify the text objects into different categories by looking at their different syntactic structures  If you want to classify articles into different categories corresponding to different authors then you generally need to look at syntactic structures  When we add entities and relations then we can enable other techniques such as knowledge graphs or information networks.
If want classify categories authors look structures.	 If you want to classify articles into different categories corresponding to different authors then you generally need to look at syntactic structures.
When techniques advanced deal	 When we add entities and relations then we can enable other techniques such as knowledge graphs or information networks  Using these more advanced feature representations allows applications that deal with entities.
advanced feature logical	 Using these more advanced feature representations allows applications that deal with entities  Finally when we add logical predicates we can integrate analysis of scattered knowledge.
Finally add logical integrate For example extracted text inferences.	 Finally when we add logical predicates we can integrate analysis of scattered knowledge  For example we can add an ontology on top of extracted information from text to make inferences.
A good example application enabled representation relevant knowledge gene inferences in.	 A good example of an application enabled by this level of representation is a knowledge assistant for biologists  This system is able to manage all the relevant knowledge from literature about a research problem such as understanding gene functions  The computer can make inferences about some of the hypotheses that a biologist might be interested in.
certain function reading It logic questions genes functions support far	 For example it could determine whether a gene has a certain function by reading literature to extract relevant facts  It could use a logic system to track answers to researchers’ questions about what genes are related to what functions  In order to support this level of application we need to go as far as logical representation.
In order support level logical This covers mainly focused representation.	 In order to support this level of application we need to go as far as logical representation  This book covers techniques mainly focused on wordbased representation.
book covers focused techniques general robust virtually mining	 This book covers techniques mainly focused on wordbased representation  These techniques are general and robust and widely used in various applications  In fact in virtually all text mining applications you need this level of representation.
widely applications fact levels sophisticated applications needed.	 These techniques are general and robust and widely used in various applications  In fact in virtually all text mining applications you need this level of representation  Still other levels can be combined in order to support more linguistically sophisticated applications as needed.
virtually mining applications need level Still order linguistically	 In fact in virtually all text mining applications you need this level of representation  Still other levels can be combined in order to support more linguistically sophisticated applications as needed  3.
3.	 3.
4 Statistical Language Models model short distribution	4 Statistical Language Models A statistical language model or just language model for short is a probability distribution over word sequences.
example threeword different pToday Wednesday 0.	 For example a language model may give the following threeword sequences different probabilities pToday is Wednesday  0 001 pToday Wednesday is  0.
pToday Wednesday 000000001 solution 0.	001 pToday Wednesday is  0 000000001 pThe equation has a solution  0.
solution 0.	000000001 pThe equation has a solution  0.
language describing general conversations inaccurate describing conversations mathematics conference solution Today	 This may be a reasonable language model for describing general conversations but it may be inaccurate for describing conversations happening at a mathematics conference where the sequence The equation has a solution may occur more frequently than Today is Wednesday.
Given language model word sequences sample In use model text model model text.	 Given a language model we can sample word sequences according to the distribution to obtain a text sample  In this sense we may use such a model to “generate” text  Thus a language model is also often called a generative model for text.
In model text model generative useful A general principled uncertainties natural language.	 In this sense we may use such a model to “generate” text  Thus a language model is also often called a generative model for text  Why is a language model useful A general answer is that it provides a principled way to quantify the uncertainties associated with the use of natural language.
called generative text language model useful A general answer principled quantify uncertainties	 Thus a language model is also often called a generative model for text  Why is a language model useful A general answer is that it provides a principled way to quantify the uncertainties associated with the use of natural language.
language A general way quantify uncertainties use	 Why is a language model useful A general answer is that it provides a principled way to quantify the uncertainties associated with the use of natural language.
allows text information	 More specifically it allows us to answer many interesting questions related to text analysis and information retrieval.
questions language answer.	 The following are some examples of questions that a language model can help answer.
Given feels likely habit word question recognition acoustic easily suggest happy John	   Given that we see John and feels how likely will we see happy as opposed to habit as the next word Answering this question can help speech recognition as happy and habit have very similar acoustic signals but a language model can easily suggest that John feels happy is far more likely than John feels habit.
feels habit word question help speech recognition happy similar acoustic model John far likely habit.	 Given that we see John and feels how likely will we see happy as opposed to habit as the next word Answering this question can help speech recognition as happy and habit have very similar acoustic signals but a language model can easily suggest that John feels happy is far more likely than John feels habit.
baseball game article likely “sports” tasks .	   Given that we observe baseball three times and game once in a news article how likely is it about the topic “sports” This will obviously directly help text categorization and information retrieval tasks  .
news article likely topic directly text categorization retrieval tasks interested news likely user use query This directly information	 Given that we observe baseball three times and game once in a news article how likely is it about the topic “sports” This will obviously directly help text categorization and information retrieval tasks    Given that a user is interested in sports news how likely would it be for the user to use baseball in a query This is directly related to information retrieval.
interested sports news likely baseball This	   Given that a user is interested in sports news how likely would it be for the user to use baseball in a query This is directly related to information retrieval.
If enumerate possible words sequence model number parameters infinite word	 If we enumerate all the possible sequences of words and give a probability to each sequence the model would be too complex to estimate because the number of parameters is potentially infinite since we have a potentially infinite number of word sequences  That is we would never have enough data to estimate these parameters.
Thus assumptions language model language model word sequence independently sequence words product probability word.	 Thus we have to make assumptions to simplify the model  The simplest language model is the unigram language model in which we assume that a word sequence results from generating each word independently  Thus the probability of a sequence of words would be equal to the product of the probability of each word.
let V	 Formally let V be the set of words in the vocabulary and w1 .
	   .
	 .
We .	 We have pw1   .
wn n∏ pwi	    wn n∏ i1 pwi  3.
model words satisfy pw 1 model essentially multinomial	 3 1 Given a unigram language model θ  we have as many parameters as the words in the vocabulary and they satisfy the constraint ∑ w∈V pw  1  Such a model essentially specifies a multinomial distribution over all the words.
1 unigram language vocabulary ∑ w∈V specifies distribution words model documents different	1 Given a unigram language model θ  we have as many parameters as the words in the vocabulary and they satisfy the constraint ∑ w∈V pw  1  Such a model essentially specifies a multinomial distribution over all the words  Given a language model θ  in general the probabilities of generating two different documents D1 and D2 would be different i.
Such model multinomial	 Such a model essentially specifies a multinomial distribution over all the words.
θ probabilities generating different D1 D2 different e.	 Given a language model θ  in general the probabilities of generating two different documents D1 and D2 would be different i e.
	e  pD1  θ  pD2  θ.
documents occurrences sense words indicate topic	 pD1  θ  pD2  θ  What kind of documents would have higher probabilities Intuitively it would be those documents that contain many occurrences of the high probability words according to pw  θ  In this sense the high probability words of θ can indicate the topic captured by θ .
What higher documents high probability according pw sense words θ topic captured illustrated Figure 3.	 What kind of documents would have higher probabilities Intuitively it would be those documents that contain many occurrences of the high probability words according to pw  θ  In this sense the high probability words of θ can indicate the topic captured by θ   For example the two unigram language models illustrated in Figure 3.
probability θ topic captured For language models Figure	 In this sense the high probability words of θ can indicate the topic captured by θ   For example the two unigram language models illustrated in Figure 3.
example language models Figure topic “text “health”	 For example the two unigram language models illustrated in Figure 3 5 suggest a topic about “text mining” and a topic about “health” respectively.
5 topic “health” text mining paper expect pD D′ blog control pD′ θ1 pD′ We expect θ2	5 suggest a topic about “text mining” and a topic about “health” respectively  Intuitively if D is a text mining paper we would expect pD  θ1  pD  θ2 while if D′ is a blog article discussing diet control we would expect the opposite pD′  θ1  pD′  θ2  We can also expect pD  θ1  pD′  θ1 and pD  θ2  pD′  θ2.
We pD′ pD D e g.	 We can also expect pD  θ1  pD′  θ1 and pD  θ2  pD′  θ2  Now suppose we have observed a document D e g.
observed D g.	 Now suppose we have observed a document D e g.
g.	g.
short abstract text assumed generated model θ like underlying model θ	 a short abstract of a text mining paper which is assumed to be generated using a unigram language model θ  and we would like to infer the underlying model θ i e.
probabilities w pw based observed D This problem statistics called estimation different popular maximum estimator seeks θ̂ likelihood	 estimate the probabilities of each word w pw  θ based on the observed D  This is a standard problem in statistics called parameter estimation and can be solved using many different methods  One popular method is the maximum likelihood ML estimator which seeks a model θ̂ that would give the observed data the highest likelihood i.
method estimator observed	 One popular method is the maximum likelihood ML estimator which seeks a model θ̂ that would give the observed data the highest likelihood i e.
best explain data	 best explain the data θ̂  arg maxθ pD  θ.
2 easy estimate unigram frequency	2 It is easy to show that the ML estimate of a unigram language model gives each word a probability equal to its relative frequency in D.
count word w D words estimate optimal observed data optimal application questionable.	3 3 4 Statistical Language Models 53 where cw D is the count of word w in D and D is the length of D or total number of words in D  Such an estimate is optimal in the sense that it would maximize the probability of the observed data but whether it is really optimal for an application is still questionable.
For example goal estimate author estimate model clearly nonoptimal estimated model zero article zero	 For example if our goal is to estimate the language model in the mind of an author of a research article and we use the maximum likelihood estimator to estimate the model based only on the abstract of a paper then it is clearly nonoptimal since the estimated model would assign zero probability to any unseen words in the abstract which would make the whole article have a zero probability unless it only uses words in the abstract.
maximum estimator Although simple language model example 3.	 We will discuss various techniques for improving the maximum likelihood estimator later by using techniques called smoothing  Although extremely simple a unigram language model is already very useful for text analysis  For example Figure 3.
Although simple unigram	 Although extremely simple a unigram language model is already very useful for text analysis.
example 3 shows unigram language estimated text data i.	 For example Figure 3 6 shows three different unigram language models estimated on three different text data samples i.
language estimated data	6 shows three different unigram language models estimated on three different text data samples i.
general highest functional words	 In general the words with the highest probabilities in all the three models are those functional words in English because such words are frequently used in any text.
After going list words Such content depending estimation different text samples perform semantic	 After going further down on the list of words one would see more contentcarrying and topical words  Such content words would differ dramatically depending on the data to be used for the estimation and thus can be used to discriminate the topics in different text samples  Unigram language models can also be used to perform semantic analysis of word relations.
words differ dramatically depending discriminate topics text samples Unigram language models word example	 Such content words would differ dramatically depending on the data to be used for the estimation and thus can be used to discriminate the topics in different text samples  Unigram language models can also be used to perform semantic analysis of word relations  For example we can use them to find what words are semantically associated with a word like computer.
Unigram models perform semantic analysis word relations.	 Unigram language models can also be used to perform semantic analysis of word relations.
Specifically obtain sample documents sentences	 Specifically we can first obtain a sample of documents or sentences where computer is mentioned.
We language model based sample pw tells occur However according model likely simply common data	 We can then estimate a language model based on this sample to obtain pw  computer  This model tells us which words occur frequently in the context of “computer ” However the most frequent words according to this model would likely be functional words in English or words that are simply common in the data but have no strong association with computer.
This occur frequently context	 This model tells us which words occur frequently in the context of “computer.
It general English background language model serve	 It is easy to see that the general English language model i e  a background language model would serve the purpose well.
normalize model pw probability word.	 So we can use the background language model to normalize the model pw  computer and obtain a probability ratio for each word.
Words high ratio values semantically occur frequently frequently	 Words with high ratio values can then be assumed to be semantically associated with computer since they tend to occur frequently in its context but not frequently in general.
More language models text information applications appear chapters documents queries language	 More applications of language models in text information systems will be further discussed as their specific applications appear in later chapters  For example we can represent both documents and queries as being generated from some language model.
introduction Sarawagi 2012.	 A comprehensive introduction to this topic can be found in Sarawagi 2008 and a useful survey can be found in Jiang 2012.
discussion information Moens 2006.	 For a discussion of this topic in the context of information retrieval see the book Moens 2006.
Unified Toolkit Analysis This accompanying	 4META A Unified Toolkit for Text Data Management and Analysis This chapter introduces the accompanying software META a free and opensource toolkit that can be used to analyze text data.
book exercises META practice different mining methods	 Throughout this book we give handson exercises with META to practice concepts and explore different text mining algorithms  Most of the algorithms and methods discussed in this book can be found in some form in the META toolkit.
Most form META include likely toolkit simplicity additions extending class Configuration files META’s infrastructure.	 Most of the algorithms and methods discussed in this book can be found in some form in the META toolkit  If META doesn’t include a technique discussed in this book it’s likely that a chapter exercise is to implement this feature yourself Despite being a powerful toolkit META’s simplicity makes feature additions relatively straightforward usually through extending a short class hierarchy  Configuration files are an integral part of META’s forwardfacing infrastructure.
META doesn’t technique book it’s implement feature powerful toolkit feature relatively usually short class	 If META doesn’t include a technique discussed in this book it’s likely that a chapter exercise is to implement this feature yourself Despite being a powerful toolkit META’s simplicity makes feature additions relatively straightforward usually through extending a short class hierarchy.
Configuration forwardfacing infrastructure.	 Configuration files are an integral part of META’s forwardfacing infrastructure.
META executables	 By default META is packaged with various executables that can be used to solve a particular task.
example user run following terminal1 classify config.	 For example for a classification experiment the user would run the following command in their terminal1  classify config.
standard configuration	toml This is standard procedure for using the default executables they take only one configuration file parameter.
configuration explained later chapter user select	 The configuration file format is explained in detail later in this chapter but essentially it allows the user to select a dataset a way 1.
If advanced functionality desired programming required META’s API applications	 If more advanced functionality is desired programming in C is required to make calls to META’s API applications programming interface.
Both configuration	 Both configuration file and API usage are documented on META’s website httpsmetatoolkit.
org Additionally META httpsforum meta	org as well as in this chapter  Additionally a forum for META exists httpsforum meta toolkit.
toolkit.	 Additionally a forum for META exists httpsforum meta toolkit.
discussion surrounding toolkit includes topics sections delve little reader working future	org containing discussion surrounding the toolkit  It includes user support topics communitywritten documentation and developer discussions  The sections that follow delve into a little more detail about particular aspects of META so the reader will be comfortable working with it in future chapters.
It support communitywritten developer	 It includes user support topics communitywritten documentation and developer discussions.
delve particular META reader working Philosophy complement body information	 The sections that follow delve into a little more detail about particular aspects of META so the reader will be comfortable working with it in future chapters  4 1 Design Philosophy META’s goal is to improve upon and complement the current body of open source machine learning and information retrieval software.
META’s current open information software source	1 Design Philosophy META’s goal is to improve upon and complement the current body of open source machine learning and information retrieval software  The existing environment of this open source software tends to be quite fragmented.
The open wide variety et al.	 The existing environment of this open source software tends to be quite fragmented  There is rarely a single location for a wide variety of algorithms a good example of this is the LIBLINEAR Fan et al.
SVMs SVMs focuses presented kernels Chang 2011.	 2008 software package for SVMs  While this is the most cited of the open source implementations of linear SVMs it focuses solely on kernelless methods  If presented with a nonlinear classification problem one would be forced to find a different software package that supports kernels such as the same authors’ LIBSVM package Chang and Lin 2011.
solely kernelless If presented classification kernels authors’ Chang This places undue researchers understanding forced understand fragmented appropriate implementations configure tool.	 While this is the most cited of the open source implementations of linear SVMs it focuses solely on kernelless methods  If presented with a nonlinear classification problem one would be forced to find a different software package that supports kernels such as the same authors’ LIBSVM package Chang and Lin 2011  This places an undue burden on the researchers and students—not only are they required to have a detailed understanding of the research problem at hand but they are now forced to understand this fragmented nature of the opensource software community find the appropriate tools in this mishmash of implementations and compile and configure the appropriate tool.
If classification supports kernels package undue burden researchers required hand understand fragmented nature community tools mishmash compile tool problem data tools single input format certain	 If presented with a nonlinear classification problem one would be forced to find a different software package that supports kernels such as the same authors’ LIBSVM package Chang and Lin 2011  This places an undue burden on the researchers and students—not only are they required to have a detailed understanding of the research problem at hand but they are now forced to understand this fragmented nature of the opensource software community find the appropriate tools in this mishmash of implementations and compile and configure the appropriate tool  Even when this is all done there is the problem of data formatting—it is unlikely that the tools have standardized upon a single input format so a certain amount of data preprocessing is now required.
This undue required understanding software appropriate appropriate Even problem data standardized single input certain detracts task impact education.	 This places an undue burden on the researchers and students—not only are they required to have a detailed understanding of the research problem at hand but they are now forced to understand this fragmented nature of the opensource software community find the appropriate tools in this mishmash of implementations and compile and configure the appropriate tool  Even when this is all done there is the problem of data formatting—it is unlikely that the tools have standardized upon a single input format so a certain amount of data preprocessing is now required  This all detracts from the actual task at hand which has a marked impact on the speed of discovery and education.
formatting—it standardized certain preprocessing required This actual task marked discovery	 Even when this is all done there is the problem of data formatting—it is unlikely that the tools have standardized upon a single input format so a certain amount of data preprocessing is now required  This all detracts from the actual task at hand which has a marked impact on the speed of discovery and education  META addresses these issues.
This marked impact speed discovery education issues.	 This all detracts from the actual task at hand which has a marked impact on the speed of discovery and education  META addresses these issues.
In particular unifying framework text analysis methods controlled experiments.	 In particular it provides a unifying framework for text indexing and analysis methods allowing users to quickly run controlled experiments.
It feature instance formats algorithm students effort	 It modularizes the feature generation instance representation data storage formats and algorithm implementations this allows for researchers and students to make seamless transitions along any of these dimensions with minimal effort  4.
4 2 META’s modularity supports visibility inner	 4 2 Setting up META 59 META’s modularity supports exploration encourages contributions and increases visibility to its inner workings.
Setting META 59 supports exploration workings.	2 Setting up META 59 META’s modularity supports exploration encourages contributions and increases visibility to its inner workings.
perfect toolkit beginning chapter follow add real After reading book learning text analysis META suit information needs building	 These facts make it a perfect companion toolkit for this book  As mentioned at the beginning of the chapter readers will follow exercises that add real functionality to the toolkit  After reading this book and learning about text data management and analysis it is envisioned readers continue to modify META to suit their text information needs building upon their newfound knowledge.
As chapter exercises real reading text readers suit knowledge free jointly	 As mentioned at the beginning of the chapter readers will follow exercises that add real functionality to the toolkit  After reading this book and learning about text data management and analysis it is envisioned readers continue to modify META to suit their text information needs building upon their newfound knowledge  Finally since META will always be free and opensource readers as a community can jointly contribute to its functionality benefiting all those involved.
After book learning text envisioned modify suit information community contribute benefiting	 After reading this book and learning about text data management and analysis it is envisioned readers continue to modify META to suit their text information needs building upon their newfound knowledge  Finally since META will always be free and opensource readers as a community can jointly contribute to its functionality benefiting all those involved.
2 META future META downloaded	 4 2 Setting up META All future sections in this book will assume the reader has META downloaded and installed.
Setting book downloaded installed Here	2 Setting up META All future sections in this book will assume the reader has META downloaded and installed  Here we’ll show how to set up META.
META actually download toolkit users software git line prerequisites The website contains instructions software particular	 META has both a website with tutorials and an online repository on GitHub  To actually download the toolkit users will check it out with the version control software git in their command line terminal after installing any necessary prerequisites  The META website contains instructions for downloading and setting up the software for a particular system configuration.
The website instructions setting configuration At time book httpsmetatoolkit.	 The META website contains instructions for downloading and setting up the software for a particular system configuration  At the time of writing this book both Linux and Mac OS are supported  Visit httpsmetatoolkit.
httpsmetatoolkit.	 At the time of writing this book both Linux and Mac OS are supported  Visit httpsmetatoolkit.
follow instructions desired	 Visit httpsmetatoolkit orgsetupguide html and follow the instructions for the desired platform.
html platform.	html and follow the instructions for the desired platform.
performed setup guide working guide sure reader META published.	 We will assume the reader has performed the steps listed in the setup guide and has a working version of META for all exercises and demonstrations in this book  There are two steps that are not mentioned in the setup guide  The first is to make sure the reader has the version of META that was current when this book was published.
steps setup guide.	 There are two steps that are not mentioned in the setup guide.
The reader META	 The first is to make sure the reader has the version of META that was current when this book was published.
2	2 0.
0 Run meta directory git reset hard v2.	0  Run the following command inside the meta directory git reset hard v2.
Run following command directory hard v2.	 Run the following command inside the meta directory git reset hard v2.
2.	2.
releases page httpsgithub toolkitmetareleasestagv2	 These are available on the META releases page httpsgithub commeta toolkitmetareleasestagv2 2.
commeta toolkitmetareleasestagv2.	commeta toolkitmetareleasestagv2.
files metabuild place paths config file	 By default the model files should be placed in the metabuild directory but you can place them anywhere as long as the paths in the config file are updated.
If needed accompanying 4.	 If any additional files or information are needed it will be provided in the accompanying section  4.
Architecture processed	 4 3 Architecture All processed data in META is stored in an index.
All data META forwardindex invertedindex IDs keyed IDs.	3 Architecture All processed data in META is stored in an index  There are two index types forwardindex and invertedindex  The former is keyed by document IDs and the latter is keyed by term IDs.
types forwardindex keyed IDs IDs.	 There are two index types forwardindex and invertedindex  The former is keyed by document IDs and the latter is keyed by term IDs.
keyed IDs keyed forwardindex modeling clas	 The former is keyed by document IDs and the latter is keyed by term IDs  forwardindex is used for applications such as topic modeling and most clas sification tasks.
forwardindex topic modeling tasks search engines k nearestneighbor Since application data interchangeable	 forwardindex is used for applications such as topic modeling and most clas sification tasks  invertedindex is used to create search engines or do classification with k nearestneighbor or similar algorithms  Since each META application takes an index as input all processed data is interchangeable between all the components.
invertedindex classification similar Since META index processed data interchangeable components.	 invertedindex is used to create search engines or do classification with k nearestneighbor or similar algorithms  Since each META application takes an index as input all processed data is interchangeable between all the components.
Since processed components.	 Since each META application takes an index as input all processed data is interchangeable between all the components.
advantage META classification If like assume cache sacrificing Index engine exercises.	 This also gives a great advantage to classification META supports outofcore classification by default If a dataset is small enough like most other toolkits assume a cache can be used such as no evictcache to keep it all in memory without sacrificing any speed  Index usage is explained in much more detail in the search engine exercises.
Index explained There corpus formats	 Index usage is explained in much more detail in the search engine exercises  There are four corpus input formats  linecorpus.
linecorpus.	 linecorpus.
document corpusname.	dat  each document appears on one line corpusname.
appears	 each document appears on one line corpusname dat.
class label corpusname.	dat labels  optional file that includes the class or label of the document on each line again corresponding to the order in corpusname.
labels optional class line corresponding order	labels  optional file that includes the class or label of the document on each line again corresponding to the order in corpusname dat.
	 There is also a corpusnamefullcorpus.
placeholder required e	 If there are no class labels a placeholder label should be required e g  “none”.
g.	g.
linecorpus files gzip corpusname.	 “none”  gzcorpus  similar to linecorpus but each of its files and metadata files are compressed using gzip compression corpusname.
files dat.	 similar to linecorpus but each of its files and metadata files are compressed using gzip compression corpusname dat.
dat version	dat gz  compressed version of corpusname.
corpusname	 compressed version of corpusname dat corpusname dat.
	dat corpusname dat labels.
	labels.
	gz.
	 compressed version of corpusname.
dat labels	 dat labels 4 4 Tokenization with META 61 libsvmcorpus.
META 61	labels 4 4 Tokenization with META 61 libsvmcorpus.
If META There datasets format information corpus configuration consult	 If only being used for classification META can also take LIBSVMformatted input to create a forwardindex  There are many ma chine learning datasets available in this format on the LIBSVM site 2 For more information on corpus storage and configuration settings we suggest the reader consult httpsmetatoolkit.
information corpus configuration reader orgoverviewtutorial.	2 For more information on corpus storage and configuration settings we suggest the reader consult httpsmetatoolkit orgoverviewtutorial.
META index “tokenization” process high level simply corpus.	4 Tokenization with META The first step in creating an index over any sort of text data is the “tokenization” process  At a high level this simply means converting individual text documents into sparse vectors of counts of terms—these sparse vectors are then typically consumed by an indexer to output an invertedindex over your corpus.
META structures text order power control text An analyzer “filter final tokenization process class followed zero reads	 META structures this text analysis process into several layers in order to give the user as much power and control over the way the text is analyzed as possible  An analyzer in most cases will take a “filter chain” that is used to generate the final tokens for its tokenization process the filter chains are always defined as a specific tokenizer class followed by a sequence of zero or more filter classes each of which reads from the previous class’s output.
“filter generate tokens tokenization specific class sequence zero previous	 An analyzer in most cases will take a “filter chain” that is used to generate the final tokens for its tokenization process the filter chains are always defined as a specific tokenizer class followed by a sequence of zero or more filter classes each of which reads from the previous class’s output.
For example lowercases tokens certain range lowercasefilter → first.	 For example here is a simple filter chain that lowercases all tokens and only keeps tokens with a certain length range icutokenizer → lowercasefilter → lengthfilter Tokenizers always come first.
split	 They define how to split a document’s string content into tokens.
	 icutokenizer.
following Unicode word	 converts documents into streams of tokens by following the Unicode standards for sentence and word segmentation  charactertokenizer.
converts Filters chained together.	 charactertokenizer  converts documents into streams of single characters  Filters come next and can be chained together.
Filters come chained	 Filters come next and can be chained together.
certain length	 lengthfilter  this filter accepts tokens that are within a certain length and rejects those that are not  icufilter.
tokens transliteration token	 this filter accepts tokens that are within a certain length and rejects those that are not  icufilter  applies an ICU International Components for Unicode3 transliteration to each token in the sequence.
icufilter transliteration For character like written	 icufilter  applies an ICU International Components for Unicode3 transliteration to each token in the sequence  For example an accented character like ı̈ is instead written as i.
applies Unicode3 transliteration token sequence example instead written	 applies an ICU International Components for Unicode3 transliteration to each token in the sequence  For example an accented character like ı̈ is instead written as i.
For character like ı̈ instead	 For example an accented character like ı̈ is instead written as i.
	 2  httpwww csie.
httpwww.	 httpwww.
csie	csie ntu.
	ntu.
edu	edu twcjlinlibsvmtoolsdatasets 3.
	 httpsite.
	csie.
icuproject.	edu twcjlinlibsvmtoolsdatasets httpsite icuproject.
4 META A Text Data Analysis listfilter.	icuproject org 62 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis listfilter.
org 62 A Data Analysis listfilter accepts rejects tokens For list reject stop words.	org 62 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis listfilter  this filter either accepts or rejects tokens based on a list  For example one could use a stop word list and reject stop words.
rejects example word reject words porter2stemmer.	 this filter either accepts or rejects tokens based on a list  For example one could use a stop word list and reject stop words  porter2stemmer.
	 porter2stemmer.
Analyzers operate token counts documents.	4 Analyzers operate on the output from the filter chain and produce token counts from documents.
analyzers.	 Here are some examples of analyzers.
counts words	 ngramwordanalyzer  Collects and counts sequences of n words tokens that have been filtered by the filter chain  ngramposanalyzer.
Collects sequences words filter ngramposanalyzer Same tags META’s	 Collects and counts sequences of n words tokens that have been filtered by the filter chain  ngramposanalyzer  Same as ngramwordanalyzer but operates on part ofspeech tags from META’s CRF implementation.
Same operates ofspeech implementation	 ngramposanalyzer  Same as ngramwordanalyzer but operates on part ofspeech tags from META’s CRF implementation  treeanalyzer.
Same ngramwordanalyzer META’s	 Same as ngramwordanalyzer but operates on part ofspeech tags from META’s CRF implementation.
treeanalyzer Collects features.	 treeanalyzer  Collects and counts occurrences of parse tree features.
Collects counts occurrences parse	 Collects and counts occurrences of parse tree features.
	 libsvmanalyzer.
Converts linecorpus sane filter users use general text requirements.	 Converts a LIBSVM linecorpus into META format  META defines a sane default filter chain that users are encouraged to use for general text analysis in the absence of any specific requirements.
META defines sane default use specify following configuration file analyzers ngramword ngram filter This text analysis words running This filter chain work languages operations including tokenization boundary detection defined Unicode possible.	 META defines a sane default filter chain that users are encouraged to use for general text analysis in the absence of any specific requirements  To use it one should specify the following in the configuration file analyzers method  ngramword ngram  1 filter  defaultchain This configures the text analysis process to consider unigrams of words gener ated by running each document through the default filter chain  This filter chain should work well for most languages as all of its operations including but not lim ited to tokenization and sentence boundary detection are defined in terms of the Unicode standard wherever possible.
To specify following ngramword ngram filter This analysis process words ated default filter This filter languages including ited tokenization sentence boundary possible To unigrams like analyzers ngram analyzers	 To use it one should specify the following in the configuration file analyzers method  ngramword ngram  1 filter  defaultchain This configures the text analysis process to consider unigrams of words gener ated by running each document through the default filter chain  This filter chain should work well for most languages as all of its operations including but not lim ited to tokenization and sentence boundary detection are defined in terms of the Unicode standard wherever possible  To consider both unigrams and bigrams the configuration file should look like the following analyzers method  ngramword ngram  1 filter  defaultchain analyzers 4.
bigrams configuration file look following method ngram 4	 To consider both unigrams and bigrams the configuration file should look like the following analyzers method  ngramword ngram  1 filter  defaultchain analyzers 4  httpsnowball.
httpsnowball.	 httpsnowball.
tartarus.	tartarus.
html tartarus.	orgalgorithmsenglishstemmer html httpsnowball tartarus.
	html httpsnowball.
	tartarus orgalgorithmsenglishstemmer.
Tokenization 63 defaultchain Each corresponding fil ter chain tokens generated vector counts.	orgalgorithmsenglishstemmer html 4 4 Tokenization with META 63 method  ngramword ngram  2 filter  defaultchain Each analyzers block defines a single analyzer and its corresponding fil ter chain as many can be used as desired—the tokens generated by each analyzer specified will be counted and placed in a single sparse vector of counts.
4 META 2 defaultchain Each analyzers block defines fil generated analyzer counted sparse	4 Tokenization with META 63 method  ngramword ngram  2 filter  defaultchain Each analyzers block defines a single analyzer and its corresponding fil ter chain as many can be used as desired—the tokens generated by each analyzer specified will be counted and placed in a single sparse vector of counts.
example following words bigram tree skeleton subtree	 For example the following configuration would combine unigram words bigram partofspeech tags tree skeleton features and subtree features.
ngram defaultchain method features skel subtree pathtogreedytaggermodel specific text di rectly file Instead filter parameter look analyzers filter defines filter	 analyzers method  ngramword ngram  1 filter  defaultchain analyzers method  ngrampos ngram  2 filter  type  icutokenizer type  ptbnormalizer crfprefix  pathtocrfmodel analyzers method  tree filter  type  icutokenizer type  ptbnormalizer features  skel subtree tagger  pathtogreedytaggermodel parser  pathtosrparsermodel If an application requires specific text analysis operations one can specify di rectly what the filter chain should look like by modifying the configuration file  Instead of filter being a string parameter as above we will change filter to look very much like the analyzers blocks each analyzer will have a series of ana lyzers filter blocks each of which defines a step in the filter chain.
filter string change like analyzer ana step filter	 Instead of filter being a string parameter as above we will change filter to look very much like the analyzers blocks each analyzer will have a series of ana lyzers filter blocks each of which defines a step in the filter chain.
All filter chains filter chain like section analyzers method ngramword type 64 Chapter Unified Data Analysis analyzers.	 All filter chains must start with a tokenizer  Here is an example filter chain for unigram words like the one at the beginning of this section analyzers method  ngramword ngram  1 analyzers filter type  icutokenizer 64 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis analyzers.
words method filter Toolkit Data Management Analysis analyzers.	 Here is an example filter chain for unigram words like the one at the beginning of this section analyzers method  ngramword ngram  1 analyzers filter type  icutokenizer 64 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis analyzers.
filter type icutokenizer Chapter A Toolkit Text Data Management filter type 2 max META classes support chains.	filter type  icutokenizer 64 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis analyzers filter type  lowercase analyzers filter type  length min  2 max  35 META provides many different classes to support building filter chains.
filter lowercase length 2 35 different classes support filter chains.	filter type  lowercase analyzers filter type  length min  2 max  35 META provides many different classes to support building filter chains.
filter type min 35 META provides support	filter type  length min  2 max  35 META provides many different classes to support building filter chains.
Please API information.	 Please look at the API documentation5 for more information.
id given class string needed “type” file 4 5 Toolkits Existing supporting fall	 The static public attribute id for a given class is the string needed for the “type” in the configuration file  4 5 Related Toolkits Existing toolkits supporting text management and analysis tend to fall into two categories.
5 Related Existing text analysis tend fall engine especially application limited support text analysismining functions.	 4 5 Related Toolkits Existing toolkits supporting text management and analysis tend to fall into two categories  The first is search engine toolkits which are especially suitable for building a search engine application but tend to have limited support for text analysismining functions.
Related Existing management tend	5 Related Toolkits Existing toolkits supporting text management and analysis tend to fall into two categories.
The engine suitable building engine limited support	 The first is search engine toolkits which are especially suitable for building a search engine application but tend to have limited support for text analysismining functions.
Examples	 Examples include the following.
Lucene.	 Lucene.
httpslucene	 httpslucene apache org Terrier.
org	org Terrier.
org IndriLemur	 httpterrier org IndriLemur  httpwww.
	 httpwww.
lemurproject second text mining tend selectively text functions support search capability.	lemurproject org The second is text mining or general data mining and machine learning toolkits which tend to selectively support some text analysis functions but generally do not support search capability.
The general mining machine generally search	org The second is text mining or general data mining and machine learning toolkits which tend to selectively support some text analysis functions but generally do not support search capability  Examples include the following.
	 Examples include the following.
Weka.	 Weka.
	 httpwww.
waikato	waikato ac.
csie.	nzmlweka LIBSVM  httpswww csie.
	 httpswww.
csie ntu.	csie ntu.
	ntu.
twc̃jlinlibsvm Stanford	edu twc̃jlinlibsvm Stanford NLP.
Stanford NLP.	twc̃jlinlibsvm Stanford NLP.
stanford.	 httpnlp stanford.
edusoftwarecorenlp Illinois NLP Curator.	edusoftwarecorenlp shtml Illinois NLP Curator.
cs illinois Learn.	cs illinois edupagesoftwareview Curator Scikit Learn.
Scikit Learn	edupagesoftwareview Curator Scikit Learn  httpscikitlearn.
NLTK httpwww.	 httpscikitlearn orgstable NLTK  httpwww.
orgstable	orgstable NLTK  httpwww.
httpwww 5.	 httpwww nltk org 5.
org 5 Visit httpsmetatoolkit.	nltk org 5  Visit httpsmetatoolkit.
5 orgdoxygennamespaces.	org 5  Visit httpsmetatoolkit orgdoxygennamespaces.
Visit	 Visit httpsmetatoolkit orgdoxygennamespaces html httpterrier.
httpwww.	orgdoxygennamespaces html httpterrier org httpwww.
org httpwww	org httpwww lemurproject.
lemurproject.	lemurproject.
	org httpwww cs.
waikato.	cs waikato.
waikato.	waikato.
	ac nzmlweka httpnlp.
httpnlp stanford	nzmlweka httpnlp stanford edusoftwarecorenlp.
stanford edusoftwarecorenlp.	stanford edusoftwarecorenlp.
	edusoftwarecorenlp shtml httpcogcomp.
However seamless search engine analysis functions building unified supporting text	orgstable httpwww nltk org However there is a lack of seamless integration of search engine capabilities with various text analysis functions which is necessary for building a unified system for supporting text management and analysis.
nltk However lack integration search functions text main design differentiates existing search text access text analysis text application.	nltk org However there is a lack of seamless integration of search engine capabilities with various text analysis functions which is necessary for building a unified system for supporting text management and analysis  A main design philosophy of META which also differentiates META from the existing toolkits is its emphasis on the tight integration of search capabilities indeed text access capabilities in general with text analysis functions enabling it to provide full support for building a power ful text analysis application.
A main design philosophy META integration search text access capabilities general functions analysis research designed modularity achieved objectoriented toolkits multiple	 A main design philosophy of META which also differentiates META from the existing toolkits is its emphasis on the tight integration of search capabilities indeed text access capabilities in general with text analysis functions enabling it to provide full support for building a power ful text analysis application  To facilitate education and research META is designed with an emphasis on modularity and extensibility achieved through objectoriented design  META can be used together with existing toolkits in multiple ways.
To education research objectoriented design.	 To facilitate education and research META is designed with an emphasis on modularity and extensibility achieved through objectoriented design.
text existing toolkit support search analysis results subset data NLP preprocess annotated text input representation fed different	 For ex ample for very largescale text applications an existing search engine toolkit can be used to support search while META can be used to further support analysis of the found search results or any subset of text data that are obtained from the orig inal large data set  NLP toolkits can be used to preprocess text data and generate annotated text data for modules in META to use as input  META can also be used to generate a text representation that would be fed into a different data mining or machine learning toolkit.
NLP toolkits text annotated use input META text representation data mining machine toolkit In form data single document	 NLP toolkits can be used to preprocess text data and generate annotated text data for modules in META to use as input  META can also be used to generate a text representation that would be fed into a different data mining or machine learning toolkit  Exercises In its simplest form text data could be a single document in .
exercise analyze We’ll A Tale Cities Dickens	txt format  This exercise will get you familiar with various techniques that are used to analyze text  We’ll use the novel A Tale of Two Cities by Charles Dickens as example text.
exercise analyze	 This exercise will get you familiar with various techniques that are used to analyze text.
called twocities located	 The book is called twocities txt and is located at httpsifaka cs.
txt httpsifaka.	txt and is located at httpsifaka.
textdatabooktwocities	eduir textdatabooktwocities txt.
txt You sentences followed setup	txt  You can also use any of your own plaintext files that have multiple English sentences  Like all future exercises we will assume that the reader followed the META setup guide and successfully compiled the executables.
Like reader META compiled	 Like all future exercises we will assume that the reader followed the META setup guide and successfully compiled the executables.
In Running profile build directory Usage	 In this exercise we’ll only be using the profile program  Running  profile from inside the build directory will print out the following usage information Usage .
profile build directory print usage information config.	profile from inside the build directory will print out the following usage information Usage  profile config.
profile config.	profile config.
OPTION perform stop words annotate POS replace words grammatical trees sort unigram words bigram freqtrigram sort trigram run cs	txt OPTION where OPTION is one or more of stem perform stemming on each word stop remove stop words pos annotate words with POS tags posreplace replace words with their POS tags parse create grammatical parse trees from file content frequnigram sort and count unigram words freqbigram sort and count bigram words freqtrigram sort and count trigram words all run all options httpsifaka cs uiuc.
cs uiuc.	cs uiuc.
	uiuc.
META Data If running information	eduirtextdatabooktwocities txt 66 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis If running  profile prints out this information then everything has been set up correctly.
META A Management Analysis If prints information	txt 66 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis If running  profile prints out this information then everything has been set up correctly.
4 Stop	 4 1  Stop Word Removal.
	 Stop Word Removal.
words If known words idea types words	 Consider the following words I the of my it to from  If it was known that a document contained these words would there be any idea what the document was about Probably not  These types of words are called stop words.
document contained idea document	 If it was known that a document contained these words would there be any idea what the document was about Probably not.
These words stop words.	 These types of words are called stop words.
content	 Specifically they are very high frequency words that do not contain content information.
required topical removed preprocessing text analysis.	 They are used because they’re grammatically required such as when connecting sentences  Since these words do not contain any topical information they are often removed as a preprocessing step in text analysis.
words topical information removed preprocessing text analysis usually ignored algorithms	 Since these words do not contain any topical information they are often removed as a preprocessing step in text analysis  Not only are these usually useless words ignored but having less data can mean that algorithms run faster .
Not usually words having run faster	 Not only are these usually useless words ignored but having less data can mean that algorithms run faster  profile config.
profile	profile config.
	txt.
idea book words present 2	 Can you still get an idea of what the book is about without these words present 4 2  Stemming.
	2  Stemming.
reducing form.	 Stemming is the process of reducing a word to a base form.
user running containing word runs	 If a user wants to find books about running documents containing the word run or runs would not match.
apply algorithm forms match retrieval task The stemming Porter2 Stemmer devel Martin It version original Porter Stemmer	 If we apply a stemming algorithm to a word it is more likely that other forms of the word will match it in an information retrieval task  The most popular stemming algorithm is the Porter2 English Stemmer devel oped by Martin Porter  It is a slightly improved version from the original Porter Stemmer from 1980.
The English Stemmer devel slightly original Stemmer 1980 examples → argues arguing argu	 The most popular stemming algorithm is the Porter2 English Stemmer devel oped by Martin Porter  It is a slightly improved version from the original Porter Stemmer from 1980  Some examples are run runs running → run argue argued argues arguing → argu lies lying  lie → lie META uses the Porter2 stemmer by default.
Some run runs running argues → → lie META default You	 Some examples are run runs running → run argue argued argues arguing → argu lies lying  lie → lie META uses the Porter2 stemmer by default  You can read more about the Porter2 stemmer here httpsnowball.
You Porter2 tartarus.	 You can read more about the Porter2 stemmer here httpsnowball tartarus.
	html  An online demo of the stemmer is also available if you’d like to play around with it httpweb engr.
stemmer like	 An online demo of the stemmer is also available if you’d like to play around with it httpweb.
illinois.	engr illinois.
html.	illinois edumassung1p2sdemo html.
edumassung1p2sdemo html.	edumassung1p2sdemo html.
	html.
Tale Two Cities profile config.	 Now that you have an idea of what stemming is run the stemmer on A Tale of Two Cities   profile config.
profile	profile config.
twocities txt	toml twocities txt stem httpsnowball tartarus.
	orgalgorithmsenglishstemmer html httpweb engr.
html httpweb.	html httpweb.
engr.	engr.
stemming meaning	edumassung1p2sdemo html Exercises 67 Like stop word removal stemming tries to keep the basic meaning behind the original text  Can you still make sense of it after it’s stemmed 4.
Can 3 PartofSpeech	 Can you still make sense of it after it’s stemmed 4 3  PartofSpeech Tagging.
	 PartofSpeech Tagging.
students words science speech POS Each based surrounding words.	 When learning English students often encounter different grammatical labels for words such as noun adjective verb etc  In linguis tics and computer science there is a much larger dichotomy of these labels called part of speech POS tags  Each word can be assigned a tag based on surrounding words.
linguis tics larger POS tags.	 In linguis tics and computer science there is a much larger dichotomy of these labels called part of speech POS tags.
assigned All number change Here’s partofspeech version AllDT hotelNN roomsNNS theDT althoughIN roomNN numberNN changeV B	 Each word can be assigned a tag based on surrounding words  Consider the following sentence All hotel rooms are pretty much the same although the room number might change  Here’s a partofspeech tagged version AllDT hotelNN roomsNNS areV BP prettyRB muchRB theDT sameJJ althoughIN theDT roomNN numberNN mightMD changeV B .
Consider following sentence All pretty number	 Consider the following sentence All hotel rooms are pretty much the same although the room number might change.
Here’s version AllDT hotelNN althoughIN roomNN .	 Here’s a partofspeech tagged version AllDT hotelNN roomsNNS areV BP prettyRB muchRB theDT sameJJ althoughIN theDT roomNN numberNN mightMD changeV B .
Above V types verbs determiner This subset 80 commonly	  Above V BP and V B are different types of verbs NN and NNS are singular and plural nouns and DT means determiner  This is just a subset of about 80 commonly used tags.
Above V V B types NNS singular plural DT means	 Above V BP and V B are different types of verbs NN and NNS are singular and plural nouns and DT means determiner.
tag instance like multiple tags depending BZ anDT .	 Not every word has a unique part of speech tag  For instance flies and like can have multiple tags depending on the context TimeNN fliesV BZ likeIN anDT arrowNN  .
instance likeIN arrowNN	 For instance flies and like can have multiple tags depending on the context TimeNN fliesV BZ likeIN anDT arrowNN .
FruitNN likeV BP .	 FruitNN fliesNNS likeV BP aDT bananaNN .
POStagging challenging.	  Such situations can make POStagging challenging.
situations POStagging challenging.	 Such situations can make POStagging challenging.
POS 97 taggers tags analysis alternate additional represen	 Nevertheless human agree ment on POS tag labeling is about 97 which is the ceiling for automatic taggers  POS tags can be used in text analysis as an alternate or additional represen tation to words.
grammatical sense program options tagging like examples word tag.	 Using these tags captures a slightly more grammatical sense of a document or corpus  The profile program has two options for POS tagging  The first annotates each word like the examples above and the second replaces each word with its POS tag.
profile tagging.	 The profile program has two options for POS tagging.
word tag .	 The first annotates each word like the examples above and the second replaces each word with its POS tag  .
	  profile config toml twocities.
profile config twocities .	profile config toml twocities txt pos .
twocities txt config.	toml twocities txt pos  profile config.
txt	txt pos .
toml twocities txt posreplace Note tagging minute complete.	toml twocities txt posreplace Note that POS tagging the book may take up to one minute to complete.
like POS When replacing sentence	 Does it look like META’s POS tagger is accurate Can you find any mistakes When replacing the words with their tags is it possible to determine what the original sentence was Experiment with the book or any other text file.
4.	 4.
trees represent sentences represent tree	 Parsing  Grammatical parse trees represent deeper syntactic knowledge from text sentences  They represent sentence phrase hierarchy as a tree structure.
Grammatical text sentences.	 Grammatical parse trees represent deeper syntactic knowledge from text sentences.
example	 Consider the example in Figure 4.
rooted S noun phrase phrase VP period Some common features production S V P depth structural	 The parse tree is rooted with S denoting Sentence the sentence is composed of a noun phrase NP followed by a verb phrase VP and period  The leaves of the tree are the words in the sentence and the preterminals the direct parents of the leaves are partofspeech tags  Some common features from a parse tree are production rules such as S → NP V P  tree depth and structural tree features.
common S → structural	 Some common features from a parse tree are production rules such as S → NP V P  tree depth and structural tree features.
categories node The runs parser profile config.	 Syntactic categories node labels alone can also be used  The following command runs the parser on each sentence in the input file  profile config.
following runs input file config.	 The following command runs the parser on each sentence in the input file  profile config.
	profile config.
toml txt parse Like complete.	toml twocities txt parse Like POStagging the parsing may also take a minute or two to complete.
POStagging complete.	txt parse Like POStagging the parsing may also take a minute or two to complete.
4	 4 5.
Frequency Analysis textprocessing	5  Frequency Analysis  Perhaps the most common textprocessing technique is frequency counting.
textprocessing technique counting This counts document corpus list idea document	 Perhaps the most common textprocessing technique is frequency counting  This simply counts how many times each unique word appears in a document or corpus  Viewing a descending list of words sorted by frequency can give you an idea of what the document is about.
This unique word appears document corpus sorted frequency idea Intuitively similar .	 This simply counts how many times each unique word appears in a document or corpus  Viewing a descending list of words sorted by frequency can give you an idea of what the document is about  Intuitively similar documents should have some of the same highfrequency words .
list document about.	 Viewing a descending list of words sorted by frequency can give you an idea of what the document is about.
	   .
	   not including stop words.
I 1 took 2grams took 1 1 1 1 3grams 1 took vacation 1	 1grams unigrams I  1 took  1 a  2 vacation  1 to  2 go  1 beach  1 2grams bigrams I took  1 took a  1 a vacation  1 vacation to  1 to go  1 go to  1 to a  1 a beach  1 Exercises 69 3grams trigrams I took a  1 took a vacation  1 a vacation to  1 .
	   .
	 .
words importance text	 As we will see in this text the unigram words document representation is of utmost importance for text representation.
This known “bagofwords” know know context This POS derived	 This representation is also known as “bagofwords” since we only know the counts of each word and no longer know the context or position  This unigram counting scheme can be used with POS tags or any other type of token derived from a document.
unigram counting POS type token derived document.	 This unigram counting scheme can be used with POS tags or any other type of token derived from a document.
toml	  profile config toml twocities.
profile config txt	profile config toml twocities txt frequnigram .
txt frequnigram	toml twocities txt frequnigram  profile config.
	profile config toml twocities.
	toml twocities txt freqbigram .
profile	profile config toml twocities.
This file	txt freqtrigram This will give the output file twocities freq.
freq	freq 1.
txt What reasonably clear Think stop words stop words gets words don’t	txt for the option freq unigram and so on  What makes the output reasonably clear Think back to stop words and stemming  Removing stop words gets rid of the noisy highfrequency words that don’t give any information about the content of the document.
rid noisy highfrequency words document aggre words single means partial run 2 instead represented	 Removing stop words gets rid of the noisy highfrequency words that don’t give any information about the content of the document  Stemming will aggre gate inflected words into a single count  This means the partial vector run  4 running  2 runs  3 would instead be represented as run  9.
aggre gate inflected words single means partial running 2 runs instead	 Stemming will aggre gate inflected words into a single count  This means the partial vector run  4 running  2 runs  3 would instead be represented as run  9.
This partial 4 running instead represented 9 Not easier frequency improve text 4.	 This means the partial vector run  4 running  2 runs  3 would instead be represented as run  9  Not only does this make it easier for humans to interpret the frequency analysis but it can improve text mining algorithms too 4.
	6  Zipf’s Law.
Zipf’s	 Zipf’s Law.
In English 10–15	 In English the top four most frequent words are about 10–15 of all word occurrences.
50 occurrences fact similar trend human language.	 The top 50 words are 35–40 of word occurrences  In fact there is a similar trend in any human language.
In human Think words.	 In fact there is a similar trend in any human language  Think back to the stop words.
graph idea word distribution given 2 unigram words Dickens Oliver Twist.	 Such a graph can give us an idea of the word distribution in a given document or collection  In Figure 4 2 we counted unigram words from another Dickens book Oliver Twist.
Figure 4 2 counted book Twist plot log log	 In Figure 4 2 we counted unigram words from another Dickens book Oliver Twist  The plot on the left is a normal x ∼ y plot and the one on the right is a log x ∼ log y plot.
2 words The left log ∼ shape	2 we counted unigram words from another Dickens book Oliver Twist  The plot on the left is a normal x ∼ y plot and the one on the right is a log x ∼ log y plot  Zipf’s law describes the shape of these plots.
The x plot right plot Zipf’s law plots.	 The plot on the left is a normal x ∼ y plot and the one on the right is a log x ∼ log y plot  Zipf’s law describes the shape of these plots.
think states apply certain	 What do you think Zipf’s law states The shape of these plots allows us to apply certain techniques to take advantage of the word distribution in natural language.
Text data analysis.	 Text data access is the foundation for text analysis.
First trieval particular avoiding unnecessary data.	 First it enables re trieval of the most relevant text data to a particular analysis problem thus avoiding unnecessary overhead from processing a large amount of nonrelevant data.
retrieval In chapters specific text pull	 Then we will formalize and motivate the problem of text retrieval  In the following chapters we will cover specific techniques for supporting text access in both push and pull modes.
Because data created humans impor tant text data management applications.	 5 1 Access Mode Pull vs  Push Because text data are created for consumption by humans humans play an impor tant role in text data analysis and management applications.
1 vs Because data created humans humans play text	1 Access Mode Pull vs  Push Because text data are created for consumption by humans humans play an impor tant role in text data analysis and management applications.
Because text play role management Specifically data application processing focus analyzing Selecting relevant text data text	 Push Because text data are created for consumption by humans humans play an impor tant role in text data analysis and management applications  Specifically humans can help select the most relevant data to a particular application problem which is beneficial since it enables us to avoid processing the huge amount of raw text data which would be inefficient and focus on analyzing the most relevant part  Selecting relevant text data from a large collection is the basic task of text access.
humans help problem huge data collection text	 Specifically humans can help select the most relevant data to a particular application problem which is beneficial since it enables us to avoid processing the huge amount of raw text data which would be inefficient and focus on analyzing the most relevant part  Selecting relevant text data from a large collection is the basic task of text access.
Selecting relevant data text generally based information need push.	 Selecting relevant text data from a large collection is the basic task of text access  This selection is generally based on a specification of the information need of an analyst a user and can be done in two modes pull and push.
selection generally information push.	 This selection is generally based on a specification of the information need of an analyst a user and can be done in two modes pull and push.
5	 Figure 5 1 describes how these modes fit together along with querying and browsing.
e.	e.
use query information engine For example need buy product interested relevant candidate products user product generally longer need information Another example process media explore information e.	 In such a case the user can use a query to find relevant information with a search engine  For example a user may have a need to buy a product and thus be interested in retrieving all the relevant opinions about candidate products after the user has purchased the product the user would generally no longer need such information  Another example is that during the process of analyzing social media data to understand opinions about an emerging event the analyst may also decide to explore information about a particular entity related to the event e.
user need retrieving candidate user longer need information.	 For example a user may have a need to buy a product and thus be interested in retrieving all the relevant opinions about candidate products after the user has purchased the product the user would generally no longer need such information.
trigger	g  a person which can also trigger a search activity.
wants explore goal searching users querying g.	 through a smartphone or simply wants to explore a topic with no fixed goal  Indeed when searching the Web users tend to mix querying and browsing e g.
Indeed searching tend querying	 Indeed when searching the Web users tend to mix querying and browsing e.
hyperlinks.	g  while traversing through hyperlinks.
	 while traversing through hyperlinks.
general browsing finding relevant information Their seeking sightseeing tourist taxi similar user looking formulate 5.	 In general we may regard querying and browsing as two complementary ways of finding relevant information in the information space  Their relation can be understood by making an analogy between information seeking and sightseeing in a physical world  When a tourist knows the exact address of an attraction the tourist can simply take a taxi directly to the attraction this is similar to when a user knows exactly what he or she is looking for and can formulate a query with the 5.
relation understood making analogy information When attraction attraction formulate query Access Mode	 Their relation can be understood by making an analogy between information seeking and sightseeing in a physical world  When a tourist knows the exact address of an attraction the tourist can simply take a taxi directly to the attraction this is similar to when a user knows exactly what he or she is looking for and can formulate a query with the 5 1 Access Mode Pull vs.
Access	1 Access Mode Pull vs.
However tourist doesn’t exact address attraction walk attraction knowledge use related browse information work browsing	 However if a tourist doesn’t know the exact address of an attraction the tourist may want to take a taxi to an approximate location and then walk around to find the attraction  Similarly if a user does not have a good knowledge about the target pages he or she can also use an approximate query to reach some related pages and then browse into truly relevant information  Thus when querying does not work well browsing can be very useful.
good target use truly information querying browsing	 Similarly if a user does not have a good knowledge about the target pages he or she can also use an approximate query to reach some related pages and then browse into truly relevant information  Thus when querying does not work well browsing can be very useful.
work browsing	 Thus when querying does not work well browsing can be very useful.
initiates recommend user mode information access useful longstanding need user analyst regarded	 In the push mode the system initiates the process to recommend a set of relevant information items to the user  This mode of information access is generally more useful to satisfy a longstanding information need of a user or analyst  For example a researcher’s research interests can be regarded as relatively stable over time.
useful satisfy information user	 This mode of information access is generally more useful to satisfy a longstanding information need of a user or analyst.
comparison e published dynamic.	 In comparison the information stream i e  published research articles is dynamic.
e In user information desirable called filtering dynamic stream user based user’s e.	e  published research articles is dynamic  In such a scenario although a user can regularly search for relevant literature information with queries it is more desirable for a recommender also called filtering system to monitor the dynamic information stream and “push” any relevant articles to the user based on the matching of the articles with the user’s interests e.
published research	 published research articles is dynamic.
In scenario regularly information desirable called filtering monitor dynamic stream articles user’s e g	 In such a scenario although a user can regularly search for relevant literature information with queries it is more desirable for a recommender also called filtering system to monitor the dynamic information stream and “push” any relevant articles to the user based on the matching of the articles with the user’s interests e g  in the form of an email.
email.	g  in the form of an email.
	 in the form of an email.
push mode appropriately called SDI scenario information disseminating users	 Another scenario of push mode is producerinitiated recommendation which can be more appropriately called selective dissemination of information SDI  In such a scenario the producer of information has an interest in disseminating the information among relevant users and would push an information item to such users.
In producer information relevant users information item	 In such a scenario the producer of information has an interest in disseminating the information among relevant users and would push an information item to such users.
There information needs need mode long	 There are broadly two kinds of information needs shortterm need and long term need  Shortterm needs are most often associated with pull mode and long term needs are most often associated with push mode.
needs associated long term push	 Shortterm needs are most often associated with pull mode and long term needs are most often associated with push mode.
information user feedback sense 76 5 Overview Text Data harder feedback information user	 Also in the case of longterm information needs it is possible to collect user feedback which can be exploited  In this sense 76 Chapter 5 Overview of Text Data Access ad hoc retrieval is much harder as we do not have much feedback information from a user i e.
76 Text Data retrieval i.	 In this sense 76 Chapter 5 Overview of Text Data Access ad hoc retrieval is much harder as we do not have much feedback information from a user i.
	e.
data recommendation learning techniques covered ex books.	 little training data for a particular query  Due to the availability of training data the problem of filtering or recommendation can usually be solved by using supervised machine learning techniques which are covered well in many ex isting books.
cover ad 5 Multimode Interactive users multimode access text push modes integrated information browsing seamlessly integrated provide maximum flexibility users query	 Thus we will cover ad hoc retrieval in much more detail than filtering and recommendation  5 2 Multimode Interactive Access Ideally the system should provide support for users to have multimode interactive access to relevant text data so that the push and pull modes are integrated in the same information access environment and querying and browsing are also seamlessly integrated to provide maximum flexibility to users and allow them to query and browse at will.
cs uiuc topic automatically based added regular search ClickThrough Search Result dining Figure	cs uiuc  eduprojsosurf where a topic map automatically constructed based on a set of queries collected in a commercial search engine has been added to a regular search ClickThrough Search Result for dining table Figure 5.
interface browsing map browsing integrated httptiman cs.	2 Sample interface of browsing with a topic map where browsing and querying are naturally integrated  httptiman cs.
cs	 httptiman cs uiuc.
	cs uiuc.
uiuc 5 Multimode Interactive 77 engine enable space flexibly.	uiuc eduprojsosurf 5 2 Multimode Interactive Access 77 engine interface to enable a user to browse the information space flexibly.
With user Querying jump.	 With this interface a user can do any of the following at any moment  Querying longrange jump.
Querying When user new query search box search shown right pane At map shown pane	 Querying longrange jump  When a user submits a new query through the search box the search results from a search engine will be shown in the right pane  At the same time the relevant part of a topic map is also shown on the left pane to facilitate browsing should the user want to.
relevant topic map left pane map walk pane interface let map.	 At the same time the relevant part of a topic map is also shown on the left pane to facilitate browsing should the user want to  Navigating on the map shortrange walk  The left pane in our interface is to let a user navigate on the map.
The pane user map.	 The left pane in our interface is to let a user navigate on the map.
clicks map node current focus displayed In local view horizontal focus labelled zoom navigate horizontal	 When a user clicks on a map node this pane will be refreshed and a local view with the clicked node as the current focus will be displayed  In the local view we show the parents the children and the horizontal neighbors of the current node in focus labelled as “center” in the interface  A user can thus zoom into a child node zoom out to a parent node or navigate into a horizontal neighbor node.
view parents children neighbors current focus “center” interface.	 In the local view we show the parents the children and the horizontal neighbors of the current node in focus labelled as “center” in the interface.
user child node horizontal	 A user can thus zoom into a child node zoom out to a parent node or navigate into a horizontal neighbor node  The number attached to a node is a score for the node that we use for ranking the nodes.
attached node map user “walk” documents reformulate region.	 The number attached to a node is a score for the node that we use for ranking the nodes  Such a map enables the user to “walk” in the information space to browse into relevant documents without needing to reformulate queries  Viewing a topic region.
Viewing	 Viewing a topic region.
The doubleclick map documents topic region The result pane documents	 The user may doubleclick on a topic node on the map to view the documents covered in the topic region  The search result pane would be updated with new results corresponding to the documents in the selected topic region.
From user’s perspective pane shows documents focused search results documents corresponding node browsing document.	 From a user’s perspective the result pane always shows the documents in the current region that the user is focused on either search results of the query or the documents corresponding to a current node on the map when browsing  Viewing a document.
view standard search In	 Viewing a document  Within the result pane a user can select any document to view as in a standard search interface  In Figure 5.
user document view search interface 5 3 dining table zoomed dining table zoomed dining furniture zoomed user kinds furniture.	 Within the result pane a user can select any document to view as in a standard search interface  In Figure 5 3 we further show an example trace of browsing in which the user started with a query dining table zoomed into asian dining table zoomed out back to dining table browsed horizontally first to dining chair and then to dining furniture and finally zoomed out to the general topic furniture where the user would have many options to explore different kinds of furniture.
In 5 browsing user query table zoomed dining table horizontally dining chair dining finally furniture user different	 In Figure 5 3 we further show an example trace of browsing in which the user started with a query dining table zoomed into asian dining table zoomed out back to dining table browsed horizontally first to dining chair and then to dining furniture and finally zoomed out to the general topic furniture where the user would have many options to explore different kinds of furniture.
browsing query dining table dining chair general options explore kinds furniture If user jump” new query map user natural extension search user’s perspective.	3 we further show an example trace of browsing in which the user started with a query dining table zoomed into asian dining table zoomed out back to dining table browsed horizontally first to dining chair and then to dining furniture and finally zoomed out to the general topic furniture where the user would have many options to explore different kinds of furniture  If this user feels that a “long jump” is needed he or she can use a new query to achieve it  Since the map can be hidden and only brought to display when the user needs it such an interface is a very natural extension of the current search interface from a user’s perspective.
jump” use new hidden brought display extension current search user’s	 If this user feels that a “long jump” is needed he or she can use a new query to achieve it  Since the map can be hidden and only brought to display when the user needs it such an interface is a very natural extension of the current search interface from a user’s perspective.
map interface perspective.	 Since the map can be hidden and only brought to display when the user needs it such an interface is a very natural extension of the current search interface from a user’s perspective.
3 Retrieval The tool data access search engine people basis engines directly extended provide mendation browsing.	3 Text Retrieval The most important tool for supporting text data access is a search engine which is why web search engines are used by many people on a daily basis  Search engines directly provide support for querying and can be easily extended to provide recom mendation or browsing.
engines support provide recom	 Search engines directly provide support for querying and can be easily extended to provide recom mendation or browsing.
In retrieval solved developing engine We structured	 In this section we discuss the problem of text retrieval TR which is solved by developing a search engine system  We specify the differences between un structured TR and structured database retrieval.
specify differences TR retrieval This	 We specify the differences between un structured TR and structured database retrieval  We then make an argument for document ranking as opposed to document selection  This provides a basis for us to discuss in the next chapter how to rank documents for a query.
ranking opposed document	 We then make an argument for document ranking as opposed to document selection.
user’s perspective problem use documents frequently temporary ad needs tasks	 From a user’s perspective the problem of TR is to use a query to find relevant documents in a collection of text documents  This is a frequently needed task because users often have temporary ad hoc information needs for various tasks 5.
support TR search engine.	 The system to support TR is a text retrieval system or a search engine.
image search engines TR represented text document consisting associated data g context article included.	 For example the current image search engines on the Web are essentially a TR system where each image is represented by a text document consisting of any associated text data with the image e g  title caption or simply textual context of the image such as the news article content where an image is included.
g title simply context image news article techniques text retrieval called search technology.	g  title caption or simply textual context of the image such as the news article content where an image is included  In industry the problem of TR is generally referred to as the search problem and the techniques for text retrieval are often called search technology or search engine technology.
textual context image news article content included search techniques called technology	 title caption or simply textual context of the image such as the news article content where an image is included  In industry the problem of TR is generally referred to as the search problem and the techniques for text retrieval are often called search technology or search engine technology.
In problem TR referred engine collections.	 In industry the problem of TR is generally referred to as the search problem and the techniques for text retrieval are often called search technology or search engine technology  The task of TR can be easy or hard depending on specific queries and specific collections.
easy queries collections For search generally finding e.	 The task of TR can be easy or hard depending on specific queries and specific collections  For example during a web search finding homepages is generally easy but finding out people’s opinions about some topic e.
For search generally opinions g.	 For example during a web search finding homepages is generally easy but finding out people’s opinions about some topic e g.
foreign harder TR difficult	S  foreign policy would be much harder  There are several reasons why TR is difficult .
policy reasons .	 foreign policy would be much harder  There are several reasons why TR is difficult .
There reasons formal information especially user	 There are several reasons why TR is difficult   a query is usually quite short and incomplete no formal language like SQL   the information need may be difficult to describe precisely especially when the user isn’t familiar with the topic and .
short incomplete like	 a query is usually quite short and incomplete no formal language like SQL .
understanding document experts disagree Due lack semantic difficulty challenging accurately retrieve relevant user’s query.	 precise understanding of the document content is difficult  In general since what counts as the correct answer is subjective even when human experts judge the relevance of documents they may disagree with each other  Due to the lack of clear semantic structures and difficulty in natural language understanding it is often challenging to accurately retrieve relevant information to a user’s query.
correct judge other.	 In general since what counts as the correct answer is subjective even when human experts judge the relevance of documents they may disagree with each other.
understanding relevant Indeed web search sufficient difficult quickly relevant	 Due to the lack of clear semantic structures and difficulty in natural language understanding it is often challenging to accurately retrieve relevant information to a user’s query  Indeed even though the current web search engines may appear to be sufficient sometimes it may still be difficult for a user to quickly locate and har vest all the relevant information for a task.
current engines simple popular informational queries complex opinions products buy symptoms work poorly.	 In general the current search engines work very well for navigational queries and simple popular informational queries but in the case where a user has a complex information need such as analyzing opinions about products to buy or researching medical information about some symptoms they often work poorly.
current search generally little users digest result engine retrieve relevant 80 Chapter 5 user documents read digest knowledge later help users information quickly text data optimize help user finish	 Moreover the current search engines generally provide little or no support to help users digest and exploit the retrieved informa tion  As a result even if a search engine can retrieve the most relevant information 80 Chapter 5 Overview of Text Data Access a user would still have to sift through a long list of documents and read them in detail to fully digest the knowledge buried in text data in order to perform their task at hand  The techniques discussed later in this book can be exploited to help users digest the found information quickly or directly analyze a large amount of text data to reveal useful and actionable knowledge that can be used to optimize decision making or help a user finish a task.
As search retrieve information Access sift list order perform task hand.	 As a result even if a search engine can retrieve the most relevant information 80 Chapter 5 Overview of Text Data Access a user would still have to sift through a long list of documents and read them in detail to fully digest the knowledge buried in text data in order to perform their task at hand.
techniques help information analyze reveal useful actionable task 5.	 The techniques discussed later in this book can be exploited to help users digest the found information quickly or directly analyze a large amount of text data to reveal useful and actionable knowledge that can be used to optimize decision making or help a user finish a task  5.
5 4	 5 4 Text Retrieval vs.
Retrieval It useful tasks information difference data important differences First	 Database Retrieval It is useful to make a comparison of the problem of TR and the similar problem of database retrieval  Both retrieval tasks are to help users find relevant information but due to the difference in the data managed by these two tasks there are many important differences  First the data managed by a search engine and a database system are different.
In databases structured field clearly according schema.	 In databases the data are structured where each field has a clearly defined meaning according to a schema.
search engine understand 1 lives address difficult answer query response free	 In contrast the data managed by a search engine are unstructured text which can be difficult for computers to understand 1 Thus even if a sentence says a person lives in a particular address it remains difficult for the computer to answer a query about the address of a person in response to a keyword query since there is no simple defined structure to free text.
sentence person difficult response defined structure text conform clearly field defined Second consequence difference supported	1 Thus even if a sentence says a person lives in a particular address it remains difficult for the computer to answer a query about the address of a person in response to a keyword query since there is no simple defined structure to free text  Therefore structured data are often easier to manage and analyze since they conform to a clearly defined schema where the meaning of each field is well defined  Second a consequence of the difference in the data is that the queries that can be supported by the two are also different.
data supported A clearly constraints table results answers query	 Second a consequence of the difference in the data is that the queries that can be supported by the two are also different  A database query clearly specifies the constraints on the fields of the data table and thus the expected retrieval results answers to the query are very well specified with no ambiguity.
A specifies constraints fields	 A database query clearly specifies the constraints on the fields of the data table and thus the expected retrieval results answers to the query are very well specified with no ambiguity.
fully language text case information need vague lack knowledge place.	 Even if the computer can fully understand the semantics of natural language text it is still often the case that the user’s information need is vague due to the lack of complete knowledge about the information to be found which is often the reason why the user wants to find the information in the first place.
example relevant parlance refers unstructured meaningful structuring employs narrow	 For example in the case of searching for relevant 1  Although common parlance refers to text as unstructured with a meaningful contrast with relational database structuring it employs a narrow sense of “structure.
Although common meaningful relational structuring sense “structure For perspective grammar structure study 5S societies structures et al.	 Although common parlance refers to text as unstructured with a meaningful contrast with relational database structuring it employs a narrow sense of “structure ” For example from a linguistics perspective grammar provides welldefined structure  To study this matter further see the 5S societies scenarios spaces structures and streams works by Fox et al.
study matter 5S societies scenarios structures streams al	 To study this matter further see the 5S societies scenarios spaces structures and streams works by Fox et al  2012 5 4 Text Retrieval vs.
Text vs Database 81 able clearly	 2012 5 4 Text Retrieval vs  Database Retrieval 81 literature to a research problem the user is unlikely able to clearly and completely specify which documents should be returned.
Database Retrieval 81 literature able clearly completely returned different specific	 Database Retrieval 81 literature to a research problem the user is unlikely able to clearly and completely specify which documents should be returned  Finally the expected results in the two applications are also different  In data base search we can retrieve very specific data elements e.
Finally results different In search	 Finally the expected results in the two applications are also different  In data base search we can retrieve very specific data elements e.
data base elements	 In data base search we can retrieve very specific data elements e.
g.	g.
specific TR able relevant documents fields search entities cation exactly “correct” direct vague TR.	 specific columns in TR we are generally only able to retrieve a set of relevant documents  With passages or fields identified in a text document a search engine can also retrieve passages but it is generally difficult to retrieve specific entities or attribute values as we can in a database  This difference is not as essential as the difference in the vague specifi cation of what exactly is the “correct” answer to a query but is a direct consequence of the vague information need in TR.
fields document retrieve passages difficult specific entities “correct” direct vague information Due useful useful engine somewhat	 With passages or fields identified in a text document a search engine can also retrieve passages but it is generally difficult to retrieve specific entities or attribute values as we can in a database  This difference is not as essential as the difference in the vague specifi cation of what exactly is the “correct” answer to a query but is a direct consequence of the vague information need in TR  Due to these differences the challenges in building a useful database and a useful search engine are also somewhat different.
This essential vague cation answer query consequence vague information Due challenges building engine	 This difference is not as essential as the difference in the vague specifi cation of what exactly is the “correct” answer to a query but is a direct consequence of the vague information need in TR  Due to these differences the challenges in building a useful database and a useful search engine are also somewhat different.
efficiency challenge engine important documents worrying answers	 While the efficiency challenge also exists in a search engine a more important challenge there is to first figure out which documents should be returned for a query before worrying about how to return the answers quickly.
applications—at database applications—it occurs In important clearly needs user	 In database applications—at least tradi tional database applications—it is also very important to maintain the integrity of the data that is to ensure no inconsistency occurs due to power failure  In TR modeling a user’s information need and search tasks is important again due to the difficulty for a user to clearly specify information needs and the difficulty in NLP  Since what counts as the best answer to a query depends on the user in TR the user is actually part of our input together with the query and document set.
algorithm computational study simulation study text real applications simulation faster application.	 In contrast in database research since the main issue is efficiency one can prove one algorithm is better than another by analyzing the computational complexity or do some simulation study  Note that however when doing simulation study to determine which algorithm is faster we also face the same problem as in text retrieval—the simulation may not accurately reflect the real applications  Thus an algorithm shown to be faster with simulation may not be actually faster for a particular application.
algorithm simulation particular	 Thus an algorithm shown to be faster with simulation may not be actually faster for a particular application.
Similarly retrieval collection turn particular application collection.	 Similarly a retrieval algorithm shown to be more effective with a test collection may turn out to be less effective for a particular application or even another test collection.
retrieval challenging research	 How to reliably evaluate retrieval algorithms is itself a challenging research topic.
widespread applications wellestablished The IR community studies information science science industry Web 1990s.	 Databases have had widespread applications in virtually every domain with a wellestablished strong industry  The IR community that studies text retrieval has been an interdisciplinary community involving library and information science and computer science but had not had a strong industry base until the Web was born in the early 1990s.
IR studies involving library information industry base born early 1990s dominated online information available engine include language grow.	 The IR community that studies text retrieval has been an interdisciplinary community involving library and information science and computer science but had not had a strong industry base until the Web was born in the early 1990s  Since then the search engine industry has dominated and as more and more online information is available the search engine technologies which include TR and other technical components such as machine learning and natural language processing will con tinue to grow.
industry dominated engine technologies technical machine	 Since then the search engine industry has dominated and as more and more online information is available the search engine technologies which include TR and other technical components such as machine learning and natural language processing will con tinue to grow.
evaluated users making engine effort spent initially	 Instead it has to be empirically evaluated by users making it a significant challenge in evaluating the effectiveness of a search engine  This is also the reason why a significant amount of effort has been spent in research of TR evaluation since it was initially studied in the 1960s.
spent initially studied 1960s The evaluation methodology TR remains topic discuss 9.	 This is also the reason why a significant amount of effort has been spent in research of TR evaluation since it was initially studied in the 1960s  The evaluation methodology of TR remains an important open research topic today we discuss it in detail in Chapter 9.
evaluation TR important topic today discuss Chapter 5.	 The evaluation methodology of TR remains an important open research topic today we discuss it in detail in Chapter 9  5.
Document Selection Document Given document collection set unordered text text	5 Document Selection vs  Document Ranking Given a document collection a set of unordered text documents the task of text retrieval can be defined as using a user query i e.
Document Ranking document collection retrieval defined user	 Document Ranking Given a document collection a set of unordered text documents the task of text retrieval can be defined as using a user query i e.
e information	e  a description of the user’s information need to identify a subset of documents that can satisfy the user’s information need.
description user’s identify satisfy user’s information computationally problem TR formally	 a description of the user’s information need to identify a subset of documents that can satisfy the user’s information need  In order to computationally solve the problem of TR we must first formally define it.
problem Thus section TR strategies V .	 In order to computationally solve the problem of TR we must first formally define it  Thus in this section we will provide a formal definition of TR and discuss highlevel strategies for solving this problem  Let V  w1 .
provide formal definition solving .	 Thus in this section we will provide a formal definition of TR and discuss highlevel strategies for solving this problem  Let V  w1 .
wN words natural wi	      wN be a vocabulary set of all the words in a particular natural language where wi is a word.
sequence V Similarly document di di1 .	    qm is a sequence of words where qi ∈ V   Similarly a document di  di1 .
qm Similarly .	  qm is a sequence of words where qi ∈ V   Similarly a document di  di1 .
.	   .
	 .
∈ V In query 5 Selection	  dim is also a sequence of words where dij ∈ V   In general a query is much shorter than a document since the query 5 5 Document Selection vs.
In query shorter query 5.	 In general a query is much shorter than a document since the query 5.
Document typed search users generally want type However	 Document Ranking 83 is often typed in by a user using a search engine system and users generally do not want to make much effort to type in many words  However this is not always the case.
However	 However this is not always the case.
dM set general subset documents collection i.	    dM is a set of text documents  In general we may assume that there exists a subset of documents in the collection i.
text	  dM is a set of text documents.
subset ⊂ relevant query q user	 In general we may assume that there exists a subset of documents in the collection i e  Rq ⊂ C which are relevant to the user’s query q that is they are relevant documents or documents useful to the user who typed in the query.
Naturally set depends query q.	 Naturally this relevant set depends on the query q.
unknown query documents Rq.	 However which documents are relevant is generally unknown the user’s query is only a “hint” at which documents should be in the set Rq.
Furthermore different query retrieve somewhat different sets documents e.	 Furthermore different users may use the same query to intend to retrieve somewhat different sets of relevant documents e.
approximation Rq compute R′q tive strategies vs.	 Thus the best a computer can do is to return an approximation of Rq which we will denote by R′q  Now how can a computer compute R′q At a high level there are two alterna tive strategies document selection vs.
compute high document	 Now how can a computer compute R′q At a high level there are two alterna tive strategies document selection vs.
selection classifier classify respect design sification d	 document ranking  In document selection we will implement a binary classifier to classify a document as either relevant or nonrelevant with respect to a particular query  That is we will design a binary clas sification function or an indicator function f q  d ∈ 0 1.
In document binary classify nonrelevant respect particular	 In document selection we will implement a binary classifier to classify a document as either relevant or nonrelevant with respect to a particular query.
strategy estimate relevance	 Using such a strategy the system must estimate the absolute relevance i.
e document An alternative documents user cutoff.	e  whether a document is relevant or not  An alternative strategy is to rank documents and let the user decide a cutoff.
document rank decide implement f d rank documents	 whether a document is relevant or not  An alternative strategy is to rank documents and let the user decide a cutoff  That is we will implement a ranking function f q  d ∈ R and rank all the documents in descending values of this ranking function.
defined partly partly implicitly threshold θ In case q	 A user would then browse the ranked list and stop whenever they consider it appropriate  In this case the set R′q is actually defined partly by the system and partly by the user since the user would implicitly choose a score threshold θ based on the rank position where he or she stopped  In this case R′q  df q  d ≥ θ.
In set R′q actually partly user implicitly score θ based stopped In ≥ needs relative documents likely	 In this case the set R′q is actually defined partly by the system and partly by the user since the user would implicitly choose a score threshold θ based on the rank position where he or she stopped  In this case R′q  df q  d ≥ θ  Using this strategy the system only needs to estimate the relative relevance of documents which documents are more likely relevant.
case R′q q d Using likely estimation relative intuitively relevance easier ranking	 In this case R′q  df q  d ≥ θ  Using this strategy the system only needs to estimate the relative relevance of documents which documents are more likely relevant  Since estimation of relative relevance is intuitively easier than that of absolute relevance we can expect it to be easier to implement the ranking strategy.
estimate documents documents likely relative relevance absolute relevance expect easier Indeed preferred	 Using this strategy the system only needs to estimate the relative relevance of documents which documents are more likely relevant  Since estimation of relative relevance is intuitively easier than that of absolute relevance we can expect it to be easier to implement the ranking strategy  Indeed ranking is generally preferred to document selection for multiple reasons.
user prescribe exact selecting documents accurate Often query 84 Chapter 5 Data overconstrained	 First due to the difficulty for a user to prescribe the exact criteria for selecting relevant documents the binary classifier is unlikely accurate  Often the query is 84 Chapter 5 Overview of Text Data Access either overconstrained or underconstrained.
documents result delivery search documents	 In the case of an overconstrained query there may be no relevant documents matching all the query words so forcing a binary decision may result in no delivery of any search result  If the query is underconstrained too general there may be too many documents matching the query resulting in overdelivery.
query documents matching resulting overdelivery Unfortunately difficult level specificity document knowledge user’s reason user information topic.	 If the query is underconstrained too general there may be too many documents matching the query resulting in overdelivery  Unfortunately it is often very difficult for a user to know the “right” level of specificity in advance before exploring the document collection due to the knowledge gap in the user’s mind which can be the reason why the user wants to find information about the topic.
know “right” level specificity gap user’s mind information topic.	 Unfortunately it is often very difficult for a user to know the “right” level of specificity in advance before exploring the document collection due to the knowledge gap in the user’s mind which can be the reason why the user wants to find information about the topic.
shown theoretically principle returning ranked relevance optimal following	 The strategy of ranking is further shown to be optimal theoretically under two assumptions based on the probability ranking principle Robertson 1997 which states that returning a ranked list of documents in descending order of predicted relevance is the optimal strategy under the following two assumptions  1.
1 document.	 1  The utility of a document to a user is independent of the utility of any other document.
independent document.	 The utility of a document to a user is independent of the utility of any other document.
	 2.
user following document sequence func f	 A user will browse the results sequentially  So the problem is the following  We have a query that has a sequence of words and a document that’s also a sequence of words and we hope to define the func tion f .
document that’s func tion	 We have a query that has a sequence of words and a document that’s also a sequence of words and we hope to define the func tion f .
document.	   that can compute a score based on the query and document.
The main challenge designing good nonrelevant	 The main challenge is designing a good ranking function that can rank all the relevant docu ments on top of all the nonrelevant ones.
definition goal retrieval model gives formalization relevance.	 That also means we have to have some way to define relevance  In particular in order to implement the program to do that we have to have a computational definition of relevance and we achieve this goal by designing a retrieval model which gives us a formalization of relevance.
2004 “pull” filtering “push” Belkin	 2004  The relation between retrieval “pull” and filtering “push” has been discussed in the article Belkin and Croft 1992.
retrieval filtering discussed Belkin 1992 retrieval discussed retrieval Rijsbergen	 The relation between retrieval “pull” and filtering “push” has been discussed in the article Belkin and Croft 1992  The contrast between information retrieval and database search was discussed in the classic in formation retrieval book by van Rijsbergen 1979.
search discussed classic retrieval Rijsbergen 1979.	 The contrast between information retrieval and database search was discussed in the classic in formation retrieval book by van Rijsbergen 1979.
2009 user interfaces design interfaces visualization browsing book Ex search particular querying	 Hearst 2009 has a systematic discussion of user interfaces of a search system which is relevant to the design of interfaces for any information system in general in particular many visualization techniques that can facilitate browsing and querying are discussed in the book  Ex ploratory search is a particular type of search tasks that often requires multimodal information access including both querying and browsing.
covered issue et al Roth 2009.	 It was covered in a spe cial issue of Communications of ACM White et al  2006 and White and Roth 2009.
More work important IR 1997 Readings Retrieval Jones Willett	 More historical work related to this as well as a set of important research papers in IR up to 1997 can be found in Readings in Information Retrieval Sparck Jones and Willett 1997.
IR history Croft 2012 In retrieval query effective practically useful models.	 A brief survey of IR history can be found in Sanderson and Croft 2012  6Retrieval Models In this chapter we introduce the two main information retrieval models vector space and query likelihood which are among the most effective and practically useful retrieval models.
models basic	 We begin with a brief overview of retrieval models in general and then discuss the two basic models i.
e.	e.
6 Over researchers fall different Zhai 2008	 6 1 Overview Over many decades researchers have designed various different kinds of retrieval models which fall into different categories see Zhai 2008 for a detailed review.
Overview Over decades designed kinds fall different	1 Overview Over many decades researchers have designed various different kinds of retrieval models which fall into different categories see Zhai 2008 for a detailed review.
Basically assume document document relevant	 First one family of the models are based on the similarity idea  Basically we assume that if a document is more similar to the query than another document is we would say the first document is more relevant than the second one.
Basically assume similar second So similarity query document.	 Basically we assume that if a document is more similar to the query than another document is we would say the first document is more relevant than the second one  So in this case the ranking function is defined as the similarity between the query and the document.
case ranking defined	 So in this case the ranking function is defined as the similarity between the query and the document.
One example space model Salton	 One wellknown example of this case is the vector space model Salton et al.
set called	 The second set of models are called probabilistic retrieval models Lafferty and Zhai 2003.
In family follow strategy assume documents random variables assume binary random 1 0 define query variable R given particular	 In this family of models we follow a very different strategy  We assume that queries and documents are all observations from random variables and we assume there is a binary random variable called R with a value of either 1 or 0 to indicate whether a document is relevant to a query  We then define the score of a document with respect to a query as the probability that this random variable R is equal to 1 given a particular document and query.
We score document query variable equal 1 document There general classic 1960s Maron 1960 Sparck Jones divergencefromrandomness model Van 2002.	 We then define the score of a document with respect to a query as the probability that this random variable R is equal to 1 given a particular document and query  There are different cases of such a general idea  One is the classic probabilistic model which dates back to work done in the 1960s and 1970s Maron and Kuhns 1960 Robertson and Sparck Jones 1976 another is the language modeling approach Ponte and Croft 1998 and yet another is the divergencefromrandomness model Amati and Van Rijsbergen 2002.
cases idea One model dates 1960s 1970s Croft divergencefromrandomness model Amati Rijsbergen	 There are different cases of such a general idea  One is the classic probabilistic model which dates back to work done in the 1960s and 1970s Maron and Kuhns 1960 Robertson and Sparck Jones 1976 another is the language modeling approach Ponte and Croft 1998 and yet another is the divergencefromrandomness model Amati and Van Rijsbergen 2002.
One model 1960s 1960 Robertson Jones modeling approach model Van Rijsbergen 2002.	 One is the classic probabilistic model which dates back to work done in the 1960s and 1970s Maron and Kuhns 1960 Robertson and Sparck Jones 1976 another is the language modeling approach Ponte and Croft 1998 and yet another is the divergencefromrandomness model Amati and Van Rijsbergen 2002.
We cover particular approach query retrieval model later chapter effective models Chapter 6 Retrieval Models retrieval framework Robertson retrieval vector space model cover vector	 We will cover a particular language modeling approach called query likelihood retrieval model in detail later in this chapter  One of the most effective retrieval models 88 Chapter 6 Retrieval Models derived from the classific probabilistic retrieval framework is BM25 Robertson and Zaragoza 2009 but since the retrieval function of BM25 is so similar to a vector space retrieval model we have chosen to cover it as a variant of the vector space model.
retrieval models 6 derived classific probabilistic retrieval framework Robertson Zaragoza retrieval model chosen cover space The model Turtle Croft Here idea inference	 One of the most effective retrieval models 88 Chapter 6 Retrieval Models derived from the classific probabilistic retrieval framework is BM25 Robertson and Zaragoza 2009 but since the retrieval function of BM25 is so similar to a vector space retrieval model we have chosen to cover it as a variant of the vector space model  The third kind of model is probabilistic inference Turtle and Croft 1990  Here the idea is to associate uncertainty to inference rules.
probabilistic Turtle Croft associate quantify follows	 The third kind of model is probabilistic inference Turtle and Croft 1990  Here the idea is to associate uncertainty to inference rules  We can then quantify the probability that we can show that the query follows from the document.
idea	 Here the idea is to associate uncertainty to inference rules.
We probability query This theoretically practice regular	 We can then quantify the probability that we can show that the query follows from the document  This family of models is theoretically appealing but in practice they are often reduced to models essentially similar to vectorspace model or a regular probabilistic retrieval model.
family models	 Finally there is also a family of models that use axiomatic thinking Fang et al.
	 2011.
idea function	 The idea is to define a set of constraints that we hope a good retrieval function satisfies.
retrieval effective diagnosing model developing improved models accordingly Zhai	 The axiomatic retrieval framework has proven effective for diagnosing deficiencies of a retrieval model and developing improved retrieval models accordingly e g  BM25 Lv and Zhai 2011.
Lv Although survived extensive ex perimentation prove robustness.	 BM25 Lv and Zhai 2011  Although many models have been proposed very few have survived extensive ex perimentation to prove effective and robustness.
	e.
1 6 Form Retrieval introduce common stateoftheart common ideas models illustrated	1 6 2 Common Form of a Retrieval Function Before we introduce specific models we first take a look at the common form of a stateoftheart retrieval model and examine some of the common ideas used in all these models  This is illustrated in Figure 6.
2 Function introduce look stateoftheart retrieval model	2 Common Form of a Retrieval Function Before we introduce specific models we first take a look at the common form of a stateoftheart retrieval model and examine some of the common ideas used in all these models.
illustrated 6 1.	 This is illustrated in Figure 6 1.
First based text This processing bagofwords representation search	 First these models are all based on the assumption of using a bagofwords representation of text  This was explained in detail in the natural language processing chapter  A bagofwords representation remains the main representation used in all the search engines.
This explained language A remains search	 This was explained in detail in the natural language processing chapter  A bagofwords representation remains the main representation used in all the search engines.
A bagofwords main representation search engines With like campaign news document d based computed word.	 A bagofwords representation remains the main representation used in all the search engines  With this assumption the score of a query like presidential campaign news with respect to a document d would be based on scores computed on each individual query word.
score presidential campaign news.	 That means the score would depend on the score of each word such as presidential campaign and news.
1 readers know Van document	 1  PL2 is another very effective model that the readers should also know of Amati and Van Rijs bergen 2002  We can see there are three different components each corresponding to how well the document matches each of the query words.
We components corresponding matches query	 We can see there are three different components each corresponding to how well the document matches each of the query words.
number heuristics.	 Inside of these functions we see a number of heuristics.
For function g word presidential document called term frequency d.	 For example one factor that affects the function g is how many times the word presidential occurs in each document  This is called a term frequency TF  We might also denote this as cpresidential  d.
denote document value	 We might also denote this as cpresidential  d  In general if the word occurs more frequently in the document the value of this function would be larger  Another factor is the document length.
Another	 Another factor is the document length.
long document occurred document occur document Finally document document	 In general if a term occurs in a long document many times it is not as significant as if it occurred the same number of times in a short document since any term is expected to occur more frequently in a long document  Finally there is a factor called document frequency  This looks at how often presidential occurs at least once in any document in the entire collection.
presidential collection DF	 This looks at how often presidential occurs at least once in any document in the entire collection  We call this the document frequency or DF of presidential.
frequency DF presidential attempts characterize	 We call this the document frequency or DF of presidential  DF attempts to characterize the popularity of the term in the collection.
general term collection overall score TF length capture stateoftheart models	 In general matching a rare term in the collection is contributing more to the overall score than matching a common term  TF DF and document length capture some of the main ideas used in pretty much all stateoftheart retrieval models  In some other models we might also use a probability to characterize this information.
In A natural Which model models regarded stateoftheart	 In some other models we might also use a probability to characterize this information  A natural question is Which model works the best It turns out that many models work equally well so here we list the four major models that are generally regarded as stateoftheart .
normalization et	 pivoted length normalization Singhal et al.
2009 90	 1996   Okapi BM25 Robertson and Zaragoza 2009 90 Chapter 6 Retrieval Models   query likelihood Ponte and Croft 1998 and .
Ponte Rijsbergen optimized models perform et	 query likelihood Ponte and Croft 1998 and   PL2 Amati and Van Rijsbergen 2002  When optimized these models tend to perform similarly as discussed in detail in Fang et al.
Amati Rijsbergen	 PL2 Amati and Van Rijsbergen 2002.
	 2011.
probably	 Among all these BM25 is probably the most popular.
common method	 It’s most likely that this has been used in virtually all search engine implementations and it is quite common to see this method discussed in research papers  We’ll talk more about this method in a later section.
We’ll section.	 We’ll talk more about this method in a later section.
summary First requires relevance proper model Second equally effective don’t winner.	 In summary the main points are as follows  First the design of a good ranking function requires a computational definition of relevance and we achieve this goal by designing a proper retrieval model  Second many models are equally effective but we don’t have a single winner.
First design good function requires definition relevance achieve retrieval single actively working problem truly model.	 First the design of a good ranking function requires a computational definition of relevance and we achieve this goal by designing a proper retrieval model  Second many models are equally effective but we don’t have a single winner  Researchers are still actively working on this problem trying to find a truly optimal retrieval model.
equally actively functions ideas representa 2 document words.	 Second many models are equally effective but we don’t have a single winner  Researchers are still actively working on this problem trying to find a truly optimal retrieval model  Finally the stateoftheart ranking functions tend to rely on the following ideas 1 bag of words representa tion and 2 TF and the document frequency of words.
retrieval model.	 Researchers are still actively working on this problem trying to find a truly optimal retrieval model.
Finally stateoftheart ranking following bag words representa tion 2 TF frequency Such ranking function adjustment	 Finally the stateoftheart ranking functions tend to rely on the following ideas 1 bag of words representa tion and 2 TF and the document frequency of words  Such information is used by a ranking function to determine the overall contribution of matching a word with an adjustment for document length.
information ranking function word adjustment combined	 Such information is used by a ranking function to determine the overall contribution of matching a word with an adjustment for document length  These are often combined in interesting ways.
These discuss combined rank later book.	 These are often combined in interesting ways  We’ll discuss how exactly they are combined to rank documents later in this book.
6 Vector Retrieval Models vector space VS retrieval ranking special case similaritybased models discussed assume query.	 6 3 Vector Space Retrieval Models The vector space VS retrieval model is a simple yet effective method of designing ranking functions for information retrieval  It is a special case of similaritybased models that we discussed previously where we assume relevance is roughly corre lated to similarity between a document and a query.
3 Retrieval Models The VS model simple effective designing functions information special similaritybased previously corre document	3 Vector Space Retrieval Models The vector space VS retrieval model is a simple yet effective method of designing ranking functions for information retrieval  It is a special case of similaritybased models that we discussed previously where we assume relevance is roughly corre lated to similarity between a document and a query.
It special similaritybased models previously roughly similarity document Whether best capture relevance formally question order notion relevance precise definition In number assumptions.	 It is a special case of similaritybased models that we discussed previously where we assume relevance is roughly corre lated to similarity between a document and a query  Whether this assumption is the best way to capture the notion of relevance formally remains an open question but in order to solve our search problem we have to convert the vague notion of relevance into a more precise definition that can be implemented with a program ming language in one way or another  In this process we inevitably have to make a number of assumptions.
assumption best capture formally solve convert vague relevance definition program another.	 Whether this assumption is the best way to capture the notion of relevance formally remains an open question but in order to solve our search problem we have to convert the vague notion of relevance into a more precise definition that can be implemented with a program ming language in one way or another.
In process number assumptions Here assume document similar query document	 In this process we inevitably have to make a number of assumptions  Here we assume that if a document is more similar to a query than another document then the first document would be assumed to be more relevant than the second one.
This documents space way relevance relevance.	 This is the basis for ranking documents in the vector space model  This is not the only way to formalize relevance we will see later there are other ways to model relevance.
This way relevance later ways	 This is not the only way to formalize relevance we will see later there are other ways to model relevance  The basic idea of VS retrieval models is actually very easy to understand.
The actually easy understand.	 The basic idea of VS retrieval models is actually very easy to understand.
Imagine high dimensional corresponds term plot space represented vectors	 Imagine a high dimensional space where each dimension corresponds to a term we can plot our documents in this space since they are represented as vectors of term magnitudes.
6 ming	 In Figure 6 2 we show a threedimensional space with three words program ming library and presidential.
2 threedimensional program ming dimension.	2 we show a threedimensional space with three words program ming library and presidential  Each term defines one dimension.
defines We vectors assume placed example vector d1 document covers terms	 Each term defines one dimension  We can consider vectors in this three dimensional space and we will assume all our documents and the query will all be placed in this vector space  For example the vector d1 repre sents a document that probably covers the terms library and presidential without really talking about programming.
We space placed vector space d1 sents terms library presidential programming What It means vector represent ignore	 We can consider vectors in this three dimensional space and we will assume all our documents and the query will all be placed in this vector space  For example the vector d1 repre sents a document that probably covers the terms library and presidential without really talking about programming  What does this mean in terms of representation of the document It means that we will rely solely on this vector to represent the original document and thus ignore everything else including e.
For vector repre sents terms library programming mean terms rely solely vector document ignore	 For example the vector d1 repre sents a document that probably covers the terms library and presidential without really talking about programming  What does this mean in terms of representation of the document It means that we will rely solely on this vector to represent the original document and thus ignore everything else including e.
	g.
order problems.	 the order of the words which may sometimes be important to keep  It is thus not an optimal representation but it is often sufficient for many retrieval problems.
It optimal	 It is thus not an optimal representation but it is often sufficient for many retrieval problems.
represented vector doesn’t presidential.	 Now this is different from another document which might be represented as a different vector d2  In this case the document covers programming and library but it doesn’t talk about presidential.
document covers programming presidential library By space representation capture dif topics	 In this case the document covers programming and library but it doesn’t talk about presidential  As you can probably guess the topic is likely about programming language and the library is actually a software library  By using this vector space representation we can intuitively capture the dif ferences between topics of documents.
By representation documents Next d3 presidential We vector space pointing directions	 By using this vector space representation we can intuitively capture the dif ferences between topics of documents  Next d3 is pointing in a direction that might be about presidential and programming  We place all documents in our collection in this vector space and they will be pointing to all kinds of directions given by these three dimensions.
place collection pointing directions given place query vector.	 We place all documents in our collection in this vector space and they will be pointing to all kinds of directions given by these three dimensions  Similarly we can place our query in this space as another vector.
Similarly query space similarity vector document In 92 Chapter	 Similarly we can place our query in this space as another vector  We can then measure the similarity between the query vector and every document vector  In this case for example we can easily see d2 seems to be the closest to the query vector 92 Chapter 6 Retrieval Models and therefore d2 will be ranked above the others.
vector example query vector d2 main vector	 We can then measure the similarity between the query vector and every document vector  In this case for example we can easily see d2 seems to be the closest to the query vector 92 Chapter 6 Retrieval Models and therefore d2 will be ranked above the others  This is the main idea of the vector space model.
In case easily d2 closest Retrieval	 In this case for example we can easily see d2 seems to be the closest to the query vector 92 Chapter 6 Retrieval Models and therefore d2 will be ranked above the others.
precise model	 To be more precise the VS model is a framework.
In assumption represent document query term Here term word phrase ngrams characters representation.	 In this framework we make some assumptions  One assumption is that we represent each document and query by a term vector  Here a term can be any basic concept such as a word or a phrase or even ngrams of characters or any other feature representation.
represent document	 One assumption is that we represent each document and query by a term vector.
Here concept phrase feature term define V terms vocabulary V dimensional	 Here a term can be any basic concept such as a word or a phrase or even ngrams of characters or any other feature representation  Each term is assumed to define one dimension  Therefore since we have V  terms in our vocabulary we define a V dimensional space.
assumed V terms space A query consist number terms.	 Each term is assumed to define one dimension  Therefore since we have V  terms in our vocabulary we define a V dimensional space  A query vector would consist of a number of elements corresponding to the weights of different terms.
V vocabulary define query vector elements terms.	 Therefore since we have V  terms in our vocabulary we define a V dimensional space  A query vector would consist of a number of elements corresponding to the weights of different terms.
query number corresponding weights terms.	 A query vector would consist of a number of elements corresponding to the weights of different terms.
case vectors retrieval query vector document asked approach search engine realize far	 The relevance in this case is measured based on the similarity between the two vectors  Therefore our retrieval function is also defined as the similarity between the query vector and document vector  Now if you were asked to write a program to implement this approach for a search engine you would realize that this explanation was far from complete.
retrieval function similarity vector vector Now approach realize far	 Therefore our retrieval function is also defined as the similarity between the query vector and document vector  Now if you were asked to write a program to implement this approach for a search engine you would realize that this explanation was far from complete.
Now implement approach search explanation complete seen actually write program this.	 Now if you were asked to write a program to implement this approach for a search engine you would realize that this explanation was far from complete  We haven’t seen many things in detail therefore it’s impossible to actually write the program to implement this.
haven’t write program That’s called space order particular function	 We haven’t seen many things in detail therefore it’s impossible to actually write the program to implement this  That’s why this is called the vector space retrieval framework  It has to be refined in order to actually suggest a particular function that can be implemented on a computer.
order actually suggest function	 It has to be refined in order to actually suggest a particular function that can be implemented on a computer.
select concepts terms.	 First it did not say how to define or select the basic concepts terms.
We assume For example different matching matched actually matched	 We clearly assume the concepts are orthogonal otherwise there will be redundancy  For example if two synonyms are somehow distinguished as two different concepts they would be defined in two different dimensions causing a redundancy or overemphasis of matching this concept since it would be as if you matched two dimensions when you actually matched only one semantic concept.
For different causing overemphasis actually	 For example if two synonyms are somehow distinguished as two different concepts they would be defined in two different dimensions causing a redundancy or overemphasis of matching this concept since it would be as if you matched two dimensions when you actually matched only one semantic concept.
Second documents vector space.	 Second it did not say how to place documents and queries in this vector space.
We saw vectors exactly This define weights.	 We saw some examples of query and document vectors but where exactly should the vector for a particular document point to This is equivalent to how to define the term weights.
This question vector importance term assign weight matched term meaningful—it indicates characterizes document.	 This is a very important question because the term weight in the query vector indicates the importance of a term depending on how you assign the weight you might prefer some terms to be matched over others  Similarly term weight in the document is also very meaningful—it indicates how well the term characterizes the document.
meaningful—it	 Similarly term weight in the document is also very meaningful—it indicates how well the term characterizes the document.
If documents engine model terms weights define measure unclear These addressed actually	 If many nonrelevant documents are returned by a search engine using this model then the chosen terms and weights must not represent the documents accurately  Finally how to define the similarity measure is also unclear  These questions must be addressed before we can have an operational function that we can actually implement using a programming language.
similarity implement programming problems	 Finally how to define the similarity measure is also unclear  These questions must be addressed before we can have an operational function that we can actually implement using a programming language  Solving these problems is the main topic of the next section.
problems main topic 3 Space 93	 Solving these problems is the main topic of the next section  6 3 Vector Space Retrieval Models 93 6.
Models 93	 6 3 Vector Space Retrieval Models 93 6 3.
Space In discuss model specific previously vector model framework doesn’t	1 Instantiation of the Vector Space Model In this section we will discuss how to instantiate a vector space model so that we can get a very specific ranking function  As mentioned previously the vector space model is really a framework it doesn’t specify many things.
mentioned space doesn’t	 As mentioned previously the vector space model is really a framework it doesn’t specify many things  For example it did not say how we should define the dimensions of the vectors.
document vector query	 It also did not say how we place a document vector or query vector into this space.
definecalculate elements document vectors Finally similarity imagine compute use In Figure	 That is how should we definecalculate the values of all the elements in the query and document vectors Finally it did not say how we should compute similarity between the query vector and the document vector  As you can imagine in order to implement this model we have to determine specifically how we should compute and use these vectors  In Figure 6.
imagine implement determine	 As you can imagine in order to implement this model we have to determine specifically how we should compute and use these vectors.
In 6.	 In Figure 6.
3 instantiation vector space instantiation vocabulary V dimensions—this instantiation let’s look place space.	3 we illustrate the simplest instantiation of the vector space model  In this instantiation we use each word in our vocabulary to define a dimension thus giving V  dimensions—this is the bagofwords instantiation  Now let’s look at how we place vectors in this space.
use vocabulary define giving bagofwords Now let’s place vectors space Here simplest bit query document means one.	 In this instantiation we use each word in our vocabulary to define a dimension thus giving V  dimensions—this is the bagofwords instantiation  Now let’s look at how we place vectors in this space  Here the simplest strategy is to use a bit vector to represent both a query and a document and that means each element xi and yi would take a value of either zero or one.
When means present query it’s If user query ones zeros.	 When it’s one it means the corresponding word is present in the document or query  When it’s zero it’s absent  If the user types in a few words for a query then the query vector would have a few ones and many many zeros.
it’s absent.	 When it’s zero it’s absent.
types zeros query vector zeros	 If the user types in a few words for a query then the query vector would have a few ones and many many zeros  The document vector in general would have more ones than the query vector but there will still be many zeros since the vocabulary is often very large.
The general ones query Many don’t occur document occur given vocabulary document.	 The document vector in general would have more ones than the query vector but there will still be many zeros since the vocabulary is often very large  Many words in the vocabulary don’t occur in a single document many words will only occasionally occur in a given document  Most words in the vocabulary will be absent in any particular document.
query space let’s commonly similarity dot dot vectors simply defined products elements vectors 6.	 Now that we have placed the documents and the query in the vector space let’s look at how we compute the similarity between them  A commonly used similarity measure is the dot product the dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors  In Figure 6.
A similarity measure dot dot product vectors defined	 A commonly used similarity measure is the dot product the dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors.
x1 plus we’ve dimensions similarity function finally simplest instantiation vector model vector dot instantiation.	3 we see that it’s the product of x1 and y1 plus the product of x2 and y2 and so on  This is only one of the many different ways of computing the similarity  So we’ve defined the dimensions the vector space and the similarity function we finally have the simplest instantiation of the vector space model It’s based on the bit vector representation dot product similarity and bag of words instantiation.
different similarity we’ve space similarity finally simplest space model dot product words	 This is only one of the many different ways of computing the similarity  So we’ve defined the dimensions the vector space and the similarity function we finally have the simplest instantiation of the vector space model It’s based on the bit vector representation dot product similarity and bag of words instantiation.
So vector similarity finally instantiation space model It’s vector finally implement ranking language rank given particular	 So we’ve defined the dimensions the vector space and the similarity function we finally have the simplest instantiation of the vector space model It’s based on the bit vector representation dot product similarity and bag of words instantiation  Now we can finally implement this ranking function using a programming language and then rank documents in our corpus given a particular query.
Now ranking programming language corpus particular process retrieval problem vector place vector define similarity.	 Now we can finally implement this ranking function using a programming language and then rank documents in our corpus given a particular query  We’ve gone through the process of modeling the retrieval problem using a vector space model  Then we made assumptions about how we place vectors in the vector space and how we define the similarity.
We’ve gone process modeling retrieval problem space model space define	 We’ve gone through the process of modeling the retrieval problem using a vector space model  Then we made assumptions about how we place vectors in the vector space and how we define the similarity.
3 individual function	3  The next step is to think about whether this individual function actually makes sense.
function	 The next step is to think about whether this individual function actually makes sense.
actually It’s thinking value end we’ve mean Please minutes think proceeding	 Can we expect this function will actually perform well It’s worth thinking about the value that we are calculating in the end we’ve got a number but what does this number mean Please take a few minutes to think about that before proceeding to the next section  6.
6.	 6.
3 2 Bit Representation In simplest vector space model 6	3 2 Behavior of the Bit Vector Representation In order to assess whether this simplest vector space model actually works well let’s look at the example in Figure 6 4.
Vector vector example Figure	2 Behavior of the Bit Vector Representation In order to assess whether this simplest vector space model actually works well let’s look at the example in Figure 6.
figure documents The	4  This figure shows some sample documents and a simple query  The query is news about presidential campaign.
documents	 This figure shows some sample documents and a simple query.
For example terms	 For this example we will examine five documents from the corpus that cover different terms in the query.
think actually	 First think about how we actually use this model to score documents.
d3 query vector want The query words rest	5 we show two documents d1 and d3 and we have the query here also  In the vector space model we want to first compute the vectors for these documents and the query  The query has four words so for these four words there would be a one and for the rest there will be zeros.
vectors documents query.	 In the vector space model we want to first compute the vectors for these documents and the query.
Document news product corresponding	 Document d1 has two ones news and about while the rest of the dimensions are zeros  Now that we have the two vectors we can compute the similarity with the dot product by multiplying the corresponding elements in each vector.
dot vector.	 Now that we have the two vectors we can compute the similarity with the dot product by multiplying the corresponding elements in each vector.
Each product similarity items.	 Each pair of vectors forms a product which represents the similarity between the two items.
don’t care zeroes vector So sum we’re pairs ones In seen result	 We actually don’t have to care about the zeroes in each vector since any product with one will be zero  So when we take a sum over all these pairs we’re just counting how many pairs of ones there are  In this case we have seen two so the result will be two.
sum we’re pairs ones In case result scoring function unique query matched	 So when we take a sum over all these pairs we’re just counting how many pairs of ones there are  In this case we have seen two so the result will be two  That means this number is the value of this scoring function it’s simply the count of how many unique query terms are matched in the document.
case result That value simply unique terms document This score.	 In this case we have seen two so the result will be two  That means this number is the value of this scoring function it’s simply the count of how many unique query terms are matched in the document  This is how we interpret the score.
number function it’s simply unique query document.	 That means this number is the value of this scoring function it’s simply the count of how many unique query terms are matched in the document.
interpret score Now d3.	 This is how we interpret the score  Now we can also take a look at d3.
d3 result distinct words	 Now we can also take a look at d3  In this case you can see the result is three because d3 matched the three distinct query words news presidential and campaign whereas d1 only matched two.
ranked That	 Based on this d3 is ranked above d1  That looks pretty good.
That looks	 That looks pretty good.
examine model In Figure 6.	 However if we examine this model in detail we will find some problems  In Figure 6.
In 6 bit vector scoring number unique query	 In Figure 6 6 we show all the scores for these five documents  The bit vector scoring function counts the number of unique query terms matched in each docu ment.
scores documents bit scoring function number matched docu ment.	6 we show all the scores for these five documents  The bit vector scoring function counts the number of unique query terms matched in each docu ment.
unique relevant The documents tied score closer d4 right d3 mentioned	 If a document matches more unique query terms then the document will be assumed to be more relevant that seems to make sense  The only problem is that there are three documents d2 d3 and d4 that are tied with a score of three  Upon closer inspection it seems that d4 should be right above d3 since d3 only mentioned presidential once while d4 mentioned it many more times.
d3 d2 campaign Intuitively d3 higher	 Another problem is that d2 and d3 also have the same score since for d2 news about and campaign were matched  In d3 it matched news presidential and campaign  Intuitively d3 is more relevant and should be scored higher than d2.
impor tant matching query But means solve	 Matching presidential is more impor tant than matching about even though about and presidential are both in the query  But this model doesn’t do that and that means we have to solve these problems.
model problems.	 But this model doesn’t do that and that means we have to solve these problems.
To need 1 define document 2.	 To summarize we talked about how to instantiate a vector space model  We need to do three things 1  define the dimensions the concept of what a document is 2.
dimensions document 2 place queries	 define the dimensions the concept of what a document is 2  decide how to place documents and queries as vectors in the vector space and 3.
define similarity Based idea instantiate model probably simplest derive.	 define the similarity between two vectors  Based on this idea we discussed a very simple way to instantiate the vector space model  Indeed it’s probably the simplest vector space model that we can derive.
it’s probably simplest define dimension zeroone bit vector In	 Indeed it’s probably the simplest vector space model that we can derive  We used each word to define a dimension with a zeroone bit vector to represent a document or a query  In this case we only care about word presence or absence ignoring the frequency.
We define dimension bit query In case care word presence absence ignoring	 We used each word to define a dimension with a zeroone bit vector to represent a document or a query  In this case we only care about word presence or absence ignoring the frequency.
care frequency measure 6 Space Retrieval Models showed function document based it.	 In this case we only care about word presence or absence ignoring the frequency  For a similarity measure we used the dot product 6 3 Vector Space Retrieval Models 97 and showed that this scoring function scores a document based on the number of distinct query words matched in it.
similarity measure dot product	 For a similarity measure we used the dot product 6.
	 We also showed that such a simple vector space model still doesn’t work well and we need to improve it.
3 In improve bit We bit essentially counts query terms	3 Improved Instantiation In this section we will improve the representation of this model from the bit vector model  We saw the bit vector representation essentially counts how many unique query terms match the document  From Figure 6.
We vector unique	 We saw the bit vector representation essentially counts how many unique query terms match the document.
From 6 d4 relevant couldn’t capture following	 From Figure 6 6 we would like d4 to be ranked above d3 and d2 is really not relevant  The problem here is that this function couldn’t capture the following characteristics.
Second presidential common carry worth	 Second matching presidential should be more important than matching about because about is a very common word that occurs everywhere it doesn’t carry that much content  It’s worth thinking at this point about why we have these issues.
It’s thinking look assumptions instantiating VS model place	 It’s worth thinking at this point about why we have these issues  If we look back at the assumptions we made while instantiating the VS model we will realize that the problem is really coming from some of those assumptions  In particular it has to do with how we place the vectors in the vector space.
fix	 Naturally in order to fix these problems we have to revisit those assumptions.
consider occurred times occurred consider frequency—the term document.	 In order to consider the difference between a document where a query term occurred multiple times and one where the query term occurred just once we have to consider the term frequency—the count of a term in a document.
6 1 With bit ignoring actual occurred.	 6 1 With the bit vector we only captured the presence or absence of a term ignoring the actual number of times that a term occurred.
1 bit vector absence actual number times Let’s add document	1 With the bit vector we only captured the presence or absence of a term ignoring the actual number of times that a term occurred  Let’s add the count information back we will represent a document by a vector with as each dimension’s weight.
Now look like change similarity.	7  Now let’s see what the formula would look like if we change this representation  The formula looks identical since we are still using the dot product similarity.
look like	 Now let’s see what the formula would look like if we change this representation.
The formula dot product inside yi different—they’re	 The formula looks identical since we are still using the dot product similarity  The difference is inside of the sum since xi and yi are now different—they’re now the counts of words in the query and the document.
sum different—they’re query document change representation different interpretation	 The difference is inside of the sum since xi and yi are now different—they’re now the counts of words in the query and the document  Because of the change in document representation the new score has a different interpretation  We can see whether this would fix the problems of the bit vector VS model.
Because score different vector model.	 Because of the change in document representation the new score has a different interpretation  We can see whether this would fix the problems of the bit vector VS model.
query	 The query vector is the same because all these words occurred exactly once in the query.
The goes d3 result documents different occurred twice.	 The same goes for d2 and d3 since none of these words has been repeated  As a result the score is also the same for both these documents  But d4 would be different here presidential occurred twice.
As score Thus instead	 As a result the score is also the same for both these documents  But d4 would be different here presidential occurred twice  Thus the corresponding dimension would be weighted as two instead of one and the score for d4 is higher.
But d4 different twice weighted instead score means TF to.	 But d4 would be different here presidential occurred twice  Thus the corresponding dimension would be weighted as two instead of one and the score for d4 is higher  This means by using TF we can now rank d4 above d2 and d3 as we had hoped to.
Thus instead This means d4 hoped	 Thus the corresponding dimension would be weighted as two instead of one and the score for d4 is higher  This means by using TF we can now rank d4 above d2 and d3 as we had hoped to  6.
hoped 3 Vector Space Unfortunately d3 identical	 This means by using TF we can now rank d4 above d2 and d3 as we had hoped to  6 3 Vector Space Retrieval Models 99 Unfortunately d2 and d3 still have identical scores.
6.	 6.
We matching solve general Is content ignore	 We would like to give more credit for matching presidential than matching about  How can we solve this prob lem in a general way Is there any way to determine which word should be treated more importantly and which word can be essentially ignored About doesn’t carry that much content so we should be able to ignore it.
word frequent match	 We call such a word a stop word  They are generally very frequent and they occur everywhere such that match ing it doesn’t have any significance.
They generally ing word presidential stop One word like occurrence word M documents higher	 They are generally very frequent and they occur everywhere such that match ing it doesn’t have any significance  Can we come up with any statistical approaches to somehow distinguish a content word like presidential from a stop word like about One difference is that a word like about occurs everywhere  If you count the occurrence of the word in the whole collection of M documents where M  5 then we would see that about has a much higher count than presidential.
approaches distinguish word like One word If count occurrence word documents 5	 Can we come up with any statistical approaches to somehow distinguish a content word like presidential from a stop word like about One difference is that a word like about occurs everywhere  If you count the occurrence of the word in the whole collection of M documents where M  5 then we would see that about has a much higher count than presidential.
This use information dimension	 This idea suggests that we could somehow use the global statistics of terms or some other information to try to decrease the weight of the about dimension in the vector rep resentation of d2.
That able d2 particular idea inverse frequency IDF retrieval functions.	 That way we’ll be able to rank d3 on top of d2  This particular idea is called the inverse document frequency IDF  It is a very important signal used in modern retrieval functions.
inverse	 This particular idea is called the inverse document frequency IDF.
important signal modern frequency documents contain	 It is a very important signal used in modern retrieval functions  The document frequency is the count of documents that contain a particular term.
inverse document reward word doesn’t	 Here we say inverse document frequency because we actually want to reward a word that doesn’t occur in many documents.
	9.
low	 We can now penalize common words which generally have a low IDF and reward informative words that have a higher IDF.
frequency total compare Intuitively IDF score campaign informative word.	 counts the document frequency the total number of documents containing w  Let’s compare the terms campaign and about  Intuitively about should have a lower IDF score than campaign since about is a less informative word.
Let’s compare campaign IDF campaign informative	 Let’s compare the terms campaign and about  Intuitively about should have a lower IDF score than campaign since about is a less informative word.
Intuitively word clarity let’s M 10 dfcampaign 1166 use base logarithm k plot function varying curve like illustrated Figure	 Intuitively about should have a lower IDF score than campaign since about is a less informative word  For clarity let’s assume M  10 000 dfabout  5000 dfcampaign  1166 and we use a base two logarithm  Let k represent dfw if you plot the IDF function by varying k then you will see a curve like the one illustrated in Figure 6.
For 10 1166 base logarithm.	 For clarity let’s assume M  10 000 dfabout  5000 dfcampaign  1166 and we use a base two logarithm.
10.	10.
general value low indicating word.	 In general you can see it would give a higher value for a low df  indicating a rare word.
	 You can also see the maximum value of this function is logM  1.
The specific function heuristic popular terms.	 The specific function is not as important as the heuristic it captures penalizing popular terms.
Retrieval Models 101 IDF function research question evaluation learn function like line reasonable IDF function defined.	3 Vector Space Retrieval Models 101 the IDF function is an open research question  With the evaluation skills you will learn in Chapter 9 you can test your different instantiations  If we use a linear function like the diagonal line as shown in the figure it may not be as reasonable as the IDF function we just defined.
” makes term it’s unlikely term	” This makes sense when the term occurs so frequently that it’s unlikely to differentiate two documents’ relevance since the term is so common.
dropping point Intuitively want focus df course works validated running experiments data set.	 But if you look at the linear representation there is no dropping off point  Intuitively we want to focus more on the discrimination of low df words rather than these common words  Of course which one works better still has to be validated by running experiments on a data set.
Intuitively words works	 Intuitively we want to focus more on the discrimination of low df words rather than these common words  Of course which one works better still has to be validated by running experiments on a data set.
documents Figure	 Let’s look at the two documents again in Figure 6 11.
weighting bit adjust TF weight	 Without IDF weighting we just had bit vectors  With IDF weighting we now can adjust the TF term frequency weight by multiplying it with the IDF weight.
weighting adjust weight IDF weight scheme IDF value	 With IDF weighting we now can adjust the TF term frequency weight by multiplying it with the IDF weight  With this scheme there is an adjustment by using the IDF value of about which is smaller than the IDF value of presidential.
With value smaller IDF IDF distinguish words informative weighting ranked d2 word d2 common uninformative word.	 With this scheme there is an adjustment by using the IDF value of about which is smaller than the IDF value of presidential  Thus the IDF will distinguish these two words based on how informative they are  Including the IDF weighting causes d3 to be ranked above d2 since it matched a rare informative word whereas d2 matched a common uninformative word.
Thus words IDF weighting causes d3 d2 informative d2 matched common word.	 Thus the IDF will distinguish these two words based on how informative they are  Including the IDF weighting causes d3 to be ranked above d2 since it matched a rare informative word whereas d2 matched a common uninformative word.
causes ranked d2 matched word d2 uninformative word shows idea	 Including the IDF weighting causes d3 to be ranked above d2 since it matched a rare informative word whereas d2 matched a common uninformative word  This shows that the idea of weighting can solve our second problem.
idea solve second problem How weighting Well let’s documents	 This shows that the idea of weighting can solve our second problem  How effective is this model in general when we use this TFIDF weighting Well let’s take a look at all the documents that we have seen before  In Figure 6.
general TFIDF let’s seen Figure	 How effective is this model in general when we use this TFIDF weighting Well let’s take a look at all the documents that we have seen before  In Figure 6.
12 documents new TFIDF scores reasonable problem simplest vector highest	12 we show all the five documents that we have seen before and their new scores using TFIDF weighting  We see the scores for the first four documents seem to be quite reasonable  But again we also see a new problem since d5 did not even have a very high score with our simplest vector space model but now d5 has the highest score.
But new problem high space d5 score.	 We see the scores for the first four documents seem to be quite reasonable  But again we also see a new problem since d5 did not even have a very high score with our simplest vector space model but now d5 has the highest score.
model d5 score This actually common designing retrieval try introduce it’s tricky “best” function open	 But again we also see a new problem since d5 did not even have a very high score with our simplest vector space model but now d5 has the highest score  This is actually a common phenomenon when designing retrieval functions when you try to fix one problem you tend to introduce other problems That’s why it’s very tricky to design an effective ranking function and why finding a “best” ranking function is an open research question.
actually phenomenon designing try problem design ranking “best” ranking question In sections discuss ideas model fix problem.	 This is actually a common phenomenon when designing retrieval functions when you try to fix one problem you tend to introduce other problems That’s why it’s very tricky to design an effective ranking function and why finding a “best” ranking function is an open research question  In the next few sections we’ll continue to discuss some additional ideas to further improve this model and try to fix this problem.
sections continue model try fix	 In the next few sections we’ll continue to discuss some additional ideas to further improve this model and try to fix this problem  6 3.
TF Transformation In previous derived weighting formula vector model model actually d5 high	4 TF Transformation In the previous section we derived a TFIDF weighting formula using the vector space model and showed that this model actually works pretty well for the examples shown in the figures—except for d5 which has a very high score.
This document position desirable Now use transformation problem Before discuss details let’s look formula weighting ranking derived.	 This document is intuitively nonrelevant so its position is not desirable  Now we’re going to discuss how to use a TF transformation to solve this problem  Before we discuss the details let’s take a look at the formula for the TFIDF weighting and ranking function we previously derived.
we’re going discuss use problem.	 Now we’re going to discuss how to use a TF transformation to solve this problem.
details formula TFIDF ranking previously derived shown Figure 6	 Before we discuss the details let’s take a look at the formula for the TFIDF weighting and ranking function we previously derived  It is shown in Figure 6 13.
shown Figure 13.	 It is shown in Figure 6 13.
13 carefully involves sum matched query sum query weight weight TFIDF	13  If you look at the formula carefully you will see it involves a sum over all the matched query terms  Inside the sum each matched query term has a particular weight this weight is TFIDF weighting.
sum term weight	 Inside the sum each matched query term has a particular weight this weight is TFIDF weighting.
It component variables total .	 It has an IDF component where we see two variables one is the total number of documents in the collection M .
The formula query term w cw d	 The other variables involved in the formula include the count of the query term w in the query and the count of w in the document represented as cw q and cw d respectively.
d5 it’s hard reason received d5 higher score document Intriguingly order matching term	 Looking at d5 again it’s not hard to realize that the reason why it has received a high score is because it has a very high count of the term campaign  Its count in d5 is four which is much higher than the other documents and has contributed to the high score of this document  Intriguingly in order to lower the score for this document we need to somehow restrict the contribution of matching this term in the document.
Its higher documents	 Its count in d5 is four which is much higher than the other documents and has contributed to the high score of this document.
multiple ously.	 Essentially we shouldn’t reward multiple occurrences so gener ously.
occurrence occurrence second occurrence seen 50 occurrences document.	 If we see an extra occurrence on top of the first occurrence that is to go from one to two then we also can say the second occurrence confirmed that it’s not an accidental mention of the word  But imagine we have seen let’s say 50 occurrences of the word in the document.
50 occurrences document.	 But imagine we have seen let’s say 50 occurrences of the word in the document.
extra occur rence evidence document word.	 Then adding one extra occur rence is not going to bring new evidence about the term because we are already sure that this document is about this word.
highcount transformation 6.	 Thus we should restrict the contribu tion of a highcount term  That is exactly the idea of TF transformation illustrated in Figure 6.
xaxis count yaxis weight.	 On the xaxis is the raw count and on the yaxis is the TF weight.
In previous ranking functions implicitly kind transformation actually binary function shown here.	 In the previous ranking functions we actually have implicitly used some kind of transformation  For example in the zeroone bit vector representation we actually used the binary transformation function as shown here.
If count	 If the count is zero then it has zero weight.
Otherwise	 Otherwise it would have a weight of one.
considered term linear function We like red	 Then we considered term count as a TF weight which is a linear function  We just saw that this is not desirable  With a logarithm we can have a sublinear transformation that looks like the red lines in the figure.
desirable logarithm sublinear transformation like lines	 We just saw that this is not desirable  With a logarithm we can have a sublinear transformation that looks like the red lines in the figure.
influence count We curve	 This will control the influence of a very high weight because it’s going to lower its influence yet it will retain the influence of a small count  We might even want to bend the curve more by applying a logarithm twice.
bend applying logarithm twice.	 We might even want to bend the curve more by applying a logarithm twice.
Researchers tried methods working linear transformation far transformation called illustrated Figure 15 BM stands k bound	 Researchers have tried all these methods and they are indeed working better than the linear transformation but so far what works the best seems to be this special transformation called BM25 TF illustrated in Figure 6 15 where BM stands for best matching  In this transformation there is a parameter k which controls the upper bound of this function.
15 best matching In transformation parameter upper function.	15 where BM stands for best matching  In this transformation there is a parameter k which controls the upper bound of this function.
In controls upper bound function easy look x multiplied k exceed numerator denominator.	 In this transformation there is a parameter k which controls the upper bound of this function  It’s easy to see this function has a upper bound because if you look at the x xk as being multiplied by k  1 the fraction will never exceed one since the numerator is always less than the denominator.
function upper bound x xk multiplied k 1 fraction exceed numerator denominator it’s upperbounded k	 It’s easy to see this function has a upper bound because if you look at the x xk as being multiplied by k  1 the fraction will never exceed one since the numerator is always less than the denominator  Thus it’s upperbounded by k  1.
it’s 1 difference BM25 TF logarithm transformation doesn’t upper actually	 Thus it’s upperbounded by k  1  This is also the difference between the BM25 TF function and the logarithm transformation which doesn’t have an upper bound  Furthermore one interesting property of this function is that as we vary k we can actually simulate different transformation functions including the two extremes that are shown in the figure.
This difference TF logarithm upper function simulate different transformation	 This is also the difference between the BM25 TF function and the logarithm transformation which doesn’t have an upper bound  Furthermore one interesting property of this function is that as we vary k we can actually simulate different transformation functions including the two extremes that are shown in the figure.
function simulate different extremes shown transformation set k number look linear function.	 Furthermore one interesting property of this function is that as we vary k we can actually simulate different transformation functions including the two extremes that are shown in the figure  When k  0 we have a zero one bit transformation  If we set k to a very large number on the other hand it’s going to look more like the linear transformation function.
set large it’s sense transformation flexible control easily It nice property simple	 If we set k to a very large number on the other hand it’s going to look more like the linear transformation function  In this sense this transformation is very flexible since it allows us to control the shape of the TF curve quite easily  It also has the nice property of a simple upper bound.
transformation nice bound	 In this sense this transformation is very flexible since it allows us to control the shape of the TF curve quite easily  It also has the nice property of a simple upper bound  This upper bound is useful to control the influence of a particular term.
nice simple bound upper particular	 It also has the nice property of a simple upper bound  This upper bound is useful to control the influence of a particular term.
For spammer count term spam	 For example we can prevent a spammer from just increasing the count of one term to spam all queries that might match this term.
bound ensures compute	 In other words this upper bound ensures that all terms will be counted when we aggregate the weights to compute a score.
To summarize sublinearity	 To summarize we need to capture some sublinearity in the TF function.
term The TF formula discussed upper robust If plug TFIDF vector model ranking BM25 TF component.	 It also avoids a dominance by one single term over all others  The BM25 TF formula we discussed has an upper bound while being robust and effective  If we plug this function into our TFIDF vector space model then we would end up having a ranking function with a BM25 TF component.
If having function BM25 TF component This stateoftheart ranking function BM25 We’ll	 If we plug this function into our TFIDF vector space model then we would end up having a ranking function with a BM25 TF component  This is very close to a stateoftheart ranking function called BM25  We’ll see the entire BM25 formula soon.
This close function	 This is very close to a stateoftheart ranking function called BM25.
We’ll entire BM25 soon	 We’ll see the entire BM25 formula soon  6 3.
So model term document.	 So far in our exploration of the vector space model we considered the TF or the count of a term in a document.
In Figure 6.	 In Figure 6.
example d6	16 we show two example documents  Document d4 is very short with only one hundred words  Conversely d6 has five thousand words.
Conversely d6 matching query words d6 query reason matched words manner topic query’s	 Conversely d6 has five thousand words  If you look at the matching of these query words we see that d6 has many more matchings of the query words one might reason that d6 may have matched these query words in a scattered manner  Perhaps d6’s topic is not really the same as the query’s topic.
If look matching query words query d6 manner In beginning d6 discussion campaign.	 If you look at the matching of these query words we see that d6 has many more matchings of the query words one might reason that d6 may have matched these query words in a scattered manner  Perhaps d6’s topic is not really the same as the query’s topic  In the beginning of d6 there is discussion of a campaign.
Perhaps d6’s topic topic.	 Perhaps d6’s topic is not really the same as the query’s topic.
In d6 discussion mention end In think higher	 In the beginning of d6 there is discussion of a campaign  This discussion may have nothing to do with the mention of presidential at the end  In general if you think about long documents they would have a higher chance to match any query since they contain more words.
This discussion mention general think long	 This discussion may have nothing to do with the mention of presidential at the end  In general if you think about long documents they would have a higher chance to match any query since they contain more words.
think documents words generate long randomly sampling words distribution eventually match sense long documents chance	 In general if you think about long documents they would have a higher chance to match any query since they contain more words  In fact if you generate a long document by randomly sampling words from the distribution of all words then eventually you probably will match any query In this sense we should penalize long documents because they naturally have a better chance to match any query.
In long randomly words words eventually probably long better match This document On penalize	 In fact if you generate a long document by randomly sampling words from the distribution of all words then eventually you probably will match any query In this sense we should penalize long documents because they naturally have a better chance to match any query  This is our idea of document length normalization  On the one hand we want to penalize a long document but on the other hand we also don’t want to overpenalize them.
idea On want long document hand overpenalize different case	 This is our idea of document length normalization  On the one hand we want to penalize a long document but on the other hand we also don’t want to overpenalize them  The reason is that a document may be long because of different reason in one case the document may be longer because it uses more words.
hand document document different longer uses For	 On the one hand we want to penalize a long document but on the other hand we also don’t want to overpenalize them  The reason is that a document may be long because of different reason in one case the document may be longer because it uses more words  For example think about a research paper article.
The different reason document longer uses words example paper	 The reason is that a document may be long because of different reason in one case the document may be longer because it uses more words  For example think about a research paper article.
use words corresponding abstract case matching document paper short abstract matching	 It would use more words than the corresponding abstract  This is the case where we probably should penalize the matching of a long document such as a full paper  When we compare matching words in such long document with matching words in the short abstract the long papers generally have a higher chance of matching query words.
This probably penalize paper compare words document matching words short abstract long papers higher chance matching query Therefore long	 This is the case where we probably should penalize the matching of a long document such as a full paper  When we compare matching words in such long document with matching words in the short abstract the long papers generally have a higher chance of matching query words  Therefore we should penalize the long documents.
matching long papers higher matching words penalize long documents.	 When we compare matching words in such long document with matching words in the short abstract the long papers generally have a higher chance of matching query words  Therefore we should penalize the long documents.
Therefore penalize case document	 Therefore we should penalize the long documents  However there is another case when the document is long—that is when the document simply has more content  Consider a case of a long document where we simply concatenated abstracts of different papers.
However long—that document content case document simply concatenated abstracts papers.	 However there is another case when the document is long—that is when the document simply has more content  Consider a case of a long document where we simply concatenated abstracts of different papers.
Consider case simply abstracts different	 Consider a case of a long document where we simply concatenated abstracts of different papers.
need right degree discourse documents needed optimal document length normalization.	 That’s why we need to be careful about using the right degree of length penalization and an understanding of the discourse structure of documents is needed for optimal document length normalization.
method worked called illustrated 6.	 A method that has worked well is called pivoted length normalization illustrated in Figure 6.
17 originally Singhal al 1996.	17 and described originally in Singhal et al  1996.
Here average reference point That means assume right normalizer	 Here the idea is to use the average document length as a pivot or reference point  That means we will assume that for the average length documents the score is about right a normalizer would be one.
length right document longer penalization it’s average length	 That means we will assume that for the average length documents the score is about right a normalizer would be one  If a document is longer than the average document length then there will be some penalization  If it’s shorter than the average document length there’s even some reward.
longer length penalization.	 If a document is longer than the average document length then there will be some penalization.
xaxis represents yaxis	 The xaxis represents the length of a document  On the yaxis we show the normalizer i.
	 On the yaxis we show the normalizer i.
e.	e.
length normalization.	 the pivoted length normalization.
The interpolation normalized controlled b.	 The formula for the normalizer is an interpolation of one and the normalized document lengths controlled by a parameter b.
varies zero degree space retrieval models Figure 18.	 By adjusting b which varies from zero to one we can control the degree of length normalization  If we plug this length normalization factor into the vector space model ranking functions that we have already examined we will end up with stateoftheart retrieval models some of which are shown in Figure 6 18.
18 Let’s called pivoted	18  Let’s take a look at each of them  The first one is called pivoted length normaliza tion.
look called pivoted tion.	 Let’s take a look at each of them  The first one is called pivoted length normaliza tion.
We weighting There query TF component	 We see that it’s basically the TFIDF weighting model that we have discussed  The IDF component appears in the last term  There is also a query TF component and in the middle there is normalized TF.
IDF term.	 The IDF component appears in the last term.
penalty documents denominator weight b.	 We also put a document length normalizer in the denominator of the TF formula which causes a penalty for long documents since the larger the denominator is the smaller the TF weight is  The document length normalization is controlled by the parameter b.
The	 The next formula is called Okapi BM25 or just BM25.
similar pivoted normalization formula IDF component bit different There factor	 It’s similar to the pivoted length normalization formula in that it has an IDF component and a query TF component  In the middle the normalization is a little bit different we have a sublinear transformation with an upper bound  There is a length normalization factor here as well.
different sublinear	 In the middle the normalization is a little bit different we have a sublinear transformation with an upper bound.
There length normalization denominator weight smaller.	 There is a length normalization factor here as well  It achieves a similar effect as discussed before since we put the normalizer in the denominator  Thus again if a document is longer the term weight will be smaller.
achieves discussed normalizer	 It achieves a similar effect as discussed before since we put the normalizer in the denominator.
longer smaller.	 Thus again if a document is longer the term weight will be smaller.
We logically represent document slowly tweaking	 We have now reached one of the bestknown retrieval functions by thinking logically about how to represent a document and by slowly tweaking formulas and considering our initial assumptions.
	 6.
6 Models So document vector space.	3 6 Further Improvement of Basic VS Models So far we have talked mainly about how to place the document vector in vector space.
Basic Models mainly place vector important determining ranking function However considerations detail.	6 Further Improvement of Basic VS Models So far we have talked mainly about how to place the document vector in vector space  This has played an important role in determining the performance of the ranking function  However there are also other considerations that we did not really examine in detail.
However assumed represent document words.	 However there are also other considerations that we did not really examine in detail  We’ve assumed that we can represent a document as a bag of words.
For example	 Obviously we can see there are many other choices  For example 6.
example 6 words words transformed option word treated matched term.	 For example 6 3 Vector Space Retrieval Models 109 stemmed words words that have been transformed into a basic root form are a viable option so all forms of the same word are treated as one and can be matched as one term.
Retrieval stemmed transformed root viable option treated term need stop word words use phrases characterizes belong	3 Vector Space Retrieval Models 109 stemmed words words that have been transformed into a basic root form are a viable option so all forms of the same word are treated as one and can be matched as one term  We also need to perform stop word removal this removes some very common words that don’t carry any content such as the a or of   We could use phrases or even latent semantic analysis which characterizes documents by which cluster words belong to.
variations terms mean	 This is actually very important as we might have variations of the terms that prevent us from matching them with each other even though they mean the same thing.
text obtain originally	 We first need to segment text to obtain word boundaries because it’s originally just a sequence of characters.
respond characters space separate language word	 A word might cor respond to one character or two characters or even three characters  It’s easier in English when we have a space to separate the words but in some other languages we may need to do some natural language processing to determine word boundaries.
There possibility far dot compute cosine vectors distance	 There is also possibility to improve the similarity function  So far we’ve used the dot product but there are other measures  We could compute the cosine of the angle between two vectors or we can use a Euclidean distance measure.
dot compute cosine angle use distance	 So far we’ve used the dot product but there are other measures  We could compute the cosine of the angle between two vectors or we can use a Euclidean distance measure.
means dot product That equivalent cosine	 That means we first normalize each vector and then we take the dot product  That would be equivalent to the cosine measure.
We formulas—but development BM25 works BM25 line people derived F BM25	 We mentioned that BM25 seems to be one of the most effective formulas—but there has also been further development in improving BM25 although none of these works have changed the BM25 fundamentally  In one line of work people have derived BM25F  Here F stands for field and this is BM25 for documents with structure.
For example abstract research web combined different scoring Essentially BM25 scores keeps i.	 For example you might consider the title field the abstract field the body of the research article or even anchor text on web pages  These can all be combined with an appropriate weight on different fields to help improve scoring for each document  Essentially this formulation applies BM25 on each field and then combines the scores but keeps global i.
Essentially scores	 Essentially this formulation applies BM25 on each field and then combines the scores but keeps global i e.
frequency	e  across all fields frequency counts.
counts.	 across all fields frequency counts.
This overcounting transformation important Chapter weight advantage	 This has the advantage of avoiding overcounting the first occurrence of the term  Recall that in the sublinear transformation of TF the first occurrence is very important 110 Chapter 6 Retrieval Models and contributes a large weight  If we do that for all the fields then the same term might have gained a large advantage in every field.
details et al	 More details can be found in Robertson et al  2004.
2004.	 2004.
researchers problem long documents BM25 address	 Another line of extension is called BM25  Here researchers have addressed the problem of overpenalization of long documents by BM25  To address this problem the fix is actually quite simple.
address fix	 To address this problem the fix is actually quite simple.
We add normalization formula But what’s interesting analytically prove fix BM25.	 We can simply add a small constant to the TF normalization formula  But what’s interesting is that we can analytically prove that by doing such a small modification we will fix the problem of overpenalization of long documents by the original BM25.
formula empirically analytically shown better BM25 Lv Zhai	 Thus the new formula called BM25 is empirically and analytically shown to be better than BM25 Lv and Zhai 2011  6 3.
6.	 6.
vector space assum respect query query vectors high defined words concepts	7 Summary In vector space retrieval models we use similarity as a notion of relevance assum ing that the relevance of a document with respect to a query is correlated with the similarity between the query and the document  Naturally that implies that the query and document must be represented in the same way and in this case we represent them as vectors in a high dimensional vector space  The dimensions are defined by words concepts or terms.
document represented way case represent vectors dimensional space dimensions	 Naturally that implies that the query and document must be represented in the same way and in this case we represent them as vectors in a high dimensional vector space  The dimensions are defined by words concepts or terms.
defined words generally need include	 The dimensions are defined by words concepts or terms  We generally need to use multiple heuris tics to design a ranking function we gave some examples which show the need for several heuristics which include .
weighting	 TF term frequency weighting and sublinear transformation .
IDF general ranking function tasks.	 IDF inverse document frequency weighting and   document length normalization  These three are the most important heuristics to ensure such a general ranking function works well for all kinds of tasks.
document normalization These important heuristics ranking works kinds	 document length normalization  These three are the most important heuristics to ensure such a general ranking function works well for all kinds of tasks.
These works tasks Finally BM25 malization powerful measures	 These three are the most important heuristics to ensure such a general ranking function works well for all kinds of tasks  Finally BM25 and pivoted length nor malization seem to be the most effective VS formulas  While there has been some work done in improving these two powerful measures their main idea remains the same.
While work main idea remains section alternative approach space representation 6.	 While there has been some work done in improving these two powerful measures their main idea remains the same  In the next section we will discuss an alternative approach to the vector space representation  6.
section discuss alternative approach representation	 In the next section we will discuss an alternative approach to the vector space representation  6.
4 Models section look ranking functions vector before.	 6 4 Probabilistic Retrieval Models In this section we will look at a very different way to design ranking functions than the vector space model that we discussed before.
In probabilistic define function based given document relevant	 In probabilistic models we define the ranking function based on the probability that a given document d is relevant 6.
query q q 0 1 relevance words introduce binary random R length	4 Probabilistic Retrieval Models 111 to a query q or pR  1  d  q where R ∈ 0 1 is a binary random variable denoting relevance  In other words we introduce a binary random variable R and we model the query and the documents as observations from random variables  Note that in the vector space model we assume that documents are all equal length vectors.
words model observations assume documents vectors.	 In other words we introduce a binary random variable R and we model the query and the documents as observations from random variables  Note that in the vector space model we assume that documents are all equal length vectors.
data observed	 Here we assumed they are the data observed from random variables.
Thus relevance.	 Thus the problem is to estimate the probability of relevance.
category models variants abilistic led retrieval discussed vector similar types els.	 In this category of models there are many different variants  The classic prob abilistic model has led to the BM25 retrieval function which we discussed in the vector space model section because its form is quite similar to these types of mod els.
retrieval called In going likelihood effective models probabilistic line models	 We will discuss another special case of probabilistic retrieval functions called language modeling approaches to retrieval  In particular we’re going to discuss the query likelihood retrieval model which is one of the most effective models in probabilistic models  There is also another line of functions called divergencefrom randomness models such as the PL2 function Amati and Van Rijsbergen 2002.
called divergencefrom randomness function	 There is also another line of functions called divergencefrom randomness models such as the PL2 function Amati and Van Rijsbergen 2002.
stateoftheart models likelihood assumption relevance given pq 1.	 It’s also one of the most effective stateoftheart retrieval models  In query likelihood our assumption is that this probability of relevance can be approximated by the probability of a query given a document and relevance pq  d  R  1.
Intuitively query retrieve document d contains document d user	 Intuitively this probability just captures the following probability if a user likes document d how likely would the user enter query q in order to retrieve document d The condition part contains document d and R  1 which can be interpreted as the condition that the user likes document d.
understand let’s look retrieval models 6.	 To understand this idea let’s first take a look at the basic idea of probabilistic retrieval models  Figure 6.
6.	 Figure 6.
19 imagined relevance documents query typed d1 user A “1” right thinks d1	19 lists some imagined relevance status values or relevance judgments of queries and documents  It shows that q1 is a query that the user typed in and d1 is a document the user has seen  A “1” in the far right column means the user thinks d1 is relevant to q1.
	 It shows that q1 is a query that the user typed in and d1 is a document the user has seen.
far right thinks	 A “1” in the far right column means the user thinks d1 is relevant to q1.
R search engine watching results In d1 associated q1 d1 clicked there’s associated d2.	 The R here can be also approximated by the clickthrough data that the search engine can collect by watching how users interact with the search results  In this case let’s say the user clicked on document d1 so there’s a one associated with the pair q1 d1  Similarly the user clicked on d2 so there’s a one associated with q1 d2.
Similarly clicked d2 q1 d2 d3 non relevant d4 d5 relevant second user issuing	 Similarly the user clicked on d2 so there’s a one associated with q1 d2  Thus d2 is assumed to be relevant to q1 while d3 is non relevant d4 is nonrelevant d5 is again relevant and so on and so forth  Perhaps the second half of the table after the ellipses is from a different user issuing the same queries.
d3 nonrelevant ellipses queries This user typed q1	 Thus d2 is assumed to be relevant to q1 while d3 is non relevant d4 is nonrelevant d5 is again relevant and so on and so forth  Perhaps the second half of the table after the ellipses is from a different user issuing the same queries  This other user typed in q1 and then found that d1 is actually not useful which is in contrast to the first user’s judgement.
half table user typed useful judgement large search ask estimate probability relevance” Simply look particular particular q likely	 Perhaps the second half of the table after the ellipses is from a different user issuing the same queries  This other user typed in q1 and then found that d1 is actually not useful which is in contrast to the first user’s judgement  We can imagine that we have a large amount of search data and are able to ask the question “how can we estimate the probability of relevance” Simply if we look at all the entries where we see a particular d and a particular q we can calculate how likely we will see a one in the third column.
q1 d1 contrast user’s search estimate probability look particular q calculate likely column q pair count times actually seen	 This other user typed in q1 and then found that d1 is actually not useful which is in contrast to the first user’s judgement  We can imagine that we have a large amount of search data and are able to ask the question “how can we estimate the probability of relevance” Simply if we look at all the entries where we see a particular d and a particular q we can calculate how likely we will see a one in the third column  We can first count how many times we see q and d as a pair in this table and then count how many times we actually have also seen a one in the third column and compute the ratio.
We count times q d pair count	 We can first count how many times we see q and d as a pair in this table and then count how many times we actually have also seen a one in the third column and compute the ratio.
Let’s compute probability d2 q1.	 Let’s take a look at some specific examples  Suppose we are trying to compute this probability for d1 d2 and d3 for q1.
Suppose probability d1 q1 What If consider q1 cases user document relevant So cases 0.	 Suppose we are trying to compute this probability for d1 d2 and d3 for q1  What is the estimated probability If we are interested in q1 and d1 we consider the two pairs containing q1 and d1 only in one of the two cases has the user said that the document is relevant  So R is equal to 1 in only one of the two cases which gives our probability a value of 0.
probability interested d1 q1 user said R 1 5.	 What is the estimated probability If we are interested in q1 and d1 we consider the two pairs containing q1 and d1 only in one of the two cases has the user said that the document is relevant  So R is equal to 1 in only one of the two cases which gives our probability a value of 0 5.
So cases value What d2 d2 R	 So R is equal to 1 in only one of the two cases which gives our probability a value of 0 5  What about d2 and d3 For d2 R is equal to 1 in both cases.
R We d2 q1.	 For d3 R is equal to 0 in both cases  We now have a score for d1 d2 and d3 for q1.
rank based probabilities—that’s basic idea probabilistic retrieval going rank volumes clickthrough data search engine learn	 We can simply rank them based on these probabilities—that’s the basic idea of probabilistic retrieval model  In our example it’s going to rank d2 above all the other documents because in all the cases given q1 and d2 R  1  With volumes of clickthrough data a search engine can learn to improve its results.
With data search simple example entries These probabilities user	 With volumes of clickthrough data a search engine can learn to improve its results  This is a simple example that shows that with even a small number of entries we can already estimate some probabilities  These probabilities would give us some sense about which document might be more useful to a user for this query.
document useful query Of course observe documents.	 These probabilities would give us some sense about which document might be more useful to a user for this query  Of course the problem is that we don’t observe all the queries and all of the documents and all the relevance values there will be many unseen documents.
problem observe documents relevance unseen documents collect documents fact unseen queries typed	 Of course the problem is that we don’t observe all the queries and all of the documents and all the relevance values there will be many unseen documents  In general we can only collect data from the documents that we have shown to the users  In fact there are even more unseen queries because you cannot predict what queries will be typed in by users.
In general documents shown users.	 In general we can only collect data from the documents that we have shown to the users.
Obviously approach won’t queries unseen documents basic retrieval	 Obviously this approach won’t work if we apply it to unseen queries or unseen documents  Nevertheless this shows the basic idea of the probabilistic retrieval model.
shows basic idea retrieval unseen documents solution In case query retrieval model approximate conditional probability d 1 Lafferty	 Nevertheless this shows the basic idea of the probabilistic retrieval model  What do we do in such a case when we have a lot of unseen documents and unseen queries The solution is that we have to approximate in some way  In the particular case called the query likelihood retrieval model we just approximate this by another conditional probability pq  d  R  1 Lafferty and Zhai 2003.
unseen queries The approximate way model conditional probability likes document liked kind	 What do we do in such a case when we have a lot of unseen documents and unseen queries The solution is that we have to approximate in some way  In the particular case called the query likelihood retrieval model we just approximate this by another conditional probability pq  d  R  1 Lafferty and Zhai 2003  We assume that the user likes the document because we have seen that the user clicked on this document and we are interested in all these cases when a user liked this particular document and want to see what kind of queries they have used.
In case called likelihood retrieval model approximate 1 We assume likes user particular document	 In the particular case called the query likelihood retrieval model we just approximate this by another conditional probability pq  d  R  1 Lafferty and Zhai 2003  We assume that the user likes the document because we have seen that the user clicked on this document and we are interested in all these cases when a user liked this particular document and want to see what kind of queries they have used.
document seen document user particular assume user based imaginary relevant document.	 We assume that the user likes the document because we have seen that the user clicked on this document and we are interested in all these cases when a user liked this particular document and want to see what kind of queries they have used  Note that we have made an interesting assumption here we assume that a user formulates the query based on an imaginary relevant document.
If look conditional obvious assumption.	 If you just look at this as a conditional probability it’s not obvious we are making this assumption.
estimate conditional probability big before.	 We have to somehow be able to estimate this conditional probability without relying on the big table from Figure 6 19  Otherwise we would have similar problems as before.
making way bypass new model works likely imaginary docu user’s formulates query probability conditional query particular doc ument fact relevant document	 By making this assumption we have some way to bypass the big table  Let’s look at how this new model works for our example  We ask the following question which of these documents is most likely the imaginary relevant docu ment in the user’s mind when the user formulates this query We quantify this probability as a conditional probability of observing this query if a particular doc ument is in fact the imaginary relevant document in the user’s mind.
ask query probability conditional particular ument imaginary relevant document We probabilities—that values	 We ask the following question which of these documents is most likely the imaginary relevant docu ment in the user’s mind when the user formulates this query We quantify this probability as a conditional probability of observing this query if a particular doc ument is in fact the imaginary relevant document in the user’s mind  We compute all these query likelihood probabilities—that is the likelihood of the query given each document  Once we have these values we can then rank these documents.
We likelihood Once documents To idea trieval model assume binary conditional	 We compute all these query likelihood probabilities—that is the likelihood of the query given each document  Once we have these values we can then rank these documents  To summarize the general idea of modeling relevance in the probabilistic re trieval model is to assume that we introduce a binary random variable R and let the scoring function be defined based on the conditional probability pR  1  d  q.
values rank modeling relevance trieval model binary random based pR d q We	 Once we have these values we can then rank these documents  To summarize the general idea of modeling relevance in the probabilistic re trieval model is to assume that we introduce a binary random variable R and let the scoring function be defined based on the conditional probability pR  1  d  q  We also talked about approximating this by using query likelihood.
summarize relevance probabilistic binary random R based probability pR d We query likelihood that’s probability query given	 To summarize the general idea of modeling relevance in the probabilistic re trieval model is to assume that we introduce a binary random variable R and let the scoring function be defined based on the conditional probability pR  1  d  q  We also talked about approximating this by using query likelihood  This means we have a ranking function that’s based on a probability of a query given the document.
approximating query likelihood means function given	 We also talked about approximating this by using query likelihood  This means we have a ranking function that’s based on a probability of a query given the document.
This means ranking function that’s probability query document This interpreted likes doc ument d	 This means we have a ranking function that’s based on a probability of a query given the document  This probability should be interpreted as the probability that a user who likes doc ument d would pose query q.
probability likes doc ument pose query q Now question course discuss section	 This probability should be interpreted as the probability that a user who likes doc ument d would pose query q  Now the question of course is how do we compute this conditional probability We will discuss this in detail in the next section  6.
Now question conditional probability We section 6	 Now the question of course is how do we compute this conditional probability We will discuss this in detail in the next section  6 4.
Likelihood Model In model pose particular 6.	4 1 The Query Likelihood Retrieval Model In the query likelihood retrieval model we quantify how likely a user would pose a particular query in order to find a particular document  Figure 6.
1 The Likelihood Retrieval Model query model quantify likely user order	1 The Query Likelihood Retrieval Model In the query likelihood retrieval model we quantify how likely a user would pose a particular query in order to find a particular document.
shows model imagines generates document’s tent In example document “presidential	20 shows how the query likelihood model assumes a user imagines some ideal document and generates a query based on that ideal document’s con tent  In this example the ideal document is about “presidential campaign news.
example document news.	 In this example the ideal document is about “presidential campaign news.
example pick word like presidential use The user pick like query word.	 For example a user might pick a word like presidential from this imaginary document and then use this as a query word  The user would then pick another word like campaign and that would be the second query word.
user pick second query word.	 The user would then pick another word like campaign and that would be the second query word.
course assumption queries.	 Of course this is only an assumption we have made about how users pose queries.
This fundamental derive retrieval implement models.	 This is why we can use this fundamental idea to further derive retrieval functions that we can implement with language models.
We’ve assumption query user’s	 We’ve made the assumption that each query word is independent and that each word is obtained from the imagined ideal document satisfying the user’s information need.
document presidential divided total number e.	 For example the probability of presidential given the document would be just the count of presidential in the document divided by the total number of words in the document i e.
document length actual formula use	 the document length  We now have an actual formula for retrieval that we can use to rank documents.
formula retrieval rank look documents	 We now have an actual formula for retrieval that we can use to rank documents  Let’s take a look at some example documents from Figure 6 21.
	 Let’s take a look at some example documents from Figure 6.
Suppose query campaign.	21  Suppose now the query is presidential campaign.
documents	 To score these documents we just count how many times we have seen presidential and how many times we have seen campaign.
We’ve presidential times d4 that’s d4 We multiply calculate documents d3	 We’ve seen presidential two times in d4 so that’s 2 d4   We also multiply by 1 d4 for the probability of campaign  Similarly we can calculate probabilities for the other two documents d3 and d2.
We 1 d4 campaign d3 length d2.	 We also multiply by 1 d4 for the probability of campaign  Similarly we can calculate probabilities for the other two documents d3 and d2  If we assume d3 and d4 have about the same length then it looks like we will rank d4 above d3 which is above d2.
If assume length looks d4 d3 d2 As expect formulation captures TF vector space update problem.	 If we assume d3 and d4 have about the same length then it looks like we will rank d4 above d3 which is above d2  As we would expect it looks like this formulation captures the TF heuristic from the vector space models  However if we try a different query like this one presidential campaign update then we might see a problem.
expect looks formulation models However different like campaign problem.	 As we would expect it looks like this formulation captures the TF heuristic from the vector space models  However if we try a different query like this one presidential campaign update then we might see a problem.
try campaign update Consider word update contain According assumption document generate query update 116 Chapter Models	 However if we try a different query like this one presidential campaign update then we might see a problem  Consider the word update none of the documents contain this word  According to our assumption that a user would pick a word from a document to generate a query the probability of obtaining a word like update 116 Chapter 6 Retrieval Models would be zero.
According assumption word document query probability zero Clearly causes problem generating query.	 According to our assumption that a user would pick a word from a document to generate a query the probability of obtaining a word like update 116 Chapter 6 Retrieval Models would be zero  Clearly this causes a problem because it would cause all these documents to have zero probability of generating this query.
it’s probability d2 d4 longer distinguish	 While it’s fine to have a zero probability for d2 which is not relevant it’s not okay to have zero probability for d3 and d4 because now we no longer can distinguish them.
assumptions ranking function.	 Clearly that’s not desirable  When one has such a result we should think about what has caused this problem examining what assumptions have been made as we derive this ranking function.
When result think examining function.	 When one has such a result we should think about what has caused this problem examining what assumptions have been made as we derive this ranking function.
let’s imagine draw word document model Figure 6 22.	 So let’s consider an improved model  Instead of drawing a word from the document let’s imagine that the user would actually draw a word from a document language model as depicted in Figure 6 22.
Here generated unigram necessarily zero probability word update model assign word.	22  Here we assume that this document is generated by using this unigram language model which doesn’t necessarily assign zero probability for the word update  In fact we assume this model does not assign zero probability for any word.
assume language model doesn’t assign word	 Here we assume that this document is generated by using this unigram language model which doesn’t necessarily assign zero probability for the word update.
fact model zero	 In fact we assume this model does not assign zero probability for any word.
If we’re different user distribution ideal based documents corpus.	 If we’re thinking this way then the generative process is a bit different the user has this model distribution of words in mind instead of a particular ideal document although the model still has to be estimated based on the documents in our corpus.
They word presidential The word occur document.	 They may pick a word such as presidential and another word such as campaign  The difference is that now we can pick a word like update even though it doesn’t occur in the document.
difference word occur document This problem zero probabilities it’s we’re user language document 6.	 The difference is that now we can pick a word like update even though it doesn’t occur in the document  This would fix our problem with zero probabilities and it’s also reasonable because we’re now thinking of what the user is looking for in a more general way via a unigram language model instead of a single fixed document  In Figure 6.
problem zero it’s way language fixed In 6.	 This would fix our problem with zero probabilities and it’s also reasonable because we’re now thinking of what the user is looking for in a more general way via a unigram language model instead of a single fixed document  In Figure 6.
In Figure	 In Figure 6.
assump tion pq probability word document’s We score rank process	 By making an independence assump tion we could have pq  d as a product of the probability of each query word in each document’s language model  We score these two documents and then rank them based on the probabilities we calculate  Let’s formally state our scoring process for query likelihood.
We score documents based probabilities state process query likelihood A q contains words q w1	 We score these two documents and then rank them based on the probabilities we calculate  Let’s formally state our scoring process for query likelihood  A query q contains the words q  w1 w2 .
q w1 w2	 A query q contains the words q  w1 w2   .
.	 .
q n The particular	  wn such that q  n  The scoring or ranking function is then the probability that we observe q given that a user is thinking of a particular document d.
product words based pw1 × .	 This is the product of probabilities of all individual words which is based on the independence assumption mentioned before pq  d  pw1  d × pw2  d × .
.	 .
pwn d.	   × pwn  d.
	 × pwn  d  6.
6 118 Models practice score query likelihood log i1 d log	 6 4 118 Chapter 6 Retrieval Models In practice we score the document for this query by using a logarithm of the query likelihood scoreq  d  log pq  d n∑ i1 log pwi  d ∑ w∈V cw q log pw  d.
4 Chapter 6 Retrieval Models In practice score document query logarithm query likelihood pq pwi ∑ log 5 small multiplied precision	4 118 Chapter 6 Retrieval Models In practice we score the document for this query by using a logarithm of the query likelihood scoreq  d  log pq  d n∑ i1 log pwi  d ∑ w∈V cw q log pw  d  6 5 We do this to avoid having numerous small probabilities multiplied together which could cause underflow and precision loss.
5 We numerous small multiplied cause precision loga order simultaneously avoiding problem equation sum V word query.	5 We do this to avoid having numerous small probabilities multiplied together which could cause underflow and precision loss  By transforming using a loga rithm we maintain the order of these documents while simultaneously avoiding the underflow problem  Note the last term in the equation above in this sum we have a sum over all the possible words in the vocabulary V and iterate through each word in the query.
don’t know document language model	 The only part we don’t know is this document language model pw  d.
estimation pw lead ranking like place document space Here different parameters different ranking functions	 There fore we can convert the retrieval problem into the problem of estimating this document language model so that we can compute the probability of a query being generated by each document  Different estimation methods for pw  d lead to dif ferent ranking functions and this is just like the different ways to place a document into a vector in the vector space model  Here there are different ways to estimate parameters in the language model which lead to different ranking functions for query likelihood.
methods lead dif functions different place vector space Here ways language different	 Different estimation methods for pw  d lead to dif ferent ranking functions and this is just like the different ways to place a document into a vector in the vector space model  Here there are different ways to estimate parameters in the language model which lead to different ranking functions for query likelihood  6.
language ranking query	 Here there are different ways to estimate parameters in the language model which lead to different ranking functions for query likelihood  6 4.
	 6 4.
4 calculating likelihood recall log probabilities words probability document	4 2 Smoothing the Document Language Model When calculating the query likelihood retrieval score recall that we take a sum of log probabilities over all of the query words using the probability of a word in the query given the document i.
	e.
language estimate language model.	 the document language model  The main task now is to estimate this document language model.
The task estimate document In look task obvious choice maximum estimation MLE 2.	 The main task now is to estimate this document language model  In this section we look into this task in more detail  First of all how do we estimate this language model The obvious choice would be the maximum likelihood estimation MLE that we have seen before in Chap ter 2.
First language model choice maximum likelihood estimation seen ter normalize document frequency count equal probability estimation	 First of all how do we estimate this language model The obvious choice would be the maximum likelihood estimation MLE that we have seen before in Chap ter 2  In MLE we normalize the word frequencies in the document by the document length  Thus all the words that have the same frequency count will have an equal probability under this estimation method.
MLE document length.	 In MLE we normalize the word frequencies in the document by the document length.
Thus words count Note probability words word formulate sampling document.	 Thus all the words that have the same frequency count will have an equal probability under this estimation method  Note that words that have not occurred in the document will have zero probability  In other words we assume the user will sample a word from the document to formulate the query and there is no chance of sampling any word that is not in the document.
occurred document assume user query sampling word document.	 Note that words that have not occurred in the document will have zero probability  In other words we assume the user will sample a word from the document to formulate the query and there is no chance of sampling any word that is not in the document.
document formulate good improve assign nonzero probability away probability mass seen words extra probability unseen sum	 In other words we assume the user will sample a word from the document to formulate the query and there is no chance of sampling any word that is not in the document  But we know that’s not good so how would we improve this In order to assign a nonzero probability to words that have not been observed in the document we would have to take away some probability mass from seen words because we need some extra probability mass for the unseen words—otherwise they won’t sum to one.
But improve order assign nonzero observed document away probability mass words extra probability words—otherwise sum To assign nonzero words	 But we know that’s not good so how would we improve this In order to assign a nonzero probability to words that have not been observed in the document we would have to take away some probability mass from seen words because we need some extra probability mass for the unseen words—otherwise they won’t sum to one  To make this transformation and to improve the MLE we will assign nonzero probabilities to words that are not observed in the data.
improve observed	 To make this transformation and to improve the MLE we will assign nonzero probabilities to words that are not observed in the data.
factor accurate representation actual Imagine paper imagine document	 Considering this factor a smoothed language model would be a more accurate representation of the actual document  Imagine you have seen the abstract of a research paper or imagine a document is just an abstract.
abstract	 Imagine you have seen the abstract of a research paper or imagine a document is just an abstract.
If words probability zero sampling outside abstract Imagine user interested actually abstract asked write text words appear abstract.	 If we assume words that don’t appear in the abstract have a probability of zero that means sampling a word outside the abstract is impossible  Imagine the user who is interested in the topic of this abstract the user might actually choose a word that is not in the abstract to use as query  In other words if we had asked this author to write more the author would have written the full text of the article which contains words that don’t appear in the abstract.
abstract user choose word use query asked author written contains don’t abstract.	 Imagine the user who is interested in the topic of this abstract the user might actually choose a word that is not in the abstract to use as query  In other words if we had asked this author to write more the author would have written the full text of the article which contains words that don’t appear in the abstract.
words asked author write author abstract model abstract that’s tricky	 In other words if we had asked this author to write more the author would have written the full text of the article which contains words that don’t appear in the abstract  So smoothing the language model is attempting to try to recover the model for the whole article  Of course we don’t usually have knowledge about the words not observed in the abstract so that’s why smoothing is actually a tricky problem.
recover article usually words observed abstract that’s tricky The unseen words.	 So smoothing the language model is attempting to try to recover the model for the whole article  Of course we don’t usually have knowledge about the words not observed in the abstract so that’s why smoothing is actually a tricky problem  The key question here is what probability should be assigned to those unseen words.
don’t usually knowledge words observed abstract that’s smoothing actually tricky question As approaches issue.	 Of course we don’t usually have knowledge about the words not observed in the abstract so that’s why smoothing is actually a tricky problem  The key question here is what probability should be assigned to those unseen words  As one would imagine there are many different approaches to solve this issue.
The key probability unseen words different solve	 The key question here is what probability should be assigned to those unseen words  As one would imagine there are many different approaches to solve this issue.
As approaches that’s retrieval probability unseen word probability model That don’t observe word corpus assume governed reference	 As one would imagine there are many different approaches to solve this issue  One idea that’s very useful for retrieval is to let the probability of an unseen word be proportional to its probability as given by a reference language model  That means if you don’t observe the word in the corpus we’re going to assume that its probability is governed by another reference language model that we construct.
idea that’s let probability given reference That means don’t we’re going reference language model construct.	 One idea that’s very useful for retrieval is to let the probability of an unseen word be proportional to its probability as given by a reference language model  That means if you don’t observe the word in the corpus we’re going to assume that its probability is governed by another reference language model that we construct.
means don’t reference construct It unseen words In natural reference LM.	 That means if you don’t observe the word in the corpus we’re going to assume that its probability is governed by another reference language model that we construct  It will tell us which unseen words have a higher probability than other unseen words  In the case of retrieval a natural choice would be to take the collection LM as the reference LM.
unseen words In retrieval natural collection reference That don’t observe going assume proportional word collection.	 It will tell us which unseen words have a higher probability than other unseen words  In the case of retrieval a natural choice would be to take the collection LM as the reference LM  That is to say if you don’t observe a word in the document we’re going to assume that the probability of this word would be proportional to the probability of the word in the whole collection.
don’t probability probability word	 That is to say if you don’t observe a word in the document we’re going to assume that the probability of this word would be proportional to the probability of the word in the whole collection.
More formally we’ll estimating word given d pseenw d αd	 More formally we’ll be estimating the probability of a word given a document as follows pw  d pseenw  d if w seen in d αd .
	 pw  C otherwise.
6 document MLE word seen let proportional collection assign words.	 6 6 If the word is seen in the document then the probability would be a discounted MLE estimate pseen  Otherwise if the word is not seen in the document we’ll let the probability be proportional to the probability of the word in the collection pw  C with the coefficient αd controlling the amount of probability mass that we assign to unseen words.
6 word document let proportional probability collection pw C coefficient αd controlling assign unseen probabilities αd	6 If the word is seen in the document then the probability would be a discounted MLE estimate pseen  Otherwise if the word is not seen in the document we’ll let the probability be proportional to the probability of the word in the collection pw  C with the coefficient αd controlling the amount of probability mass that we assign to unseen words  Regardless of whether the word w is seen in the document or not all these probabilities must sum to one so αd is constrained.
word let probability pw probability assign unseen	 Otherwise if the word is not seen in the document we’ll let the probability be proportional to the probability of the word in the collection pw  C with the coefficient αd controlling the amount of probability mass that we assign to unseen words.
seen document probabilities αd	 Regardless of whether the word w is seen in the document or not all these probabilities must sum to one so αd is constrained.
24 In sum query corpus Although vocabulary word frequency	24  In this formula we have a sum over all the query words written in the form of a sum over the corpus vocabulary  Although we sum over words in the vocabulary in effect we are just taking a sum of query words since each word is weighted by its frequency in the query.
formula query form	 In this formula we have a sum over all the query words written in the form of a sum over the corpus vocabulary.
Although vocabulary words	 Although we sum over words in the vocabulary in effect we are just taking a sum of query words since each word is weighted by its frequency in the query.
Such way write sum convenient	 Such a way to write this sum is convenient in some transformations.
decompose sum matched words	 Using this form we can decompose this sum into two parts one over all the query words that are matched in the document and the other over all the words that are not matched.
These unmatched probability smoothing We rewrite sum query minus words d ∈ V αd .	 These unmatched words have a different form of probability because of our assumption about smoothing  We can then rewrite the second sum of query words not matched in d as a difference between the scores of all words in the vocabulary minus all the query words matched in d  This is actually quite useful since part of the sum over all w ∈ V can now be written as q log αd .
sum V q log Additionally sum query like vector space vector.	 This is actually quite useful since part of the sum over all w ∈ V can now be written as q log αd   Additionally the sum of query words matched in d is in terms of words that we observe in the query  Just like in the vector space model we are now able to take a sum of terms in the intersection of the query vector and the document vector.
vector sum terms query	 Just like in the vector space model we are now able to take a sum of terms in the intersection of the query vector and the document vector.
If look rewriting shown 25 benefits.	 If we look at this rewriting further as shown in Figure 6 25 we can see how it actually would give us two benefits.
ranking function.	25 we can see how it actually would give us two benefits  The first benefit is that it helps us better understand the ranking function.
helps we’re collection heuristics similar second allows efficiently need consider matched	 The first benefit is that it helps us better understand the ranking function  In particular we’re going to show that from this formula we can see the connection of smoothing using a collection language model with weighting heuristics similar to TFIDF weighting and length normalization  The second benefit is that it also allows us to compute the query likelihood more efficiently since we only need to consider terms matched in the query.
particular going smoothing collection similar The compute query likelihood efficiently consider main sum query	 In particular we’re going to show that from this formula we can see the connection of smoothing using a collection language model with weighting heuristics similar to TFIDF weighting and length normalization  The second benefit is that it also allows us to compute the query likelihood more efficiently since we only need to consider terms matched in the query  We see that the main part of the formula is a sum over the matching query terms.
We matching query words After smooth language	 We see that the main part of the formula is a sum over the matching query terms  This is much better than if we take the sum over all the words  After we smooth the document using the collection language model we would have nonzero probabilities for all the words w ∈ V .
After document language model probabilities w new form easier interesting term scored ignored ranking.	 After we smooth the document using the collection language model we would have nonzero probabilities for all the words w ∈ V   This new form of the formula is much easier to compute  It’s also interesting to note that the last term is independent of the document being scored so it can be ignored for ranking.
This new form easier	 This new form of the formula is much easier to compute.
note won’t documents value final score.	 It’s also interesting to note that the last term is independent of the document being scored so it can be ignored for ranking  Ignoring this term won’t affect the order of the documents since it would just be the same value added onto each document’s final score.
won’t order added document’s final score.	 Ignoring this term won’t affect the order of the documents since it would just be the same value added onto each document’s final score.
sum weight.	 Inside the sum we also see that each matched query term would contribute a weight.
weight vector frequency query like vector dot word appears	 This weight looks like TFIDF weighting from the vector space models  First we can already see it has a frequency of the word in the query just like in the vector space model  When we take the dot product the word frequency in the query appears in the sum as a vector element from the query vector.
query vector element vector The term document vector encodes TF IDF term word seen probability tend	 When we take the dot product the word frequency in the query appears in the sum as a vector element from the query vector  The corresponding term from the document vector encodes a weight that has an effect similar to TF IDF weighting  pseen is related to the term frequency in the sense that if a word occurs very frequently in the document then the seen probability will tend to be larger.
corresponding document vector TF IDF weighting term word document tend weighting.	 The corresponding term from the document vector encodes a weight that has an effect similar to TF IDF weighting  pseen is related to the term frequency in the sense that if a word occurs very frequently in the document then the seen probability will tend to be larger  This term is really doing something like TF weighting.
pseen occurs frequently seen probability larger This like weighting.	 pseen is related to the term frequency in the sense that if a word occurs very frequently in the document then the seen probability will tend to be larger  This term is really doing something like TF weighting.
This term like In denominator achieve IDF effect pw popularity	 This term is really doing something like TF weighting  In the denominator we achieve the IDF effect through pw  C or the popularity of the term in the collection.
achieves effect	 Intuitively however it achieves a similar effect to the VS interpretation.
We normalization In αd related	 We also have something related to the length normalization  In particular αd might be related to document length.
particular related length.	 In particular αd might be related to document length.
words	 If the document is short the number of unseen words is expected to be large and we need to do more smoothing in this case.
convenient means We need smooth collection model formula looks like document length	 This formulation is quite convenient since it means we don’t have to think about the specific way of doing smoothing  We just need to assume that if we smooth with the collection language model then we would have a formula that looks like TF IDF weighting and document length normalization.
assume smooth collection model like TF weighting document normalization interesting fixed ranking Note logarithm scoring turned product	 We just need to assume that if we smooth with the collection language model then we would have a formula that looks like TF IDF weighting and document length normalization  It’s also interesting that we have a very fixed form of the ranking function  Note that we have not heuristically put a logarithm here but have used a logarithm of query likelihood for scoring and turned the product into a sum of logarithms of probabilities.
interesting Note heuristically turned	 It’s also interesting that we have a very fixed form of the ranking function  Note that we have not heuristically put a logarithm here but have used a logarithm of query likelihood for scoring and turned the product into a sum of logarithms of probabilities.
logarithm product logarithms probabilities If implement TFIDF weighting don’t	 Note that we have not heuristically put a logarithm here but have used a logarithm of query likelihood for scoring and turned the product into a sum of logarithms of probabilities  If we only want to heuristically implement TFIDF weighting we don’t necessarily have to have a logarithm.
want logarithm drop	 If we only want to heuristically implement TFIDF weighting we don’t necessarily have to have a logarithm  Imagine if we drop this logarithm we would still have TF and IDF weighting.
In nice property models we’ll derivation.	” In summary a nice property of probabilistic models is that by following some assumptions and probabilistic rules we’ll get a formula by derivation.
If heuris	 If we heuris tically design the formula we may not necessarily end up having such a specific form.
Additionally need smoothing model document scoring necessary representing ment.	 Additionally we talked about the need for smoothing a document language model  Otherwise it would give zero probability for unseen words in the document which is not good for scoring a query with an unseen word  It’s also necessary to improve the accuracy of estimating the model representing the topic of this docu ment.
words document	 Otherwise it would give zero probability for unseen words in the document which is not good for scoring a query with an unseen word.
It’s necessary topic docu ment retrieval use model proba bility.	 It’s also necessary to improve the accuracy of estimating the model representing the topic of this docu ment  The general idea of smoothing in retrieval is to use the collection language model to give us some clue about which unseen word would have a higher proba bility.
proportional assumption we’ve shown formula retrieval models contains vector heuristics TFIDF weighting document	 That is the probability of the unseen word is assumed to be proportional to its probability in the entire collection  With this assumption we’ve shown that we can derive a general ranking formula for query likelihood retrieval models that au tomatically contains the vector space heuristics of TFIDF weighting and document length normalization.
assumption we’ve derive ranking likelihood contains vector heuristics TFIDF	 With this assumption we’ve shown that we can derive a general ranking formula for query likelihood retrieval models that au tomatically contains the vector space heuristics of TFIDF weighting and document length normalization.
We rewriting function based query space actual given assumptions unlike heuristically function However need smooth language exactly language model section.	 We also saw that through some rewriting the scoring of such a ranking function is primarily based on a sum of weights on matched query terms also just like in the vector space model  The actual ranking function is given to us automatically by the probabilistic derivation and assumptions we have made unlike in the vector space model where we have to heuristically think about the forms of each function  However we still need to address the question how exactly should we smooth a document language model How exactly should we use the reference language model based on the collection to adjust the probability of the MLE of seen terms This is the topic of the next section.
function given unlike heuristically think need question exactly seen section.	 The actual ranking function is given to us automatically by the probabilistic derivation and assumptions we have made unlike in the vector space model where we have to heuristically think about the forms of each function  However we still need to address the question how exactly should we smooth a document language model How exactly should we use the reference language model based on the collection to adjust the probability of the MLE of seen terms This is the topic of the next section.
However need question smooth language model use model based collection MLE 6 4.	 However we still need to address the question how exactly should we smooth a document language model How exactly should we use the reference language model based on the collection to adjust the probability of the MLE of seen terms This is the topic of the next section  6 4.
	 6.
	4.
3 Specific smoothing section likelihood collection model retrieval following∑ q cw pseenw pw .	3 Specific smoothing methods From the last section we showed how to smooth the query likelihood retrieval model with the collection language model  We end up having a retrieval function that looks like the following∑ w∈d q cw q log pseenw  d αd   pw  C q log αd .
7 matched terms sum We saw previous sum.	 6 7 We can see it’s a sum of all the matched query terms and inside the sum it’s a count of terms in the query with some weight for the term in the document  We saw in the previous section how TF and IDF are captured in this sum.
function programming we’d figure particular know estimate	 If we wanted to implement this function using a programming language we’d still need to figure out a few variables in particular we’re going to need to know how to estimate the probability of a word and how to set αd .
order think methods .	 In order to answer these questions we have to think about specific smoothing methods where we define pseen and αd .
	 We’re going to talk about two different smoothing methods.
document That gives word counts normal ized The observed	26 shows how we estimate the document language model by using MLE  That gives us word counts normal ized by the total number of words in the document  The idea of using this method is to maximize the probability of the observed text.
ized words idea text word like observed text it’s zero probability.	 That gives us word counts normal ized by the total number of words in the document  The idea of using this method is to maximize the probability of the observed text  As a result if a word like network is not observed in the text it’s going to get zero probability.
idea method probability result network text it’s	 The idea of using this method is to maximize the probability of the observed text  As a result if a word like network is not observed in the text it’s going to get zero probability.
word like observed text it’s	 As a result if a word like network is not observed in the text it’s going to get zero probability.
idea collection reference word zero	 The idea of smoothing is to rely on the collection reference model where this word is not going to have a zero probability helping us decide what nonzero probability should be assigned to such a word.
JelinekMercer smoothing interpolation maximum estimate model smoothing λ ∈ 0 1 Thus particular smoothing	 In JelinekMercer smoothing we do a linear interpolation between the maximum likelihood estimate and the collection language model  This is con trolled by the smoothing parameter λ ∈ 0 1  Thus λ is a smoothing parameter for this particular smoothing method.
This trolled smoothing λ 0 1 λ smoothing particular	 This is con trolled by the smoothing parameter λ ∈ 0 1  Thus λ is a smoothing parameter for this particular smoothing method.
Thus smoothing parameter method.	 Thus λ is a smoothing parameter for this particular smoothing method.
larger λ weight probabilities.	 The larger λ is the more smoothing we have putting more weight on the background probabilities.
let’s works here.	 So let’s see how it works for some of the words here.
example com probability text MLE background probability Since d 100 100 In background 0.	 For example if we com pute the smoothed probability for the word text we get the MLE estimate in the document interpolated with the background probability  Since text appears ten times in d and d  100 our MLE estimate is 10 100   In the background we have ptext  C  0.
background ptext C 0.	 In the background we have ptext  C  0.
001 λ pMLEw d pw 1 .	001 giving our smoothed probability of pseenw  d  1 − λ   pMLEw  d  λ   pw  C 1 − λ .
d λ	 pMLEw  d  λ .
λ	 10 100 λ .
	001.
	 In Figure 6.
consider network In case zero .	26 we also consider the word network which does not appear in d  In this case the MLE estimate is zero and its smoothed probability is 0  λ   pw  C  λ .
MLE estimate probability 0 λ	 In this case the MLE estimate is zero and its smoothed probability is 0  λ .
0	 pw  C  λ   0 001.
001.	 0 001.
	001.
You method that’s word collection method discuss called ing Bayesian face zero words network.	 You can see now that αd in this smoothing method is just λ because that’s the coefficient in front of the probability of the word given by the collection language model  The second smoothing method we will discuss is called Dirichlet prior smooth ing or Bayesian smoothing  Again we face the problem of zero probability for words like network.
second smoothing method called Dirichlet	 The second smoothing method we will discuss is called Dirichlet prior smooth ing or Bayesian smoothing.
face zero probability like network.	 Again we face the problem of zero probability for words like network.
use language combine different way.	 Just like JelinekMercer smoothing we’ll use the collection language model but in this case we’re going to combine it with the MLE esimate in a some what different way.
formula seen interpolation MLE probability model αd λ coefficient μ Figure	 The formula first can be seen as an interpolation of the MLE probability and the collection language model as before  Instead however αd is not simply a fixed λ but a dynamic coefficient which takes μ  0 as a parameter  Based on Figure 6.
27 constant document smaller coefficient long docu expect fixedcoefficient	27 we can see if we set μ to a constant the effect is that a long document would actually get a smaller coefficient here  Thus a long docu ment would have less smoothing as we would expect so this seems to make more sense than fixedcoefficient smoothing.
Thus ment smoothing expect sense smoothing d μ sum giving model coefficient	 Thus a long docu ment would have less smoothing as we would expect so this seems to make more sense than fixedcoefficient smoothing  The two coefficients d dμ and μ dμ would still sum to one giving us a valid probability model  This smoothing can be un derstood as a dynamic coefficient interpolation.
The d μ sum valid probability	 The two coefficients d dμ and μ dμ would still sum to one giving us a valid probability model.
way formula—which easier remember—is d pw μ 6.	 Another way to understand this formula—which is even easier to remember—is to rewrite this smoothing method in this form pw  d  cw d  μ   pw  C d  μ   6.
	 6.
μ2 cw — μ 10 0 001 μ μ cw — d — μ μ LM text mining association query 3100 1100	001 pwd    μ2 0 ∞pwC cw d  μpwC — d  μ p“text”d 10  μ  0 001 — 100  μ μ — 100  μ cw d — d d — d  μ μ — d  μ Collection LM PwCUnigram LM   pwθ … text mining association database … query network 10100 5100 3100 3100 1100 0100 text 10 mining 5 association 3 database 3 algorithm 2 … query 1 efficient 1 the 0.
001 μ μ — d μ μ d μ LM text query network 10100 3100 1100 text 10 association 3 1 0.	001 — 100  μ μ — 100  μ cw d — d d — d  μ μ — d  μ Collection LM PwCUnigram LM   pwθ … text mining association database … query network 10100 5100 3100 3100 1100 0100 text 10 mining 5 association 3 database 3 algorithm 2 … query 1 efficient 1 the 0.
1	1 a 0.
02 database 0.	02 database 0.
01 … text 0	01 … text 0 001 network 0.
6 27 Smoothing retrieval Dirichlet prior smoothing.	001 mining 0 0009 … Figure 6 27 Smoothing the query likelihood retrieval function with linear interpolation Dirichlet prior smoothing.
0009 …	0009 … Figure 6.
likelihood retrieval Dirichlet prior	27 Smoothing the query likelihood retrieval function with linear interpolation Dirichlet prior smoothing.
	 We pretend every word w has μ   pw  C additional pseudocounts.
pw C extra numerator valid distribution.	 pw  C additional pseudocounts  Since we add this extra probability mass in the numerator we have to renormalize in order to have a valid probability distribution.
add numerator distribution.	 Since we add this extra probability mass in the numerator we have to renormalize in order to have a valid probability distribution.
Since w∈V pw μ number pseudocounts look For counts observe added proportional text entire corpus.	 Since ∑ w∈V pw  C  1 we can add a μ in the denominator which is the total number of pseudocounts we added for each w in the numerator  Let’s also take a look at this specific example again  For the word text we will have ten counts that we actually observe but we also added some pseudocounts which are proportional to the probability of text in the entire corpus.
Let’s specific probability entire set μ 3000 3000 counts smoothed	 Let’s also take a look at this specific example again  For the word text we will have ten counts that we actually observe but we also added some pseudocounts which are proportional to the probability of text in the entire corpus  Say we set μ  3000 meaning we will add 3000 extra word counts into our smoothed model.
actually observe added pseudocounts probability 3000 word counts smoothed model.	 For the word text we will have ten counts that we actually observe but we also added some pseudocounts which are proportional to the probability of text in the entire corpus  Say we set μ  3000 meaning we will add 3000 extra word counts into our smoothed model.
Say set meaning 3000 extra smoothed	 Say we set μ  3000 meaning we will add 3000 extra word counts into our smoothed model.
We want allocated text ptext 001	 We want some portion of the 3000 counts to be allocated to text since ptext  C  0 001 we’ll assign 0.
001 assign 0 001	001 we’ll assign 0 001 .
word.	001   3000 counts to that word.
goes word network d observe zero counts μ extra pseudocounts smoothed document scored probability.	 The same goes for the word network for d we observe zero counts but also add μ   pnetwork  C extra pseudocounts for our smoothed probability  In Dirichlet prior smoothing αd will actually depend on the current document being scored since d is used in the smoothed probability.
extra smoothed αd actually probability interpolation constant.	 pnetwork  C extra pseudocounts for our smoothed probability  In Dirichlet prior smoothing αd will actually depend on the current document being scored since d is used in the smoothed probability  In the JelinekMercer linear interpolation αd  λ which is a constant.
In prior αd current document smoothed probability In interpolation αd constant For Dirichlet αd μ dμ interpolation coefficient	 In Dirichlet prior smoothing αd will actually depend on the current document being scored since d is used in the smoothed probability  In the JelinekMercer linear interpolation αd  λ which is a constant  For Dirichlet prior we have αd μ dμ which is the interpolation coefficient applied to the collection language model.
In JelinekMercer linear interpolation αd λ For interpolation For slightly variables consult	 In the JelinekMercer linear interpolation αd  λ which is a constant  For Dirichlet prior we have αd μ dμ which is the interpolation coefficient applied to the collection language model  For a slightly more detailed derivation of these variables the reader may consult Appendix A.
1 − d	 pw  C 1 − λ   pMLEw  d  λ   pw  C λ .
d λ C C − λ .	 pMLEw  d  λ   pw  C λ   pw  C 1  1 − λ λ .
1 − λ .	 pw  C λ   pw  C 1  1 − λ λ .
λ .	 pw  C 1  1 − λ λ .
cw d pw	 cw d d   pw  C .
C plugging query formula d w∈q d log λ λ .	 pw  C   6 9 Then plugging this into the entire query likelihood retrieval formula we get scoreJMq  d ∑ w∈q d cw q log 1  1 − λ λ .
6 entire formula log 1 λ	 6 9 Then plugging this into the entire query likelihood retrieval formula we get scoreJMq  d ∑ w∈q d cw q log 1  1 − λ λ .
plugging entire query retrieval formula ∑ w∈q q 1 − λ cw d .	9 Then plugging this into the entire query likelihood retrieval formula we get scoreJMq  d ∑ w∈q d cw q log 1  1 − λ λ   cw d d .
.	 cw d d .
.	 pw  C .
10 q term section current document scored.	 6 10 We ignore the q log αd additive term derived in the previous section since αd  λ does not depend on the current document being scored.
ignore q additive αd We’ll strikingly model	10 We ignore the q log αd additive term derived in the previous section since αd  λ does not depend on the current document being scored  We’ll end up having a ranking function that is strikingly similar to a vector space model since it is a sum over all the matched query terms.
We’ll end ranking function query terms logarithm non	 We’ll end up having a ranking function that is strikingly similar to a vector space model since it is a sum over all the matched query terms  The value of the logarithm term is non negative.
The negative We clearly numerator sublinearly weighting pw C frequent numerator be.	 The value of the logarithm term is non negative  We see very clearly the TF weighting in the numerator which is scaled sublinearly  We also see the IDFlike weighting which is the pw  C term in the denominator the more frequent the term is in the entire collection the more discounted the numerator will be.
clearly scaled sublinearly pw C entire	 We see very clearly the TF weighting in the numerator which is scaled sublinearly  We also see the IDFlike weighting which is the pw  C term in the denominator the more frequent the term is in the entire collection the more discounted the numerator will be.
We term term	 We also see the IDFlike weighting which is the pw  C term in the denominator the more frequent the term is in the entire collection the more discounted the numerator will be.
second fraction ratio ratio means probability w background.	 The second fraction can also be considered as the ratio of two probabilities if the ratio is greater than one it means the probability of w in d is greater than appearing by chance in the background.
If seeing actually likely observing collection heuristic kind assumptions.	 If the ratio is less than one the chance of seeing w in d is actually less likely than observing it in the collection  What’s also important to note is that we received this weighting function auto matically by making various assumptions whereas in the vector space model we had to go through those heuristic design choices in order to get this  These are the advantages of using this kind of probabilistic reasoning where we have made ex plicit assumptions.
What’s important weighting making assumptions space model heuristic design advantages kind probabilistic assumptions precisely probabilities.	 What’s also important to note is that we received this weighting function auto matically by making various assumptions whereas in the vector space model we had to go through those heuristic design choices in order to get this  These are the advantages of using this kind of probabilistic reasoning where we have made ex plicit assumptions  We know precisely why we have a logarithm here and precisely why we have these probabilities.
advantages probabilistic We precisely precisely makes length normalization.	 These are the advantages of using this kind of probabilistic reasoning where we have made ex plicit assumptions  We know precisely why we have a logarithm here and precisely why we have these probabilities  We have a formula that makes sense and does TFIDF weighting and document length normalization.
We precisely formula makes sense TFIDF normalization Let’s function	 We know precisely why we have a logarithm here and precisely why we have these probabilities  We have a formula that makes sense and does TFIDF weighting and document length normalization  Let’s look at the complete function for Dirichlet prior smoothing now.
sense weighting normalization.	 We have a formula that makes sense and does TFIDF weighting and document length normalization.
cw d .	 cw d d  μ d  μ .
11 d .	11 therefore pseenw  d αd .
C dμ pwC pwC 1 cw d	 pw  C cw dμ pwC dμ μ pwC dμ 1  cw d μ .
pwC pwC dμ 1 d	pwC dμ μ pwC dμ 1  cw d μ .
pw We scoreDIRq w∈q cw q log cw .	 pw  C   6 12 We can now substitute this into the complete formula scoreDIRq  d ∑ w∈q d cw q log 1  cw d μ .
substitute scoreDIRq w∈q log μ	 6 12 We can now substitute this into the complete formula scoreDIRq  d ∑ w∈q d cw q log 1  cw d μ .
We substitute d w∈q q log 1 cw pw q μ d .	12 We can now substitute this into the complete formula scoreDIRq  d ∑ w∈q d cw q log 1  cw d μ   pw  C q log μ μ  d .
13 function looks similar scoring tion We scaled nonnegative logarithm TF IDF computed exact	13 The form of the function looks very similar to the JelinekMercer scoring func tion  We compute a ratio that is sublinearly scaled by a nonnegative logarithm  Both TF and IDF are computed in almost the exact same way.
We compute scaled nonnegative IDF exact way.	 We compute a ratio that is sublinearly scaled by a nonnegative logarithm  Both TF and IDF are computed in almost the exact same way.
TF computed exact way.	 Both TF and IDF are computed in almost the exact same way.
large small effectively rewarding	 If d is large then less extra mass is added onto the final score if d is small more extra mass is added to the score effectively rewarding a short document.
summarize section talked smoothing Jelinek Mercer fixed coefficient linear adds current word smoothing able retrieval assumptions clearly articulated vector Even explicitly define VS naturally weighting length normaliza tion inclusion	 To summarize this section we’ve talked about two smoothing methods Jelinek Mercer which is doing the fixed coefficient linear interpolation and Dirichlet prior which adds pseudo counts proportional to the probability of the current word in the background collection  In most cases we can see by using these smoothing methods we will be able to reach a retrieval function where the assumptions are clearly articulated making them less heuristic than some of the vector space mod els  Even though we didn’t explicitly set out to define the popular VS heuristics in the end we naturally arrived at TFIDF weighting and document length normaliza tion perhaps justifying their inclusion in the VS models.
need set smoothing Overall shows probabilistic model strategies vec	 Still we need to set these smoothing parameters or estimate them in some way  Overall this shows that by using a probabilistic model we follow very different strategies than the vec tor space model.
shows different strategies vec model end functions look having final form dictated	 Overall this shows that by using a probabilistic model we follow very different strategies than the vec tor space model  Yet in the end we end up with retrieval functions that look very similar to the vector space model  Some advantages here are having assumptions clearly stated and a final form dictated by a probabilistic model.
end end similar model Some advantages clearly stated final dictated probabilistic model.	 Yet in the end we end up with retrieval functions that look very similar to the vector space model  Some advantages here are having assumptions clearly stated and a final form dictated by a probabilistic model.
This section concludes discussion query likelihood retrieval derive seen following.	 This section also concludes our discussion of the query likelihood probabilistic retrieval models  Let’s recall what assumptions we have made in order to derive the functions that we have seen the following.
The modeled query	 1  The relevance can be modeled by the query likelihood i e.
modeled query i.	 The relevance can be modeled by the query likelihood i.
d q	 pR  d  q ≈ pq  d  2.
allowing probability words 3.	 2  Query words are generated independently allowing us to decompose the probability of the whole query into a product of probabilities of observed words in the query  3.
3 If word probability collection background 4.	 3  If a word is not seen in the document its probability is proportional to its probability in the collection smoothing with the background collection  4.
Finally smoothing If choice form function seen	 4  Finally we made one of two assumptions about the smoothing using either JelinekMercer smoothing or Dirichlet prior smoothing  If we make these four assumptions then we have no choice but to take the form of the retrieval function that we have seen earlier.
If seen normalization.	 If we make these four assumptions then we have no choice but to take the form of the retrieval function that we have seen earlier  Fortunately the function has a nice property in that it implements TFIDF weighting and document length normalization.
Fortunately function implements TFIDF length normalization	 Fortunately the function has a nice property in that it implements TFIDF weighting and document length normalization  In practice these functions also work very well.
In practice work functions heuristic model.	 In practice these functions also work very well  In that sense these functions are less heuristic compared with the vector space model.
In space Bibliographic Notes Further brief 2 2008 The space model length Singhal	 In that sense these functions are less heuristic compared with the vector space model  Bibliographic Notes and Further Reading A brief review of many different kinds of retrieval models can be found in Chapter 2 Zhai 2008  The vector space model with pivoted length normalization was pro posed and discussed in detail in Singhal et al.
Further different kinds models 2	 Bibliographic Notes and Further Reading A brief review of many different kinds of retrieval models can be found in Chapter 2 Zhai 2008.
vector space pro discussed	 The vector space model with pivoted length normalization was pro posed and discussed in detail in Singhal et al  1996.
	 1996.
retrieval function Zaragoza comprehensive survey language retrieval Zhai 2008 treatment retrieval heuristics	 A useful reference for the BM25 retrieval function is Robertson and Zaragoza 2009  A comprehensive survey of language models for information retrieval can be found in Zhai 2008  A formal treatment of retrieval heuristics is given in Fang et al.
A comprehensive information 2008 A formal treatment retrieval heuristics al model proposed et al.	 A comprehensive survey of language models for information retrieval can be found in Zhai 2008  A formal treatment of retrieval heuristics is given in Fang et al  2004 and a diagnostic eval uation method for assessing deficiencies of a retrieval model is proposed in Fang et al.
A formal retrieval et	 A formal treatment of retrieval heuristics is given in Fang et al.
improved	 2011 where multiple improved basic retrieval functions are also derived.
7Feedback In system.	 7Feedback In this chapter we will discuss feedback in a TR system.
search results retrieval illustrated Figure 1.	 Feedback takes the results of a user’s actions or previous search results to improve retrieval results  This is illustrated in Figure 7 1.
	1.
type standard returns depth	 We can see the user would type in a query and then the query would be sent to a standard search engine which returns a ranked list of results we discussed this in depth in Chapter 6.
search results user.	 These search results would be shown to the user.
user judgements useful	 The user can make judgements about whether each returned document is useful or not.
For example document good useful document relevance	 For example the user may say one document is good or one document is not very useful  Each decision on a document is called a relevance judgment.
document called relevance	 Each decision on a document is called a relevance judgment.
feedback feedback user search results learn exactly users The feedback judgements collection improve future	 This overall process is a type of relevance feedback because we’ve got some feedback information from the user based on the judgements of the search results  As one would expect this can be very useful to the retrieval system since we should be able to learn what exactly is interesting to a particular user or users  The feedback module would then take these judgements as input and also use the document collection to try to improve future rankings.
As able exactly interesting user The use document rankings.	 As one would expect this can be very useful to the retrieval system since we should be able to learn what exactly is interesting to a particular user or users  The feedback module would then take these judgements as input and also use the document collection to try to improve future rankings.
module judgements input use document collection improve future mentioned query results accurately user main idea feedback.	 The feedback module would then take these judgements as input and also use the document collection to try to improve future rankings  As mentioned it would typically involve updating the query so the system can now rank the results more accurately for the user this is the main idea behind relevance feedback.
mentioned typically updating accurately feedback relevance judgements generally feedback called relevance blind	 As mentioned it would typically involve updating the query so the system can now rank the results more accurately for the user this is the main idea behind relevance feedback  These types of relevance judgements are reliable but the users generally don’t want to make extra effort unless they have to  There is another form of feedback called pseudo relevance feedback or blind feedback.
types relevance reliable generally want extra There called	 These types of relevance judgements are reliable but the users generally don’t want to make extra effort unless they have to  There is another form of feedback called pseudo relevance feedback or blind feedback.
There feedback	 There is another form of feedback called pseudo relevance feedback or blind feedback.
	 Let’s say we assume the top k  10 documents are relevant.
Then use documents learn But help In documents similar	 Then we will use these documents to learn and to improve the query  But how could this help if the top ranked documents are random In fact the top documents are actually similar to relevant documents even if they are not relevant.
ranked random In documents actually relevant documents relevant.	 But how could this help if the top ranked documents are random In fact the top documents are actually similar to relevant documents even if they are not relevant.
Otherwise high ranked So possible related set relevant 7 Feedback Results d1	 Otherwise how would they have appeared high in the ranked list So it’s possible to learn some related terms to the query from this set anyway regardless whether the user says that a document is relevant or not  134 Chapter 7 Feedback Results d1 3.
Chapter Feedback Results	 134 Chapter 7 Feedback Results d1 3.
5 dk 5 Judgments d1 d3 dk – Updated Feedback Figure 7.	5 d2 2 4 … dk 0 5 … Judgments d1 d2 – d3 … dk – … Document collection Query Updated query Feedback Retrieval engine User Figure 7.
4 dk Judgments d2 … collection Query Updated Retrieval 7 How information	4 … dk 0 5 … Judgments d1 d2 – d3 … dk – … Document collection Query Updated query Feedback Retrieval engine User Figure 7 1 How feedback is part of an information retrieval system.
recall analyze word word	 You may recall that we talked about using language models to analyze word associations by learning related words to the word computer see Chapter 3.
results k match probabilities counting model terms frequent retrieved set frequent	 Then the results will be those documents that contain computer  We take the top k results that match computer well and we estimate term probabilities by counting them in this set for our topic language model  Lastly we use the background language model to choose the terms that are frequent in this retrieved set but not frequent in the whole collection.
If ideas learn terms related added query expand query don’t necessarily words like query.	 If we contrast these two ideas what we can find is that we’ll learn some related terms to computer  These related words can then be added to the original query to expand the query which helps find documents that don’t necessarily match computer but match other words like program and software that may not have been in the original query.
Unfortunately feedback reliable set useful called implicit involve ask	 Unfortunately pseudo relevance feedback is not completely reliable we have to arbitrarily set a cutoff and hope that the ranking function is good enough to get at least some useful documents  There is also another feedback method called implicit feedback  In this case we still involve users but we don’t have to explicitly ask them to make judgements.
called case users observe users observing clickthroughs.	 There is also another feedback method called implicit feedback  In this case we still involve users but we don’t have to explicitly ask them to make judgements  Instead we are going to observe how the users interact with the search results by observing their clickthroughs.
users	 In this case we still involve users but we don’t have to explicitly ask them to make judgements.
assume use document search engine results text actually user text implicit use query.	 We can even assume that we’re going to use only the snippet here in a document that is displayed on the search engine results page the text that’s actually seen by the user  We can assume this displayed text is probably relevant or interesting to the user since they clicked on it  This is the idea behind implicit feedback and we can again use this information to update the query.
This idea implicit feedback use update This modern engines—think collect user search results.	 This is the idea behind implicit feedback and we can again use this information to update the query  This is a very important technique used in modern search engines—think about how Google and Bing can collect user activity to improve their search results.
important technique modern search collect user results feedback.	 This is a very important technique used in modern search engines—think about how Google and Bing can collect user activity to improve their search results  To summarize we talked about three types of feedback.
summarize feedback use relevance require reliable.	 To summarize we talked about three types of feedback  In relevance feedback we use explicit relevance judgements which require some user effort but this method is the most reliable.
talked feedback document involving user	 We talked about pseudo feedback where we simply assumed the top k document are relevant without involving the user at all.
In query showing	 In this case we can actually do this automatically for each query before showing the user the final results page.
method user explicit effort	 Lastly we mentioned implicit feedback where we use clickthrough data  While this method does involve users the user doesn’t have to make explicit effort to make judgements on the results.
apply feedback query likelihood retrieval models sections feedback obtained matter following feedback	 Next we will discuss how to apply feedback techniques to both the vector space and query likelihood retrieval models  The future sections do not make any note of how the feedback documents are obtained since no matter how they are obtained they would be dealt with the same way by each of the following two feedback methods.
note obtained obtained way feedback methods Vector This section feedback retrieval	 The future sections do not make any note of how the feedback documents are obtained since no matter how they are obtained they would be dealt with the same way by each of the following two feedback methods  7 1 Feedback in the Vector Space Model This section is about feedback in the vector space retrieval model.
Model This section space model.	 7 1 Feedback in the Vector Space Model This section is about feedback in the vector space retrieval model.
discussed feedback based previous queries retrieval accuracy future positive particular examples specific query.	 As we have discussed feedback in a TR system is based on learning from previous queries to improve retrieval accuracy in future queries  We will have positive examples which are the documents that we assume to be relevant to a particular query and we have negative examples which are nonrelevant to a specific query.
We positive examples documents relevant query examples specific The gets documents strategy employed previous	 We will have positive examples which are the documents that we assume to be relevant to a particular query and we have negative examples which are nonrelevant to a specific query  The way the system gets these judged documents depends on the particular feedback strategy that is employed which was discussed in the previous section.
method model query vector.	 The general method in the vector space model for feedback is to modify our query vector.
We old terms query terms query method vector space feedback decades	 We might adjust weights of old terms or assign weights to new terms in the query vector  As a result the query will usually have more terms which is why this is often called query expansion  The most effective method for the vector space model feedback was proposed several decades ago and is called Rocchio feedback.
terms called query method model proposed decades ago Rocchio	 As a result the query will usually have more terms which is why this is often called query expansion  The most effective method for the vector space model feedback was proposed several decades ago and is called Rocchio feedback  We illustrate this idea in Figure 7.
effective space feedback Rocchio illustrate Figure	 The most effective method for the vector space model feedback was proposed several decades ago and is called Rocchio feedback  We illustrate this idea in Figure 7.
illustrate idea 7 2 addition vector query vector center positive − negative documents.	 We illustrate this idea in Figure 7 2 by using a twodimensional display of all the documents in the collection in addition to the query vector q  The query vector is in the center and the  positive or − negative represent documents.
2 display documents vector q vector − negative represent use similarity function similar denoting	2 by using a twodimensional display of all the documents in the collection in addition to the query vector q  The query vector is in the center and the  positive or − negative represent documents  When we have a query vector and use a similarity function to find the most similar documents we are drawing this dotted circle denoting the topranked documents.
similar documents drawing topranked documents Of topranked motivation feedback place.	 When we have a query vector and use a similarity function to find the most similar documents we are drawing this dotted circle denoting the topranked documents  Of course not all the topranked documents will be positive and this is the motivation behind feedback in the first place.
course motivation place.	 Of course not all the topranked documents will be positive and this is the motivation behind feedback in the first place.
Our goal accuracy dotted	 Our goal is to move the query vector to some position to improve the retrieval accuracy shifting the dotted circle of similarity.
we’re vector closer away vectors Algebraically qm q β Dr	 Geometrically we’re talking about moving a vector closer to some vectors and away from other vectors  Algebraically it means we have the following formula using the arrow vector notation for clarity qm  α   q  β Dr .
dj − Dn	 ∑ dj∈Dr dj − γ Dn .
7 1 query transformed modified i.	 ∑ dj∈Dn dj  7 1 where q is the original query vector that is transformed into qm the modified i.
1 q query vector.	1 where q is the original query vector that is transformed into qm the modified i e  expanded vector.
expanded	e  expanded vector.
	 expanded vector.
Additionally parameters α β control original	 Additionally we have the parameters α  β  and γ which are weights that control the amount of movement of the original vector.
In terms original boosted documents factor β terms factor Another term centroid vector term documents centroid away negative centroid.	 In terms of movement we see that the terms in the original query are boosted by a factor of α and terms from positive documents are boosted by a factor of β while terms from negative documents are shrunk by a factor of γ   Another interpretation of the second term the sum over positive documents is the centroid vector of relevant feedback documents while the third term is the centroid vector of the negative feedback documents  In this sense we shift the original query towards the relevant centroid and away from the negative centroid.
sense query centroid Thus	 In this sense we shift the original query towards the relevant centroid and away from the negative centroid  Thus the average over these two terms computes one dimension’s weight in the centroid of these vectors.
average computes vectors 7 Space Model After performed operations vector documents	 Thus the average over these two terms computes one dimension’s weight in the centroid of these vectors  7 1 Feedback in the Vector Space Model 137 After we have performed these operations we will get a new query vector which can be used again to score documents in the index.
	 7.
1 Feedback Space Model After score This new vector reflect vector nonrelevant	1 Feedback in the Vector Space Model 137 After we have performed these operations we will get a new query vector which can be used again to score documents in the index  This new query vector will then reflect the move of the original query vector toward the relevant centroid vector and away from the nonrelevant centroid vector.
This query centroid vector away nonrelevant vector Let’s look	 This new query vector will then reflect the move of the original query vector toward the relevant centroid vector and away from the nonrelevant centroid vector  Let’s take a look at a detailed example depicted below.
scheme search engine feedback weights query vector Say given documents vectors prefix.	 It’s not necessary to know what type of weighting scheme this search engine is using since in Rocchio feedback we will only be adding and subtracting term weights from the query vector  Say we are given five feedback documents whose term vectors are denoted as relevant with a  prefix.
given documents vectors denoted prefix The negative feedback prefixed −.	 Say we are given five feedback documents whose term vectors are denoted as relevant with a  prefix  The negative feedback documents are prefixed with −.
	 news about pres.
0 0 0.	5 0 1 0 0 0.
0 d2 5 0 0.	0 − d2  1 5 0 1 0.
5 0 1	5 0 1 0 0 2.
0 2.	1 0 0 2.
0 2 0	0 2 0 2 0 0.
0.	0 2 0 0.
	0 0.
0 1	0 d3  1 5 0 0 3.
5	5 0 0 3.
0 0	0 2 0 0 0 0.
0	0 0 0 0.
0 5 0.	0 0 0 d4  1 5 0.
0	0 4.
	0 0.
0 0 − 5 0.	0 0 0 − d5  1 5 0.
− d5 1 5 0 0	0 − d5  1 5 0 0 0.
0.	5 0 0 0.
0 0.	0 0.
0 6	0 6 0 2.
0 For Rocchio compute centroid positive tive feedback documents documents average case centroid	0 0 0 For Rocchio feedback we first compute the centroid of the positive and nega tive feedback documents  The centroid of the positive documents would have the average of each dimension and the case is the same for the negative centroid news about pres.
0 For Rocchio feedback positive centroid positive dimension case centroid	0 For Rocchio feedback we first compute the centroid of the positive and nega tive feedback documents  The centroid of the positive documents would have the average of each dimension and the case is the same for the negative centroid news about pres.
The centroid average case negative campaign Cr	 The centroid of the positive documents would have the average of each dimension and the case is the same for the negative centroid news about pres  campaign food text Cr  1 51.
campaign 51.	 campaign food text Cr  1 51.
51.	51.
0 0	5 2 0 0 3.
04	0 3 04 0 2 2.
	04.
0 2 0.	0 2 2 02 0 2 0.
	02 0 2 0 0 0.
0 0.	0 0.
− Cn	0 − Cn  1.
51.	51.
51 3 0.	51 5 3 0.
3	5 3 0.
	10.
10.	10.
0 0	0 3 0 0 0.
0 02	0 0 02 06.
	02.
06.	06.
0 0	0 3 0 02.
0 0.	0 3 0.
0 Now centroids modify query create query qm α	0 Now that we have the two centroids we modify the original query to create the expanded query qm qm  α .
β Cr γ Cn 1.	 q  β   Cr − γ   Cn α  1.
γ	 Cr − γ   Cn α  1 5β − 1.
Cn	 Cn α  1.
067γ α 3.	067γ  α  3.
5β 33γ 0.	5β  α  2β − 2 67γ  −1 33γ  0.
33γ	33γ  0.
β control influence γ vector modified query	 We have β to control the influence of the relevant centroid vector Cr   Finally we have γ  which is the nonrelevant centroid Cn weight  Shifting the original query vector q by these amounts yields our modified query qm.
Finally γ weight.	 Finally we have γ  which is the nonrelevant centroid Cn weight.
original q amounts yields modified query	 Shifting the original query vector q by these amounts yields our modified query qm.
vector better closer away nonrelevant feedback.	 Due to the movement of the query vector we should match the relevant documents much better since we moved q closer to them and away from the nonrelevant documents—this is precisely what we want from feedback.
Therefore vector terms contain highest small	 Therefore we often truncate this vector and only retain the terms which contain the highest weights considering only a small number of words.
One documents taking average doesn’t tell exactly On hand positive documents tend direction respect query.	 One reason is because negative documents distract the query in all directions so taking the average doesn’t really tell us where exactly it should be moving to  On the other hand positive documents tend to be clustered together and they are often in a consistent direction with respect to the query.
Because examples set parameter important means relatively high weight α query We trust small sample documents reformulate query	 Because of this effect we sometimes don’t use the negative examples or set the parameter γ to be small  It’s also important to avoid overfitting which means we have to keep relatively high weight α on the original query terms  We don’t want to overly trust a small sample of documents and completely reformulate the query without regard to its original meaning.
important avoid overfitting means relatively weight query terms.	 It’s also important to avoid overfitting which means we have to keep relatively high weight α on the original query terms.
Those original typed important direction This true pseudo trustworthy method robust effective making popular	 Those original terms are typed in by the user because the user decided that those terms were important Thus we bias the modified vector towards the original query direction  This is especially true for pseudo relevance feedback since the feedback documents are less trustworthy  Despite these issues the Rocchio method is usually robust and effective making it a very popular method for feedback.
This relevance feedback trustworthy issues method usually robust effective method	 This is especially true for pseudo relevance feedback since the feedback documents are less trustworthy  Despite these issues the Rocchio method is usually robust and effective making it a very popular method for feedback.
Despite issues method robust effective 2 Feedback feedback retrieval.	 Despite these issues the Rocchio method is usually robust and effective making it a very popular method for feedback  7 2 Feedback in Language Models This section is about feedback for language modeling in the query likelihood model of information retrieval.
query function making assumptions term	 Recall that we derive the query likelihood ranking function by making various assumptions such as term independence.
family functions well.	 As a basic retrieval function that family of functions worked well.
query allow model This model retrieval vector space model Despite form language retrieval query likelihood covers query special	 However we have a way to generalize the query likelihood function that will allow us to include feedback documents more easily it’s called a KullbackLeibler divergence retrieval model or KLdivergence retrieval model for short  This model actually makes the query likelihood retrieval function much closer to the vector space model  Despite this the new form of the language model retrieval can still be regarded as a generalization of query likelihood in that it covers query likelihood without feedback as a special case.
This function space model language generalization likelihood special achieved estimation	 This model actually makes the query likelihood retrieval function much closer to the vector space model  Despite this the new form of the language model retrieval can still be regarded as a generalization of query likelihood in that it covers query likelihood without feedback as a special case  Here the feedback can be achieved through query model estimation or updating.
Despite new model generalization likelihood query special	 Despite this the new form of the language model retrieval can still be regarded as a generalization of query likelihood in that it covers query likelihood without feedback as a special case.
achieved model updating This Rocchio feedback updates vector	 Here the feedback can be achieved through query model estimation or updating  This is very similar to Rocchio feedback which updates the query vector in this case we update the query language model instead.
similar feedback updates vector update query model Figure difference original query KLdivergence model.	 This is very similar to Rocchio feedback which updates the query vector in this case we update the query language model instead  Figure 7 3 shows the difference between our original query likelihood formula and the generalized KLdivergence model.
Figure 3 shows original likelihood formula KLdivergence query likelihood retrieval	 Figure 7 3 shows the difference between our original query likelihood formula and the generalized KLdivergence model  On top we have the query likelihood retrieval function.
query generalized KLdivergence model.	3 shows the difference between our original query likelihood formula and the generalized KLdivergence model.
retrieval model term frequency distribution.	 The KLdivergence retrieval model generalizes the query term frequency into a probabilistic distribution.
distribution difference able query way.	 This distribution is the only difference which is able to characterize the user’s query in a more general way.
different ways—including	 This query lan guage model can be estimated in many different ways—including using feedback information.
enables query 140 7 Feedback θD DθQ θD Document D Feedback docs … model Full feedback 1 – αθF α 0 θQ′ θQ θQ′ Query 7 Modelbased feedback.	 This enables feedback information to be incorporated into the query more easily  140 Chapter 7 Feedback θD DθQ  θD Document D Results Feedback docs F  d1 d2 … dnGenerative model Full feedback No feedback θQ θQ′  1 – αθQ  αθF α  0 α  1 θQ′  θQ θQ′  θF θF Query Q Figure 7 4 Modelbased feedback.
140 Chapter Feedback θD Document D F … 1 αθQ αθF θQ′ θF θF Figure	 140 Chapter 7 Feedback θD DθQ  θD Document D Results Feedback docs F  d1 d2 … dnGenerative model Full feedback No feedback θQ θQ′  1 – αθQ  αθF α  0 α  1 θQ′  θQ θQ′  θF θF Query Q Figure 7.
Modelbased formulas look identical generalized formula	4 Modelbased feedback  So the two formulas look almost identical except that in the generalized formula we have a probability of a word given by a query language model.
formulas given model Still add document nonzero language	 So the two formulas look almost identical except that in the generalized formula we have a probability of a word given by a query language model  Still we add all the words that are in the document and have nonzero probability for the query language model.
Still words probability query language generalization	 Still we add all the words that are in the document and have nonzero probability for the query language model  Again this becomes a generalization of summing over all the matching query words.
Again query We recover setting query model relative frequency eliminates n	 Again this becomes a generalization of summing over all the matching query words  We can recover the original query likelihood formula by simply setting the query language model to be the relative frequency of a word in the query which eliminates the query length term n  q which is a constant.
4 shows estimate document language language model denoted	4 shows that we first estimate a document language model then we estimate a query language model and we compute the KLdivergence often denoted by D.
.	.
language query terms called feedback positive Rocchio	  We compute a language model from the documents containing the query terms called the feedback language model θF   This feedback language model is similar to the positive centroid Cr in Rocchio feedback.
We compute model documents containing query called feedback model θF This feedback similar Rocchio feedback combined query linear Rocchio.	 We compute a language model from the documents containing the query terms called the feedback language model θF   This feedback language model is similar to the positive centroid Cr in Rocchio feedback  This model can be combined with the original query language model using a linear interpolation which produces an updated model again just like Rocchio.
feedback similar Cr feedback combined language interpolation produces updated model We parameter α feedback docu	 This feedback language model is similar to the positive centroid Cr in Rocchio feedback  This model can be combined with the original query language model using a linear interpolation which produces an updated model again just like Rocchio  We have a parameter α ∈ 0 1 that controls the strength of the feedback docu ments.
produces We parameter 0 1 controls docu If α feedback ignore	 This model can be combined with the original query language model using a linear interpolation which produces an updated model again just like Rocchio  We have a parameter α ∈ 0 1 that controls the strength of the feedback docu ments  If α  0 there is no feedback if α  1 we receive full feedback and ignore the original query.
We 0 controls strength If α 0 feedback α 1 query.	 We have a parameter α ∈ 0 1 that controls the strength of the feedback docu ments  If α  0 there is no feedback if α  1 we receive full feedback and ignore the original query.
If feedback α receive Of The main question	 If α  0 there is no feedback if α  1 we receive full feedback and ignore the original query  Of course these extremes are generally not desirable  The main question is how to compute this θF .
question compute discuss approaches .	 The main question is how to compute this θF   Now we’ll discuss one of the approaches to estimate θF .
discuss θF .	 Now we’ll discuss one of the approaches to estimate θF .
	5.
Let’s tive documents means One approach estimate language documents feedback model documents.	 Let’s say we are observing the posi tive documents which are collected by users’ judgements the top k documents from a search clickthrough logs or some other means  One approach to estimate a language model over these documents is to assume these documents are gen erated from some ideal feedback language model as we did before this entails normalizing all the frequency counts from all the feedback documents.
approach model erated ideal language normalizing frequency counts feedback	 One approach to estimate a language model over these documents is to assume these documents are gen erated from some ideal feedback language model as we did before this entails normalizing all the frequency counts from all the feedback documents.
But What As language model actually common words like	 But is this distribution good for feedback What would the topranked words in θF be As depicted in the language model on the right in Figure 7 6 the highscoring words are actually common words like the.
words	6 the highscoring words are actually common words like the.
Clearly need In fact word Chapter	 Clearly we need to get rid of these stop words  In fact we have already seen one way to do that by using a background language model while learning word associations in Chapter 2.
way learning word	 In fact we have already seen one way to do that by using a background language model while learning word associations in Chapter 2.
unwanted words background language	 What we can do is to assume that those unwanted words are from the background language model.
use maximum likelihood single	 If we use a maximum likelihood estimate a single model would have been forced to assign high probabilities to a word like the because it occurs so frequently.
It language model goal probabilities words machine generated work follows.	 It is appropriate to use the background language model to achieve this goal because this model will assign high probabilities to these common words  We assume the machine that generated these words would work as follows.
assume machine words follows Imagine coin decide distribution topic background words.	 We assume the machine that generated these words would work as follows  Imagine we flip a coin to decide what distribution to use topic words or background words.
Imagine coin decide topic words 1 shows background model know word model.	 Imagine we flip a coin to decide what distribution to use topic words or background words  With the probability of λ ∈ 0 1 the coin shows up as heads and then we’re going to use the background language model  Once we know we will use the background LM we can then sample a word from that model.
λ ∈ 0 coin heads we’re going background model use LM word model.	 With the probability of λ ∈ 0 1 the coin shows up as heads and then we’re going to use the background language model  Once we know we will use the background LM we can then sample a word from that model.
Once background sample word probability − decide use generate word mixed actually	 Once we know we will use the background LM we can then sample a word from that model  Alternatively with probability 1 − λ we decide to use an unknown topic model to generate a word  This is a mixture model because there are two distributions that are mixed together and we actually don’t know when each distribution is used.
− decide unknown model	 Alternatively with probability 1 − λ we decide to use an unknown topic model to generate a word.
mixture actually distribution feedback mixture generate words random way according underlying	 This is a mixture model because there are two distributions that are mixed together and we actually don’t know when each distribution is used  We can treat this feedback mixture model as a single distribution in that we can still ask it to generate words and it will still give us a word in a random way according to the underlying models  Which word will show up depends on both the topic distribution and background distribution.
feedback mixture single distribution generate way according Which topic distribution distribution depend mixing λ λ going prefer background	 We can treat this feedback mixture model as a single distribution in that we can still ask it to generate words and it will still give us a word in a random way according to the underlying models  Which word will show up depends on both the topic distribution and background distribution  In addition it would also depend on the mixing parameter λ if λ is high it’s going to prefer the background distribution.
result like exactly common high probability distribution topic high probabilities common feedback collection.	 As a result it doesn’t have to assign high probabilities to words like the which is exactly what we want  It would then assign high probabilities to other words that are common in the topic distribution but not having high probability in the background distribution  As a result this topic model must assign high probabilities to the words common in the feedback documents yet not common across the whole collection.
probabilities topic distribution having high distribution.	 It would then assign high probabilities to other words that are common in the topic distribution but not having high probability in the background distribution.
assign probabilities collection.	 As a result this topic model must assign high probabilities to the words common in the feedback documents yet not common across the whole collection.
it’s parameters	 We assume it will be fixed to some value  Assuming it’s fixed then we only have word probabilities θ as parameters just like in the simplest unigram language model.
model θF max θ θ ∑ d∈F w log λ	 This gives us the following formula to estimate the feedback lan guage model θF  arg max θ log pF  θ arg max θ ∑ d∈F ∑ w cw d   log 1 − λ .
θ	 pw  θ  λ .
C 2 choose distribution log feedback documents model This	 pw  C 7 2 We choose this probability distribution θF to maximize the log likelihood of the feedback documents under our model  This is the same idea as the maximum likelihood estimator.
idea estimator.	 This is the same idea as the maximum likelihood estimator.
interpolated original query	 Once we have done that we ob tain this θF that can be interpolated with the original query model to do feedback.
Of course algo parameters Lafferty involving models combined model models topic Figure 7.	 Of course in practice it isn’t feasible to try all values of θ  so we use the EM algo rithm to estimate its parameters Zhai and Lafferty 2001  Such a model involving multiple component models combined together is called a mixture model and we will further discuss such models in more detail in the topic analysis chapter Chapter 17  Figure 7.
Figure 7 6 examples web collection feedback use use parameters λ	 Figure 7 6 shows some examples of the feedback model learned from a web docu ment collection for performing pseudo feedback  We just use the top 10 documents and we use the mixture model with parameters λ  0.
We use use 0 9 0	 We just use the top 10 documents and we use the mixture model with parameters λ  0 9 and λ  0 7.
9	9 and λ  0 7.
	7.
query feed mixture model words learned approach described For example security naturally occur	 We select the top ten documents returned by the search engine for this query and feed them to the mixture model  The words in the two tables are learned using the approach we described  For example the words airport and security still show up as high probabilities in each case naturally because they occur frequently in the topranked documents.
The words tables described For example words security high case frequently topranked documents.	 The words in the two tables are learned using the approach we described  For example the words airport and security still show up as high probabilities in each case naturally because they occur frequently in the topranked documents.
compare we’ll common words use model often.	 If we compare the two tables we see that when λ is set to a smaller value we’ll still see some common words when we don’t use the background model often.
don’t rely background topic model common words.	 If we don’t rely much on the background model we still have to use the topic model to account for the common words.
Setting explaining words feedback	 Setting λ to a very high value uses the background model more often to explain these words and there is no burden on explaining the common words in the feedback documents.
As topic model discriminative—it words.	 As a result the topic model is very discriminative—it contains all the relevant words without common words.
144 7 model approach query likelihood function model allows model query include feedback	 144 Chapter 7 Feedback To summarize this section discussed feedback in the language model approach we transform our original query likelihood retrieval function to a more general KL divergence model  This generalization allows us to use a language model for the query which can be manipulated to include feedback documents.
method discriminates words relevant query words words major scenarios relevance feedback implicit feedback use query model esti language models.	 We described a method for estimating the parameters in this feedback model that discriminates between topic words relevant to the query and background words useless stop words  In this chapter we talked about the three major feedback scenarios relevance feedback pseudo feedback and implicit feedback  We talked about how to use Rocchio to do feedback in the vectorspace model and how to use query model esti mation for feedback in language models.
chapter talked major feedback implicit We talked Rocchio vectorspace model use model models We talked mixture model parameters	 In this chapter we talked about the three major feedback scenarios relevance feedback pseudo feedback and implicit feedback  We talked about how to use Rocchio to do feedback in the vectorspace model and how to use query model esti mation for feedback in language models  We briefly talked about the mixture model for its estimation although there are other methods to estimate these parameters that we mention later on in the book.
We use feedback model mation briefly mixture estimation mention	 We talked about how to use Rocchio to do feedback in the vectorspace model and how to use query model esti mation for feedback in language models  We briefly talked about the mixture model for its estimation although there are other methods to estimate these parameters that we mention later on in the book.
mixture parameters mention book Bibliographic comparison relevance techniques Buckley Pseudorelevance feedback lar positive results TREC e.	 We briefly talked about the mixture model for its estimation although there are other methods to estimate these parameters that we mention later on in the book  Bibliographic Notes and Further Reading An early empirical comparison of various relevance feedback techniques can be found in Salton and Buckley 1990  Pseudorelevance feedback has become popu lar after positive results being observed in TREC experiments e.
g Buckley Xu Croft	g  Buckley 1994 Xu and Croft 1996.
comparison approaches Lv 2009 Lv Zhai methods query pseudo Due large engine log data feedback users’ interaction web engines users Joachims	 A comparison of feedback approaches in language models is available in Lv and Zhai 2009  The positional relevance model proposed in Lv and Zhai 2010 appears to be one of the most effective methods for estimating a query language model for pseudo feedback  Due to the availability of a large amount of search engine log data implicit feedback based on users’ interaction behavior has become a very important and very effective technique to enable web search engines to improve their accuracy over time as more and more users are using the systems though the interpretation of user clickthroughs must take position bias into con sideration which is discussed in detail in Joachims et al.
implicit 2003 In web feedback form ranking	 A bibliography on implicit feedback can be found in Kelly and Teevan 2003  In the web search era implicit feedback is often implemented in the form of using feedback features in a ranking function using machine learning i.
implemented feedback features ranking learning i.	 In the web search era implicit feedback is often implemented in the form of using feedback features in a ranking function using machine learning i.
For thorough discussion	 For a more thorough discussion of mining query logs see the tutorial Silvestri 2010.
	 146 Chapter 7 Feedback 7 12.
12.	12.
methods assumed relevant In reality actually nonrelevant advantage ranked feedback In feedback differently depending similar Consider space query	 In the feedback methods we discussed in this chapter we assumed we only had sets of relevant and nonrelevant documents  In reality we actually have two ranked lists of relevant and nonrelevant documents  How can we take advantage of these ranked lists for feedback In other words how can we treat feedback documents differently depending on how similar they are to the original query Consider the vector space model the query likelihood model or both.
reality lists nonrelevant How advantage lists feedback In documents differently space	 In reality we actually have two ranked lists of relevant and nonrelevant documents  How can we take advantage of these ranked lists for feedback In other words how can we treat feedback documents differently depending on how similar they are to the original query Consider the vector space model the query likelihood model or both.
advantage ranked In words feedback Consider model query 7.	 How can we take advantage of these ranked lists for feedback In other words how can we treat feedback documents differently depending on how similar they are to the original query Consider the vector space model the query likelihood model or both  7.
general Tokenizer This takes strings large separate tokens	 In general an IR system consists of four components  Tokenizer  This component takes in documents as raw strings and determines how to separate the large document string into separate tokens or features.
Indexer.	 Indexer.
This module processes indexes data Indexer run	 This is the module that processes documents and indexes them with appropriate data structures  An Indexer can be run offline.
offline The quickly include supporting uments.	 An Indexer can be run offline  The main chal lenges are to index large amounts of documents quickly with a limited amount of memory  Other challenges include supporting addition and deletion of doc uments.
The main chal amounts quickly memory Other addition	 The main chal lenges are to index large amounts of documents quickly with a limited amount of memory  Other challenges include supporting addition and deletion of doc uments.
Other supporting uments ScorerRanker.	 Other challenges include supporting addition and deletion of doc uments  ScorerRanker.
module documents challenge documents efficiently.	 ScorerRanker  This is the module that takes a query and returns a ranked list of documents  Here the challenge is to implement a retrieval model efficiently so that we can score documents efficiently.
module query returns list	 This is the module that takes a query and returns a ranked list of documents.
Here model efficiently efficiently.	 Here the challenge is to implement a retrieval model efficiently so that we can score documents efficiently.
This responsible feedback.	 FeedbackLearner  This is the module that is responsible for relevance feed back or pseudo feedback.
responsible implicit available modern search module fairly sophisticated.	 This is the module that is responsible for relevance feed back or pseudo feedback  When there is a lot of implicit feedback information such as user clickthroughs available as in a modern web search engine this learning module can be fairly sophisticated.
When lot implicit clickthroughs available modern engine learning module sophisticated discussed chapter chapter added search engines.	 When there is a lot of implicit feedback information such as user clickthroughs available as in a modern web search engine this learning module can be fairly sophisticated  It was discussed in detail in the previous chapter so in this chapter we will just outline how it may be added to an existing system  For the first three items there are fairly standard techniques that are essentially used in all current search engines.
items fairly essentially engines The implementing depend applications Despite common feedback previous	 For the first three items there are fairly standard techniques that are essentially used in all current search engines  The techniques for implementing feedback 148 Chapter 8 Search Engine Implementation however highly depend on the learning approaches and applications  Despite this we did discuss some common methods for feedback in the previous chapter.
techniques implementing depend	 The techniques for implementing feedback 148 Chapter 8 Search Engine Implementation however highly depend on the learning approaches and applications.
Despite common chapter additionally investigate optimizations These ensure information speed disk	 Despite this we did discuss some common methods for feedback in the previous chapter  We will additionally investigate two additional optimizations  These are not required to ensure the correctness of an information retrieval system but they will enable such a system to be much more efficient in both speed and disk usage.
additionally additional These required correctness enable efficient disk	 We will additionally investigate two additional optimizations  These are not required to ensure the correctness of an information retrieval system but they will enable such a system to be much more efficient in both speed and disk usage.
These required ensure speed	 These are not required to ensure the correctness of an information retrieval system but they will enable such a system to be much more efficient in both speed and disk usage.
Compression documents index consume	 Compression  The documents we index could consume hundreds of gigabytes or terabytes.
Caching designing document storage mercy Thus cache facing disk.	1 Caching  Even after designing and compressing an efficient data structure for document retrieval storage the system will still be at the mercy of the hard disk speed  Thus it is common practice to add a cache between the front facing API and the document index on disk.
Even storage hard disk Thus add API document index	 Even after designing and compressing an efficient data structure for document retrieval storage the system will still be at the mercy of the hard disk speed  Thus it is common practice to add a cache between the front facing API and the document index on disk.
able save information disk ing reduced.	 The cache will be able to save frequentlyaccessed term information so the number of slow disk seeks dur ing querytime is reduced.
sections discuss components 8.	 The following sections in this chapter discuss each of the above components in turn  8.
1 Tokenizer Document step mining represent document.	 8 1 Tokenizer Document tokenization is the first step in any text mining task  This determines how we represent a document.
previous chapter resent corresponds single word stored particular When retrieval vectors prefer alternate count smoothed term count TFIDF	 We saw in the previous chapter that we often rep resent documents as document vectors where each index corresponds to a single word  The value stored in the index is then a raw count of the number of occurrences of that word in a particular document  When running information retrieval scoring functions on these vectors we usu ally prefer some alternate representation of term count such as smoothed term count or TFIDF weighting.
running retrieval functions vectors alternate term count TFIDF weighting.	 When running information retrieval scoring functions on these vectors we usu ally prefer some alternate representation of term count such as smoothed term count or TFIDF weighting.
different discuss string represented term IDs “words” integer IDs instead efficiency	 Furthermore we’d like our scorer to be able to use different 1  As we will discuss the string terms themselves are almost always represented as term IDs and most of the processing on “words” is done on integer IDs instead of strings for efficiency  8.
1 Tokenizer functions necessary weight use TFIDF	1 Tokenizer 149 scoring functions as necessary storing only TFIDF weight would then require us to always use TFIDF weighting.
segment document countable tokens kind appear The scorer formulate retrieval discussed chapter.	 Therefore a tokenizer’s job is to segment the document into countable features or tokens  A document is then represented by how many and what kind of tokens appear in it  The raw counts of these tokens are used by the scorer to formulate the retrieval scoring functions that we discussed in the previous chapter.
The basic tokenizer whitespace This simply words whitespace whitespacetokenizerMr.	 The most basic tokenizer we will consider is a whitespace tokenizer  This tok enizer simply delimits words by their whitespace  Thus whitespacetokenizerMr.
enizer simply whitespacetokenizerMr	 This tok enizer simply delimits words by their whitespace  Thus whitespacetokenizerMr  Quill’s book is very very long.
Thus	 Thus whitespacetokenizerMr  Quill’s book is very very long.
book long.	 Quill’s book is very very long.
result	 could result in Mr.
special case	 There is a special case here where the period after Mr.
split word mr.	 is not split since it forms a unique word mr.
1 quill 1 book long .	 1 quill 1 ’s 1 book 1 is 1 very 2 long 1 .
exercises Chapter ways We words tree features combination.	 Look back to the exercises from Chapter 4 to see some different ways in which we can represent text  We could use bigram words POStags grammatical parse tree features or any combination.
We bigram grammatical stop common stemming.	 We could use bigram words POStags grammatical parse tree features or any combination  Common words stop words could be removed and words could also be reduced to their common stem stemming.
words removed reduced stem Again 4	 Common words stop words could be removed and words could also be reduced to their common stem stemming  Again the exercises in Chapter 4 give good examples of these transformations using META.
exercises transformations In essence indexer shouldn’t care term tokenizer tokenizer document IDs.	 Again the exercises in Chapter 4 give good examples of these transformations using META  In essence the indexer and scorer shouldn’t care how the term IDs were generated this is solely the job of the tokenizer  Another common task of the tokenizer is to assign document IDs.
In essence IDs generated solely Another common document It efficient refer	 In essence the indexer and scorer shouldn’t care how the term IDs were generated this is solely the job of the tokenizer  Another common task of the tokenizer is to assign document IDs  It is much more efficient to refer to documents as unique numbers as opposed to strings such as homejeremydocsfile473.
efficient refer documents numbers opposed strings homejeremydocsfile473	 It is much more efficient to refer to documents as unique numbers as opposed to strings such as homejeremydocsfile473 txt.
parisons comparisons addition space argument terms	txt  It’s much faster to do integer com parisons than string comparisons in addition to integers taking up much less space  The same argument may be made for string terms vs.
integer integers The terms term	 It’s much faster to do integer com parisons than string comparisons in addition to integers taking up much less space  The same argument may be made for string terms vs  term IDs.
The argument vs.	 The same argument may be made for string terms vs.
map terms documents counts In C internally mapstdstring	 term IDs  Finally it will almost always be necessary to map terms to counts or documents to counts  In C we could of course use some structure internally such as stdunordered mapstdstring uint64t.
map terms counts course use stdunordered hash table O1 lookup particular	 Finally it will almost always be necessary to map terms to counts or documents to counts  In C we could of course use some structure internally such as stdunordered mapstdstring uint64t  As you know using a hash table like this gives amortized O1 lookup time to find a uint64t corresponding to a particular stdstring.
know table like amortized corresponding	 As you know using a hash table like this gives amortized O1 lookup time to find a uint64t corresponding to a particular stdstring.
structure takes access integer	 This data structure takes up less space and allows true O1 access to each uint64t using a term ID integer as the index into the stdvector.
Thus term ID IDs quill→ id 1 like 1 1 1 2	 Thus for term ID 57 we would look up index 57 in the array  Using term IDs and the second tokenizer example we could set mr → term id 0 quill→ term id 1 and so on then our document vector looks like 1 1 1 1 1 2 1 1.
term IDs tokenizer mr.	 Using term IDs and the second tokenizer example we could set mr.
quill→ term id looks 1 larger dimensions count	→ term id 0 quill→ term id 1 and so on then our document vector looks like 1 1 1 1 1 2 1 1  Of course a real document vector would be much larger and much sparser—that is most of the dimensions will have a count of zero.
Of course document count zero feature	 Of course a real document vector would be much larger and much sparser—that is most of the dimensions will have a count of zero  This process is also called feature generation.
This called	 This process is also called feature generation.
building blocks document meaningful ways compare	 It defines the building blocks of our document objects and gives us meaningful ways to compare them.
As mentioned tokenization critical indexer	 As mentioned in the Introduction tokenization is perhaps the most critical component of our indexer since all downstream operations depend on its output.
8 2 designed able example Wikipedia dump GB uncompressed	 8 2 Indexer Modern search engines are designed to be able to index data that is much larger than the amount of system memory  For example a Wikipedia database dump is about 40 GB of uncompressed text.
2 Modern designed text time book larger memory personal systems science	2 Indexer Modern search engines are designed to be able to index data that is much larger than the amount of system memory  For example a Wikipedia database dump is about 40 GB of uncompressed text  At the time of writing this book this is much larger than the amount of memory in common personal systems although it is quite a common dataset for computer science researchers.
example dump GB text time writing common science	 For example a Wikipedia database dump is about 40 GB of uncompressed text  At the time of writing this book this is much larger than the amount of memory in common personal systems although it is quite a common dataset for computer science researchers.
At time personal researchers.	 At the time of writing this book this is much larger than the amount of memory in common personal systems although it is quite a common dataset for computer science researchers.
Furthermore files want return necessary term statistics fast search engine query sufficient	 Furthermore when running queries on our indexed files we want to ensure that we can return the necessary term statistics fast enough to ensure a usable search engine  Scanning over every document in the corpus to match terms in the query will not be sufficient even for relatively small corpora.
inverted engine.	 An inverted index is the main data structure used in a search engine.
g phrase file specific.	g  it can be used to check whether a phrase is matched  This information is stored in the postings file since it is document specific.
check	 it can be used to check whether a phrase is matched.
8 1 shows representation lexicon files.	 Figure 8 1 shows a representation of the lexicon and postings files.
postings files integer bit byte	1 shows a representation of the lexicon and postings files  The arrows in the image are actually integer offsets that represent bit or byte indices into the postings file.
The Term ID 56 Document 78 Offset Of store → 78 443	 The information we receive could be Term ID 56 Document frequency 78 Total number of occurrences 443 Offset into postings file 8923754 Of course the actual lexicon would just store 56 → 78 443 8923754.
If seek large postings like ID Doc ID count 1 Doc count position 89	 If we seek to position 8923754 in the large postings file we could see something like Term ID 56 Doc ID 4 count 1 position 56 Doc ID 7 count 9 position 4 position 89 position  .
.	 .
.	.
position position.	 Doc ID 24 count 19 position 1 position 67 position.
Search position position.	   152 Chapter 8 Search Engine Implementation Doc ID 90 count 4 position 90 position 93 position.
Search Engine ID 90 position	  152 Chapter 8 Search Engine Implementation Doc ID 90 count 4 position 90 position 93 position .
152 Engine ID count position 93 .	 152 Chapter 8 Search Engine Implementation Doc ID 90 count 4 position 90 position 93 position  .
	 .
	.
ID count Doc count 2 position 34 position	 Doc ID 141 count 1 position 100 Doc ID 144 count 2 position 34 position 89 .
	 .
position information Notice doc positions fact advantage postings large size file.	 which is the counts and position information for the 78 documents that term ID 56 appears in  Notice how the doc IDs and positions are stored in increasing order this is a fact we will take advantage of when compressing the postings file  Also make note of the large difference in size of the lexicon and postings file.
Notice compressing file large difference lexicon postings	 Notice how the doc IDs and positions are stored in increasing order this is a fact we will take advantage of when compressing the postings file  Also make note of the large difference in size of the lexicon and postings file.
know store postings file store ID count positions document term documents length number	 For each entry in the lexicon we know we will only store three values per term  In the postings file we store at least three values doc ID count positions for each document that the term appears in  If the term appears in all documents we’d have a list of the length of the number of documents in the corpus.
ID positions document term length corpus.	 In the postings file we store at least three values doc ID count positions for each document that the term appears in  If the term appears in all documents we’d have a list of the length of the number of documents in the corpus.
unique lexicon file disk seeked pointers	 This is true for all unique terms  For this reason we often assume that the lexicon can fit into main memory and the postings file resides on disk and is seeked into based on pointers from the lexicon.
For lexicon resides	 For this reason we often assume that the lexicon can fit into main memory and the postings file resides on disk and is seeked into based on pointers from the lexicon.
creating structures set popular sortingbased approach.	 Indexing is the process of creating these data structures based on a set of tok enized documents  A popular approach for indexing is the following sortingbased approach.
Scan document	 Scan the raw document stream sequentially.
tokenization document	 In tokenization assign each document an ID.
document obtain term IDs scanning documents collect counts pair build documents When reach write index disk.	 Tokenize each document to obtain term IDs creating new term IDs as needed  While scanning documents collect term counts for each termdocument pair and build an inverted index for a subset of documents in memory  When we reach the limit of memory write the incomplete inverted index into the disk.
scanning documents counts pair index reach limit memory incomplete disk It format	 While scanning documents collect term counts for each termdocument pair and build an inverted index for a subset of documents in memory  When we reach the limit of memory write the incomplete inverted index into the disk  It will be the same format as the resulting postings file just smaller.
When reach inverted disk.	 When we reach the limit of memory write the incomplete inverted index into the disk.
file process incomplete indices written	 It will be the same format as the resulting postings file just smaller  Continue this process to generate many incomplete inverted indices called “runs” all written on disk.
Continue process inverted called written disk runs pairwise produce sorted	 Continue this process to generate many incomplete inverted indices called “runs” all written on disk  Merge all these runs in a pairwise manner to produce a single sorted by term ID postings file.
manner single ID postings file algorithm essentially	 Merge all these runs in a pairwise manner to produce a single sorted by term ID postings file  This algorithm is essentially the merge function from mergesort.
essentially function postings file created lexicon scanning offset ID.	 This algorithm is essentially the merge function from mergesort  Once the postings file is created create the lexicon by scanning through the postings file and assigning the offset values for each term ID.
Once create lexicon offset Figure shows terms originally document ID order.	 Once the postings file is created create the lexicon by scanning through the postings file and assigning the offset values for each term ID  Figure 8 2 shows how documents produce terms originally in document ID order.
2 shows documents produce document	2 shows how documents produce terms originally in document ID order.
mapping terms forward maps list occur type search.	 Instead of mapping terms to documents a forward index maps documents to a list of terms that occur in them  This type of setup is useful when doing other operations aside from search.
useful example access entire document’s content	 This type of setup is useful when doing other operations aside from search  For example clustering or classification would need to access an entire document’s content at once.
Using we’d postings occur specific Thus index structure ID.	 Using an inverted index to do this is not efficient at all since we’d have to scan the entire postings file to find all the terms that occur in a specific document  Thus we have the forward index structure that records a term vector for each document ID.
Thus index structure term ID In inverted greatly query time.	 Thus we have the forward index structure that records a term vector for each document ID  In the next section we’ll see how using the inverted termtodocument mapping can greatly decrease query scoring time.
There efficiency aspects relevant forward	 There are other efficiency aspects that are relevant to the forward index as well such as compression and caching.
Scorer Now score moment don’t forward document terms To we’d	 8 3 Scorer Now that we have our inverted index how can we use it to efficiently score queries Imagine for a moment that we don’t have an inverted index we only have the forward index which maps document IDs to a list of terms that occur in them  To score a query vector we’d need to iterate through every single entry i.
Scorer Now index use queries Imagine moment don’t forward maps list occur them.	3 Scorer Now that we have our inverted index how can we use it to efficiently score queries Imagine for a moment that we don’t have an inverted index we only have the forward index which maps document IDs to a list of terms that occur in them.
score vector need iterate	 To score a query vector we’d need to iterate through every single entry i.
forward function	e  document in the forward index and run a scoring function on the each document query pair.
document index document query	 document in the forward index and run a scoring function on the each document query pair.
fetch scoresd score Most likely terms performed ranking scoring exactly benefit match query documents scores.	fetch docsw do scoresd  scoresd  score termcount end for end for return top k documents from scores Most likely many documents do not contain any of the query terms especially if stop word removal is performed which means that their ranking score will be zero  Why should we bother scoring these documents anyway This is exactly how we can benefit from an inverted index we can only score documents that match at least one query term—that is we will only score documents that will have nonzero scores.
exactly match query term—that score	 Why should we bother scoring these documents anyway This is exactly how we can benefit from an inverted index we can only score documents that match at least one query term—that is we will only score documents that will have nonzero scores.
assume documents containing appear scoring computation algorithm inverted index 8.	 We assume and verify in practice that scoring only documents containing terms that appear in the query results in much less scoring computation  This leads us to our first scoring algorithm using the inverted index  8.
1 index built query inverted Algorithm	 8 3 1 Termatatime Ranking Once an inverted index is built scoring a query termbyterm can be done efficiently on an inverted index Idx using Algorithm 8.
1 inverted index index Idx 8.	1 Termatatime Ranking Once an inverted index is built scoring a query termbyterm can be done efficiently on an inverted index Idx using Algorithm 8.
accumulated document.	 Create document score accumulators as needed variables that hold the accumulated score for each document.
document term update accumulator weighting method score example BM25 As score final scores documents	 Scan the inverted index entries for the current term and for each entry corresponding to a document containing the term update its score accumulator based on some term weighting method the score term function  This could be for example Okapi BM25  As we finish processing all the query terms the score accumulators should have the final scores for all the documents that contain at least one query term.
This accumulators final scores contain term.	 This could be for example Okapi BM25  As we finish processing all the query terms the score accumulators should have the final scores for all the documents that contain at least one query term.
As finish query terms scores query Note accumulator doesn’t query	 As we finish processing all the query terms the score accumulators should have the final scores for all the documents that contain at least one query term  Note that we don’t need to create a score accumulator if the document doesn’t match any query term.
Note don’t need create score accumulator doesn’t query In reality object contains current term background information function operate Once	 Note that we don’t need to create a score accumulator if the document doesn’t match any query term  In reality the fetchdocs function would return some object that contains in formation about the current term in the document such as count background probability or any other necessary information that the scoreterm function would need to operate  Once we’ve iterated through all the query terms the score accumulators have been finalized.
Once query score accumulators finalized need k query sorting index	 Once we’ve iterated through all the query terms the score accumulators have been finalized  We just need to sort these documents by their accumulated scores and return usually the top k  Again we save time in this sorting operation by only sorting documents that contained a query term as opposed to sorting every single document in the index even if its score is zero.
	 8.
huge documents smaller iterating multiple score document once.	 While this is a huge improvement over all documents in the index we can still make this data structure smaller  Instead of iterating through each document multiple times for each matched query term occurrence we can instead score an entire document at once.
Instead multiple term entire document Since searches topk time.	 Instead of iterating through each document multiple times for each matched query term occurrence we can instead score an entire document at once  Since most if not all searches are topk searches we can only keep the top k documents at any one time.
Since searches k This document holds scored Otherwise scoring document surpass terms scored.	 Since most if not all searches are topk searches we can only keep the top k documents at any one time  This is only possible if we have the complete score for each document in our structure that holds scored documents  Otherwise as with termatatime scoring a document may start out with a lower score than another only to surpass it as more terms are scored.
structure scored	 This is only possible if we have the complete score for each document in our structure that holds scored documents.
index postings data need complete document added priority assign high priorities documents low adding document k time	 Using the inverted index we can get a list of document IDs and postings data that need to be scored  As we score a complete document it is added on the priority queue  We assign high priorities to documents with low scores this is so that after adding the k  1st document we can in Olog k time remove the lowestscore document and only hold onto the top k.
We documents scores 1st document remove lowestscore Once iterated document easily sort 8.	 We assign high priorities to documents with low scores this is so that after adding the k  1st document we can in Olog k time remove the lowestscore document and only hold onto the top k  Once we’ve iterated through all the document IDs we can easily sort the k documents and return them  See Algorithm 8.
iterated easily sort 8	 Once we’ve iterated through all the document IDs we can easily sort the k documents and return them  See Algorithm 8 2.
2 We use k docu need store scores k	2  We can use a similar priority queue approach while extracting the top k docu ments from the termatatime score accumulators but we would still need to store all the scores before finding the top k  8.
priority approach k score finding 3.	 We can use a similar priority queue approach while extracting the top k docu ments from the termatatime score accumulators but we would still need to store all the scores before finding the top k  8 3.
	 8 3.
For example store newspaper	 For example our index may store newspaper articles with dates as metadata.
In topk suppose return	 In our topk search suppose we want to only return documents that were written within the past year.
document filtering problem ignore range scores inserting structure In documentatatime skip document context document.	 This is a common document filtering problem  With termatatime ranking we can ignore documents that are not in the correct date range by not updating their scores in the score accumulator thus not inserting those document IDs into the structure  In documentatatime ranking we can simply skip the document if it doesn’t pass the filter when creating the context for that particular document.
With termatatime ranking date updating scores score accumulator inserting document IDs structure In doesn’t creating context	 With termatatime ranking we can ignore documents that are not in the correct date range by not updating their scores in the score accumulator thus not inserting those document IDs into the structure  In documentatatime ranking we can simply skip the document if it doesn’t pass the filter when creating the context for that particular document.
ranking simply skip document doesn’t creating context	 In documentatatime ranking we can simply skip the document if it doesn’t pass the filter when creating the context for that particular document.
8 Implementation	 156 Chapter 8 Search Engine Implementation Algorithm 8.
document list matching terms q fetch docsw	2 Documentatatime Ranking context    maps a document to a list of matching terms for w ∈ q do for d  count ∈ Idx fetch docsw do contextd.
appendcount queue score term 0 ∈ term score end queue score	appendcount end for end for priority queue    low score is treated as high priority for d  term counts ∈ context do score  0 for count ∈ term counts do score  score  score termcount end for priority queue pushd  score if priority queue.
score priority queue size k queue lowest end sorted priority queue Filters complex filter function document returns documents.	pushd  score if priority queue size  k then priority queue pop  removes lowest score so far end if end for Return sorted documents from priority queue Filters can be as complex as desired since a filter is essentially just a Boolean function that takes a document and returns whether or not it should be returned in the list of scored documents.
size k priority end sorted desired essentially Boolean documents The optional scoring access document metadata usually index order filter documents contain terms.	size  k then priority queue pop  removes lowest score so far end if end for Return sorted documents from priority queue Filters can be as complex as desired since a filter is essentially just a Boolean function that takes a document and returns whether or not it should be returned in the list of scored documents  The filtering function can then be an optional parameter to the scoring function which has access to the document metadata store usually a database and a forward index in order to filter documents that contain certain terms.
pop score queue complex Boolean document returns scored documents filtering parameter function database forward index order filter	pop  removes lowest score so far end if end for Return sorted documents from priority queue Filters can be as complex as desired since a filter is essentially just a Boolean function that takes a document and returns whether or not it should be returned in the list of scored documents  The filtering function can then be an optional parameter to the scoring function which has access to the document metadata store usually a database and a forward index in order to filter documents that contain certain terms.
Sharding sharding ular engine easily achieved stopping number number desired	3 4 Index Sharding Index sharding is the concept of keeping more than one inverted index for a partic ular search engine  This is easily achieved by stopping the postings chunk merging process when the number of chunks is equal to the number of desired shards.
All final chunk broken But inverted index cluster nodes probably algorithm matching terms shard results 8.	 All the same data is stored as one final chunk but it’s just broken down into several pieces  But why would we want multiple inverted index chunks Consider when we have the number of shards equal to the number of threads or cluster nodes in our search system  You can probably imagine an algorithm where each thread searches for matching terms in its respective shard and the final search results 8.
number threads You probably imagine thread shard search results Implementation 157	 But why would we want multiple inverted index chunks Consider when we have the number of shards equal to the number of threads or cluster nodes in our search system  You can probably imagine an algorithm where each thread searches for matching terms in its respective shard and the final search results 8 4 Feedback Implementation 157 are then merged together.
algorithm thread matching terms respective shard	 You can probably imagine an algorithm where each thread searches for matching terms in its respective shard and the final search results 8.
4 157 algorithm results—is discuss	4 Feedback Implementation 157 are then merged together  This type of algorithm design—distributing the work and then merging the results—is a very common paradigm called Map Reduce  We will discuss its generality and many other applications in future chapters.
applications chapters.	 We will discuss its generality and many other applications in future chapters.
Implementation Chapter feedback information retrieval We vector query feedback.	 8 4 Feedback Implementation Chapter 7 discussed feedback in a standard information retrieval system  We saw two implementations of feedback the vector space Rocchio feedback and the query likelihood mixture model for feedback.
Implementation information	4 Feedback Implementation Chapter 7 discussed feedback in a standard information retrieval system.
saw implementations feedback likelihood feedback Both implemented metadata previous	 We saw two implementations of feedback the vector space Rocchio feedback and the query likelihood mixture model for feedback  Both can be implemented with the inverted index and document metadata we’ve described in the previous sections.
Both metadata previous	 Both can be implemented with the inverted index and document metadata we’ve described in the previous sections.
For feedback forward query running Rocchio algorithm set model method requires language learned feedback achieved counts The term	 For Rocchio feedback we can use the forward index to obtain the vectors of both the query and feedback documents running the Rocchio algorithm on that set of vectors  The mixture model feedback method requires a language model to be learned over the feedback documents again this can be achieved efficiently by using the term counts from the forward index  The only other information needed is the corpus background probabilities for each term which can be stored in the term lexicon.
The method language documents efficiently forward index The information probabilities possible create “inmemory”	 The mixture model feedback method requires a language model to be learned over the feedback documents again this can be achieved efficiently by using the term counts from the forward index  The only other information needed is the corpus background probabilities for each term which can be stored in the term lexicon  With this information it is now possible to create an online or “inmemory” pseudofeedback method.
The information corpus term term With create “inmemory” Recall looks returned documents assumes	 The only other information needed is the corpus background probabilities for each term which can be stored in the term lexicon  With this information it is now possible to create an online or “inmemory” pseudofeedback method  Recall that pseudofeedback looks at the top k returned documents from search and assumes they are relevant.
With online method pseudofeedback looks returned documents search assumes relevant.	 With this information it is now possible to create an online or “inmemory” pseudofeedback method  Recall that pseudofeedback looks at the top k returned documents from search and assumes they are relevant.
k returned documents assumes The following online	 Recall that pseudofeedback looks at the top k returned documents from search and assumes they are relevant  The following process could be used to enable online feedback.
following enable feedback.	 The following process could be used to enable online feedback.
user’s documents forward index modify query Rocchio model interpolate model query search query new results user.	 Run the user’s original query  Use the top k documents and the forward index to either modify the query vec tor Rocchio or estimate a language model and interpolate with the feedback model query likelihood  Rerun the search with the modified query and return the new results to the user.
Rerun new results user simple implement query soon results	 Rerun the search with the modified query and return the new results to the user  There are both advantages and disadvantages to this simple feedback model  For one it requires very little memory and disk storage to implement since each modified query is “forgotten” as soon as the new results are returned.
There For memory new returned.	 There are both advantages and disadvantages to this simple feedback model  For one it requires very little memory and disk storage to implement since each modified query is “forgotten” as soon as the new results are returned.
little storage modified query new returned Thus additional storage index The downside computationally search	 For one it requires very little memory and disk storage to implement since each modified query is “forgotten” as soon as the new results are returned  Thus we don’t need to create any additional storage structures for the index  The downside is that all the processing is done at query time which could be quite computationally expensive especially when using a search engine with many users.
downside users.	 The downside is that all the processing is done at query time which could be quite computationally expensive especially when using a search engine with many users.
The opposite tradeoff modified database form running Of queries database size adding index Search Engine query vectors match query.	 The completely opposite tradeoff is to store every modified query in a database and look up its expanded form running the search function only once  Of course this is infeasible as the number of queries would quickly make the database explode in size not to mention that adding more documents to the index would 158 Chapter 8 Search Engine Implementation invalidate the stored query vectors since the new documents might also match the original query.
quickly mention adding index Implementation documents match	 Of course this is infeasible as the number of queries would quickly make the database explode in size not to mention that adding more documents to the index would 158 Chapter 8 Search Engine Implementation invalidate the stored query vectors since the new documents might also match the original query.
expanded query	 only storing the very frequently expanded queries or using query similarity to search for a similar query that has been saved.
section applicable feedback methods consider adopt terms expanded Of method There stored way.	 The caching techniques discussed in a later section are also applicable to feedback methods so consider how to adopt them from caching terms to caching expanded queries  Of course this only touches on the pseudofeedback method  There is also clickthrough data which can be stored in a database and relevance judgements which can be stored the same way.
data relevance	 There is also clickthrough data which can be stored in a database and relevance judgements which can be stored the same way.
8 5 Another integer compress large postings file A it’s main memory.	 8 5 Compression Another technical component in a retrieval system is integer compression which is applied to compress the very large postings file  A compressed index is not only smaller but also faster when it’s loaded into main memory.
Compression Another integer compression applied postings file compressed index smaller faster loaded memory general integers exploit nonuniform distribution values.	5 Compression Another technical component in a retrieval system is integer compression which is applied to compress the very large postings file  A compressed index is not only smaller but also faster when it’s loaded into main memory  The general idea of compressing integers and compression in general is to exploit the nonuniform distribution of values.
compressed index faster memory The general compressing integers compression general exploit nonuniform values.	 A compressed index is not only smaller but also faster when it’s loaded into main memory  The general idea of compressing integers and compression in general is to exploit the nonuniform distribution of values.
The idea compressing exploit nonuniform assign frequent values.	 The general idea of compressing integers and compression in general is to exploit the nonuniform distribution of values  Intuitively we will assign a short code to values that are frequent at the price of using longer codes for rare values.
The compression rate related taking consider—skewed distributions need random access particular position postings decompressing decompress vious data.	 The optimal compression rate is related to the entropy of the random variable taking the values that we consider—skewed distributions would have lower entropy and are thus easier to compress  It is important that all of our compression methods need to support random access decoding that is we could like to seek to a particular position in the postings file and start decompressing without having to decompress all the pre vious data.
methods need random access decoding like seek particular postings start pre data index entries exploit compress document IDs distributed uniformly bution term frequent inverted IDs leading small gaps.	 It is important that all of our compression methods need to support random access decoding that is we could like to seek to a particular position in the postings file and start decompressing without having to decompress all the pre vious data  Because inverted index entries are stored sequentially we may exploit this fact to compress document IDs and position information based on their gaps  The document IDs would otherwise be distributed relatively uniformly but the distri bution of their gaps would be skewed since when a term is frequent its inverted list would have many document IDs leading to many small gaps.
Because index sequentially exploit fact IDs position The relatively distri gaps skewed frequent list document leading	 Because inverted index entries are stored sequentially we may exploit this fact to compress document IDs and position information based on their gaps  The document IDs would otherwise be distributed relatively uniformly but the distri bution of their gaps would be skewed since when a term is frequent its inverted list would have many document IDs leading to many small gaps.
Consider following example document IDs 23 34 35	 Consider the following example of a list of document IDs 23 25 34 35 39 43 49 51 57 59   .
.	    .
.	.
159 space frequent 2 2 6	5 Compression 159 up less space and are more frequent 23 2 9 1 4 4 6 2 6 2     .
	 .
actual document offset value.	    To get the actual document ID values simply add the offset to the previous value.
	.
actual document simply add	 To get the actual document ID values simply add the offset to the previous value.
So ID 23 2 25.	 So the first ID is 23 and the second is 23  2  25.
In following types compression values	 In this section we will discuss the following types of compression which may or may not operate on gapencoded values .
unary bitwise encoding bitwise	 unary encoding bitwise   γ encoding bitwise .
δencoding vByte	 δencoding bitwise   vByte block and .
block.	 vByte block and   frame of reference block.
5 1 Bitwise like fixed like writing binary	 8 5 1 Bitwise compression With bitwise compression instead of writing out strings representing numbers like “1624” or fixed bytewidth chunks like a 4byte integer as “00000658” we are writing raw binary numbers.
5 1 Bitwise bitwise instead numbers “1624” 4byte writing	5 1 Bitwise compression With bitwise compression instead of writing out strings representing numbers like “1624” or fixed bytewidth chunks like a 4byte integer as “00000658” we are writing raw binary numbers.
1 Bitwise compression With instead strings numbers “1624” 4byte integer “00000658” writing representation number	1 Bitwise compression With bitwise compression instead of writing out strings representing numbers like “1624” or fixed bytewidth chunks like a 4byte integer as “00000658” we are writing raw binary numbers  When the representation ends the next number begins.
There bitwise compression means performing bit order	 When the representation ends the next number begins  There is no fixed width or length of the number representations  Using bitwise compression means performing some bit operations for every bit that is encoded in order to “build” the compressed integer back into its original form.
fixed width length number compression operations encoded compressed	 There is no fixed width or length of the number representations  Using bitwise compression means performing some bit operations for every bit that is encoded in order to “build” the compressed integer back into its original form.
method write followed acts delimiter stop reading 1 3 → 001 5 → 00001 0000000000000000001 Note number	 Unary encoding is the simplest method  To write the integer k we simply write k − 1 zeros followed by a one  The one acts as a delimiter and lets us know when to stop reading 1 → 1 2 → 01 3 → 001 4 → 0001 5 → 00001 19 → 0000000000000000001 Note that we can’t encode the number zero—this is true of most other methods as well.
write integer simply − 1	 To write the integer k we simply write k − 1 zeros followed by a one.
An example 4 3 4 8 Chapter 8 Search Engine As beginning support access numbers larger numbers	 An example of a unaryencoded sequence is 000100100010000000101000100001  4 3 4 8 2 4 5  160 Chapter 8 Search Engine Implementation As long as the lexicon has a pointer to the beginning of a compressed integer we can easily support random access decoding  We also have the property that small numbers take less space while larger numbers take up more space.
property concept	 We also have the property that small numbers take less space while larger numbers take up more space  The next two compression methods are built on the concept of unary encoding.
built encode number	 The next two compression methods are built on the concept of unary encoding  Gamma  To encode a number with γ encoding first simply write the number in binary.
Gamma simply write k bits string.	 Gamma  To encode a number with γ encoding first simply write the number in binary  Let k be the number of bits in your binary string.
number encoding write number	 To encode a number with γ encoding first simply write the number in binary.
number binary	 Let k be the number of bits in your binary string.
Read k bits Note Delta.	 Read the one and additional k bits in binary  Note that all γ codes will have an odd number of bits  Delta.
Note γ codes number bits.	 Note that all γ codes will have an odd number of bits.
short number encoding unary prefix 1 → → 3 → 0101 4 01101 → 000010011 001010011 47 00000101111 0011001111 γ code start position k read	 In short δencoding is γ encoding a number and then γ encoding the unary prefix including the one 1 → 1 → 1 2 → 010 → 0100 3 → 011 → 0101 4 → 00100 → 01100 5 → 00101 → 01101 19 → 000010011 → 001010011 47 → 00000101111 → 0011001111 To decode decode the γ code at your start position to get an integer k  Write a one and then read the next k  1 bits in binary including the one you wrote.
compression bits γ eventually numbers larger.	 As you can see the δ compression at first starts off to have more bits than the γ encoding but eventually becomes more efficient as the numbers get larger.
It probably method better terms	 It probably depends on the particular dataset the distribution of integers as to which compression method would be better in terms of compression ratio.
ratio simply size compressed size 3 files 2 8.	 A compression ratio is simply the uncompressed size divided by the compressed size  Thus a compression ratio of 3 is better in that the compressed files are smaller than a compression ratio of 2  8.
compression 3 smaller compression 2.	 Thus a compression ratio of 3 is better in that the compressed files are smaller than a compression ratio of 2.
5.	 8 5.
2 Block compression While grained distribution model processing read order read attempts issue reading	2 Block compression While bitwise encoding can achieve a very high compression ratio due to its fine grained distribution model its downside is the amount of processing that is re quired to encode and decode  Every single bit needs to be read in order to read one integer  Block compression attempts to alleviate this issue by reading bytes at a time instead of bits.
Block compression attempts time In compression operation opposed operation e.	 Block compression attempts to alleviate this issue by reading bytes at a time instead of bits  In block compression schemes only one bitwise operation per byte is usually required as opposed to at least one operation per bit in the pre vious three schemes e.
In compression opposed operation vious e count required	 In block compression schemes only one bitwise operation per byte is usually required as opposed to at least one operation per bit in the pre vious three schemes e g  count how many bit operations are required to δencode the integer 47.
g operations required 47 compression seeks reduce number CPU required decoding expense	g  count how many bit operations are required to δencode the integer 47  Block compression seeks to reduce the number of CPU instructions required in decoding at the expense of using more storage.
compression seeks number CPU expense storage The compression	 Block compression seeks to reduce the number of CPU instructions required in decoding at the expense of using more storage  The two block compression methods we will investigate deal mainly with bytes instead of bits.
methods instead	 The two block compression methods we will investigate deal mainly with bytes instead of bits.
reading binary	 The flag signals whether the decoder should keep reading the binary number.
The zero adding processed Notice backwards For follow shift byte add × k k number read far.	 The decoder works by keeping a sum which starts at zero and adding each byte into the sum as it is processed  Notice how the bytes are “chained” together backwards  For every “link” we follow we left shift the byte to add by 7 × k where k is the number of bytes read so far.
Notice bytes left × number read far.	 Notice how the bytes are “chained” together backwards  For every “link” we follow we left shift the byte to add by 7 × k where k is the number of bytes read so far.
Therefore sum 0100000 0011100 0000001 14 00111000000000b 000000100000000000000b 20000d encoding reference encoding block k k integers block method sequence numbers Take block size 45 47 70	 Therefore the sum we have to decode the integer 20000 is 0100000  0  0011100  7  0000001  14 which is the same as 0100000b 00111000000000b 000000100000000000000b 000000100111000100000b 20000d In this method the “blocks” that we are encoding are bytes  Frame of reference encoding takes a block size k and encodes k integers at a time so a block in this method is actually a sequence of numbers  Take the following block of size k  8 as an example 45 47 51 59 63 64 70 72.
Frame block encodes time block sequence	 Frame of reference encoding takes a block size k and encodes k integers at a time so a block in this method is actually a sequence of numbers.
size 8 example 70 block subtracting value 25 27 min 45.	 Take the following block of size k  8 as an example 45 47 51 59 63 64 70 72  We transform this block by subtracting the minimum value in the list from each element 0 2 6 14 18 19 25 27 min  45.
similar gap discussed instead bitwise compression δencoding smallest possible Since store integers 5 bits 11001 11011 min bytes 5.	 Up to this point this is similar to the gap encoding discussed previously  However instead of encoding each value with a bitwise compression such as γ  or δencoding we will simply use binary with the smallest number of bits possible  Since the maximum number in this chunk is 27 we will store each of the integers in 5 bits 00000 00010 00110 01110 10010 10011 11001 11011 min  45 bytes  5.
Since chunk store integers 5 01110 11011 5.	 Since the maximum number in this chunk is 27 we will store each of the integers in 5 bits 00000 00010 00110 01110 10010 10011 11001 11011 min  45 bytes  5.
look like 45 5 0000000010001100111010010100111100111011 5 convenient g.	 This might look like 45 5 0000000010001100111010010100111100111011 where 45 and 5 could be stored however is convenient e g.
reduces number operations chunks 5 add base value 45 nice minimum maximum possible stored integer document ID skip chunk decompress 45	 This method also reduces the number of bitwise operations since we read chunks of 5 bits and add them to the base value 45 to recreate the sequence  Another nice side effect is that we know the minimum value and maximum possible value stored in this chunk therefore if we are looking for a particular integer say for a document ID we know we can skip this chunk and not decompress it if ID  45 or ID  45  25.
nice effect maximum integer skip ID 45 45 25.	 Another nice side effect is that we know the minimum value and maximum possible value stored in this chunk therefore if we are looking for a particular integer say for a document ID we know we can skip this chunk and not decompress it if ID  45 or ID  45  25.
8 While designed inverted index For reason realworld engine sort caching structure stored objects.	 8 6 Caching While we designed our inverted index structure to be very efficient we still have the issue of disk latency  For this reason it is very common for a realworld search engine to also employ some sort of caching structure stored in memory for postings data objects.
6 designed structure disk	6 Caching While we designed our inverted index structure to be very efficient we still have the issue of disk latency.
The basic idea cache accessed To disk.	 The basic idea of a cache is to make frequently accessed objects fast to acquire  To accomplish this we attempt to keep the frequently accessed objects in memory so we don’t need to seek to the disk.
objects don’t objects access lists.	 To accomplish this we attempt to keep the frequently accessed objects in memory so we don’t need to seek to the disk  In our case the objects we access are postings lists.
cutoff cache address issues.	 But how do we know which terms to store Even if we knew which terms are most queried how do we set a cutoff for memory consumption The two cache designs we describe address both these issues.
1 mostfrequently 2 set size doesn’t	 That is 1 the mostfrequently used items are fast to access and 2 we can set a maximum size for our cache so it doesn’t get too large.
1 consider LRU cache displayed 8.	 8 6 1 LRU cache We first consider the least recently used LRU cache as displayed in Figure 8.
	6.
1 LRU We displayed Figure	1 LRU cache We first consider the least recently used LRU cache as displayed in Figure 8 3.
assuming retrieve search cache ID	3  The LRU algorithm is as follows assuming we want to retrieve the postings list for term ID x  First search the cache for term ID x.
ID If exists cache corresponding x exist cache disk	 First search the cache for term ID x  If it exists in the cache return the postings list corresponding to x  If it doesn’t exist in the cache retrieve it from disk and insert it into the cache.
list x If cache retrieve disk cache exceed maximum remove postings list.	 If it exists in the cache return the postings list corresponding to x  If it doesn’t exist in the cache retrieve it from disk and insert it into the cache  If this causes the cache to exceed its maximum size remove the leastrecently used postings list.
If doesn’t retrieve cache.	 If it doesn’t exist in the cache retrieve it from disk and insert it into the cache.
If maximum size	 If this causes the cache to exceed its maximum size remove the leastrecently used postings list.
want term lists O1 operations.	 We want searching the cache to be fast so we use a hash table to store term IDs as keys and postings lists as values  This enables O1 amortized insert find and delete operations.
interesting cache	 The interesting part of the LRU cache is how to determine the access order of the objects.
To values doubly linked list Once element list time.	 To do this we link together the values as a doubly linked list  Once an element is inserted or accessed it is moved to the head front of the list in O1 time.
need LRU element end constant	 When we need to remove the LRU item we look at the element at the tail end of the linked list and delete it also in constant time  8.
8	 8 6.
cache double LRU cache originally search	2 DBLRU cache The double barrel LRU cache was originally used in the popular Lucene search engine.
	 Figure 8.
First primary ID x it.	 First search primary for term ID x if it exists return it.
secondary delete return	 If it’s in secondary delete it  Insert it in primary and return it.
Insert If reach size clear contents secondary swap	 Insert it in primary and return it  If this causes primary to reach the maximum size clear the entire contents of secondary  Then swap the two hash tables.
reach maximum contents Then swap	 If this causes primary to reach the maximum size clear the entire contents of secondary  Then swap the two hash tables.
swap hash	 Then swap the two hash tables.
it’s secondary retrieve insert	 If it’s not in secondary retrieve it from disk and insert it into secondary  This cache has a rough hierarchy of usage primary contains elements that are more frequently accessed than secondary.
cache rough elements accessed	 This cache has a rough hierarchy of usage primary contains elements that are more frequently accessed than secondary.
So emptied free While precise LRU simpler access	 So when the cache fills the secondary table is emptied to free memory  While the temporal accuracy of the DBLRU cache is not as precise as the LRU cache it is a much simpler setup which translates to faster access times.
temporal DBLRU precise LRU faster times.	 While the temporal accuracy of the DBLRU cache is not as precise as the LRU cache it is a much simpler setup which translates to faster access times.
As speed accuracy	 As usual there is a tradeoff between the speed and accuracy of these two caches.
2	 2  httpslucene.
httpslucene org Reading classic book search engines book aging	 httpslucene apache org Bibliographic Notes and Further Reading A classic reference book for the implementation of search engines is the book Man aging Gigabytes Witten et al.
Notes Further Reading classic engines book Witten	apache org Bibliographic Notes and Further Reading A classic reference book for the implementation of search engines is the book Man aging Gigabytes Witten et al.
	 1999.
Information Manning al Information Retrieval Practice et al.	 Other books on information retrieval such as Introduction to Information Retrieval Manning et al  2008 and Search Engines Information Retrieval in Practice Croft et al.
et 2009 Engines et al 2010 implementations search engines.	 2008 and Search Engines Information Retrieval in Practice Croft et al  2009 and Information Retrieval Imple menting and Evaluating Search Engines Büttcher et al  2010 also have an excellent coverage of implementations of search engines.
2009 Imple menting Evaluating Engines et 2010 excellent search	 2009 and Information Retrieval Imple menting and Evaluating Search Engines Büttcher et al  2010 also have an excellent coverage of implementations of search engines.
Evaluation chapter focuses evaluation text retrieval systems.	 9Search Engine Evaluation This chapter focuses on the evaluation of text retrieval TR systems.
previous talked number different ranking know works best In evaluate retrieval methods focus chapter.	 In the previous chapter we talked about a number of different TR methods and ranking functions but how do we know which one works the best In order to answer this question we have to compare them and that means we’ll have to evaluate these retrieval methods  This is the main focus of this chapter.
This main	 This is the main focus of this chapter.
We evaluation.	 We start out with the methodology behind evaluation.
Then compare ranked relevance issues evaluation	 Then we compare the retrieval of sets with the retrieval of ranked lists as well as judgements with multiple levels of relevance  We end with practical issues in evaluation followed by exercises.
We end issues exercises.	 We end with practical issues in evaluation followed by exercises.
evaluation figure retrieval works best important knowledge wouldn’t know idea works	 There are two main reasons the first is that we have to use evaluation to figure out which retrieval method works the best  This is very important for advancing our knowledge otherwise we wouldn’t know whether a new idea works better than an old idea.
This know new idea better compared engine evaluation rely problem.	 This is very important for advancing our knowledge otherwise we wouldn’t know whether a new idea works better than an old idea  Previously in this book Chapter 6 we discussed the problem of text retrieval and compared it with database retrieval  Search engine evaluation must rely on users so this becomes a very challenging problem.
engine evaluation users involved	 Search engine evaluation must rely on users so this becomes a very challenging problem  Because of this we must determine how we can get users involved and draw a fair comparison of different methods.
determine fair different methods.	 Because of this we must determine how we can get users involved and draw a fair comparison of different methods.
second reason evaluation utility overall TR opposed methods.	 The second reason to perform evaluation is to assess the actual utility of an overall TR system as opposed to specific methods.
In users retrieval Typically user users inter	 In this case measures must reflect the utility to the actual users in the real application as opposed to measures on each individual retrieval result  Typically this has been done via user studies—where human users inter act with the corpus via the system.
studies—where act	 Typically this has been done via user studies—where human users inter act with the corpus via the system.
In methods use correlated measures determine method works This ally test main we’ll chapter.	 In the case of comparing different methods the measures we use all need to be correlated with the utility to the users  The measures only need to be good enough to determine which method works better  This is usu ally done by using a test collection which is a main idea that we’ll be talking about in this chapter.
important comparing improving general.	 This has been very important for comparing different algorithms and for improving search engines systems in general.
1 What Measure There aspects engine measure—here case relevant nonrelevant	1 What to Measure There are many aspects of a search engine we can measure—here are the three major ones  Effectiveness or accuracy  How accurate are the search results In this case we’re measuring a system’s capability of ranking relevant documents on top of nonrelevant ones.
How quickly user results How In space	 Efficiency  How quickly can a user get the results How large are the computing resources that are needed to answer a query In this case we need to measure the space and time overhead of the system.
results answer In time Usability real Here interfaces things typically user studies.	 How quickly can a user get the results How large are the computing resources that are needed to answer a query In this case we need to measure the space and time overhead of the system  Usability  How useful is the system for real user tasks Here interfaces and many other things are also important and we typically have to do user studies.
useful tasks Here user	 How useful is the system for real user tasks Here interfaces and many other things are also important and we typically have to do user studies.
book going talk effectiveness accu measures dimensions search engines evaluating	 In this book we’re going to talk mainly about the effectiveness and accu racy measures because the efficiency and usability dimensions are not unique to search engines they are needed for evaluating other software systems.
good coverage material reading	 There is also very good coverage of such material in other books so we suggest the reader consult Harman 2011 for further reading in this area.
	 9.
2 Cranfield Methodology developed 1960s strategy	1 2 Cranfield Evaluation Methodology The Cranfield evaluation methodology was developed in the 1960s and is a strategy for laboratory testing of system components.
Methodology The evaluation 1960s strategy It’s actually evaluation evaluating empirical example processing empirically need this.	2 Cranfield Evaluation Methodology The Cranfield evaluation methodology was developed in the 1960s and is a strategy for laboratory testing of system components  It’s actually a methodology that has been very useful not only for search engine evaluation but also for evaluating virtually all kinds of empirical tasks  For example in image processing or other fields where the problem is empirically defined we typically would need to use a method such as this.
methodology engine virtually empirical tasks.	 It’s actually a methodology that has been very useful not only for search engine evaluation but also for evaluating virtually all kinds of empirical tasks.
different ideas.	 Once such a test collection is built it can be used again and again to test different algorithms or ideas.
test collection similar search set queries topics simulate	 The assembled test collection of documents is similar to a real document collection in a search application  We can also have a sample set of queries or topics that simulate the user’s information need.
methodology algorithms reused times fair comparison methods exactly criteria corpus relevance differ ent algorithms.	 This methodology is very useful for evaluating retrieval algorithms because the test set can be reused many times  It also provides a fair comparison for all the methods since the evaluation is exactly the same for each one  That is we have the same criteria same corpus and same relevance judgements to compare the differ ent algorithms.
It provides fair comparison	 It also provides a fair comparison for all the methods since the evaluation is exactly the same for each one.
That criteria judgements algorithms.	 That is we have the same criteria same corpus and same relevance judgements to compare the differ ent algorithms.
allows compare new algorithm years In	 This allows us to compare a new algorithm with an old algorithm that was invented many years ago by using the same approach  In Figure 9.
1 Cranfield methodology works mentioned set shown here.	1 we illustrate how the Cranfield evaluation methodology works  As mentioned we need a set of queries that are shown here.
As need set Q2 We document D2	 As mentioned we need a set of queries that are shown here  We have Q1 Q2 and so on  We also need the document collection D1 D2 .
Q2 document D2	 We have Q1 Q2 and so on  We also need the document collection D1 D2 .
need document D2 .	 We also need the document collection D1 D2     .
.	   .
	 .
figure plus specifying plus	 and on the far right side of the figure we have the relevance judgments which are plus or minus annotations on each document specifying whether it is relevant plus or not relevant minus.
binary respect query levels	 Essentially these are binary judgments of documents with respect to a specific query since there are only two levels of relevance.
relevant judged respect Q1.	 For example D1 and D2 are judged as being relevant to Q1  D3 is judged as nonrelevant with respect to Q1.
D3 judged nonrelevant respect These Qi system.	 D3 is judged as nonrelevant with respect to Q1  These Qi judgements are created by users that interact with each system.
users Once judgements	 These Qi judgements are created by users that interact with each system  Once we have these judgements we can compare two or more systems.
Each run documents returns.	 Once we have these judgements we can compare two or more systems  Each query is run on each system and we investigate the documents that each system returns.
Let’s	 Each query is run on each system and we investigate the documents that each system returns  Let’s say the query is Q1.
	 Let’s say the query is Q1.
In RA results RB ranked B relevant approximation Let’s As like There documents	 In the figure we have RA as ranked results from system A and RB as the ranked results from system B  Thus RA is system A’s approximation of relevant documents and RB is system B’s approximation  Let’s take a look at these results—which is better As a user which one would you like There are some differences and there are some documents that are returned by both systems.
look As user like differences	 Let’s take a look at these results—which is better As a user which one would you like There are some differences and there are some documents that are returned by both systems.
good A B better returned instead two.	 That’s good system A is precise  On the other hand we can also say system B is better because it returned more relevant documents it returned three instead of two.
hand B returned relevant returned instead	 On the other hand we can also say system B is better because it returned more relevant documents it returned three instead of two.
possible writing literature	 On the other hand imagine a user might need to have as many relevant documents as possible for example in writing a literature survey.
9 examine basic measures systems.	 9 2 Evaluation of Set Retrieval In this section we examine the basic measures for evaluation of text retrieval systems.
Retrieval In section examine basic evaluation systems We basic measures systems evaluation metrics discussed section sets returned order taken	2 Evaluation of Set Retrieval In this section we examine the basic measures for evaluation of text retrieval systems  We discuss how to design these basic measures and how to quantitatively compare two systems  Although the systems return a ranked list of documents the evaluation metrics discussed in this section deal with sets of returned documents that is the order of the returned results is not taken into account.
design measures compare systems list deal documents order These intuition design	 We discuss how to design these basic measures and how to quantitatively compare two systems  Although the systems return a ranked list of documents the evaluation metrics discussed in this section deal with sets of returned documents that is the order of the returned results is not taken into account  These measures and their intuition are used to design other more sophisticated methods but are also quite valuable on their own.
evaluation documents returned taken intuition sophisticated methods valuable own.	 Although the systems return a ranked list of documents the evaluation metrics discussed in this section deal with sets of returned documents that is the order of the returned results is not taken into account  These measures and their intuition are used to design other more sophisticated methods but are also quite valuable on their own.
intuition sophisticated 9.	 These measures and their intuition are used to design other more sophisticated methods but are also quite valuable on their own  9.
1 Precision return	2 1 Precision and Recall Let’s return to Figure 9 1.
set better—system discuss actually quantify performance relevant documents corpus Q1.	1  Which set of results is better—system A’s or system B’s We can now discuss how to actually quantify their performance  Suppose we have a total of ten relevant documents in the corpus for the current query Q1.
results better—system B’s discuss performance total relevant documents corpus current query Q1.	 Which set of results is better—system A’s or system B’s We can now discuss how to actually quantify their performance  Suppose we have a total of ten relevant documents in the corpus for the current query Q1.
Suppose total current Q1 Of shown	 Suppose we have a total of ten relevant documents in the corpus for the current query Q1  Of course the relevance judgements shown on the right did not include all the ten  9.
right ten.	 Of course the relevance judgements shown on the right did not include all the ten.
Retrieval 171 imagine query Intuitively thought A	 9 2 Evaluation of Set Retrieval 171 We have only seen three relevant documents there but we can imagine there are other documents judged for this query  Intuitively we thought that system A is better because it did not have much noise.
Intuitively A noise	 Intuitively we thought that system A is better because it did not have much noise  In particular we have seen out of three results two are relevant.
	 On the other hand in system B we have five results and only three of them are relevant.
100 mean relevant A 2 3	 100 precision would mean all the retrieved documents are relevant  Thus in this case system A has a precision of 2 3  0.
3 66.	 Thus in this case system A has a precision of 2 3  0 66.
5	66  System B has 3 5  0.
B	 System B has 3 5  0 60.
mentioned preferred relevant So total relevant actually retrieved captured measure called measures documents result.	 But we also mentioned that system B might be preferred by some other users who wish to retrieve as many relevant documents as possible  So in that case we have to compare the number of total relevant documents to the number that is actually retrieved  This is captured in another measure called recall which measures the completeness of coverage of relevant documents in your retrieval result.
number total relevant number actually captured measures completeness coverage relevant documents	 So in that case we have to compare the number of total relevant documents to the number that is actually retrieved  This is captured in another measure called recall which measures the completeness of coverage of relevant documents in your retrieval result.
measure measures relevant retrieval result.	 This is captured in another measure called recall which measures the completeness of coverage of relevant documents in your retrieval result.
We assume Here got 10	 We assume that there are ten relevant documents in the collection  Here we’ve got two of them from system A so the recall is 2 10  0 20.
10 0 20.	 Here we’ve got two of them from system A so the recall is 2 10  0 20.
3 10	20  System B has 3 10  0.
System 10 0.	 System B has 3 10  0.
Therefore recall.	30  Therefore system B is better according to recall.
These basic foundation evaluating search engines.	 These two measures are the very basic foundation for evaluating search engines.
example machine recall kinds	 They are very important because they are also widely used in many other evaluation problems  For example if you look at the applications of machine learning you tend to see precision and recall numbers being reported for all kinds of tasks.
For look applications machine tend precision reported kinds let’s define measures evaluate documents.	 For example if you look at the applications of machine learning you tend to see precision and recall numbers being reported for all kinds of tasks  Now let’s define these two measures more precisely and how these measures are used to evaluate a set of retrieved documents.
That means considering relevant documents We distinguish cases depending situation shown Figure 9 2.	 That means we are considering that approximation of a set of relevant documents  We can distinguish the results in four cases depending on the situation of a document as shown in Figure 9 2.
set document relevant depending useful categories.	 A document is either retrieved or not retrieved since we’re talking about the set of results  The document can be also relevant or not relevant depending on whether the user thinks this is a useful document  We now have counts of documents in each of the four categories.
categories.	 We now have counts of documents in each of the four categories.
case documents defined dividing b number	 In this case the denominator is all the retrieved documents  Recall is defined by dividing a by the sum of a and b where a  b is the total number of relevant documents.
dividing b Precision recall relevant documents measures differ	 Recall is defined by dividing a by the sum of a and b where a  b is the total number of relevant documents  Precision and recall are focused on looking at a the number of retrieved relevant documents  The two measures differ based on the denominator of the formula.
measures differ based formula So result 1 didn’t there’s nonrelevant returned.	 The two measures differ based on the denominator of the formula  So what would be an ideal result If precision and recall are both 1 0 that means all the results that we returned are relevant and we didn’t miss any relevant documents there’s no single nonrelevant document returned.
result precision recall	 So what would be an ideal result If precision and recall are both 1.
tends possible tend include non documents precision documents results	 In reality however high recall tends to be associated with low precision as you go down the list to try to get as many relevant documents as possible you tend to include many non relevant documents which decreases the precision  We often are interested in the precision up to ten documents for web search  This means we look at the top ten results and see how many documents among them are actually relevant.
We interested precision	 We often are interested in the precision up to ten documents for web search.
2 Measure Precision There recall combine Fβ measure displayed 9.	2 2 The F Measure Combining Precision and Recall There tends to be a tradeoff between precision and recall so it is natural to combine them  One metric that is often used is called the Fβ measure displayed in Figure 9.
2 The Precision precision recall combine	2 The F Measure Combining Precision and Recall There tends to be a tradeoff between precision and recall so it is natural to combine them  One metric that is often used is called the Fβ measure displayed in Figure 9 3.
case harmonic mean parameter simplification written form indicates precision	 In the case where β  1 it’s a harmonic mean of precision and recall  Considering the parameter β and after some simplification we can see the F measure may be written in the form on the righthand side of the figure  Often β set to one which indicates an equal preference towards precision and recall.
In β 1 case called combined precision recall	 In the case where β  1 we have a special case of the F measure often called F1  This is a popular measure that is often used as a combined precision and recall score.
β precision	 If β is not equal to one it controls the emphasis between precision and recall.
It’s easy large precision high.	 It’s easy to see that if you have a large precision or large recall then the F1 measure would be high.
what’s interesting tradeoff recall captured score order formulation combine arithmetic mean likely natural way	 But what’s interesting is how the tradeoff between precision and recall is captured in the F1 score  In order to understand the formulation we can first ask the natural question Why not combine them using a simple arithmetic mean That would be likely the most natural way of combining them.
understand formulation ask natural simple arithmetic mean That likely way combining Why F1	 In order to understand the formulation we can first ask the natural question Why not combine them using a simple arithmetic mean That would be likely the most natural way of combining them  Why is this not as good as F1 i.
Why	 Why is this not as good as F1 i.
e.	e.
mean mean dominated large values.	 what’s the problem with an arithmetic mean The arithmetic mean tends to be dominated by large values.
This recall returning Then recall low precision This relatively average.	 This is not the desirable effect because one can easily have a perfect recall by returning all the documents Then we have a perfect recall and a low precision  This will still give a relatively high average.
relatively high Such results contrast similar.	 This will still give a relatively high average  Such search results are clearly not very useful for users even though the average using this formula would be relatively high  In contrast the F1 score will reward a case where precision and recall are roughly similar.
Such average relatively In contrast F1 So extremely result them.	 Such search results are clearly not very useful for users even though the average using this formula would be relatively high  In contrast the F1 score will reward a case where precision and recall are roughly similar  So it would penalize a case with an extremely high result for only one of them.
contrast reward precision case extremely	 In contrast the F1 score will reward a case where precision and recall are roughly similar  So it would penalize a case with an extremely high result for only one of them.
This means F1 tradeoff simple arithmetic	 This means F1 encodes a different tradeoff between them than a simple arithmetic mean.
example shows important methodology try problem think g.	 This example shows a very important methodology when we try to solve a problem you might naturally think of one solution e g.
	g.
Once important differences makes sense	 Once you have multiple ideas it’s important to analyze their differences and then think about which one makes more sense in a real scenario.
summarize talked addresses retrieval talked question relevant documents retrieved basic information evaluation well.	 To summarize we talked about precision which addresses the question are the retrieval results all relevant We also talked about recall which addresses the question have all the relevant documents been retrieved These two are the two basic measures in information retrieval evaluation  They are used for many other tasks as well.
well.	 They are used for many other tasks as well.
We combine	 We talked about F measure as a way to combine precision and recall.
We tradeoff recall users’ search tasks	 We also talked about the tradeoff between precision and recall and it turns out to depend on the users’ search tasks and preferences  9.
Ranked List In considered relevant appeared results measure In document’s account	 9 3 Evaluation of a Ranked List In the previous section we only considered whether a relevant document appeared in the results or not—a binary measure  In this section we will see how we can take each document’s position into account when assigning an evaluation score.
3 In section document In document’s account	3 Evaluation of a Ranked List In the previous section we only considered whether a relevant document appeared in the results or not—a binary measure  In this section we will see how we can take each document’s position into account when assigning an evaluation score.
position assigning precision recall ways measure	 In this section we will see how we can take each document’s position into account when assigning an evaluation score  We saw that precision and recall are the two basic ways to quantitatively measure the performance of a search result.
But Chapter 5 problem ranked relevant	 But as we talked about in depth in Chapter 5 the text retrieval problem is a ranking problem not a classification one  Thus we need to evaluate the quality of a ranked list as opposed to whether a relevant document was returned anywhere in the results.
Thus evaluate relevant results How precision list Naturally look different cutoffs ranked list user browsing user sequentially list user stop	 Thus we need to evaluate the quality of a ranked list as opposed to whether a relevant document was returned anywhere in the results  How can we use precision and recall to evaluate a ranked list Naturally we will have to look at precision and recall at different cutoffs since a ranked list of relevant documents is determined by where the user stops browsing  If we assume the user sequentially browses the list of results the user would stop at some point.
use evaluate list precision recall relevant documents determined user stops If list stop point determine set.	 How can we use precision and recall to evaluate a ranked list Naturally we will have to look at precision and recall at different cutoffs since a ranked list of relevant documents is determined by where the user stops browsing  If we assume the user sequentially browses the list of results the user would stop at some point  That point would determine the size of the set.
If assume results stop point determine set.	 If we assume the user sequentially browses the list of results the user would stop at some point  That point would determine the size of the set.
determine set that’s cutoff	 That point would determine the size of the set  Therefore that’s the most important cutoff that we have to consider when we compute the precisionrecall.
compute user	 Therefore that’s the most important cutoff that we have to consider when we compute the precisionrecall  Without knowing where exactly the user would stop we have to consider all the possible positions where they might stop.
positions stop.	 Without knowing where exactly the user would stop we have to consider all the possible positions where they might stop.
precisionrecall Figure	 A precisionrecall curve does exactly this as illustrated in Figure 9.
document recall Note relevant D2 relevant	4 What if the user stops at the first document What’s the precisionrecall at this point Since D1 is relevant the precision is one out of one since we have one document and it is relevant  What about the recall Note that we are assuming that there are ten relevant documents for this query in the collection so it’s one out of ten  What if the user stops at the second position The precision is the same since both D1 and D2 are relevant 100 or two out of two.
assuming stops second D1 D2 100	 What about the recall Note that we are assuming that there are ten relevant documents for this query in the collection so it’s one out of ten  What if the user stops at the second position The precision is the same since both D1 and D2 are relevant 100 or two out of two.
position interesting case recall change lower recall won’t change	 If the user stops at the third position we have an interesting case because we don’t have any additional relevant documents so the recall does not change  However the precision is lower because we have two out of three relevant documents  The recall won’t change until we see another relevant document.
The won’t document In case point D5 There precision	 The recall won’t change until we see another relevant document  In this case that point is at D5  There the recall has increased to three out of ten and the precision is three out of five.
point recall increased	 In this case that point is at D5  There the recall has increased to three out of ten and the precision is three out of five.
As D8 precision documents	 There the recall has increased to three out of ten and the precision is three out of five  As you can see if we keep doing this we can also get to D8 and have a precision of four out of eight because there are eight documents and four of them are relevant.
assume like this.	 When can we get a recall of five out of ten In this list we don’t have it  For convenience we often assume that the precision is zero in a situation like this.
convenience assume zero situation This pessimistic assumption order way measure called clearly	 For convenience we often assume that the precision is zero in a situation like this  This is a pessimistic assumption since the actual precision would be higher but we make this assumption in order to have an easy way to compute another measure called average precision that we will discuss soon  Note that we’ve made some assumptions that are clearly not accurate.
assumption precision higher assumption order way discuss we’ve clearly okay comparison text retrieval	 This is a pessimistic assumption since the actual precision would be higher but we make this assumption in order to have an easy way to compute another measure called average precision that we will discuss soon  Note that we’ve made some assumptions that are clearly not accurate  But this is okay for the relative comparison of two text retrieval methods.
Note we’ve clearly accurate.	 Note that we’ve made some assumptions that are clearly not accurate.
measure accurately tell works better.	 As long as the deviation is not biased toward any particular retrieval method the measure is acceptable since we can still accurately tell which method works better.
avoid it’s perfectly fine transformation measures	 As long as you can avoid that it’s perfectly fine to do a transformation of these measures that preserves the order.
4 On values.	4  On the xaxis are the recall values.
Furthermore As figure precision recalls curves exactly like matter methods exact precision not.	 Furthermore we can link these points to form a curve  As you see in the figure we assumed all the precision values at the highlevel recalls are zero  Although the real curves will not be exactly like this it doesn’t matter that much for comparing two methods whether we get the exact precision values here or not.
figure zero.	 As you see in the figure we assumed all the precision values at the highlevel recalls are zero.
like In Figure 9 compare PRcurves graph.	 Although the real curves will not be exactly like this it doesn’t matter that much for comparing two methods whether we get the exact precision values here or not  In Figure 9 5 we compare two systems by plotting their PRcurves on the same graph.
In 5 compare PRcurves graph.	 In Figure 9 5 we compare two systems by plotting their PRcurves on the same graph.
compare plotting shown blue.	5 we compare two systems by plotting their PRcurves on the same graph  System A is shown in red and system B is shown in blue.
A shown B recall better	 System A is shown in red and system B is shown in blue  Which one is better On the left system A is clearly better since for the same level of recall the precision value by system A is better than system B.
better On A level recall general curve problem case like right graph—this curves cross	 Which one is better On the left system A is clearly better since for the same level of recall the precision value by system A is better than system B  In general the higher the curve is the better  The problem is that we might see a case like the right graph—this actually happens quite often where the two curves cross each other.
In higher curve like graph—this happens	 In general the higher the curve is the better  The problem is that we might see a case like the right graph—this actually happens quite often where the two curves cross each other.
The right graph—this actually better This problem actually face.	 The problem is that we might see a case like the right graph—this actually happens quite often where the two curves cross each other  In this case which one is better This is a real problem that you might actually have to face.
In better This real build search old that’s blue Then results shown red	 In this case which one is better This is a real problem that you might actually have to face  Suppose you build a search engine and you have an old algorithm that’s shown here in blue as system B  Then you have come up with a new idea and test it with results shown in red as system A.
Suppose build old algorithm that’s shown B Then new red	 Suppose you build a search engine and you have an old algorithm that’s shown here in blue as system B  Then you have come up with a new idea and test it with results shown in red as system A.
better Or algorithm you’re behave A don’t like B.	 The question is is your new method better than the old method Or more practically do you have to replace the algorithm that you’re already using in your search engine with another new algorithm If you make the replacement the search engine would behave like system A here whereas if you don’t do that it will be like system B.
Now A like	 Now some users might like system A while other users might like system B.
That means depends high recall high precision.	 That means it depends on whether the user cares about high recall or low recall with high precision.
In case better unlikely examine	 Which system is better for that task In this case system B is better because the user is unlikely to examine many results i e.
user doesn’t care On user wants emphasize high documents don’t miss chance	e  the user doesn’t care about high recall only the first few results being useful  On the other hand if a user wants to determine whether an idea has been thought of before they will want to emphasize high recall so that they see as many relevant documents as possible and don’t miss the chance to find the idea.
user care	 the user doesn’t care about high recall only the first few results being useful.
Therefore users A.	 Therefore those users would favor system A.
better Again users task necessarily depict	 But this brings us back to the original question which one is better Again this actually depends on the users or more precisely the users’ task  You may not necessarily be able to come up with one number that would accurately depict the performance.
come performance You	 You may not necessarily be able to come up with one number that would accurately depict the performance  You have to look at the overall picture.
look number systems different need number values area	 You have to look at the overall picture  Despite this it can be beneficial to have one number to compare the systems so that we can easily make a lot of different system comparisons we need a number to summarize the range of precisionrecall values  One way is to look at the area underneath the curve—the average precision.
Despite easily number way area underneath going recall point consider	 Despite this it can be beneficial to have one number to compare the systems so that we can easily make a lot of different system comparisons we need a number to summarize the range of precisionrecall values  One way is to look at the area underneath the curve—the average precision  Basically we’re going to take a look at every different recall point and consider the precision.
Basically precision.	 Basically we’re going to take a look at every different recall point and consider the precision.
retrieving relevant cases precision divide total documents	 The precisions we add up correspond to retrieving the first relevant document the second and so on  In the example in the figure we missed many relevant documents so in all of these cases we assume that they have zero precision  Finally we take the average and divide it by ten which is the total number of relevant documents in the collection.
example figure assume	 In the example in the figure we missed many relevant documents so in all of these cases we assume that they have zero precision.
total relevant documents	 Finally we take the average and divide it by ten which is the total number of relevant documents in the collection.
sum number favors retrieve small denominator relevant	 Note that we’re not dividing this sum by four which is the number of retrieved relevant documents  Dividing by four is a common mistake this favors a system that would retrieve very few documents making the denominator very small  In the correct formula the denominator is ten the total number of relevant documents.
In number relevant	 In the correct formula the denominator is ten the total number of relevant documents.
This area combining recall precision define average list L n avpL pi 9.	 This will allow us to compute the area under the PRcurve combining recall and precision  Mathematically we can define average precision on a ranked list L where L  n as avpL  1 Rel n∑ i1 pi 9.
ranked list L avpL 1 n∑	 Mathematically we can define average precision on a ranked list L where L  n as avpL  1 Rel n∑ i1 pi 9.
1 denotes documents set relevant collection relevant ignore setting 0.	1 where pi denotes the precision at rank i of the documents in L and Rel is the set of all relevant documents in the collection  If Di is not relevant we would ignore the contribution from this rank by setting pi  0.
Di ignore contribution 0 relevant obtain relevant position If document .	 If Di is not relevant we would ignore the contribution from this rank by setting pi  0  If Di is relevant to obtain pi we divide the number of relevant documents we’ve seen so far by the current position in the list which is i  If the first relevant document is at the second rank then p2  1 2 .
relevant document second	 If the first relevant document is at the second rank then p2  1 2 .
If document seventh rank p7 7 formula Figure	 If the third relevant document is at the seventh rank then p7  3 7   Let’s use this formula to calculate the average precision of the documents returned in Figure 9.
Let’s formula average precision Figure 4.	 Let’s use this formula to calculate the average precision of the documents returned in Figure 9 4.
4 9.	4  Figure 9.
Figure 6 shows measure small change document.	 Figure 9 6 shows the calculation  This measure is sensitive to a small change in position of a relevant document.
small position If relevant document	6 shows the calculation  This measure is sensitive to a small change in position of a relevant document  If we move the third or fourth relevant document up it would increase the averages.
This	 This measure is sensitive to a small change in position of a relevant document.
Conversely relevant decrease There good ranking individual relevant document It differences ranked lists that’s exactly want.	 Conversely if we move any relevant document down then it would decrease  There fore this is a good measure because it’s sensitive to the ranking of each individual relevant document  It can distinguish small differences between two ranked lists and that’s exactly what we want.
differences ranked lists that’s want.	 It can distinguish small differences between two ranked lists and that’s exactly what we want.
effective precision relevant ranked	 But if we use this measure to compare two or more systems it wouldn’t be as effective since precision alone is not sensitive to where these four relevant documents are ranked in the list.
contrast precision better subtle rank overall	 In contrast average precision is a much better measure since subtle differences in rank affect the overall score  9 3.
9.	 9.
3.	3.
For ex perform query happens poorly accurate systems’ capability.	 For ex ample one system may perform very well with one query on which another system happens to perform poorly using only this query would not give an accurate assess ment of each systems’ capability.
Using average precision Naturally calculate arithmetic	 Using more queries then requires the researcher to take an average of the average precision over all these queries  Naturally we can simply calculate an arithmetic mean.
arithmetic mean.	 Naturally we can simply calculate an arithmetic mean.
called average In case queries	 In fact this would give us what’s called mean average precision MAP  In this case we take arithmetic mean of all the average precisions over several queries or topics.
case Let L	 In this case we take arithmetic mean of all the average precisions over several queries or topics  Let L L1 L2 .
L1	 Let L L1 L2     .
different	    Lm be the ranked lists returned from running m different queries.
1 m i1 avpLi 2 9.	 Then we have MAPL  1 m m∑ i1 avpLi  9 2 9.
9 9 Evaluation List 179 Recall score.	 9 2 9 3 Evaluation of a Ranked List 179 Recall our discussion about the F1 score.
9 3 Recall discussion score In situation arithmetic mean average precisions We concluded arithmetic	2 9 3 Evaluation of a Ranked List 179 Recall our discussion about the F1 score  In this situation is an arithmetic mean of average precisions acceptable We concluded before that the arithmetic mean of precision and recall was not as good as the harmonic mean.
179 score situation mean average acceptable We arithmetic recall	3 Evaluation of a Ranked List 179 Recall our discussion about the F1 score  In this situation is an arithmetic mean of average precisions acceptable We concluded before that the arithmetic mean of precision and recall was not as good as the harmonic mean.
average precisions acceptable We mean precision harmonic mean similar think alternative average	 In this situation is an arithmetic mean of average precisions acceptable We concluded before that the arithmetic mean of precision and recall was not as good as the harmonic mean  Here we have a similar situation we should think about the alternative ways of aggregating the average precisions.
Here similar ways average precisions Another way precisions geometric precision	 Here we have a similar situation we should think about the alternative ways of aggregating the average precisions  Another way is the geometric mean using the geometric mean to consolidate the average precisions is called geometric mean average precision or gMAP for short.
precisions called geometric average gMAP short define gMAPL 9 space gMAPL exp 1	 Another way is the geometric mean using the geometric mean to consolidate the average precisions is called geometric mean average precision or gMAP for short  We define it below mathematically as gMAPL m∏ i1 avpLi 1 m 9 3 or in log space as gMAPL  exp 1 m m∑ i1 ln avpLi .
gMAPL 1 .	3 or in log space as gMAPL  exp 1 m m∑ i1 ln avpLi .
You’ve multiple topics queries precision You wish performance strategy think scenarios difference argument F1 sum	 You’ve tested multiple topics queries and have the average precision for each topic  You wish to consider the overall performance but which strategy would you use Can you think of scenarios where using one of them would make a difference That is is there a situation where one measure would give different rankings of the two methods Similar to our argument about F1 we realize in the arithmetic mean the sum is dominated by large values.
overall performance strategy use Can scenarios difference situation different rankings argument F1 arithmetic	 You wish to consider the overall performance but which strategy would you use Can you think of scenarios where using one of them would make a difference That is is there a situation where one measure would give different rankings of the two methods Similar to our argument about F1 we realize in the arithmetic mean the sum is dominated by large values.
Here easy gMAP low don’t good low improve difficult gMAP	 Here a large value means that the query is relatively easy  On the other hand gMAP tends to be affected more by low values—those are the queries that don’t have good performance the average precision is low  If you wish to improve the search engine for those difficult queries then gMAP would be preferred.
low values—those queries precision low If engine difficult	 On the other hand gMAP tends to be affected more by low values—those are the queries that don’t have good performance the average precision is low  If you wish to improve the search engine for those difficult queries then gMAP would be preferred.
1 position rank single	 That is 1 r where r is the position rank of the single relevant document.
ranked 1 it’s second position it’s set topics mean reciprocal MRR.	 If that document is ranked on the very top then the reciprocal rank would be 1 1  1  If it’s ranked at the second position then it’s 1 2 and so on  This means we can also take an average of all the reciprocal ranks over a set of topics which gives us the mean reciprocal rank MRR.
it’s position 1 2 reciprocal gives MRR known search relevant item.	 If it’s ranked at the second position then it’s 1 2 and so on  This means we can also take an average of all the reciprocal ranks over a set of topics which gives us the mean reciprocal rank MRR  It’s a very popular measure for known item search or any problem where you have just one relevant item.
means average reciprocal topics gives reciprocal measure	 This means we can also take an average of all the reciprocal ranks over a set of topics which gives us the mean reciprocal rank MRR  It’s a very popular measure for known item search or any problem where you have just one relevant item.
r user If it’s actually sift documents meaningful measure reciprocal rank r	 We can see this r is quite meaningful it indicates how much effort a user would have to make in order to find that one relevant document  If it’s ranked on the top it’s low effort if it’s ranked at 100 then you actually have to presumably sift through 100 documents in order to find it  Thus r is also a meaningful measure and the reciprocal rank will take the reciprocal of r instead of using it directly.
ranked it’s documents reciprocal r directly question r performance item thought directly measure.	 If it’s ranked on the top it’s low effort if it’s ranked at 100 then you actually have to presumably sift through 100 documents in order to find it  Thus r is also a meaningful measure and the reciprocal rank will take the reciprocal of r instead of using it directly  The usual question also applies here Why not just simply use r If you were to design a ratio to measure the performance of a system where there is only one relevant item you might have thought about using r directly as the measure.
Thus meaningful instead directly.	 Thus r is also a meaningful measure and the reciprocal rank will take the reciprocal of r instead of using it directly.
The applies simply use performance relevant thought measure right number	 The usual question also applies here Why not just simply use r If you were to design a ratio to measure the performance of a system where there is only one relevant item you might have thought about using r directly as the measure  After all that measures the user’s effort right But think about if you take an average of this over a large number of topics  Again it would make a difference.
effort right think topics	 After all that measures the user’s effort right But think about if you take an average of this over a large number of topics  Again it would make a difference.
1 r The difference appears topics Just MAP sum dominated r	 A larger r corresponds to a small 1 r   The difference appears when there are many topics  Just like MAP this sum will be dominated by large values of r .
Those large values low list.	 So what are those values Those are basically large values that indicate lower ranked results  That means the relevant items rank very low down on the list.
That means relevant relevant documents lower portion	 That means the relevant items rank very low down on the list  The average would then be dominated by the relevant documents that are ranked in the lower portion of the list.
The dominated relevant documents portion care highly ranked documents taking rank difference	 The average would then be dominated by the relevant documents that are ranked in the lower portion of the list  From a user’s perspective we care more about the highly ranked documents so by taking this transformation by using reciprocal rank we emphasize more on the difference on the top.
Think rank rank 100 1000 Is summary showed curve characterize ranked list.	 Think about the difference between rank one and rank two and the difference between rank 100 and 1000 using each method  Is one more preferable than the other In summary we showed that the precisionrecall curve can characterize the overall accuracy of a ranked list.
actual utility ranked ranked user examine	 We emphasized that the actual utility of a ranked list depends on how many top ranked results a user would examine some users will examine more than others.
methods precision methods summarize multiple average precision	 Average precision is a standard measure for comparing two ranking methods it combines precision and recall while being sensitive to the rank of every relevant document  We concluded this section with three methods to summarize multiple average precision values MAP gMAP and MRR  9.
methods summarize precision MAP gMAP	 We concluded this section with three methods to summarize multiple average precision values MAP gMAP and MRR.
Evaluation Multilevel Judgements evaluate text	4 Evaluation with Multilevel Judgements In this section we will explain how to evaluate text retrieval systems when there are multiple levels of relevance judgments.
relevance highly relevant documents documents	 Earlier we made the point that relevance is a matter of degree  We often are able to distinguish very highly relevant documents from documents that are still useful but with a lower relevance.
We able distinguish documents documents example relevance relevant marginally	 We often are able to distinguish very highly relevant documents from documents that are still useful but with a lower relevance  In Figure 9 7 we show an example of three relevance levels level three for highly relevant two for marginally relevant and one for nonrelevant.
How We use average precision binary relevance values level gained categories gMAP depend precision use look relevant	 How do we evaluate a new system using these judgements We can’t use average precision since it only operates on binary relevance values if we treat level two and three as only one level then we lose the information gained from comparing these two categories  MAP gMAP and MRR depend on average precision so we can’t use them either  Let’s look at the top relevant results when using these judgments.
can’t Let’s look results judgments imagine user	 MAP gMAP and MRR depend on average precision so we can’t use them either  Let’s look at the top relevant results when using these judgments  We imagine the user would mostly care about the top ten results.
Let’s We user care	 Let’s look at the top relevant results when using these judgments  We imagine the user would mostly care about the top ten results.
user care	 We imagine the user would mostly care about the top ten results.
multilevel “gains” roughly correspond information gains viewing document.	 We call these multilevel judgements “gains” since they roughly correspond to how much information a user gains when viewing a document.
This document user sum traversing list ri let n	 This gain usually matches the utility of a document from a user’s perspective  If we assume the user stops at ten documents we can cumulatively sum the information gain from traversing the list of returned documents  Let ri be the gain of result i and let i range from one to n where we set n to ten in our example.
If user stops sum information list returned documents result let example.	 If we assume the user stops at ten documents we can cumulatively sum the information gain from traversing the list of returned documents  Let ri be the gain of result i and let i range from one to n where we set n to ten in our example.
gain CGL i1 9.	 We then have the cumulative gain CG as CGL n∑ i1 ri   9.
5 user	5 If the user looks at more documents the cumulative gain is more.
Thus gain gives gain user user examines	 Thus cumulative gain gives us some idea about how much total gain the user would have if the user examines all these documents.
CG sum relevant marginally don’t Ideally gains The second	 Looking at the CG sum of the top four documents we know there is only one highly relevant document one marginally relevant document two non relevant documents we don’t know where they are ranked in the list  Ideally we want those with gains of three to be ranked on the top  But how can we capture that intuition The second three is not as good as the first three at the top.
Ideally ranked But The contribution gain	 Ideally we want those with gains of three to be ranked on the top  But how can we capture that intuition The second three is not as good as the first three at the top  That means the contribution of gain from different documents has to be weighted by their position.
That means contribution position The need assume user second discounted little possibility notice position positionbased penalty.	 That means the contribution of gain from different documents has to be weighted by their position  The document at position one doesn’t need to be discounted because you can assume that the user always sees this document but the second one will be discounted a little bit because there’s a small possibility that the user wouldn’t notice it  We divide this gain by a weight based on the position in order to capture this positionbased penalty.
cumulative n∑ i2 ri 9 document’s	 The discounted cumulative gain does exactly this DCGL  r1 n∑ i2 ri log2 i   9 6 Each document’s gain is discounted by dividing by a logarithm of its position in the list.
Each document’s gain discounted position Thus document.	6 Each document’s gain is discounted by dividing by a logarithm of its position in the list  Thus a lowly ranked document would not contribute as much gain as a highly ranked document.
Thus ranked document contribute That D5’s relevance score close	 Thus a lowly ranked document would not contribute as much gain as a highly ranked document  That means if for example you switch the position of D5 and D2 then the overall DCG score would increase since D5’s relevance score of three is discounted less by being close to the top.
switch D5 overall DCG discounted close point gain utility ranked list	 That means if for example you switch the position of D5 and D2 then the overall DCG score would increase since D5’s relevance score of three is discounted less by being close to the top  At this point we have a discounted cumulative gain for measuring the utility of a ranked list with multiple levels of judgment.
It ideal DCG The DCG ranked list relevant sorted	 9 7 It is simply DCG normalized by the ideal DCG IDCG for a particular query  The IDCG is the DCG of an ideal ranked list with the most relevant documents at the top sorted in decreasing order of relevance.
For example collection rated	 For example imagine that we have nine documents in the whole collection rated three.
Then ideal list documents All that’s	 Then our ideal ranked list would have put all these nine documents on the very top  All this would be followed by a two because that’s the best we could do after we have run out of threes.
DCG ideal	 Then we can compute the DCG for this ideal ranked list.
actual DCG best result doesn’t affect systems systems.	 Essentially we compare the actual DCG with the best result you can possibly get for this query  This doesn’t affect the relative comparison of systems for just one topic because this ideal DCG is the same for all the systems.
This relative topic	 This doesn’t affect the relative comparison of systems for just one topic because this ideal DCG is the same for all the systems.
topics—if don’t different scales DCG highly documents course relevance level.	 The difference is when we have multiple topics—if we don’t do normalization different topics will have different scales of DCG  For a query like this one we have nine highly relevant documents but of course that will not always be the case  Thus NDCG is used for measuring relevance based on much more than one relevance level.
For highly case NDCG measuring based relevance level general way basically measure applied task	 For a query like this one we have nine highly relevant documents but of course that will not always be the case  Thus NDCG is used for measuring relevance based on much more than one relevance level  In a more general way this is basically a measure that can be applied through any ranked task with a large range of judgments.
judgments dependant	 Furthermore the scale of the judgments can be dependant on the application at hand.
The idea summarize total k documents measure	 The main idea of this measure is to summarize the total utility of the top k documents you always choose a cutoff and then you measure the total utility.
contribution finally performs	 It discounts the contribution from lowly ranked documents and finally it performs normalization to ensure comparability across queries  9.
5 Issues Evaluation In queries	 9 5 Practical Issues in Evaluation In order to create a test collection we have to create a set of queries a set of documents and a set of relevance judgments.
turns challenges queries representative.	 It turns out that each requirement has its own challenges  First the documents and queries must be representative.
represent real users interact We order avoid task ensure exist relevant	 They must represent real queries and real documents that users interact with  We also have to use many queries and many documents in order to avoid biased conclusions  In order to evaluate a highrecall retrieval task we must ensure there exist many relevant doc uments for each query.
queries	 We also have to use many queries and many documents in order to avoid biased conclusions.
If relevant document methods terms complete judge queries effort human effort it’s laborintensive task.	 If a query has only one relevant document in the collection then it’s not very informative to compare different methods using such a query because there is not much room to see a difference  In terms of relevance judgements the challenge is to ensure complete judge ments of all the documents for all the queries while simultaneously minimizing human effort  Because we have to use human effort to label these documents it’s a very laborintensive task.
relevance judge ments queries label documents it’s laborintensive result usually impossible label documents especially considering like	 In terms of relevance judgements the challenge is to ensure complete judge ments of all the documents for all the queries while simultaneously minimizing human effort  Because we have to use human effort to label these documents it’s a very laborintensive task  As a result it’s usually impossible to actually label all of the documents for all the queries especially considering a data set like the Web.
laborintensive task usually especially considering like	 Because we have to use human effort to label these documents it’s a very laborintensive task  As a result it’s usually impossible to actually label all of the documents for all the queries especially considering a data set like the Web.
correlate evaluation	 It’s also challenging to correlate the evaluation measures with the perceived utility of users.
carefully capture preferences With certain probability mathematically uation scores	 We have to consider carefully what the users care about and then design measures to capture their preferences  With a certain probability we can mathematically quantify whether the eval uation scores of two systems are indeed different.
significance gives chance This use queries data	 The way we do this is with a statistical significance test  The significance test gives us an idea as to how likely a difference in evaluation scores is due to random chance  This is the reason why we have to use a lot of queries the more data points we have the more confident we can be in our measure.
significance idea difference scores chance This reason points	 The significance test gives us an idea as to how likely a difference in evaluation scores is due to random chance  This is the reason why we have to use a lot of queries the more data points we have the more confident we can be in our measure.
reason queries	 This is the reason why we have to use a lot of queries the more data points we have the more confident we can be in our measure.
9 precision A experiments.	 Figure 9 8 displays some sample average precision results from system A and system B in two different experiments.
They identi look average precisions trust given average feel confident.	 They happen to be identi cal in experiment one and two  Yet if you look at the exact average precisions for different queries you will realize that in one case you might feel that you can trust the conclusion here given by the average  In the other case you might not feel as confident.
exact queries realize average MAP easily	 Yet if you look at the exact average precisions for different queries you will realize that in one case you might feel that you can trust the conclusion here given by the average  In the other case you might not feel as confident  Based on only the MAP score we can easily say that system B is better.
4 twice 0 Clearly	4 which is twice as much as 0 2  Clearly that’s better performance.
better look experiments results experiment B better precisions consistently	 Clearly that’s better performance  But if you look at these two experiments and look at the detailed results you will see that we’ll be more confident to say that in experiment 1 that system B is in fact better since the average precisions are consistently better than system A’s.
answer test tests variance scores score different If according makes result unreliable.	 How can we quantitatively answer this question This is why we need to do a statistical significance test  The idea behind these tests is to assess the variance in average precision scores or any other score across these different queries  If there’s a big variance that means that the results could fluctuate according to different queries which makes the result unreliable.
tests different queries If means results fluctuate different unreliable So let’s case.	 The idea behind these tests is to assess the variance in average precision scores or any other score across these different queries  If there’s a big variance that means that the results could fluctuate according to different queries which makes the result unreliable  So let’s look at these results again in the second case.
Figure different test.	 In Figure 9 9 we show two different ways to compare them  One is a sign test.
	9 we show two different ways to compare them  One is a sign test.
sign test sign.	 One is a sign test  If system B is better than system A then we have a plus sign.
B better If minus sign Using A better.	 If system B is better than system A then we have a plus sign  If system A is better we have a minus sign  Using this we have four cases where system B is better and three cases where system A is better.
A sign B A better.	 If system A is better we have a minus sign  Using this we have four cases where system B is better and three cases where system A is better.
cases better cases A	 Using this we have four cases where system B is better and three cases where system A is better.
results appear If flip seven coins minus tails easily	 Intuitively these results appear random  If you flip seven coins using plus to denote heads and minus to denote tails then these could easily be the results of just randomly flipping the seven coins.
A value fact In means surely	 A p value is the probability that this result is in fact from random fluctuation  In this case the probability is one it means it surely is a random fluctuation.
surely random fluctuation.	 In this case the probability is one it means it surely is a random fluctuation.
There different significance tests quantify likelihood signedrank test test signs magnitude scores.	 There are many different significance tests that we can use to quantify the likelihood that the observed difference in the results is simply due to random fluctuations  A particularly interesting test is the Wilcoxon signedrank test  It’s a nonparametric test and we not only look at the signs but also consider the magnitude of the difference in scores.
nonparametric test look	 It’s a nonparametric test and we not only look at the signs but also consider the magnitude of the difference in scores.
Another parametric t assumed In similar example outcome	 Another is the parametric t test where a normal distribution is assumed  In any event we would draw a similar conclusion in our example case the outcome is very likely to be random.
In example outcome random.	 In any event we would draw a similar conclusion in our example case the outcome is very likely to be random.
For study suggest start Smucker	 For further study on this and other statistical significance tests we suggest the reader start with Smucker et al.
2007.	 2007.
illustrate pvalues 9.	 To illustrate the concept of pvalues consider the distribution in Figure 9.
mean	 This is a normal distribution with a mean of zero in the center.
probability deviating zero subtract A’s B’s versa difference fact	 This curve shows the probability that we would observe values that are deviating from zero here when we subtract system A’s MAP from system B’s MAP or even vice versa  Based on the picture we see that if a difference is observed at the dot shown in the figure then the chance is very high that this is in fact a random observation.
Based dot likely observation fluctuation 95 outcomes.	 Based on the picture we see that if a difference is observed at the dot shown in the figure then the chance is very high that this is in fact a random observation  We can define the region of likely observation due to random fluctuation we usually use the value 95 of all outcomes.
We define region observation random outcomes.	 We can define the region of likely observation due to random fluctuation we usually use the value 95 of all outcomes.
tails difference fluctuation 5	 If you observe a value in the tails on the side then the difference is unlikely from random fluctuation only 5 likely.
determines lines confident believing 1 chance fluctuations farther exact significance	 This 95 value determines where the lines are drawn on the x axis  If we are only confident in believing a 1 chance is due to random fluctuations then the vertical lines are redrawn farther from the mean determining the exact x values where the lines are drawn depends on the specific significance test used.
If confident 1 random fluctuations vertical lines x values lines depends specific significance need use incorrect better another.	 If we are only confident in believing a 1 chance is due to random fluctuations then the vertical lines are redrawn farther from the mean determining the exact x values where the lines are drawn depends on the specific significance test used  The takeaway message here is that we need to use many queries to avoid jumping to an incorrect conclusion that one system is better than another.
ways statistical significance test deter lines random chance	 There are many different ways of doing this statistical significance test which is essentially deter mining where to place the boundary lines between random chance and an actual difference in systems.
let’s discuss problem relevance	 Now let’s discuss the problem of making relevance judgements.
The can’t documents cases choose ranking types systems.	 The question is if we can’t afford judging all the documents in the collection which subset should we judge The solution here is pooling  This is a strategy that has been used in many cases to solve this problem  First choose a diverse set of ranking methods these are different types of retrieval systems.
This strategy set	 This is a strategy that has been used in many cases to solve this problem  First choose a diverse set of ranking methods these are different types of retrieval systems.
set methods help nominate relevant documents.	 First choose a diverse set of ranking methods these are different types of retrieval systems  We hope these methods can help us nominate likely relevant documents.
The goal documents That return ranking function value systems point suggest documents.	 The goal is to pick out the relevant documents so the users can make judgements on them  That way we would have each system return the top k documents according to its ranking function  The k value can vary between systems but the point is to ask them to suggest the most likely relevant documents.
That k according ranking The vary suggest relevant We combine sets assessors judge.	 That way we would have each system return the top k documents according to its ranking function  The k value can vary between systems but the point is to ask them to suggest the most likely relevant documents  We then simply combine all these top k sets to form a pool of documents for human assessors to judge.
The k vary suggest likely relevant simply Of duplicated systems retrieved documents.	 The k value can vary between systems but the point is to ask them to suggest the most likely relevant documents  We then simply combine all these top k sets to form a pool of documents for human assessors to judge  Of course there will be many duplicated documents since many systems might have retrieved the same documents.
systems documents.	 Of course there will be many duplicated documents since many systems might have retrieved the same documents.
We include human data	 We can include as many possible random documents as possible  Then the human assessors would make complete judgements on this data set or pool.
remaining unjudged documents annotators need time effort them.	 The remaining unjudged documents are assumed to be nonrelevant and the human annotators do not need to spend time and effort manually judging them.
If large participates pool it’s unlikely topranked documents However evaluating contributed documents assumed	 If the pool is large enough this assumption is perfectly fine  That means if your system participates in contributing to the pool then it’s unlikely that it will be penalized since the topranked documents have all been judged  However this is problematic for evaluating a new system that may not have contributed to the pool since the documents it returns may not have been judged and are assumed to be nonrelevant.
That means contributing unlikely	 That means if your system participates in contributing to the pool then it’s unlikely that it will be penalized since the topranked documents have all been judged.
result method judge documents engine application documents method clicked	 Of course the users don’t see which result is from which method so the users would judge those results or click on those documents in a search engine application  In this case then the system can keep track of the clicked documents and see if one method has contributed more to the clicked documents.
In case method clicked documents tends click results method better retrieval interfaces.	 In this case then the system can keep track of the clicked documents and see if one method has contributed more to the clicked documents  If the user tends to click on one of the results from one method then that method may be better  AB testing can also be used to compare two different retrieval interfaces.
If user tends method better testing interfaces Text retrieval	 If the user tends to click on one of the results from one method then that method may be better  AB testing can also be used to compare two different retrieval interfaces  Text retrieval evaluation is extremely important since the task is empirically defined.
testing	 AB testing can also be used to compare two different retrieval interfaces.
users there’s tell	 If we don’t rely on users there’s no way to tell whether one method works better.
conclusions strategy Cranfield kinds	 If we have an inappropriate experiment design we might misguide our research or applications drawing the wrong conclusions  The main strategy is the Cranfield evaluation methodology for all kinds of empirical evaluation tasks not just for search engines.
The evaluation methodology empirical search engines.	 The main strategy is the Cranfield evaluation methodology for all kinds of empirical evaluation tasks not just for search engines.
Bibliographic Reading information AI evaluation meth odology established early information important early book Information Harman 2011 ex comprehensive topic particularly providing cal methodology uation	 Bibliographic Notes and Further Reading Evaluation has always been an important research problem in information re trieval and in empirical AI problems in general  The Cranfield evaluation meth odology was established in 1960s by early pioneers of information retrieval re searchers important early papers on the topic can be found in Sparck Jones and Willett 1997  The book Information Retrieval Evaluation by Harman 2011 is an ex cellent comprehensive introduction to this topic particularly in providing a histori cal view of the development of the IR evaluation methodology and initiatives of eval uation such as TREC.
The Retrieval Harman 2011 introduction topic cal view IR eval TREC test 2010 useful work The teractive IR excellent introduction interactive studies	 The book Information Retrieval Evaluation by Harman 2011 is an ex cellent comprehensive introduction to this topic particularly in providing a histori cal view of the development of the IR evaluation methodology and initiatives of eval uation such as TREC  Sanderson’s book on test collection evaluation Sanderson 2010 is another very useful survey of research work on evaluation  The book on in teractive IR evaluation by Kelly is yet another excellent introduction to interactive IR evaluation via user studies Kelly 2009.
evaluation Sanderson useful interactive	 Sanderson’s book on test collection evaluation Sanderson 2010 is another very useful survey of research work on evaluation  The book on in teractive IR evaluation by Kelly is yet another excellent introduction to interactive IR evaluation via user studies Kelly 2009.
The book teractive evaluation introduction 2009.	 The book on in teractive IR evaluation by Kelly is yet another excellent introduction to interactive IR evaluation via user studies Kelly 2009.
10Web In chapter retrieval web search	 10Web Search In this chapter we discuss one of the most important applications of text retrieval web search engines.
scalability handle completeness coverage information How serve answering Before search focused libraries serious.	 First this is a scalability challenge  How can we handle the size of the web and ensure completeness of coverage of all its information be it textual or not How can we serve many users quickly by answering all their queries Before the web was born the scale of search was relatively small usually focused on libraries so these questions were not serious.
attempt heighten particular rank scored g.	 Search engine optimization is the attempt to heighten a particular page’s rank by taking advantage of how pages are scored e g.
words relevant actual content creating fake particular popular approaches detect spamming Spirin Han	g  adding many words that are not necessarily relevant to the actual content or creating many fake links to a particular page to make it seem more popular than it really is  Many different approaches have been designed to detect and prevent such spamming practices Spirin and Han 2012.
Many dynamic	 Many different approaches have been designed to detect and prevent such spamming practices Spirin and Han 2012  The third challenge is the dynamic nature of the web.
challenge web New pages created	 The third challenge is the dynamic nature of the web  New pages are constantly created and updated very quickly  This makes it harder to keep the index fresh with the most recent content.
New This recent These order high quality search	 New pages are constantly created and updated very quickly  This makes it harder to keep the index fresh with the most recent content  These are just some of the challenges that we have to solve in order to build a high quality web search engine.
This makes fresh recent build high web Despite leverage improve search	 This makes it harder to keep the index fresh with the most recent content  These are just some of the challenges that we have to solve in order to build a high quality web search engine  Despite these challenges there are also some interesting opportunities that we can leverage to improve search results.
high quality web	 These are just some of the challenges that we have to solve in order to build a high quality web search engine.
Despite challenges interesting opportunities leverage search results example imagine scoring.	 Despite these challenges there are also some interesting opportunities that we can leverage to improve search results  For example we can imagine that using links between pages can improve scoring.
opportunities web search.	 Due to these challenges and opportunities there are new techniques that have been developed specifically for web search.
techniques addressing spam spam pages ranked high There achieve robust	 There are also techniques that have been developed for addressing the spam problem  We’ll have to prevent those spam pages from being ranked high  There are also techniques to achieve robust ranking in the light of search engine optimizers.
pages ranked There engine We’re going use variety easy spam	 We’ll have to prevent those spam pages from being ranked high  There are also techniques to achieve robust ranking in the light of search engine optimizers  We’re going to use a wide variety of signals to rank pages so that it’s not easy to spam the search engine with one particular trick.
light search engine signals it’s easy particular link techniques allow leveraging information networked web.	 There are also techniques to achieve robust ranking in the light of search engine optimizers  We’re going to use a wide variety of signals to rank pages so that it’s not easy to spam the search engine with one particular trick  The third line of techniques is link analysis these are techniques that can allow us to improve search results by leveraging extra information about the networked nature of the web.
We’re variety signals rank it’s spam engine	 We’re going to use a wide variety of signals to rank pages so that it’s not easy to spam the search engine with one particular trick.
line techniques allow search nature use ranking—not link analysis kinds features layout web link	 The third line of techniques is link analysis these are techniques that can allow us to improve search results by leveraging extra information about the networked nature of the web  Of course we will use multiple features for ranking—not just link analysis  We can also exploit all kinds of features like the layout of web pages or anchor text that describes a link to another page.
multiple link analysis We kinds features pages text describes	 Of course we will use multiple features for ranking—not just link analysis  We can also exploit all kinds of features like the layout of web pages or anchor text that describes a link to another page.
exploit kinds web anchor text page The search	 We can also exploit all kinds of features like the layout of web pages or anchor text that describes a link to another page  The first component of a web search engine is the crawler.
component crawler This web content wish second downloaded pages create	 The first component of a web search engine is the crawler  This is a program that downloads web page content that we wish to search  The second component is the indexer which will take these downloaded pages and create an inverted index.
second component indexer downloaded pages index component The browser search allow	 The second component is the indexer which will take these downloaded pages and create an inverted index  The third component is retrieval which answers a user’s query by talking to the user’s browser  The browser will show the search results and allow the user to interact with the web.
retrieval talking	 The third component is retrieval which answers a user’s query by talking to the user’s browser.
The search allow interact web interactions user allow discussed 7 discussed Chapter	 The browser will show the search results and allow the user to interact with the web  These interactions with the user allow opportunities for feedback discussed in Chapter 7 and evaluation discussed in Chapter 9.
These user allow 7 evaluation discussed Chapter	 These interactions with the user allow opportunities for feedback discussed in Chapter 7 and evaluation discussed in Chapter 9.
section	 In the next section we will discuss crawling  We’ve already described all indexing steps except crawling in detail in Chapter 8.
We’ve	 We’ve already described all indexing steps except crawling in detail in Chapter 8.
crawling discussion challenges indexing.	 After our crawling discussion we move onto the particular challenges of web indexing.
technique learning rank features ranking.	 The last technique we discuss is learning to rank which is a way to combine many different features for ranking.
Web Crawling robot parses web.	 10 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.
Web robot crawls traverses downloads pages need pages pages pages’ queue page’s	1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links  We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied.
Building toy crawler seed fetch web new We explore page’s Building real tricky	 Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links  We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied  Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with.
One robustness server there’s trap dynamically pages attract circles issue don’t overload server crawling	 One issue is robustness What if the server doesn’t respond or returns unparseable garbage What if there’s a trap that generates dynamically generated pages that attract your crawler to keep crawling the same site in circles Yet another issue is that we don’t want to overload one particular server with too many crawling requests.
A called robots site paths	 A file called robots txt at the root of the site tells crawlers which paths they are not allowed to crawl.
going topic pages automobiles.	 Here we’re going to crawl some pages about a particular topic e g  all pages about automobiles.
automobiles.	 all pages about automobiles.
typically start results crawl more.	 This is typically going to start with a query that you use to get some results  Then you gradually crawl more.
gradually crawl An version forum forum In case	 Then you gradually crawl more  An even more extreme version of focused crawling is for example downloading and indexing all forum posts on a particular forum  In this case we might have a URL such as httpwww.
refers post	comboardsid3 which refers to the third post on the forum.
it’s important delay requests server challenge	 In this scenario it’s especially important to add a delay between requests so that the server is not overwhelmed  Another challenge in crawling is to find new pages that have been created since the crawler last ran.
challenging new linked old page.	 This is very challenging if the new pages have not been linked to any old page.
able create web search	 Let’s say you want to be able to create a web search engine.
Clearly data web.	 Clearly you first crawl data from the web.
need updated	 In the future we just need to crawl the updated pages.
This determine page needs recrawled page created consider page updated	 This is a very interesting research question how can we determine when a page needs to be recrawled or even when a new page has been created There are two major factors to consider here the first of which is whether a particular page would be updated frequently.
hasn’t probably frequently hand it’s example gets updated multiple times day.	 If the page is a static page that hasn’t been changed for months it’s probably not necessary to recrawl it every day since it’s unlikely that it will be changed frequently  On the other hand if it’s for example a sports score page that gets updated very frequently you may need to recrawl even multiple times on the same day.
On it’s score page updated frequently times day.	 On the other hand if it’s for example a sports score page that gets updated very frequently you may need to recrawl even multiple times on the same day.
The second factor accessed users If it’s important fresh it’s it’s maintain freshness.	 The second factor to consider is how frequently a particular page is accessed by users of the search engine system  If it’s a highutility page it’s more important to ensure it is fresh  Compare it with another page that has never been fetched by any users for a year even though that unpopular page has been changed a lot it’s probably not necessary to crawl that page—or at least it’s not as urgent—to maintain its freshness.
important fresh fetched unpopular page changed it’s crawl urgent—to freshness.	 If it’s a highutility page it’s more important to ensure it is fresh  Compare it with another page that has never been fetched by any users for a year even though that unpopular page has been changed a lot it’s probably not necessary to crawl that page—or at least it’s not as urgent—to maintain its freshness.
users year page lot it’s it’s maintain 10.	 Compare it with another page that has never been fetched by any users for a year even though that unpopular page has been changed a lot it’s probably not necessary to crawl that page—or at least it’s not as urgent—to maintain its freshness  10.
10 Web In section create index After crawler gigabytes terabytes step indexer	 10 2 Web Indexing In this section we will discuss how to create a webscale index  After our crawler delivers gigabytes or terabytes of data the next step is to use the indexer to create the inverted index.
2 Indexing create index.	2 Web Indexing In this section we will discuss how to create a webscale index.
gigabytes step indexer create	 After our crawler delivers gigabytes or terabytes of data the next step is to use the indexer to create the inverted index.
	 The two main challenges are scalability and efficiency.
Also data quickly address challenges Google innovations file help manage machines.	 Also because the data is so large it’s beneficial to process the data in parallel so that we can produce the index quickly  To address these challenges Google has made a number of innovations  One is the Google File System which is a general distributed file system that can help programmers manage files stored on a cluster of machines.
To Google	 To address these challenges Google has made a number of innovations.
Google distributed file programmers stored machines The second general computation.	 One is the Google File System which is a general distributed file system that can help programmers manage files stored on a cluster of machines  The second is MapReduce which is a general software framework for supporting parallel computation.
known open implementation applications.	 Hadoop is the most well known open source implementation of MapReduce now used in many applications.
uses simple manage specific file lookup table know exactly actually	 It uses a very simple centralized management mechanism to manage all the specific locations of files  That is it maintains a file namespace and lookup table to know where exactly each file is actually stored.
That maintains file lookup know file stored.	 That is it maintains a file namespace and lookup table to know where exactly each file is actually stored.
application client talks GFS master obtains specific stores files machines chunks 64 chunks.	 The application client talks to the GFS master node which obtains specific locations of the files to process  This filesystem stores its files on machines in fixedsize chunks each data file is separated into many 64 MB chunks.
data separated 64 MB These chunks ensure reliability All details worry	 This filesystem stores its files on machines in fixedsize chunks each data file is separated into many 64 MB chunks  These chunks are replicated to ensure reliability  All of these details are something that the programmer doesn’t have to worry about and it’s all taken care of by this filesystem.
These ensure programmer doesn’t worry it’s taken application programmer normal	 These chunks are replicated to ensure reliability  All of these details are something that the programmer doesn’t have to worry about and it’s all taken care of by this filesystem  From the application perspective the programmer would see a normal file.
The program know stored process file feature transfer directly application chunk efficient	 The program doesn’t have to know where exactly it’s stored and can just invoke high level operators to process the file  Another feature is that the data transfer is directly between application and chunk servers so it’s efficient in this sense as well.
Another data chunk efficient On Google MapReduce par programming.	 Another feature is that the data transfer is directly between application and chunk servers so it’s efficient in this sense as well  On top of the GFS Google proposed MapReduce as a general framework for par allel programming.
Google MapReduce general allel This supports tasks building framework	 On top of the GFS Google proposed MapReduce as a general framework for par allel programming  This supports tasks like building an inverted index  Like GFS this framework hides low level features from the programmer.
like inverted framework low level pro grammer minimum effort cluster	 This supports tasks like building an inverted index  Like GFS this framework hides low level features from the programmer  As a result the pro grammer can make minimum effort to create an application that can be run on a large cluster in parallel.
details framework communications	 Some of the lowlevel details hidden in the framework are communications load balancing and task execution.
MapReduce dispatch Again this.	 Here the MapReduce mechanism would know that the task has not been completed and would automat ically dispatch the task on other servers that can do the job  Again the programmer doesn’t have to worry about this.
In number key	 In MapReduce the input data are separated into a number of key value pairs.
What exactly depend	 What exactly the value is will depend on the data.
Each function programmer writes map value pairs	 Each pair will be then sent to a map function which the programmer writes  The map function will then process these key value pairs and generate a number of other key value pairs.
different key All outputs result grouped	 Of course the new key is usually different from the old key that’s given to map as input  All the outputs of all the calls to map are collected and sorted based on the key  The result is that all the values that are associated with the same key will be grouped together.
All outputs calls based key The result values grouped together.	 All the outputs of all the calls to map are collected and sorted based on the key  The result is that all the values that are associated with the same key will be grouped together.
For unique values This data reduce function.	 For each unique key we now have a set of values that are attached to this key  This is the data that is sent to the reduce function.
key.	 Each reduce instance will handle a different key.
This input produce value framework programmer	 This function processes its input which is a key and a set of values to produce another set of key  value pairs as the output  This is the general framework of MapReduce  Now the programmer only needs to write the map function and the reduce function.
This general Now programmer needs write function reduce function Everything taken MapReduce framework.	 This is the general framework of MapReduce  Now the programmer only needs to write the map function and the reduce function  Everything else is taken care of by the MapReduce framework.
Now map function taken MapReduce framework With input processed map processed parallel	 Now the programmer only needs to write the map function and the reduce function  Everything else is taken care of by the MapReduce framework  With such a framework the input data can be partitioned into multiple parts which are processed in parallel first by map and then processed again in parallel once we reach the reduce stage.
taken MapReduce With framework data parts map Figure	 Everything else is taken care of by the MapReduce framework  With such a framework the input data can be partitioned into multiple parts which are processed in parallel first by map and then processed again in parallel once we reach the reduce stage  Figure 10.
shows	3 shows an example of word counting.
containing generate occur kind counting assess popularity achieving search So solve counting parts file parallel	 The input is files containing tokenized words and the output that we want to generate is the number of occur rences of each word  This kind of counting would be useful to assess the popularity of a word in a large collection or achieving an effect of IDF weighting for search  So how can we solve this problem One natural thought is that this task can be done in parallel by simply counting different parts of the file in parallel and combining all the counts.
counting assess popularity achieving IDF weighting One thought task file	 This kind of counting would be useful to assess the popularity of a word in a large collection or achieving an effect of IDF weighting for search  So how can we solve this problem One natural thought is that this task can be done in parallel by simply counting different parts of the file in parallel and combining all the counts.
That’s	 That’s precisely the idea of what we can do with MapReduce we can parallelize lines in this input file.
assume input map function pair number The 1 World sent amap function words	 More specifically we can assume the input to each map function is a key  value pair that represents the line number and the string on that line  The first line is the pair 1 Hello World Bye World  This pair will be sent to amap function that counts the words in this line.
map pseudocode simple simply needs iterate line Collect collector collector value map	 The map pseudocode shown at the bottom of the figure is quite simple  It simply needs to iterate over all the words in this line and then just call a Collect function which means it would then send the word and the counter to the collector  The collector would then try to sort all these key value pairs from different map functions.
It simply needs words line Collect function means word sort key pairs different functions The specifies way process	 It simply needs to iterate over all the words in this line and then just call a Collect function which means it would then send the word and the counter to the collector  The collector would then try to sort all these key value pairs from different map functions  The programmer specifies this function as a way to process each part of the data.
The sort key different	 The collector would then try to sort all these key value pairs from different map functions.
The specifies function way	 The programmer specifies this function as a way to process each part of the data.
course second line different instance map output.	 Of course the second line will be handled by a different instance of the map function which will produce a similar output.
At stage collected multiple Each word line pairs sort word.	 At this stage you can see we have collected multiple pairs  Each pair is a word and its count in the line  Once we see all these pairs then we can sort them based on the key which is the word.
Each pair based key word.	 Each pair is a word and its count in the line  Once we see all these pairs then we can sort them based on the key which is the word.
key	 Once we see all these pairs then we can sort them based on the key which is the word.
word number	 Each word now is attached to a number of values i e  a number of counts.
number counts These word different lines These new value pairs function.	 a number of counts  These counts represent the occurrences of this word in different lines  These new key  value pairs will then be fed into a reduce function.
represent word different These key function.	 These counts represent the occurrences of this word in different lines  These new key  value pairs will then be fed into a reduce function.
These key reduce Figure 4 shows function finishes counting	 These new key  value pairs will then be fed into a reduce function  Figure 10 4 shows how the reduce function finishes the job of counting the total occurrences of this word.
Figure	 Figure 10.
total precisely want	 Finally we output the key and the total count which is precisely what we want as the output of this whole program.
document document We parallel modify example inverted	 What’s missing is the document IDs and the specific frequency counts of words in each particular document  We can modify this slightly to actually build an inverted index in parallel  Let’s modify our wordcounting example to create an inverted index.
We slightly index modify example create index.	 We can modify this slightly to actually build an inverted index in parallel  Let’s modify our wordcounting example to create an inverted index.
Let’s wordcounting create inverted	 Let’s modify our wordcounting example to create an inverted index  Figure 10 5 illustrates this example.
assume input map function key value pair key document denotes string content document map function previous simply	5 illustrates this example  Now we assume the input to map function is a key  value pair where the key is a document ID and the value denotes the string content of all the words in that document  The map function will do something very similar to what we have seen in the previous example it simply groups all the counts of this word in this document together generating new pairs.
key pair value denotes The function seen previous example simply counts word document pairs In word count word document followed ID.	 Now we assume the input to map function is a key  value pair where the key is a document ID and the value denotes the string content of all the words in that document  The map function will do something very similar to what we have seen in the previous example it simply groups all the counts of this word in this document together generating new pairs  In the new pairs each key is a word and the value is the count of this word in this document followed by the document ID.
The function similar example simply groups counts pairs word value count word	 The map function will do something very similar to what we have seen in the previous example it simply groups all the counts of this word in this document together generating new pairs  In the new pairs each key is a word and the value is the count of this word in this document followed by the document ID.
inverted index keeps	 Later in the inverted index we would like to keep this document ID information so the map function keeps track of it.
After mechanism words feed data function We reduce function’s looks inverted entry word word	 After the map function there is a sorting mechanism that would group the same words together and feed this data into the reduce function  We see the reduce function’s input looks like an inverted index entry  It’s just the word and all the documents that contain the word and the frequency of the word in those documents.
word frequency word	 It’s just the word and all the documents that contain the word and the frequency of the word in those documents.
chunk data	 All we need to do is simply to concatenate them into a continuous chunk of data and this can be then stored on the filesystem  The reduce function is going to do very minimal work.
1 construction Algorithm	1 can be used for this inverted index construction  Algorithm 10.
reduce function concatenates input given ID key construct inverted large scale.	 The reduce function simply concatenates all the input that it has been given and as single entry for this document ID key  Despite its simplicity this MapReduce function allows us to construct an inverted index at a very large scale.
MapReduce index Data processed machines programmer details construction web	 Despite its simplicity this MapReduce function allows us to construct an inverted index at a very large scale  Data can be processed by different machines and the programmer doesn’t have to take care of the details  This is how we can do parallel index construction for web search.
parallel construction web search scale indexing requires techniques techniques.	 This is how we can do parallel index construction for web search  To summarize web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques.
new traditional indexing store index like	 To summarize web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques  Mainly we have to store the index on multiple machines and this is usually done by using a distributed file system like the GFS.
index GFS creating large MapReduce	 Mainly we have to store the index on multiple machines and this is usually done by using a distributed file system like the GFS  Second it requires creating the index in parallel because it’s so large  This is done by using the MapReduce framework.
Second requires creating index large MapReduce	 Second it requires creating the index in parallel because it’s so large  This is done by using the MapReduce framework.
MapReduce It’s MapReduce general support applications aside	 This is done by using the MapReduce framework  It’s important to note that the both the GFS and MapReduce framework are very general so they can also support many other applications aside from indexing  10.
GFS	 It’s important to note that the both the GFS and MapReduce framework are very general so they can also support many other applications aside from indexing.
Link Analysis going continue search links	3 Link Analysis In this section we’re going to continue our discussion of web search particularly focusing on how to utilize links between pages to improve search.
previous section talked create GFS Now index	 In the previous section we talked about how to create a large index on using MapReduce on GFS  Now that we have our index we want to see how we can improve ranking of pages on the web.
improve web.	 Now that we have our index we want to see how we can improve ranking of pages on the web.
Of supporting web search aren’t sufficient reasons.	 Of course standard IR models can be applied here in fact they are important building blocks for supporting web search but they aren’t sufficient for the following reasons.
tend people entry page—this tional people interested types queries navigational navigate targeted	 First on the web we tend to have very different information needs  For example people might search for a web page or entry page—this is different from the tradi tional library search where people are primarily interested in collecting literature information  These types of queries are often called navigational queries where the purpose is to navigate into a particular targeted page.
These types navigate targeted page queries link navigational facebook finance.	 These types of queries are often called navigational queries where the purpose is to navigate into a particular targeted page  For such queries we might benefit from using link information  For example navigational queries could be facebook or yahoo finance.
For information For navigational queries yahoo	 For such queries we might benefit from using link information  For example navigational queries could be facebook or yahoo finance.
For example finance.	 For example navigational queries could be facebook or yahoo finance.
user trying pages explicitly bar	 The user is simply trying to get to those pages without explicitly typing in the URL in the address bar of the browser.
documents information text hierarchical organization page hyper pages document Finally information	 Secondly web documents have much more information than pure text there is hierarchical organization and annotations such as the page layout title or hyper links to other pages  These features provide an opportunity to use extra context in formation of the document to improve scoring  Finally information quality greatly varies.
These provide formation document improve scoring Finally information quality All consider factors standard giving difficult page’s	 These features provide an opportunity to use extra context in formation of the document to improve scoring  Finally information quality greatly varies  All this means we have to consider many factors to improve the standard ranking algorithm giving us a more robust way to rank the pages and making it more difficult for spammers to manipulate one signal to improve a single page’s ranking.
	 Finally information quality greatly varies.
All means factors giving robust way signal improve page’s ranking 3 Link 201 text”summary indicate doc Description link Figure 10.	 All this means we have to consider many factors to improve the standard ranking algorithm giving us a more robust way to rank the pages and making it more difficult for spammers to manipulate one signal to improve a single page’s ranking  10 3 Link Analysis 201 Authority “Extra text”summary for a doc Links indicate the utility of a doc Hub Description “anchor text” What does a link tell us Figure 10.
Link “Extra Links “anchor text” tell useful information result concerns number	3 Link Analysis 201 Authority “Extra text”summary for a doc Links indicate the utility of a doc Hub Description “anchor text” What does a link tell us Figure 10 6 Links provide useful information about pages  As a result of all these concerns researchers have made a number of major extensions to the standard ranking algorithms.
6 useful extensions ranking One links main topic section.	6 Links provide useful information about pages  As a result of all these concerns researchers have made a number of major extensions to the standard ranking algorithms  One is to exploit links to improve scoring which is the main topic of this section.
exploit scoring	 One is to exploit links to improve scoring which is the main topic of this section.
Of feedback techniques machine techniques there.	 Of course that belongs in the category of feedback techniques and machine learning techniques are often used there.
based standard BM25 Chapter 6 information scoring search systems.	 Many of them are based on standard models such as BM25 that we talked about in Chapter 6  Link information is one of the important features used in combined scoring functions in modern web search systems.
Link information features web 10.	 Link information is one of the important features used in combined scoring functions in modern web search systems  Figure 10.
We description right side.	6 shows a snapshot of a part of the web  We can see there are many links that connect different pages and in the center there is a description of a link that’s pointing to the document on the right side.
We center link pointing document	 We can see there are many links that connect different pages and in the center there is a description of a link that’s pointing to the document on the right side.
It actually incredibly useful engines provides extra description	 It is actually incredibly useful for search engines because it provides some extra description of the page being pointed to.
wants Amazon.	 For example if someone wants to bookmark the Amazon.
The query provides pointed to—the entry anchor page evidence	 The query would match this anchor text in the page  This actually provides evidence for matching the page that’s been pointed to—the Amazon entry page  Thus if you match the anchor text that describes the link to a page it provides good evidence for the relevance of the page being pointed to.
Thus anchor text describes 202 Search 6 patterns utility document.	 Thus if you match the anchor text that describes the link to a page it provides good evidence for the relevance of the page being pointed to  202 Chapter 10 Web Search On the bottom of Figure 10 6 there are some patterns of links which may indicate the utility of a document.
10 10 indicate utility	 202 Chapter 10 Web Search On the bottom of Figure 10 6 there are some patterns of links which may indicate the utility of a document.
indicate utility received inlinks	6 there are some patterns of links which may indicate the utility of a document  For example on the right side you can see a page has received many inlinks meaning many other pages are pointing to this page.
right page received inlinks meaning pages page.	 For example on the right side you can see a page has received many inlinks meaning many other pages are pointing to this page.
shows left points pages	 This shows that this page is quite useful  On the left side you can see a page that points to many other pages  This is a central page that would allow you to see many other pages.
On left points	 On the left side you can see a page that points to many other pages.
central page allow pages We case authority second hub page link information text case additional scores web pages characterize page hub authority.	 This is a central page that would allow you to see many other pages  We call the first case an authority page and the second case a hub page  This means the link information can help in two ways one is to provide extra text for matching in the case of anchors and the other is to provide some additional scores for the web pages to characterize how likely a page is a hub or an authority.
authority case hub	 We call the first case an authority page and the second case a hub page.
link information help matching additional web characterize likely hub authority	 This means the link information can help in two ways one is to provide extra text for matching in the case of anchors and the other is to provide some additional scores for the web pages to characterize how likely a page is a hub or an authority  10 3.
3 1 PageRank Google’s PageRank link link	 10 3 1 PageRank Google’s PageRank a main technique that was used originally for link analysis is a good example of leveraging page link information.
3.	3.
PageRank Google’s link link information page authority The intuition links like literature.	1 PageRank Google’s PageRank a main technique that was used originally for link analysis is a good example of leveraging page link information  PageRank captures page popularity which is another word for authority  The intuition is that links are just like citations in literature.
links like citations page page paper paper page cited assume useful.	 The intuition is that links are just like citations in literature  Think about one page pointing to another page this is very similar to one paper citing another paper  Thus if a page is cited often we can assume this page is more useful.
intuition implements approach simplest PageRank inlink It improves	 PageRank takes advantage of this intuition and implements it in a principled approach  In its simplest sense PageRank is essentially doing citation counting or inlink counting  It improves this simple idea in two ways.
In simplest counting improves simple	 In its simplest sense PageRank is essentially doing citation counting or inlink counting  It improves this simple idea in two ways.
It improves idea	 It improves this simple idea in two ways.
	 One is to consider indirect citations.
means look inlinks	 This means you don’t just look at the number of inlinks rather you also look at the inlinks of your inlinks recursively.
If pages	 If your inlinks themselves have many inlinks your page gets credit from that  In short if important pages are pointing to you you must also be important.
This	 This is the concept of indirect citations or cascading citations.
Again understand idea research	 Again we can understand this idea by considering research papers.
Clearly consider The idea it’s good observed Assume citation count.	 Clearly this is a case where we would like to consider indirect links which is exactly what PageRank does  The other idea is that it’s good to smooth the citations to accommodate potential citations that have not yet been observed  Assume that every page has a nonzero pseudo citation count.
Assume page nonzero pseudo count trying link pseudo citations everyone.	 Assume that every page has a nonzero pseudo citation count  Essentially you are trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone.
links link actually	 Essentially you are trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone.
example Figure	 Let’s take a look at this example in detail illustrated in Figure 10 7.
3 edges connecting assume walker	3 Link Analysis 203 page and the edges between documents are hyperlinks connecting them to each other  Let’s assume that a random surfer or random walker can be on any of these pages.
d1 surfer bored If able reach link	 So if the random surfer is at d1 with some probability that random surfer will follow the links to either d3 or d4  The random surfing model also assumes that the surfer might get bored sometimes and decide to ignore the actual links randomly jumping to any page on the web  If the surfer takes that option they would be able to reach any of the other pages even though there is no link directly to that page.
surfer option reach pages link directly page.	 If the surfer takes that option they would be able to reach any of the other pages even though there is no link directly to that page.
model ask likely reach particular page” This computes The PageRank document di proportional	 Based on this model we can ask the question “How likely on average would the surfer reach a particular page” This probability is precisely what PageRank computes  The PageRank score of a document di is the average probability that the surfer visits di  Intuitively this should be proportional to the inlink count.
If page inlinks higher chance link	 Intuitively this should be proportional to the inlink count  If a page has a high number of inlinks then it would have a higher chance of being visited since there will be more opportunities of having the surfer follow a link there.
high higher visited opportunities This random captures idea counting But point reach	 If a page has a high number of inlinks then it would have a higher chance of being visited since there will be more opportunities of having the surfer follow a link there  This is how the random surfing model captures the idea of counting the inlinks  But it also considers the indirect inlinks if the pages that point to di have themselves a lot of inlinks that would mean the random surfer would very likely reach one of them.
This surfing model inlinks.	 This is how the random surfing model captures the idea of counting the inlinks.
considers indirect inlinks lot mean random likely them.	 But it also considers the indirect inlinks if the pages that point to di have themselves a lot of inlinks that would mean the random surfer would very likely reach one of them.
This increases way capture Mathematically represent played center Figure	 This increases the chance of visiting di  This is a nice way to capture both indirect and direct links  Mathematically we can represent this document network as a matrix M  dis played in the center of Figure 10.
This nice way direct links matrix	 This is a nice way to capture both indirect and direct links  Mathematically we can represent this document network as a matrix M  dis played in the center of Figure 10.
Mathematically represent center	 Mathematically we can represent this document network as a matrix M  dis played in the center of Figure 10.
7 Each stands page row probability going d1.	7  Each row stands for a starting page  For example row one would indicate the probability of going to any of the four pages from d1.
row starting For row indicate d1.	 Each row stands for a starting page  For example row one would indicate the probability of going to any of the four pages from d1.
For indicate probability d1.	 For example row one would indicate the probability of going to any of the four pages from d1.
	 We see there are only two nonzero entries.
randomly they’d probability We columns 0 0 0 1 12 matrix d1 d2 12 0 N ∑ j1 Mij Figure 10.	 Each is one half since d1 is pointing to only two other pages thus if we can randomly choose to visit either of them from d1 they’d each have a probability of 1 2   We have zeros for the first two columns for 0 1 0 12 0 0 1 12 Transition matrix Mij  probability of going from di to dj d1 d2 d3 d4 12 0 0 0 M 12 0 0 0 N ∑ j1 Mij  1 Figure 10.
Example corresponding	7 Example of a web graph and the corresponding transition matrix.
204 Chapter 10 Web doesn’t link	 204 Chapter 10 Web Search d1 since d1 doesn’t link to itself and it doesn’t link to d2.
Now probability visiting On probability visiting time	 Now how can we compute the probability of a surfer visiting a particular page On the lefthand side is the probability of visiting page dj at time t  1 the next time count.
captures possibilities reaching dj time 1 random surfing reach random chooses strategy α factor α	 The equation captures the two possibilities of reaching a page dj at time t  1 through random surfing or following a link  The first part of the equation captures the probability that the random surfer would reach this page by following a link  The random surfer chooses this strategy with probability 1 − α thus there is a factor of 1 − α before this term.
surfer strategy 1 α factor 1 term This possible time .	 The random surfer chooses this strategy with probability 1 − α thus there is a factor of 1 − α before this term  This term sums over all the possible N pages that the surfer could have been at time t .
Inside	 Inside the sum is the product of two probabilities.
surfer That’s	 One is the probability that the surfer was at di at time t   That’s ptdi.
That’s dj Mij	 That’s ptdi  The other is the transition probability from di to dj which we know is represented as Mij .
di dj know represented .	 The other is the transition probability from di to dj which we know is represented as Mij .
The second sum difference 1 .	 The second part is a similar sum  The only difference is that now the transition probability is uniform 1 N .
The probability uniform 1 This probability random jumping probability jumping allows PageRank captures transition matrix.	 The only difference is that now the transition probability is uniform 1 N   This part captures the probability of reaching this page through random jumping where α is the probability of random jumping  This also allows us to see why PageRank captures a smoothing of the transition matrix.
allows PageRank smoothing transition transition matrix 1	 This also allows us to see why PageRank captures a smoothing of the transition matrix  You can think this 1 N comes from another transition matrix that has all the elements as 1 N .
clear merge parts.	 It is then clear that we can merge the two parts.
matrix combination I sense ensure there’s transition matrix average satisfy index.	 Because they are of the same form we can imagine there’s a different matrix that’s a combination of this M and the uniform matrix I   In this sense PageRank uses this idea of smoothing to ensure that there’s no 0 entry in the transition matrix  Now we can imagine that if we want to compute average probabilities they would satisfy this equation without considering the time index.
PageRank uses ensure there’s 0 matrix.	 In this sense PageRank uses this idea of smoothing to ensure that there’s no 0 entry in the transition matrix.
imagine want compute considering	 Now we can imagine that if we want to compute average probabilities they would satisfy this equation without considering the time index.
precisely N variables.	 Similarly there are also precisely N variables  This means we now have a system of N linear equations with N variables.
This N problem boils p again.	 This means we now have a system of N linear equations with N variables  The problem boils down to solving this system of equations which we can write in the following form The vector p equals the transpose of a matrix multiplied by p again.
matrix fact matrix linear equation equa solved iterative	 The trans posed matrix is in fact the sum from 1 to N written in matrix form  Recall from linear algebra that this is precisely the equation for an eigenvector  Thus this equa tion can be solved by using an iterative algorithm.
equa solved algorithm.	 Thus this equa tion can be solved by using an iterative algorithm.
called power p.	 In this iterative algorithm called power iteration we simply start with a random p.
transposed concrete	 We then repeatedly update p by multiplying the transposed matrix expression by p  Let’s look at a concrete example set α  0.
Let’s look 0.	 Let’s look at a concrete example set α  0.
	2.
means chance randomly jumping page entire web following We matrix transition jumping.	 This means that there is a 20 chance of randomly jumping to a page on the entire web and an 80 chance of randomly following a link from the current page  We have the original transition matrix M as before that encodes the actual links in the graph  Then we have this uniform smoothing transition matrix I representing random jumping.
original transition M graph transition I random jumping.	 We have the original transition matrix M as before that encodes the actual links in the graph  Then we have this uniform smoothing transition matrix I representing random jumping.
	 Then we have this uniform smoothing transition matrix I representing random jumping.
We form A The PageRank algorithm randomly p If rewrite matrix multiplication A want compute d1 multiply A scores	 We combine them together with interpolation via α to form another matrix we call A The PageRank algorithm will randomly initialize p first and then iteratively update it by using matrix multiplication  If we rewrite this matrix multiplication in terms of just A we’ll get the following If you want to compute the updated value for d1 you multiply the top row in A by the column vector of PageRank scores from the previous iteration.
matrix following value row column scores This vector started values iteratively multiply scores repeat converge.	 If we rewrite this matrix multiplication in terms of just A we’ll get the following If you want to compute the updated value for d1 you multiply the top row in A by the column vector of PageRank scores from the previous iteration  This is how we update the vector we started with some initial values and iteratively multiply the matrices together which generates a new set of scores  We repeat this multiplication until the values in p converge.
We multiplication converge.	 We repeat this multiplication until the values in p converge.
That pages page propagated document documents transfers network.	 That is we’ll look at all the pages that are pointing to a page and combine their scores with the propagated score in order to get the next score for the current document  We repeat this for all documents which transfers probability mass across the network.
In PageRank efficient matrices link page	 In practice the calculation of the PageRank score is actually quite efficient because the matrices are sparse—that means that if there isn’t a link into the current page we don’t have to worry about it in the calculation.
address zero	 The normalization is to address the potential problem of zero outlinks.
extensions PageRank called PageRank.	 There are many extensions to PageRank  One extension is to do queryspecific PageRank also called Personalized PageRank.
queryspecific Personalized	 One extension is to do queryspecific PageRank also called Personalized PageRank.
Instead assume random jumping jump By align	 Instead they jump to only those pages that are relevant to the query  For example if the query is about sports then we could assume that when we do random jumping we randomly jump to a sports page  By doing this our PageRank scores align with sports.
query jumping scores	 For example if the query is about sports then we could assume that when we do random jumping we randomly jump to a sports page  By doing this our PageRank scores align with sports.
By scores Therefore current query sports use PageRank score rank results.	 By doing this our PageRank scores align with sports  Therefore if you know the current query is about sports we can use this specialized PageRank score to rank the results.
algorithm particularly imagine compute person’s PageRank you’ll meaningful scores people	 PageRank is a general algorithm that can be used in many other applications such as network analysis particularly in social networks  We can imagine if you compute a person’s PageRank score on a social network where a link indicates a friendship relation you’ll get some meaningful scores for people  10.
	 10 3.
	3.
There algorithm HITS scores	 There is another algorithm we will discuss called HITS that is designed to compute both these scores for each page.
good contain collection links HITS algorithm help scoring hubs	 Hub pages are those that point to good authority pages or contain some collection of knowledge in the form of links  The main idea of the HITS algorithm is a reinforcement mechanism to help improve the scoring for both hubs and authorities.
main HITS algorithm reinforcement hubs assume authorities hubs That means you’re score.	 The main idea of the HITS algorithm is a reinforcement mechanism to help improve the scoring for both hubs and authorities  It will assume that good authorities are cited by good hubs  That means if you’re cited by many pages with good hub scores then that increases your authority score.
It good hubs cited pages hub authority good hubs	 It will assume that good authorities are cited by good hubs  That means if you’re cited by many pages with good hub scores then that increases your authority score  Similarly good hubs are those that point to good authorities.
That cited pages scores authorities.	 That means if you’re cited by many pages with good hub scores then that increases your authority score  Similarly good hubs are those that point to good authorities.
good point good authority pages score increased.	 Similarly good hubs are those that point to good authorities  So if you are pointing to many good authority pages then your hub score would be increased.
So good pages PageRank graph network Briefly	 So if you are pointing to many good authority pages then your hub score would be increased  Like PageRank HITS is also quite general and has many applications in graph and network analysis  Briefly we’ll describe how it works.
	 Figure 10.
shows matrix A contains position Aij links dj zero	8 shows the following  First we construct the adjacency matrix A it contains a 1 at position Aij if di links to dj and a zero otherwise.
define hub authority scores pages points second define sum	 We define the hub score of a page hdi as a sum of the authority scores of all the pages that it points to  In the second equation we define the authority score of a page adi as a sum of the hub scores of all pages that point to it.
authority score sum hub pages point This reinforcement mechanism written matrix form.	 In the second equation we define the authority score of a page adi as a sum of the hub scores of all pages that point to it  This forms an iterative reinforcement mechanism  These two equations can be also written in matrix form.
written hub vector equal product adjacency matrix vector.	 These two equations can be also written in matrix form  The hub vector is equal to the product of the adjacency matrix and the authority vector.
hub equal matrix	 The hub vector is equal to the product of the adjacency matrix and the authority vector.
Similarly second equation authority product AT	 Similarly the second equation can be written as the authority vector is equal to the product of AT multiplied by the hub vector.
What’s look matrix plug authority one.	 What’s interesting is that if you look at the matrix forms you can plug the authority equation into the first one.
actually eliminate authority vector	 That is you can actually eliminate the authority vector completely and you get the equation of only hub scores.
hub formula problem eliminate obtain equation	 We can do the same trick for the hub formula  Thus although we framed the problem as computing hubs and authorities we can actually eliminate one of them to obtain the equation for the other.
hubs obtain other.	 Thus although we framed the problem as computing hubs and authorities we can actually eliminate one of them to obtain the equation for the other.
similar problem.	 Mathematically then we would be computing a very similar problem.
We need normalize adjacency matrix iteration allow growth	 We still need to normalize the adjacency matrix after each iteration  This would allow us to control the growth of the values otherwise they would grow larger and larger.
allow control growth values larger larger.	 This would allow us to control the growth of the values otherwise they would grow larger and larger.
To section useful In particular text representation HITS algorithms major analysis search.	 To summarize this section has shown that link information is very useful  In particular the anchor text is an important feature in the text representation of a page  We also talked about the PageRank and HITS algorithms as two major link analysis algorithms in web search.
text important feature representation	 In particular the anchor text is an important feature in the text representation of a page.
Both generate standard ranking	 Both can generate scores for pages that can be used in addition to standard IR ranking functions.
PageRank useful variants graphs networks Rank section discuss learning different fea	 PageRank and HITS are very general algorithms with useful variants so they have many applications in analyzing other graphs or networks aside from the web  10 4 Learning to Rank In this section we discuss using machine learning to combine many different fea tures into a single ranking function to optimize search results.
10 In machine different fea optimize we’ve discussed rank documents.	 10 4 Learning to Rank In this section we discuss using machine learning to combine many different fea tures into a single ranking function to optimize search results  Previously we’ve discussed a number of ways to rank documents.
Rank discuss learning different optimize results.	4 Learning to Rank In this section we discuss using machine learning to combine many different fea tures into a single ranking function to optimize search results.
rank	 Previously we’ve discussed a number of ways to rank documents.
We talked BM25 query matching text talked ap PageRank help The combine features ranking This ranking improve accuracy improve robustness function that’s features	 We talked about some retrieval models like BM25 or query likelihood these can generate a contentbased score for matching document text with a query  We also talked about the linkbased ap proaches like PageRank that can give additional scores to help us improve ranking  The question now is how can we combine all these features and potentially many other features to do ranking This will be very useful for ranking web pages not only just to improve accuracy but also to improve the robustness of the ranking function so that’s it not easy for a spammer to just perturb one or a few features to promote a page.
We linkbased proaches like scores The question combine features potentially accuracy improve ranking perturb	 We also talked about the linkbased ap proaches like PageRank that can give additional scores to help us improve ranking  The question now is how can we combine all these features and potentially many other features to do ranking This will be very useful for ranking web pages not only just to improve accuracy but also to improve the robustness of the ranking function so that’s it not easy for a spammer to just perturb one or a few features to promote a page.
The potentially This useful improve robustness function easy page.	 The question now is how can we combine all these features and potentially many other features to do ranking This will be very useful for ranking web pages not only just to improve accuracy but also to improve the robustness of the ranking function so that’s it not easy for a spammer to just perturb one or a few features to promote a page.
The general learning rank machine features optimizing different features We assume given querydocument d number	 The general idea of learning to rank is to use machine learning to combine these features optimizing the weight on different features to generate the best ranking function  We assume that given a querydocument pair q  d we can define a number of features.
These don’t necessarily based features score respect according retrieval likelihood normalization PL2	 These features don’t necessarily have to be content based features  They could be a score of the document with respect to the query according to a retrieval function such as BM25 query likelihood pivoted length normalization PL2 etc.
There linkbased score like application models	 There also can be a linkbased score like PageRank or HITS or an application of retrieval models to the anchor text of the page which are the descriptions of links that point to d.
This Whether assumption Naturally question	 This is of course just an assumption  Whether this assumption really makes sense is still an open question  Naturally the next question is how to estimate those parameters.
assumption estimate parameters.	 Whether this assumption really makes sense is still an open question  Naturally the next question is how to estimate those parameters.
Naturally parameters know features features low weight use	 Naturally the next question is how to estimate those parameters  How do we know which features should have high weight and which features should have low weight This is a task of training or learning  In this approach we use training data.
high features weight	 How do we know which features should have high weight and which features should have low weight This is a task of training or learning.
In use This data judged users know relevance judgments know highly ranked judgments clickthrough information Chapter	 In this approach we use training data  This is data that have been judged by users so we already know the relevance judgments  We know which documents should be highly ranked for which queries and this information can be based on real judgments by users or can be approximated by just using clickthrough information as we discussed in Chapter 7.
highly information real 7 optimize search engine’s accuracy e.	 We know which documents should be highly ranked for which queries and this information can be based on real judgments by users or can be approximated by just using clickthrough information as we discussed in Chapter 7  We will try to optimize our search engine’s retrieval accuracy using e.
try engine’s g.	 We will try to optimize our search engine’s retrieval accuracy using e g.
g.	g.
training parameters The	 MAP or NDCG on the training data by adjusting these parameters  The training data would look like a table of tuples.
look elements judgment.	 The training data would look like a table of tuples  Each tuple has three elements the query the document and the judgment.
elements query document specific that’s regression This different actually simpler	 Each tuple has three elements the query the document and the judgment  Let’s take a look at a specific method that’s based on logistic regression This is one of many different methods and actually one of the simpler ones.
Let’s based different ones In simply combination ith feature value like.	 Let’s take a look at a specific method that’s based on logistic regression This is one of many different methods and actually one of the simpler ones  In this approach we simply assume the relevance of a document with respect to the query is related to a linear combination of all the features  Here we have Xi to denote the ith feature value and we can have as many features as we would like.
assume relevance respect linear combination Xi ith feature features	 In this approach we simply assume the relevance of a document with respect to the query is related to a linear combination of all the features  Here we have Xi to denote the ith feature value and we can have as many features as we would like.
weight βi.	 The weight of feature Xi is controlled by a parameter βi.
βi function.	 A larger βi would mean the feature would have a higher weight and it would contribute more to the scoring function.
function following vance We know probability relevance range 0 1 scoring function	 The specific form of the function also gives the following probability of rele vance We know that the probability of relevance is within the range 0 1 and we assume that the scoring function is a transformed form of the linear combination of features.
regression instead combination allows connect relevance 0 If rewrite combination probability function score.	 Thus the reason why we use the logistic regression instead of linear re gression is to map this combination onto the range 0 1  This allows us to connect the probability of relevance which is between 0 and 1 to a linear combination of arbitrary coefficients  If we rewrite this combination of weights into a probability function we will get the predicted score.
connect 1 linear arbitrary weights probability predicted score weights high value document	 This allows us to connect the probability of relevance which is between 0 and 1 to a linear combination of arbitrary coefficients  If we rewrite this combination of weights into a probability function we will get the predicted score  If this combination of features and weights gives us a high value then the document is more likely relevant.
If score weights value document hypothesis way features relevance.	 If we rewrite this combination of weights into a probability function we will get the predicted score  If this combination of features and weights gives us a high value then the document is more likely relevant  This isn’t necessarily the best hypothesis but it is a simple way to connect these features with the probability of relevance.
combination features weights gives value document	 If this combination of features and weights gives us a high value then the document is more likely relevant.
This necessarily best way connect relevance estimate parameters truly applied look example shown 10.	 This isn’t necessarily the best hypothesis but it is a simple way to connect these features with the probability of relevance  The next task is to see how we estimate the parameters so that the function can truly be applied that is we need to estimate the β values  Let’s take a look at a simple example shown in Figure 10.
	9.
example	 In this example we have three features.
We PageR query Lastly score text	 We might also have a topicsensitive PageR ank score that would depend on the query  Lastly we have a BM25 score on the anchor text of the document.
Lastly score text	 Lastly we have a BM25 score on the anchor text of the document.
feature par ticular document pair case document judgment	 These are then the three feature values for a par ticular document query pair  In this case the document is d1 and the judgment says that it’s relevant.
case judgment it’s	 In this case the document is d1 and the judgment says that it’s relevant.
course overly instances point We use likelihood estimator relevance status document based feature	 Of course this is an overly simplified example where we just have two instances but it’s sufficient to illustrate the point  We use the maximum likelihood estimator to estimate the parameters  That is we’re going to predict the relevance status of the document based on the feature values.
parameters going status document based feature The likelihood relevance status model We relevance	 We use the maximum likelihood estimator to estimate the parameters  That is we’re going to predict the relevance status of the document based on the feature values  The likelihood of observing the relevance status of these two documents using our model is We hypothesize that the probability of relevance is related to the features in this way.
The likelihood observing relevance status documents model hypothesize features going values β relevance d1 value expression d2 hope value close	 The likelihood of observing the relevance status of these two documents using our model is We hypothesize that the probability of relevance is related to the features in this way  We’re going to see for what values of β we can predict the relevance effectively  The expression for d1 should give a higher value than the expression for d2 in fact we hope d1’s value is close to one since it’s a relevant document.
The expression d2 fact d1’s close relevant	 We’re going to see for what values of β we can predict the relevance effectively  The expression for d1 should give a higher value than the expression for d2 in fact we hope d1’s value is close to one since it’s a relevant document.
value hope d1’s value close it’s mathematically	 The expression for d1 should give a higher value than the expression for d2 in fact we hope d1’s value is close to one since it’s a relevant document  Let’s see how this can be mathematically expressed.
expressed expressing probability document talking We values.	 Let’s see how this can be mathematically expressed  It’s similar to expressing the probability of a document only we are not talking about the probability of words but the probability of relevance  We need to plug in the X values.
similar probability talking words relevance.	 It’s similar to expressing the probability of a document only we are not talking about the probability of words but the probability of relevance.
need plug values The β values probability	 We need to plug in the X values  The β values are still unknown but this expression gives us the probability that this document is relevant if we assume such a model.
β unknown expression probability document	 The β values are still unknown but this expression gives us the probability that this document is relevant if we assume such a model.
d1 relevant	 We want to maximize this probability for d1 since this is a relevant document.
probability This 1 relevance That’s probability predicting relevance values.	 For the second document we want to predict the probability that the document is nonrelevant  This means we have to compute 1 minus the probability of relevance  That’s the reasoning behind this whole expression then it’s our probability of predicting these two relevance values.
means probability	 This means we have to compute 1 minus the probability of relevance.
it’s relevance values The equation probability observing R 1 0 d2 goal adjust values expression	 That’s the reasoning behind this whole expression then it’s our probability of predicting these two relevance values  The whole equation is our probability of observing a R  1 and R  0 for d1 and d2 respectively  Our goal is then to adjust the β values to make the whole expression reach its maximum value.
The R 0 d2	 The whole equation is our probability of observing a R  1 and R  0 for d1 and d2 respectively.
goal adjust value words look choose	 Our goal is then to adjust the β values to make the whole expression reach its maximum value  In other words we will look at the function and choose β values to make this expression as large as possible.
words look large possible learn use expression new query features.	 In other words we will look at the function and choose β values to make this expression as large as possible  After we learn the regression parameters we can use this expression for any new query and new document once we have their features.
learn regression parameters use query new features.	 After we learn the regression parameters we can use this expression for any new query and new document once we have their features.
formula applied particular There regressionbased They generally attempt theoretically optimize retrieval measure	 This formula is then applied to generate a ranking score for a particular query  There are many more advanced learning algorithms than the regressionbased reproaches  They generally attempt to theoretically optimize a retrieval measure such as MAP or NDCG.
generally theoretically retrieval	 They generally attempt to theoretically optimize a retrieval measure such as MAP or NDCG.
One imagine We larger d1 won’t retrieval perspective bad.	 One can imagine that while our prediction may not be too bad the ranking can be wrong  We might have a larger probability of relevance for d2 than d1  So that won’t be good from a retrieval perspective even though by likelihood the function is not bad.
won’t likelihood function try correct problem solve.	 So that won’t be good from a retrieval perspective even though by likelihood the function is not bad  More advanced approaches will try to correct this problem  Of course then the challenge is that the optimization problem will be harder to solve.
problem Of course solve case probabilities relevance 0.	 More advanced approaches will try to correct this problem  Of course then the challenge is that the optimization problem will be harder to solve  In contrast we might have another case where we predicted probabilities of relevance around 0.
In predicted relevance	 In contrast we might have another case where we predicted probabilities of relevance around 0.
user.	9 the ranking will still be acceptable to a user.
rank general They problems retrieval problems.	 These learning to rank approaches are actually quite general  They can be ap plied to many other ranking problems aside from retrieval problems.
problems For exam systems summarization solved To summarize talked combine predict	 They can be ap plied to many other ranking problems aside from retrieval problems  For exam ple recommender systems computational advertising summarization and many other relevant applications can all be solved using this approach  To summarize we talked about using machine learning to combine features to predict a ranking result.
For exam recommender advertising summarization relevant solved talked machine combine features	 For exam ple recommender systems computational advertising summarization and many other relevant applications can all be solved using this approach  To summarize we talked about using machine learning to combine features to predict a ranking result.
To summarize ranking use machine learning information ago Chapter learning feedback.	 To summarize we talked about using machine learning to combine features to predict a ranking result  Actually the use of machine learning in information re trieval began many decades ago  Rocchio feedback discussed in Chapter 7 was a machine learning approach applied to learn the optimal feedback.
use machine information trieval began ago.	 Actually the use of machine learning in information re trieval began many decades ago.
7 learning approach rithms massive training data form This provides knowledge machine learning methods applied leverage this.	 Rocchio feedback discussed in Chapter 7 was a machine learning approach applied to learn the optimal feedback  Many algo rithms are driven by the availability of massive amounts of training data in the form of clickthroughs  This data provides much useful knowledge about relevance and so machine learning methods are applied to leverage this.
form This provides knowledge relevance learning methods applied need chine types ranking.	 Many algo rithms are driven by the availability of massive amounts of training data in the form of clickthroughs  This data provides much useful knowledge about relevance and so machine learning methods are applied to leverage this  The need for ma chine learning is also driven by the desire to combine many different feature types to predict an accurate ranking.
This useful machine	 This data provides much useful knowledge about relevance and so machine learning methods are applied to leverage this.
driven combine different feature accurate ranking web drives web search Using combating spam.	 The need for ma chine learning is also driven by the desire to combine many different feature types to predict an accurate ranking  web search especially drives this need since there are more features available on the web that can be taken advantage of for search  Using many different features also increases the robustness of the scoring function which is useful in combating spam.
web search features web taken advantage	 web search especially drives this need since there are more features available on the web that can be taken advantage of for search.
features robustness function Modern search use kind machine learning features current engines Google	 Using many different features also increases the robustness of the scoring function which is useful in combating spam  Modern search engines all use some kind of machine learning techniques to combine many features to optimize ranking and this is a major feature of current engines such as Google and Bing.
engines kind optimize ranking feature current engines Google Bing 10.	 Modern search engines all use some kind of machine learning techniques to combine many features to optimize ranking and this is a major feature of current engines such as Google and Bing  10.
Web chapter concludes talk future trends search information retrieval sys tems general To search it’s information	 10 5 The Future of Web Search Since this chapter concludes our coverage of search engines we briefly talk about some possible future trends of web search and intelligent information retrieval sys tems in general  To further improve the accuracy of a search engine it’s important to consider special cases of information need.
engines briefly web search retrieval improve search important special particular engines	5 The Future of Web Search Since this chapter concludes our coverage of search engines we briefly talk about some possible future trends of web search and intelligent information retrieval sys tems in general  To further improve the accuracy of a search engine it’s important to consider special cases of information need  One particular trend is to have more and more specialized and customized search engines which can be called vertical search engines.
To improve search engine consider information	 To further improve the accuracy of a search engine it’s important to consider special cases of information need.
particular trend search engines search effective search user special information Due possible personalization.	 One particular trend is to have more and more specialized and customized search engines which can be called vertical search engines  These vertical search engines can be expected to be more effective than the current general search engines because they could assume that a particular user belongs to a special group that might have a common information need  Due to this customization it’s also possible to do personalization.
effective general particular user special information Due possible personalization personalized understanding	 These vertical search engines can be expected to be more effective than the current general search engines because they could assume that a particular user belongs to a special group that might have a common information need  Due to this customization it’s also possible to do personalization  The search can be personalized because we have a better understanding of the users.
Due customization personalization The search advantages documents documents.	 Due to this customization it’s also possible to do personalization  The search can be personalized because we have a better understanding of the users  Restrict ing the domain of the search engine can also have some advantages in handling the documents because we would have a better understanding of these documents.
search personalized	 The search can be personalized because we have a better understanding of the users.
Restrict ing domain search documents documents example particular ambiguous problem	 Restrict ing the domain of the search engine can also have some advantages in handling the documents because we would have a better understanding of these documents  For example particular words may not be ambiguous in such a domain so we can bypass the problem of ambiguity  10.
example particular ambiguous problem ambiguity.	 For example particular words may not be ambiguous in such a domain so we can bypass the problem of ambiguity.
5 The 213 Another expect able	5 The Future of Web Search 213 Another trend we can expect to see is search engines that are able to learn over time a form of lifetime learning or lifelong learning.
This attractive search able	 This is very attractive because that means the search engine will be able to selfimprove.
As people use better.	 As more people use it the search engine will become better and better.
happening search engines feedback More typed better trend integration information	 This is already happening because the search engines can learn from the relevance feedback  More users use it and the quality of the search engine allows for the popular queries that are typed in by many users to retrieve better results  A third trend might be the integration of information access.
search allows popular	 More users use it and the quality of the search engine allows for the popular queries that are typed in by many users to retrieve better results.
recommendation fullfledged man In access pull access	 Search navigation and recommendation might be combined to form a fullfledged information man agement system  In the beginning of this book we talked about push access versus pull access these modes can be combined.
search detects unsatisfied In future document information need note document Currently advertising imagine integrated multi information	 For example if a search engine detects that a user is unsatisfied with search results a “note” may be made  In the future if a new document is crawled that matches the user’s information need recorded in the note this new document could be pushed to the user  Currently most of the cases of information recommendation are advertising but in the future you can imagine recommendation is seamlessly integrated into the system with multi mode information access.
future crawled matches need recorded document pushed imagine try support user tasks.	 In the future if a new document is crawled that matches the user’s information need recorded in the note this new document could be pushed to the user  Currently most of the cases of information recommendation are advertising but in the future you can imagine recommendation is seamlessly integrated into the system with multi mode information access  Another trend is that we might see systems that try to go beyond search to support user tasks.
Currently information seamlessly integrated multi mode access Another systems reason want problem decision perform	 Currently most of the cases of information recommendation are advertising but in the future you can imagine recommendation is seamlessly integrated into the system with multi mode information access  Another trend is that we might see systems that try to go beyond search to support user tasks  After all the reason why people want to search is to solve a problem or to make a decision to perform a task.
people want perform task For consumers search order product shopping	 After all the reason why people want to search is to solve a problem or to make a decision to perform a task  For example consumers might search for opinions about products in order to purchase a product so it would be beneficial to support the whole shopping workflow.
For example opinions product	 For example consumers might search for opinions about products in order to purchase a product so it would be beneficial to support the whole shopping workflow.
We think system—especially nodes If nodes triangle able specify information We triangle Service	 We can think about any intelligent system—especially intelligent information systems—specified by three nodes  If we connect these nodes into a triangle then we’ll able to specify an information system  We can call this triangle the DataUser Service Triangle.
triangle DataUser	 We can call this triangle the DataUser Service Triangle  The three questions you ask are as follows  .
questions ask .	 The three questions you ask are as follows  .
	 .
serving	 Who are you serving   What kind of data are you managing .
What managing What kind Chapter Search Users employees Online … Browsing Mining … Blog articles Literature Email 10.	 What kind of data are you managing   What kind of service are you providing 214 Chapter 10 Web Search Users Lawyers Scientists UIUC employees Online shoppers … Services Search Browsing Mining Task support … Data Web pages News articles Blog articles Literature Email … Figure 10.
kind Chapter 10 Web Users Lawyers Scientists Browsing Mining … Data pages Blog Literature 10 These information different	 What kind of service are you providing 214 Chapter 10 Web Search Users Lawyers Scientists UIUC employees Online shoppers … Services Search Browsing Mining Task support … Data Web pages News articles Blog articles Literature Email … Figure 10 10 The DataUserService triangle  These questions specify an information system there are many different ways to connect them.
These questions specify different connect	 These questions specify an information system there are many different ways to connect them  Depending on how they are connected we can specify all different types of systems  Let’s consider some examples.
specify systems.	 Depending on how they are connected we can specify all different types of systems.
consider On 10 kinds	 Let’s consider some examples  On the top of Figure 10 10 there are different kinds of users.
users different information service functions imagine connect ways.	10 there are different kinds of users  On the left side there are different types of data or information and on the bottom there are different service functions  Now imagine you can connect all these in different ways.
different functions Now imagine different ways.	 On the left side there are different types of data or information and on the bottom there are different service functions  Now imagine you can connect all these in different ways.
For connect pages web connect employees organization enterprise documents support	 For example if you connect everyone with web pages and support search and browsing you get web search  If we connect university employees with organization documents or enterprise documents and support the search and browsing we get enterprise search.
If university employees enterprise documents support browsing	 If we connect university employees with organization documents or enterprise documents and support the search and browsing we get enterprise search.
able support automatically section research paper	 For example we might be able to provide support for automatically generating a related works section for a research paper this would be closer to task support.
Then intelligent connect online product reviews shopping experience.	 Then we can imagine this intelligent information system would be a type of literature assistant  If we connect online shoppers with blog articles or product reviews then we can help these people improve their shopping experience.
If online blog articles reviews help experience.	 If we connect online shoppers with blog articles or product reviews then we can help these people improve their shopping experience.
Imagine analysis major	 Imagine a system that can provide an analysis of these emails to find that the major complaints of the cus tomers.
generating customer intelligently attaching	 We can imagine a system that could provide task support by automatically generating a response to a customer email by intelligently attaching a promotion message if appropriate.
If it’s com generic response tell expect aim people productivity Figure	 If it’s a com plaint then you might be able to automatically generate some generic response first and tell the customer that he or she can expect a detailed response later  All of these aim to help people to improve their productivity  Figure 10.
10 technology characterizes systems angles.	 Figure 10 10 shows the trend of technology and characterizes intelligent infor mation systems with three angles.
center figure there’s representation means current search provides search support users users keyword seeing bagofwords Current search documents.	 In the center of the figure there’s a triangle that connects keyword queries to search a bagofwords representation  That means the current search engines basically provides search support to users and mostly model users based on keyword queries seeing the data through a bagofwords representa tion  Current search engines don’t really “understand” information in the indexed documents.
search engines users queries data	 That means the current search engines basically provides search support to users and mostly model users based on keyword queries seeing the data through a bagofwords representa tion.
Consider trends node advanced Imagine queries search history understand user’s context information major information systems.	 Consider some trends to push each node toward a more advanced func tion away from the center  Imagine if we can go beyond keyword queries look at the user search history and then further model the user to completely understand the user’s task environment context or other information  Clearly this is pushing for personalization and a more complete user model which is a major direction in order to build intelligent information systems.
Imagine look search history model understand user intelligent information systems document tation.	 Imagine if we can go beyond keyword queries look at the user search history and then further model the user to completely understand the user’s task environment context or other information  Clearly this is pushing for personalization and a more complete user model which is a major direction in order to build intelligent information systems  On the document side we can also go beyond a bagofwords implementation to have an entityrelation represen tation.
pushing personalization complete model tation recognize names relations potentially	 Clearly this is pushing for personalization and a more complete user model which is a major direction in order to build intelligent information systems  On the document side we can also go beyond a bagofwords implementation to have an entityrelation represen tation  This means we’ll recognize people’s names their relations locations and any other potentially useful information.
means we’ll recognize people’s names potentially	 This means we’ll recognize people’s names their relations locations and any other potentially useful information.
effort engine better service future like knowledge representation add inference intelligent This largescale semantic initially	 Once we can get to that level without much manual human effort the search engine can provide a much better service  In the future we would like to have a knowledge representation where we can perhaps add some inference rules making the search engine more intelligent  This calls for largescale semantic analysis and perhaps this is initially more feasible for vertical search engines.
calls feasible vertical search it’s easier particular	 This calls for largescale semantic analysis and perhaps this is initially more feasible for vertical search engines  That is it’s easier to make progress in one particular domain.
On need information access need help digest information data mining convert text real knowledge knowledge making.	 On the service side we see we need to go beyond search to support information access in general search is only one way to get access to information  Going beyond access we also need to help people digest information once it is found and this step has to do with analysis of information or data mining  We have to find patterns or convert the text information into real knowledge that can be used in application or actionable knowledge that can be used for decision making.
access information analysis information We patterns convert text information actionable Furthermore user finishing task.	 Going beyond access we also need to help people digest information once it is found and this step has to do with analysis of information or data mining  We have to find patterns or convert the text information into real knowledge that can be used in application or actionable knowledge that can be used for decision making  Furthermore the knowledge will be used to help a user improve productivity in finishing a task.
real making.	 We have to find patterns or convert the text information into real knowledge that can be used in application or actionable knowledge that can be used for decision making.
knowledge improve task intelligent systems support.	 Furthermore the knowledge will be used to help a user improve productivity in finishing a task  In this dimension we anticipate that future intelligent information systems will provide interactive task support.
combined intelligence users	 We should emphasize interactive here because it’s important to optimize the combined intelligence of users and the system.
user machine intelligent	 That is the user and the machine can collaborate in an intelligent and efficient way.
intelligent systems hope insights motivate additional chapters book Notes Further classic Page et	 This is the big picture of future intelligent information systems and this hope fully can provide us with some insights about how to make further innovations on top of what we have today and also motivate the additional techniques to be covered in the later chapters of the book  Bibliographic Notes and Further Reading The classic reference for PageRank is Page et al.
Notes Reading The classic reference al.	 Bibliographic Notes and Further Reading The classic reference for PageRank is Page et al.
1999	 1999 and that for HITS is Kleinberg 1999.
Lin Dyer provides introduction ing text applications use MapReduce gives research rank search engine issue shortterm information need.	 Lin and Dyer 2010 provides an excellent introduction to us ing MapReduce for text processing applications including particularly a detailed treatment of how to use MapReduce for constructing an inverted index  Liu 2009 gives an excellent survey of research work on learning to rank  11Recommender Systems In our many discussions of search engine systems we have addressed the issue of shortterm ad hoc information need.
In discussions engine addressed issue shortterm ad	 11Recommender Systems In our many discussions of search engine systems we have addressed the issue of shortterm ad hoc information need.
need brary information needs documents dynamic information	 This is a temporary need from a static information source where the user pulls relevant information  Examples are li brary or web search  Conversely most users also have longterm information needs such as filtering or recommending documents or any other item type from a dynamic information source here the user is pushed information by a system.
delivery users filtering similar use	 Although there is some distinction between a recommender system emphasizing delivery of useful items to users and a filtering system em phasizing exclusion of useless items the techniques used are similar so we will use recommender and filtering interchangeably for convenience.
Unlike hoc lot feedback information user making feedback information In documents delivered dynamic A decision relevance user soon “arrives.	 Unlike ad hoc search where we may not get much feedback from a user in filtering we can expect to collect a lot of feedback information from the user making it important to learn from the feedback information to improve filtering performance  In filtering documents are delivered from some dynamic information source  A system must make a binary decision regarding the relevance of a document to a user as soon as it “arrives.
documents delivered source relevance document user soon “arrives This difficult search rely cutoff.	 In filtering documents are delivered from some dynamic information source  A system must make a binary decision regarding the relevance of a document to a user as soon as it “arrives ” This is more difficult than search where we can simply provide a ranked list and rely on a user to flexibly set the cutoff.
A binary decision document ” This difficult search simply provide ranked user flexibly hand collect information user easier nonrelevant	 A system must make a binary decision regarding the relevance of a document to a user as soon as it “arrives ” This is more difficult than search where we can simply provide a ranked list and rely on a user to flexibly set the cutoff  On the other hand since we can collect feedback information we can expect to get more and more information about what the user likes making it easier to distinguish relevant documents from nonrelevant ones.
search user	” This is more difficult than search where we can simply provide a ranked list and rely on a user to flexibly set the cutoff.
hand feedback information distinguish relevant	 On the other hand since we can collect feedback information we can expect to get more and more information about what the user likes making it easier to distinguish relevant documents from nonrelevant ones.
.	 .
filtering x	 Contentbased filtering look at what u likes and characterize x .
look 222 Chapter Recommender Systems users like users item	 Collaborative filtering is to look at what other 222 Chapter 11 Recommender Systems similar users like and assume that if those other users who are similar to you like an item you may also like it.
collaborative applied filtering case items specific application want combine optimize	 Note that if we can get user ratings of items collaborative filtering can be applied to recommend any item  Contentbased filtering however can only be applied to a case where we know how to measure similarity of items  In any specific application we will want to combine the two approaches to optimize the filtering performance.
Contentbased filtering applied similarity items.	 Contentbased filtering however can only be applied to a case where we know how to measure similarity of items.
In specific application want filtering retrieval thresholding component There	 In any specific application we will want to combine the two approaches to optimize the filtering performance  Contentbased filtering can usually be done by extending a retrieval system to add a thresholding component  There are two challenges associated with threshold setting.
First beginning initial user learn feedback Many threshold proposed.	 First at the beginning we must set an initial threshold without requiring much information from a user  Second over time we need to learn from feedback to optimize the threshold  Many threshold learning methods have been proposed.
time need feedback optimize Many practice betagamma method discuss good enough.	 Second over time we need to learn from feedback to optimize the threshold  Many threshold learning methods have been proposed  In practice a simple thresholding strategy such as the betagamma threshold setting method we will discuss is often good enough.
threshold	 Many threshold learning methods have been proposed.
In practice simple thresholding strategy basic rating object x weighted similar users	 In practice a simple thresholding strategy such as the betagamma threshold setting method we will discuss is often good enough  The basic idea behind collaborative filtering is to predict the rating of a current active user u for object x based on a weighted average of the ratings of x given by similar users to u.
idea filtering rating current object x given similar Thus approach involving simply user u users Each user represented i.	 The basic idea behind collaborative filtering is to predict the rating of a current active user u for object x based on a weighted average of the ratings of x given by similar users to u  Thus we can think of this approach involving two steps In the first step we simply “retrieve” similar users to the current user u where similarity is often defined as the similarity between the two vectors for two users  Each user can be represented by a rating vector i.
Thus think In simply user u similarity Each rating	 Thus we can think of this approach involving two steps In the first step we simply “retrieve” similar users to the current user u where similarity is often defined as the similarity between the two vectors for two users  Each user can be represented by a rating vector i e.
e ratings vectors based similarity correlation perform	e  all the ratings given by this user  The similarity of two vectors can be measured based on the cosine similarity or Pearson correlation of the two vectors which tends to perform very well empirically.
second step ratings x weight correlation	 In the second step we compute a weighted average of the ratings of x given by all these retrieved similar users where the weight is the correlation between the active user and the corresponding user to the weight.
discussed Therefore read reader basic concepts contentbased followed section userbased collaborative filtering.	 As we will see in this chapter many recommender systems are extensions of the information retrieval systems and techniques we have discussed previously  Therefore it may be beneficial to read Chapter 6 before this one if the reader is unfamiliar with the basic concepts or terminology  We continue this chapter with contentbased recommendation followed by a section on userbased recommen dation collaborative filtering.
read 6 reader basic terminology continue chapter section recommen collaborative 11.	 Therefore it may be beneficial to read Chapter 6 before this one if the reader is unfamiliar with the basic concepts or terminology  We continue this chapter with contentbased recommendation followed by a section on userbased recommen dation collaborative filtering  11.
11 1 Contentbased Recommendation Figure	 11 1 Contentbased Recommendation Figure 11.
1 11.	1 Contentbased Recommendation Figure 11.
information	1 shows a generic information filtering system where a stream of content is absorbed by a filtering system.
Based content liked item	 Based on either the content of the item or the other users that liked the item the system decides whether or not to pass the item along to the user.
inspect considering As shown	 In this section the filtering system will inspect the content of the item and compare it to both the user’s preferences and feedback without considering information from other users  As shown in Figure 11.
11.	 As shown in Figure 11.
initial profile Next decide threshold current shown learning feedback time.	 This is fed into the system as the initial user profile  Next there is a utility function to help the system make decisions it helps the system decide where to set an acceptance threshold θ determining whether or not the current item should be shown to the user  The learning module adjusts its parameters based on the user’s feedback over time.
utility help decisions threshold determining item user’s	 Next there is a utility function to help the system make decisions it helps the system decide where to set an acceptance threshold θ determining whether or not the current item should be shown to the user  The learning module adjusts its parameters based on the user’s feedback over time.
learning adjusts based user’s time Typically information need opportunities observe views indicate recommended	 The learning module adjusts its parameters based on the user’s feedback over time  Typically in information filtering applications the users’ information need is stable  Due to this the system would have many opportunities to observe the user if the user views a recommended item since the user can indicate whether the recommended item was relevant or not.
Due opportunities views recommended user item	 Due to this the system would have many opportunities to observe the user if the user views a recommended item since the user can indicate whether the recommended item was relevant or not.
Thus collect user’s classifier know case ranking MAP afford waiting user decision real	 Thus such feedback can be longterm allowing the system to collect much information about this user’s interests which is then used to improve the classifier  How do we know this filtering system actually performs well In this case we cannot use the ranking evaluation measures such as MAP or NDCG because we can’t afford waiting for a significant number of documents to rank them to make a decision for the user the system must make a decision in real time.
know evaluation waiting documents decision decision	 How do we know this filtering system actually performs well In this case we cannot use the ranking evaluation measures such as MAP or NDCG because we can’t afford waiting for a significant number of documents to rank them to make a decision for the user the system must make a decision in real time.
In words trying linear 3 .	 In general this decision is whether the item is above the acceptance threshold θ or not  In other words we’re trying to decide absolute relevance  One common strategy is to use a utility function and below is an example of a linear utility function U  3 .
In trying absolute	 In other words we’re trying to decide absolute relevance.
utility 3	 One common strategy is to use a utility function and below is an example of a linear utility function U  3 .
11.	 R′ 11.
delivered user documents delivered user way	1 where R is the set of relevant documents delivered to the user and R′ is the set of nonrelevant documents delivered to the user that the user rejected  In a way we can treat this as a gambling game.
delivers good	 If the system delivers one good item let’s say you win 3 or you gain 3.
clear utility function deliver good items.	 It’s clear that if you want to maximize this utility function your strategy should be to deliver as many good items as possible while simultaneously minimizing the delivery of bad items.
interesting coefficients.	 One interesting question here is how to set these coefficients.
We showed 3 ask “are −1 How output If 10 big penalty delivering bad deliver delivering chance	 We just showed a 3 and a −2 as the possible coefficients but we can ask the question “are they reasonable” What about other choices We could have 10 and −1 or 1 and −10  How would these utility functions affect the system’s output If we use 10 and −1 you will see that while we get a big reward for delivering a good document we incur only a small penalty for delivering a bad one  Intuitively the system would be encouraged to deliver more documents since delivering more documents gives a better chance of obtaining a high reward.
documents delivering gives better chance obtaining If choose 1 case incurred deliver bad one.	 Intuitively the system would be encouraged to deliver more documents since delivering more documents gives a better chance of obtaining a high reward  If we choose 1 and −10 it is the opposite case we don’t really get such a big prize if a good document is delivered while a large loss is incurred if we deliver a bad one.
choose −10 don’t large loss The case deliver relevant	 If we choose 1 and −10 it is the opposite case we don’t really get such a big prize if a good document is delivered while a large loss is incurred if we deliver a bad one  The system in this case would be very reluctant to deliver many documents and has to be absolutely sure that it’s a relevant one.
absolutely sure relevant	 The system in this case would be very reluctant to deliver many documents and has to be absolutely sure that it’s a relevant one.
short utility function based application different different users.	 In short the utility function has to be designed based on a specific application preference potentially different for different users.
Initialization	 Initialization module.
started limited text description Decision module.	 Gets the system started based only on a very limited text description or very few examples from the user  Decision module.
	 Decision module.
Learning user relevance judgments ered	 Learning module  Learn from limited user relevance judgments on the deliv ered documents.
user relevance judgments deliv documents deliver	 Learn from limited user relevance judgments on the deliv ered documents  If we don’t deliver a document to the user we’d never know whether the user likes it or not.
document user likes modules maximize function extend filtering.	 If we don’t deliver a document to the user we’d never know whether the user likes it or not  All these modules would have to be optimized to maximize the utility function U   To solve these problems we will talk about how to extend a retrieval system for information filtering.
solve talk extend filtering reuse retrieval know score documents queries profile text description	 To solve these problems we will talk about how to extend a retrieval system for information filtering  First we can reuse retrieval techniques to do scoring we know how to score documents against queries and measure the similarity between a profile text description and a document.
retrieval techniques know score queries similarity profile use score θ filtering deliver	 First we can reuse retrieval techniques to do scoring we know how to score documents against queries and measure the similarity between a profile text description and a document  We can use a score threshold θ for the filtering decision  If scored  θ  we say document d is relevant and we are going to deliver it to the user.
We score θ filtering decision.	 We can use a score threshold θ for the filtering decision.
scored d going Of course need learn use techniques	 If scored  θ  we say document d is relevant and we are going to deliver it to the user  Of course we still need to learn from the history and for this we can use the traditional feedback techniques to learn to improve scoring such as Rocchio.
know learn θ We set update information.	 What we don’t know how to do yet is learn how to set θ   We need to set it initially and then we have to learn how to update it over time as more documents are delivered to the user and we have more information.
Figure	 Figure 11.
shows like vector problems.	3 shows what the system might look like if we generalized a vector space model for filtering problems.
The utility filtering says sent user feedback.	 The evaluation would be based on the utility for the filtering results  If it says yes the document will be sent to the user and then the user could give some feedback.
The	 The feedback information would be used to both adjust the threshold and change the vector representation.
learning feedback search learning new bit challenges threshold learning.	 In this sense vector learning is essentially the same as query modification or feedback in search  The threshold learning is a new component that we need to talk a little bit more about  There are some interesting challenges in threshold learning.
threshold data filtering	 There are some interesting challenges in threshold learning  Figure 11 4 depicts the type of data that you can collect in the filtering system.
second documents user	 The second one is not relevant  We have many documents for which we don’t know the status since their scores are less than θ and are not shown to the user for judging.
We documents know censored creates learning optimal θ Secondly labeled challenging require training	 We have many documents for which we don’t know the status since their scores are less than θ and are not shown to the user for judging  Thus the judged documents are not a random sample it’s biased or censored data which creates some difficulty for learning an optimal θ   Secondly there are in general very little labeled data and very few relevant data which make it challenging for machine learning approaches which require a large amount of training data.
want explore document space user want non relevant unsatisfied bit misses user response document This explore hand don’t want explore overdeliver nonrelevant	 This means we want to explore the document space to see if the user might be interested in the documents that we have not yet labeled but we don’t want to show the user too many non relevant documents or they will be unsatisfied with the system  So how do we do that We could lower the threshold a little bit and deliver some near misses to the user to see what their response to this extra document is  This is a tradeoff because on one hand you want to explore but on the other hand you don’t want to explore too much since you would overdeliver nonrelevant information.
tradeoff want want explore overdeliver nonrelevant information learned user user	 This is a tradeoff because on one hand you want to explore but on the other hand you don’t want to explore too much since you would overdeliver nonrelevant information  Exploitation means you would take advantage of the information learned about the user  Say you know the user is interested in this particular topic so you don’t want to deviate that much.
know user topic don’t want deviate user	 Say you know the user is interested in this particular topic so you don’t want to deviate that much  However if you don’t deviate at all then you don’t explore at all and you might miss the opportunity to learn another interest of the user  Clearly this is a dilemma and a difficult problem to solve.
Clearly dilemma difficult utility optimization U The strategy optimize based data.	 Clearly this is a dilemma and a difficult problem to solve  Why don’t we just use the empirical utility optimization strategy to optimize U The problem is that this strategy is used to optimize the threshold based on historical data.
Why optimize optimize historical data.	 Why don’t we just use the empirical utility optimization strategy to optimize U The problem is that this strategy is used to optimize the threshold based on historical data.
That data candidate threshold track utility observed θ This difficulty	 That is you can compute the utility on the training data for each candidate score threshold keeping track of the highest utility observed given a θ   This doesn’t account for the exploration that we just mentioned and there is also the difficulty of biased training samples.
This account	 This doesn’t account for the exploration that we just mentioned and there is also the difficulty of biased training samples.
discuss threshold learning Zhai 1998.	 We’ll discuss one particular approach called betagamma threshold learning Zhai et al  1998.
The basic Given ranked documents training sorted xaxis relevance specific utility U utility value .	 The basic idea of the betagamma threshold learning algorithm is as follows  Given a ranked list of all the documents in the training database sorted by their scores on the xaxis their relevance and a specific utility U  we can plot the utility value at each different cutoff position θ .
Given ranked database sorted xaxis relevance utility value different cutoff .	 Given a ranked list of all the documents in the training database sorted by their scores on the xaxis their relevance and a specific utility U  we can plot the utility value at each different cutoff position θ .
Each cutoff threshold	 Each cutoff position corresponds to a score threshold  Figure 11.
5 shows configuration choice determines point help dynamically number database The optimal point utility threshold.	 Figure 11 5 shows this configuration and how a choice of α determines a cutoff point between the optimal and the zero utility points and how β and γ help us to adjust α dynamically according to the number of judged examples in the training database  The optimal point θopt is the point when we would achieve the maximum utility if we had chosen this threshold.
5 shows configuration cutoff point zero β adjust number training	5 shows this configuration and how a choice of α determines a cutoff point between the optimal and the zero utility points and how β and γ help us to adjust α dynamically according to the number of judged examples in the training database.
θopt point achieve The threshold zero threshold Between point values.	 The optimal point θopt is the point when we would achieve the maximum utility if we had chosen this threshold  The θzero threshold is the zero utility threshold  Between these two θ values give us a safe point to explore the potential cutoff values.
The threshold safe explore cutoff values zero utility threshold optimal	 The θzero threshold is the zero utility threshold  Between these two θ values give us a safe point to explore the potential cutoff values  As one can see from the formula the threshold will be just the interpolation of the zero utility threshold and the optimal threshold via the interpolation parameter α.
Between values As formula threshold interpolation threshold	 Between these two θ values give us a safe point to explore the potential cutoff values  As one can see from the formula the threshold will be just the interpolation of the zero utility threshold and the optimal threshold via the interpolation parameter α.
question set α optimal This multiple point safe necessarily reach Rather we’re use parameters α’s value information.	 Now the question is how we should set α and deviate from the optimal utility point  This can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore only up to the θzero point which is still a safe point but not necessarily reach all the way to it  Rather we’re going to use other parameters to further define α’s value given some additional information.
depend factors mechanism explore θzero safe necessarily way parameters information parameter controls based	 This can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore only up to the θzero point which is still a safe point but not necessarily reach all the way to it  Rather we’re going to use other parameters to further define α’s value given some additional information  The β parameter controls the deviation from θopt  which can be based on our previously observed documents i.
going use α’s value given	 Rather we’re going to use other parameters to further define α’s value given some additional information.
The controls deviation θopt based previously observed documents i.	 The β parameter controls the deviation from θopt  which can be based on our previously observed documents i.
interesting γ parameter influence number set N .	 the training data  What’s more interesting is the γ parameter which controls the influence of the number of examples in the training data set N .
What’s γ data set	 What’s more interesting is the γ parameter which controls the influence of the number of examples in the training data set N .
small explore seen examples sure space But observe probably don’t explore much.	 In other words when N is very small the algorithm will try to explore more meaning that if we have seen only a few examples we’re not sure whether we have exhausted the space of interest  But as we observe many data points from the user we feel that we probably don’t have to explore as much.
But observe points probably explore exploration examples exploration θopt approach particularly tasks.	 But as we observe many data points from the user we feel that we probably don’t have to explore as much  This gives us a dynamic strategy for exploration the more examples we have seen the less exploration we are going to do so the threshold will be closer to θopt   This approach has worked well in some empirical studies particularly on the TREC filtering tasks.
This examples approach particularly It’s welcomes arbitrary lower	 This gives us a dynamic strategy for exploration the more examples we have seen the less exploration we are going to do so the threshold will be closer to θopt   This approach has worked well in some empirical studies particularly on the TREC filtering tasks  It’s also convenient that it welcomes any arbitrary utility function with an appropriate lower bound.
This approach tasks It’s welcomes function bound It explicitly addresses exploration exploration	 This approach has worked well in some empirical studies particularly on the TREC filtering tasks  It’s also convenient that it welcomes any arbitrary utility function with an appropriate lower bound  It explicitly addresses the exploration exploration tradeoff and uses θzero as a safeguard.
welcomes arbitrary appropriate	 It’s also convenient that it welcomes any arbitrary utility function with an appropriate lower bound.
addresses tradeoff θzero explore	 It explicitly addresses the exploration exploration tradeoff and uses θzero as a safeguard  That is we’re never going to explore further than the zero utility point.
going explore point.	 That is we’re never going to explore further than the zero utility point.
If 11.	 If you take the analogy of gambling you 11.
The problem approach bound conservative There learning proposed solving research 11.	 The problem is of course that this approach is purely heuristic and the zero utility lower bound is often too conservative in practice  There are more advanced machine learning projects that have been proposed for solving these problems it is actually a very active research area  11.
machine learning proposed area.	 There are more advanced machine learning projects that have been proposed for solving these problems it is actually a very active research area.
	 11.
basic idea infer individual interests preferences based collaborative finds set users set users predicts preferences.	 The basic idea is to infer individual interests or preferences based solely on similar users  Given a user collaborative filtering finds a set of similar users  Based on the set of similar users it predicts the current user’s preferences.
This assumptions.	 Based on the set of similar users it predicts the current user’s preferences  This method makes some assumptions.
method assumptions preferences example favor users	 This method makes some assumptions  Users with a common interest will have similar preferences Users with similar preferences share the same interest For example if a user has an interest in information retrieval they might favor papers published in SIGIR  If users favor SIGIR papers then they might have an interest in IR.
preferences example information retrieval favor papers users papers IR text doesn’t matter previous looked item contentbased	 Users with a common interest will have similar preferences Users with similar preferences share the same interest For example if a user has an interest in information retrieval they might favor papers published in SIGIR  If users favor SIGIR papers then they might have an interest in IR  The text content of items doesn’t matter This is in sharp contrast to the previous section where we looked at item similarity through contentbased filtering.
users SIGIR papers The sharp contrast contentbased	 If users favor SIGIR papers then they might have an interest in IR  The text content of items doesn’t matter This is in sharp contrast to the previous section where we looked at item similarity through contentbased filtering.
infer similar The idea Figure	 Here we will infer an individual’s interest based on other similar users  The general idea is displayed in Figure 11.
general rank users	 The general idea is displayed in Figure 11 6 given a user u we will rank other users based on similarity u1 .
	      um.
um preferences m	  um  We then predict user preferences based on the preferences of these m other users  The preference is on a common set of items o1 .
We predict items	 We then predict user preferences based on the preferences of these m other users  The preference is on a common set of items o1   .
The preference o1	 The preference is on a common set of items o1 .
	 .
	 .
If user	 If we arrange the users and objects into a matrix X we can consider the user ui and the object oj as the point ui  oj in the matrix.
Again note all.	 Again note that the exact content of each item doesn’t matter at all.
We applied text documents.	 We only consider the relationship between the users and the items  This makes this ap proach very general since it can be applied to any items—not just text documents.
This movies ratings e	 This makes this ap proach very general since it can be applied to any items—not just text documents  Those items could be movies or products and the users could give ratings e g.
users ratings g.	 Those items could be movies or products and the users could give ratings e g.
	 one through five.
Some users movies movies it’s examined object	 Some users have watched movies and rated them but most movies for a given user are unrated since it’s unlikely that a user has examined all items in the object space.
item unknown values job collaborative filtering value element large number preferences us.	 Thus many item entries have unknown values and it is the job of collaborative filtering to infer the value of a element in this matrix based on other known values  One other assumption we have to make is that there are a sufficiently large number of user preferences available to us.
assumption sufficiently preferences available us.	 One other assumption we have to make is that there are a sufficiently large number of user preferences available to us.
number preferences movies sparsity cold assume unknown	 For example we need an appreciable number of ratings by users for movies that indicate their preferences for those particular movies  If we don’t have sufficient data there will be a data sparsity problem and that’s often called the cold start problem  We assume an unknown function f .
We function object rating.	 We assume an unknown function f     that maps a user and object to a rating.
maps user object matrix output function pairs This problems know training hope unseen	 that maps a user and object to a rating  In the matrix X we have observed there are some output values of this function and we want to infer the value of this function for other pairs that don’t have values  This is very similar to other machine learning problems where we would know the values of the function on some training data and we hope to predict the values of this function on some unseen test data.
learning problems training data hope function test	 This is very similar to other machine learning problems where we would know the values of the function on some training data and we hope to predict the values of this function on some unseen test data.
users current user.	 similar users to the current user.
users preference user average	 Then we use those users to predict the preference of the current user  Let ni be the average rating of all objects by user ui.
Let average rating user We ni subtracting rating ratings This necessary different users comparable generally	 Let ni be the average rating of all objects by user ui  We need ni so we can normalize the ratings of objects by this user by subtracting the average rating from all the ratings  This is necessary so that the ratings from different users will be comparable some users might be more generous and generally give higher ratings while others might be more critical and have a lower average rating.
We need normalize ratings user This necessary different users comparable generous ratings critical rating	 We need ni so we can normalize the ratings of objects by this user by subtracting the average rating from all the ratings  This is necessary so that the ratings from different users will be comparable some users might be more generous and generally give higher ratings while others might be more critical and have a lower average rating  So their ratings can not be directly compared with each other or aggregated together which is why we first normalize.
This ratings different comparable generous generally ratings lower	 This is necessary so that the ratings from different users will be comparable some users might be more generous and generally give higher ratings while others might be more critical and have a lower average rating.
In interested recommending look similar users liked object Mathe matically rating user combination users.	 In particular we are interested in recommending oj to ua  The idea here is to look at whether similar users to this user have liked this object or not  Mathe matically the predicted rating of this user on this object is a combination of the normalized ratings of different users.
Mathe matically user object combination normalized different	 Mathe matically the predicted rating of this user on this object is a combination of the normalized ratings of different users.
We’re picking sum users users contribute user Naturally similarity ua particular	 We’re picking a sum of all the users but not all users contribute equally to the average each user’s weight controls the influence of a user on the prediction  Naturally the weight is related to the similarity between ua and a particular user ui.
contribution like preference ua We normalized ratings	 The more similar they are the more contribution we would like user ui to make in predicting the preference of ua  We have the following formulas  First using the normalized ratings Vij  Xij − ni 11.
We formulas Vij Xij − 11.	 We have the following formulas  First using the normalized ratings Vij  Xij − ni 11.
First normalized write normalized rating V̂aj k .	 First using the normalized ratings Vij  Xij − ni 11 2 we can write the predicted normalized rating V̂aj  k .
3	3 where w.
function 1∑m 11 V̂aj ∈ 0	   is the similarity function and k is the normalizer k  1∑m i1 wua  ui 11 4 that ensures V̂aj ∈ 0 1.
Once predicted normalized range uses X̂aj 11 want write program implement filtering problem	 Once we have the predicted normalized rating we trans form it into the rating range that ua uses X̂aj  V̂aj  na  11 5 If we want to write a program to implement this collaborative filtering we still face the problem of determining the weighting function.
If want write program filtering weighting formula easy implement.	 11 5 If we want to write a program to implement this collaborative filtering we still face the problem of determining the weighting function  Once we know this then the formula is very easy to implement.
5 implement function Once formula definitions define different	5 If we want to write a program to implement this collaborative filtering we still face the problem of determining the weighting function  Once we know this then the formula is very easy to implement  Specific definitions of the weighting function define the different interpretations of the collaborative filtering rating estimate.
formula definitions function different interpretations collaborative rating	 Once we know this then the formula is very easy to implement  Specific definitions of the weighting function define the different interpretations of the collaborative filtering rating estimate.
	 Specific definitions of the weighting function define the different interpretations of the collaborative filtering rating estimate.
One Coefficient range judged items users tended ratings items lower similar items Another cosine measure vectors vector cosine discussed retrieval We’ll clustering Chapter	 One popular approach is the Pearson Correlation Coefficient This is a sum of a common range of judged items and measures whether the two users tended to all give higher ratings to similar items or lower ratings to similar items  Another measure is the cosine measure which treats the rating vectors as vectors in the vector space and measures the cosine of the angle between the two vectors As we’ve discussed previously in this book this measure has been used in the vector space model for retrieval  We’ll also see how it is used in clustering in Chapter 14.
Chapter In note user similarity items actually information items It items products	 We’ll also see how it is used in clustering in Chapter 14  In all these cases note that the user similarity is based on their preferences on items and we did not actually use any content information of these items  It didn’t matter what these items are they can be movies books products or text documents.
items actually content didn’t books products documents.	 In all these cases note that the user similarity is based on their preferences on items and we did not actually use any content information of these items  It didn’t matter what these items are they can be movies books products or text documents.
didn’t matter	 It didn’t matter what these items are they can be movies books products or text documents.
This allows approach applied wide problems.	 This allows such an approach to be applied to a wide range of problems.
example missing values default values average ratings	 There are some practical issues to deal with here as well for example there will be many missing values  We could set them to default values or the average ratings of other users.
default values users That simple actually try use values improve fact memory based filtering judgements missing	 We could set them to default values or the average ratings of other users  That will be a simple solution but there are advantages to approaches that can actually try to predict those missing values and then use the predicted values to improve the similarity measure  In fact in memory based collaborative filtering we can predict judgements with missing values.
That advantages approaches actually try missing values improve measure In collaborative imagine apply iterative predicted values improve function.	 That will be a simple solution but there are advantages to approaches that can actually try to predict those missing values and then use the predicted values to improve the similarity measure  In fact in memory based collaborative filtering we can predict judgements with missing values  As you can imagine we could apply an iterative approach where we first do some preliminary prediction and then use the predicted values to further improve the similarity function.
seen called frequency IUF Here ratings.	 Another idea which is quite similar to the idea of IDF that we have seen in text research is called the inverse user frequency or IUF  Here the idea is to look at where the two users share similar ratings.
Here idea look users item	 Here the idea is to look at where the two users share similar ratings  If the item is a popular item that has been viewed by many people it’s not as informative.
popular it’s item says similarity users.	 If the item is a popular item that has been viewed by many people it’s not as informative  Conversely if it’s a rare item that has not been viewed by many users then it says more about their similarity emphasizing more on similarity of items that are not viewed by many users.
Let’s systems sense low.	 Let’s summarize our discussion of recommender systems  In some sense the filtering task of recommendation is easy and in another sense the task is rather difficult  It’s easy because the user expectation is low.
It’s user expectation That recommendation better	 It’s easy because the user expectation is low  That is any recommendation is better than none.
That none.	 That is any recommendation is better than none.
items it’s easy 11.	 Unless you recommend only noisy items or useless documents any information would be appreciated  Thus in that sense it’s easy  11.
easy 3 Recommender 233 considered harder afford waiting items belief better others.	 Thus in that sense it’s easy  11 3 Evaluation of Recommender Systems 233 Filtering can also be considered a much harder task because you have to make a binary decision and can’t afford waiting for many items to enhance your belief that one is better than others.
11 Evaluation afford waiting belief better Let’s soon articles decide user.	 11 3 Evaluation of Recommender Systems 233 Filtering can also be considered a much harder task because you have to make a binary decision and can’t afford waiting for many items to enhance your belief that one is better than others  Let’s think about news filtering as soon as the system detects the news articles you have to decide whether the news would be interesting to a user.
3 Systems Filtering considered decision enhance better think news news articles decide news interesting	3 Evaluation of Recommender Systems 233 Filtering can also be considered a much harder task because you have to make a binary decision and can’t afford waiting for many items to enhance your belief that one is better than others  Let’s think about news filtering as soon as the system detects the news articles you have to decide whether the news would be interesting to a user.
news it’s data sparseness If think problem it’s past	 If you wait for a few days even an accurate recommendation of the most relevant news is not interesting  Another reason why it’s hard is due to data sparseness  If you think of this as a learning problem in collaborative filtering for example it’s purely based on learning from the past ratings.
it’s data sparseness If learning problem collaborative example based past	 Another reason why it’s hard is due to data sparseness  If you think of this as a learning problem in collaborative filtering for example it’s purely based on learning from the past ratings.
If don’t problem For use preferences.	 If you don’t have many ratings there’s really not much you can do  As we mentioned there are strategies that have been proposed to solve the problem  For example we can use more user information to assess their similarity instead of just using the item preferences.
problem We look content similarity.	 As we mentioned there are strategies that have been proposed to solve the problem  For example we can use more user information to assess their similarity instead of just using the item preferences  We also talked about the two strategies for a filtering task one is contentbased where we look at item content similarity.
For use user information instead item preferences talked filtering contentbased content	 For example we can use more user information to assess their similarity instead of just using the item preferences  We also talked about the two strategies for a filtering task one is contentbased where we look at item content similarity.
We filtering look item content	 We also talked about the two strategies for a filtering task one is contentbased where we look at item content similarity.
vs pull strategies getting access text data.	 We talked about push vs  pull as two strategies for getting access to the text data.
pull access text data push mode engines	 pull as two strategies for getting access to the text data  Recommender systems aid users in push mode whereas search engines assist users in pull mode.
Evaluation Systems In setups filtering pairs ratings r̂ actual useritem A called rootmean error RMSE Due square RMSE errors The similarity MAE reader differences gMAP MAP.	3 Evaluation of Recommender Systems In evaluation setups for collaborative filtering we have a set P of pairs of predicted ratings r̂ and actual ratings r across all useritem pairs  A very common measure called rootmean squared error RMSE has a mathematical formula that follows its name Due to the square in RMSE RMSE is more affected by larger errors  The similarity between RMSE and MAE should remind the reader of the differences between gMAP and MAP.
A common measure called RMSE mathematical formula larger The MAE reader MAP.	 A very common measure called rootmean squared error RMSE has a mathematical formula that follows its name Due to the square in RMSE RMSE is more affected by larger errors  The similarity between RMSE and MAE should remind the reader of the differences between gMAP and MAP.
The remind gMAP Both quantify difference values true measure ordinal	 The similarity between RMSE and MAE should remind the reader of the differences between gMAP and MAP  Both these measures quantify the difference in values between r̂ and the true rating r   Using such a measure is natural when we have ratings on some ordinal scale.
Using ratings scale.	 Using such a measure is natural when we have ratings on some ordinal scale.
note accuracy recommended	 One important note is that these measures only capture the accuracy of predicted ratings in an actual recommender system the top k elements are recommended to the user.
Despite low MAE possible actually relevant user.	 Despite having a low RMSE or MAE it may be possible that the top documents may not actually be relevant to the user.
case k 1 recommend high To combat issue rank particular NDCG list r .	 In the extreme case where k  1 we may recommend the one element that received an erroneous high score  To combat this issue we can rank all ratings for a particular user and then use an information retrieval metric such as NDCG to view the list as a whole when compared to the true rating r .
ratings user view true rating information pushes	 To combat this issue we can rank all ratings for a particular user and then use an information retrieval metric such as NDCG to view the list as a whole when compared to the true rating r   In information filtering tasks a system pushes items to a user if it thinks the user would like the item.
user item user determines suggestion For user it’s use metrics Chapter	 Once the user sees the item the user then determines if the suggestion was a good one  For a single user it’s easy to see that we can use some evaluation metrics from information retrieval as discussed in Chapter 9.
single it’s metrics discussed 9 Since pushed available ranksensitive measures.	 For a single user it’s easy to see that we can use some evaluation metrics from information retrieval as discussed in Chapter 9  Since items are pushed to users as soon as they become available we can’t use any ranksensitive measures.
soon available ranksensitive Still pushed compute precision score similar measures.	 Since items are pushed to users as soon as they become available we can’t use any ranksensitive measures  Still we can examine the set of all documents pushed to the user in some time period and compute statistics like precision recall F1 score and other similar measures.
examine documents pushed user compute precision similar measures In users pushed items.	 Still we can examine the set of all documents pushed to the user in some time period and compute statistics like precision recall F1 score and other similar measures  In a true information filtering system there will be many users who receive all pushed items.
way aggregate scores metrics e	 A simple way to aggregate scores would be to take an average of the individual user metrics e g.
g	g  average F1 score across all users.
score recommendations Since set peruser different different seen	 average F1 score across all users  However this may not be the best measure if some users have more recommendations than others  Since θ is set on a peruser basis different users will aggregate different numbers of seen documents.
measure users θ peruser users aggregate numbers seen documents.	 However this may not be the best measure if some users have more recommendations than others  Since θ is set on a peruser basis different users will aggregate different numbers of seen documents.
Since peruser basis users aggregate numbers seen	 Since θ is set on a peruser basis different users will aggregate different numbers of seen documents.
real users training complete profiles	 Furthermore in a real system users may not all join at the same time or some users may have more training data available than others if they have more complete user profiles or if they have been in the system longer.
For advantageous instead average The sum user’s user’s It compute precision recall number documents seen.	 For these reasons it could be advantageous to instead take a weighted average of user metrics as an overall metric  The weight may be assigned such that all weights sum to one and each user’s weight is determined by that user’s total number of judgements  It may also be interesting to compute the precision or recall over time where time is measured as the number of documents that the filtering system has seen.
The user’s user’s number interesting time time documents filtering seen user.	 The weight may be assigned such that all weights sum to one and each user’s weight is determined by that user’s total number of judgements  It may also be interesting to compute the precision or recall over time where time is measured as the number of documents that the filtering system has seen  A variant of this is to measure time based on the number of elements judged by the user which are only those elements that are shown to the user.
time measured documents seen A time	 It may also be interesting to compute the precision or recall over time where time is measured as the number of documents that the filtering system has seen  A variant of this is to measure time based on the number of elements judged by the user which are only those elements that are shown to the user.
A variant time number elements user Ideally documents increases overall precision precision documents Once flat line likely	 A variant of this is to measure time based on the number of elements judged by the user which are only those elements that are shown to the user  Ideally as the number of documents increases the overall precision or precision of the last k documents should increase  Once the precision has reached a flat line we are most likely at θopt given the current system setup.
As note recommender types valuable evaluation metric That evaluation significantly investigated overall sure course depend evaluation metric multiple metrics	 As a final note for both recommender system types it is valuable to find those users affecting the evaluation metric the most  That is are there any outliers that cause the evaluation metric to be significantly lower than expected If so these users can be further investigated and the overall system may be adjusted to en sure their satisfaction  Of course these outliers depend heavily on the evaluation metric used so using multiple metrics will give the most complete view of user satisfaction.
outliers evaluation significantly expected sure satisfaction metrics Further For reading suggest Herlocker et al.	 That is are there any outliers that cause the evaluation metric to be significantly lower than expected If so these users can be further investigated and the overall system may be adjusted to en sure their satisfaction  Of course these outliers depend heavily on the evaluation metric used so using multiple metrics will give the most complete view of user satisfaction  Bibliographic Notes and Further Reading For further reading we suggest that the reader consult Herlocker et al.
course depend heavily complete et Shani	 Of course these outliers depend heavily on the evaluation metric used so using multiple metrics will give the most complete view of user satisfaction  Bibliographic Notes and Further Reading For further reading we suggest that the reader consult Herlocker et al  2004 Shani and Gunawardana 2011.
Bibliographic Notes Further For reading Herlocker al 2004 Gunawardana 2011.	 Bibliographic Notes and Further Reading For further reading we suggest that the reader consult Herlocker et al  2004 Shani and Gunawardana 2011.
Shani	 2004 Shani and Gunawardana 2011.
Ricci et al learning recommender systems general.	 Ricci et al  2010 is a comprehensive resource for learning more about recommender systems in general.
comprehensive resource learning systems	 2010 is a comprehensive resource for learning more about recommender systems in general.
More setting al description collaborative filtering al.	 More information about the betagamma threshold setting algorithm can be found in Zhai et al  2000  A description of memorybased collaborative filtering algorithm can be found in Breese et al.
provides comparison	 1998 which also provides a comparison of different collaborative filtering algorithms.
A recent comparison algorithms Cacheda	 A more recent comparison of multiple collaborative filtering algorithms can be found in the Cacheda et al.
	 2011.
book chapter techniques processing text data useful knowledge decision making Part II extent need user equivalently specific application emphasized.	 In Part III of the book starting from this chapter we will cover techniques for further processing relevant text data so as to extract and discover useful actionable knowledge that can be directly used for decision making or supporting a user’s task  One difference between Part II and Part III is the extent to which the information need of a user or equivalently a specific application is emphasized.
difference Part information need specific data access general users digest text help additional text information closely relevance role techniques covered	 One difference between Part II and Part III is the extent to which the information need of a user or equivalently a specific application is emphasized  Specifically since the purpose of text data access is in general to connect users with the right information at the right time so that they can further digest and exploit the relevant text data with or without the help of additional text analysis techniques the concept of information need and the closely related concept of relevance play an important role in all the techniques covered in Part II.
purpose text data users right information time digest data information need closely important Part example essential search user’s information equally role recommender systems.	 Specifically since the purpose of text data access is in general to connect users with the right information at the right time so that they can further digest and exploit the relevant text data with or without the help of additional text analysis techniques the concept of information need and the closely related concept of relevance play an important role in all the techniques covered in Part II  For example queries play an essential role in any search engines and accurate modeling of a user’s interest and information need plays an equally important role in any recommender systems.
For play essential role engines accurate modeling user’s need plays role systems.	 For example queries play an essential role in any search engines and accurate modeling of a user’s interest and information need plays an equally important role in any recommender systems.
In Part generally relevant longer emphasize goal understand content text interesting trends polarity eventually useful finishing user task view analysis case general users mining operators probe interactive manner Multiple operators	 In Part III however we generally can assume that the text data to be considered are all relevant so we will see that we no longer emphasize the information need so much but instead will emphasize the goal toward understand the content of text in more detail and find any interesting patterns in text data such as topical trends or sentiment polarity so as to eventually extract and discover actionable knowledge directly useful for finishing a user task  As such we will attempt to view the process of text data analysis as a special case of the general process of data mining where users would use various data mining operators to probe and analyze the data in an interactive manner  Multiple operators may be combined.
view text analysis special process mining use mining interactive manner Multiple operators combined view sensors” text data subjective text data kinds data objective jointly nontext data	 As such we will attempt to view the process of text data analysis as a special case of the general process of data mining where users would use various data mining operators to probe and analyze the data in an interactive manner  Multiple operators may be combined  Specifically we will view humans as “subjective sensors” of our world and text data as data generated by such subjective sensors making text data more similar to other kinds of data generated by objective machine sensors and enabling us to naturally discuss how to jointly analyze text and nontext data together.
separation data access text analysis stage separation Part II Part III book sophisticated stages interleaved involving zoomed documents analysis documents choose inside particular specific subset analysis applied obtained	 However we must point out that the separation of the text data access stage and text data analysis stage thus also the separation of Part II and Part III in the book is somewhat artificial since in a sophisticated application these two stages are often interleaved and an iterative process involving both stages is often followed  For example after a user has zoomed into a set of relevant documents and performed an analysis task such as clustering of documents into topical clusters the user may also choose to further search inside a particular cluster to further zoom into a specific subset of documents and additional analysis operators such as sentiment analysis may then be applied to this newly obtained smaller subset.
For user relevant documents performed analysis task documents topical clusters user inside zoom documents newly smaller combined provide functions	 For example after a user has zoomed into a set of relevant documents and performed an analysis task such as clustering of documents into topical clusters the user may also choose to further search inside a particular cluster to further zoom into a specific subset of documents and additional analysis operators such as sentiment analysis may then be applied to this newly obtained smaller subset  Moreover techniques from both Part II and Part III can often be combined to provide more useful functions to users e.
Part II Part combined provide useful functions summarization enhance	 Moreover techniques from both Part II and Part III can often be combined to provide more useful functions to users e g  summarization can be naturally combined with a search engine or recommender system and they may enhance each other e.
	g.
naturally engine recommender enhance	 summarization can be naturally combined with a search engine or recommender system and they may enhance each other e.
	g.
Nevertheless separate readers overall picture highlevel relations 12 The easily seen process lot data basis.	 Nevertheless we have chosen to separate them so as to allow the readers to see a meaningful overall picture of all the techniques we covered and their highlevel relations  12 1 Motivation Applications of Text Data Analysis The importance of text data to our lives can be easily seen from the fact that we all process a lot of text data on a daily basis.
Applications Text Analysis importance text easily process lot	 12 1 Motivation Applications of Text Data Analysis The importance of text data to our lives can be easily seen from the fact that we all process a lot of text data on a daily basis.
Motivation Applications Analysis The text fact process text	1 Motivation Applications of Text Data Analysis The importance of text data to our lives can be easily seen from the fact that we all process a lot of text data on a daily basis.
data manual acceptable timecritical applications.	 However as the amount of text data increases the manual processing of text data would not be feasible or acceptable especially for timecritical applications.
analysis kind labor improve sorting save time.	 In general we may distinguish two kinds of text analysis applications  One kind is those that can replace our current manual labor in digesting text content they help improve our productivity but do not do anything beyond what we humans can do  For example automatic sorting of emails would save us a lot of time.
replace text content help improve productivity emails time.	 One kind is those that can replace our current manual labor in digesting text content they help improve our productivity but do not do anything beyond what we humans can do  For example automatic sorting of emails would save us a lot of time.
save	 For example automatic sorting of emails would save us a lot of time.
example intelligent literature reveal genes synthesizing relations genedisease articles suggesting opportunity design targeting genes coverage knowledge text data possible text analysis virtually specific examples provide text analysis covered subsequent	 For example an intelligent biomedical literature anlayzer may reveal a chain of associations of genes and diseases by synthesizing genegene relations and genedisease relations scattered in many different research articles thus suggesting a potential opportunity to design drugs targeting some of the genes for treatment of a disease  Due to the broad coverage of knowledge in text data and our reliance on text data for communications it is possible to imagine text analysis applications in virtually any domain  Below are just a few specific examples that may provide some application contexts for understanding the text analysis techniques covered in the subsequent chapters.
Due broad knowledge text communications possible specific contexts understanding chapters application domain text business	 Due to the broad coverage of knowledge in text data and our reliance on text data for communications it is possible to imagine text analysis applications in virtually any domain  Below are just a few specific examples that may provide some application contexts for understanding the text analysis techniques covered in the subsequent chapters  One important application domain of text analysis is business intelligence.
Below examples application text subsequent chapters One application domain analysis business	 Below are just a few specific examples that may provide some application contexts for understanding the text analysis techniques covered in the subsequent chapters  One important application domain of text analysis is business intelligence.
This opportunity form reviews If develop techniques tap information extract opinions product gain business feedback	 This can be a good opportunity for leveraging text data in the form of product reviews on the Web  If we can develop and master text mining techniques to tap into such an information source to extract the knowledge and opinions of people about these products then we can help these product managers gain business intelligence or gain feedback from their customers.
If mining tap information help product managers gain gain feedback application scientific research di encoded literature Scientists interested discover related	 If we can develop and master text mining techniques to tap into such an information source to extract the knowledge and opinions of people about these products then we can help these product managers gain business intelligence or gain feedback from their customers  Another important application domain is scientific research where timely di gestion of knowledge encoded in literature articles is essential  Scientists are also interested in knowing the trends of research topics or learning about discover ies in fields related to their own.
Another important application scientific timely knowledge essential topics discover fields related important communities different similar	 Another important application domain is scientific research where timely di gestion of knowledge encoded in literature articles is essential  Scientists are also interested in knowing the trends of research topics or learning about discover ies in fields related to their own  This problem is especially important in biology research—different communities tend to use different terminologies yet they’re stating very similar problems.
knowing trends learning fields own.	 Scientists are also interested in knowing the trends of research topics or learning about discover ies in fields related to their own.
problem especially important research—different communities different they’re stating similar problems How covered communities vocabularies problem question examples leverage text knowledge optimize	 This problem is especially important in biology research—different communities tend to use different terminologies yet they’re stating very similar problems  How can we integrate the knowledge that is covered in different communities using different vocabularies to help study a particular problem Answering such a question speeds up scientific discovery  There are many more such examples where we can leverage text data to discover usable knowledge to optimize our decisionmaking processes.
There examples leverage data optimize decisionmaking	 There are many more such examples where we can leverage text data to discover usable knowledge to optimize our decisionmaking processes.
In sensor “listen” produced real social dia data real monitor relevant ing help improve policy benefit early discovery disaster analyzing real time.	 In general we can imagine building an intelligent sensor system to “listen” to all the text data produced in real time especially social me dia data such as tweets which report realworld events almost in real time and monitor interesting patterns relevant to an application  For example perform ing sentiment analysis on people’s opinions about policies can help better un derstand society’s response to a policy and thus potentially improve the policy if needed  Disaster response and management would benefit early discovery of any warning signs of a natural disaster which is possible through analyzing tweets in real time.
enhance Just allows allows things far era big data “datascope” allow useful knowledge data data unique help “see” kinds knowledge text knowledge thoughts easy kinds	 In general “big data” can enhance our perception  Just as a microscope allows us to see things in the “micro world” and a telescope allows us to see things far away in the era of big data we may envision a “datascope” would allow us to “see” useful hidden knowledge buried in large amounts of data  As a special kind of data text data presents unique opportunities to help us “see” virtually all kinds of knowledge we encode in text especially knowledge about people’s opinions and thoughts which may not be easy to see in other kinds of data.
2 vs.	 12 2 Text vs.
2	2 Text vs.
compare humans subjective physical network	 We can compare humans as subjective sensors to physical sensors such as a network sensor or a thermometer.
monitors signal thermometer temperature world	 Any sensor monitors the real world in some way it senses some signal from the real world and then reports the signal as various forms of data  For example a thermometer would sense the temperature of the real world and then report the temperature as data in a format like Fahrenheit or Celsius.
Similarly GPS	 Similarly a geosensor would sense its geographical location and then report it as GPS coordinates.
Interestingly think real world perspective form data.	 Interestingly we can also think of humans as subjective sensors that observe the real world from their own perspective  Humans express what they have observed in the form of text data.
express	 Humans express what they have observed in the form of text data.
subjective sensor expresses observed illustrated Figure	 In this sense a human is actually a subjective sensor of what is happening in the world who then expresses what’s observed in the form of data—text data  This idea is illustrated in Figure 12.
mining dealing world related dealing non data text data Of usually produced sensors different formats numerical categorical relational.	 In a data mining scenario we would be dealing with data about our world that are related to a particular problem  Most problems would be dealing with both non text data and text data  Of course the nontext data are usually produced by physical sensors and can exist in many different formats such as numerical categorical or relational.
text	 Most problems would be dealing with both non text data and text data.
data usually exist formats numerical categorical Text data knowledge preferences	 Of course the nontext data are usually produced by physical sensors and can exist in many different formats such as numerical categorical or relational  It could even be multimedia data like video or speech  Text data is also very important because they contain knowledge about users especially preferences and opinions.
data video contain users especially opinions.	 It could even be multimedia data like video or speech  Text data is also very important because they contain knowledge about users especially preferences and opinions.
By treating data data data	 By treating text data as data observed from human sensors we can examine all this data together in the same framework.
The data defined turn data knowledge advantage change This Figure 12	 The data mining problem can then be defined as to turn all such data into actionable knowledge that we can take advantage of to change the world for the better  This is illustrated in Figure 12 2.
This Figure	 This is illustrated in Figure 12 2.
	2.
course data need different algorithms particular For require understand video content effective general kinds data useful particular kind data best need develop algorithm.	 Of course for different kinds of data we generally need different algorithms each suitable for mining a particular kind of data  For example video data would require computer vision to understand video content which would facilitate more effective general mining  We also have many general algorithms that are applicable to all kinds of data those algorithms of course are very useful but for a particular kind of data in order to achieve the best mining results we generally would still need to develop a specialized algorithm.
example data require vision content facilitate We general kinds data particular data order achieve results useful mining text data.	 For example video data would require computer vision to understand video content which would facilitate more effective general mining  We also have many general algorithms that are applicable to all kinds of data those algorithms of course are very useful but for a particular kind of data in order to achieve the best mining results we generally would still need to develop a specialized algorithm  This part of the book will cover specialized algorithms that are particularly useful for mining and analyzing text data.
We general course useful order achieve results need specialized algorithm This cover specialized algorithms analyzing Looking closely Figure 12.	 We also have many general algorithms that are applicable to all kinds of data those algorithms of course are very useful but for a particular kind of data in order to achieve the best mining results we generally would still need to develop a specialized algorithm  This part of the book will cover specialized algorithms that are particularly useful for mining and analyzing text data  Looking at the text mining problem more closely in Figure 12.
This book cover algorithms text data Looking text mining 12.	 This part of the book will cover specialized algorithms that are particularly useful for mining and analyzing text data  Looking at the text mining problem more closely in Figure 12.
text mining problem 12 3 similar mining focusing text text knowledge real world especially completing require	 Looking at the text mining problem more closely in Figure 12 3 we see that the problem is similar to general data mining except that we’ll be focusing more on text  We need text mining algorithms to help us turn text data into actionable knowledge that we can use in the real world especially for decision making or for completing whatever tasks require text data support.
3 problem similar we’ll focusing We text data knowledge use world decision require text problems data mining kinds data nontextual.	3 we see that the problem is similar to general data mining except that we’ll be focusing more on text  We need text mining algorithms to help us turn text data into actionable knowledge that we can use in the real world especially for decision making or for completing whatever tasks require text data support  Many realworld problems of data mining also tend to have other kinds of data that are nontextual.
need turn text actionable completing tasks text support.	 We need text mining algorithms to help us turn text data into actionable knowledge that we can use in the real world especially for decision making or for completing whatever tasks require text data support.
Many problems data nontextual.	 Many realworld problems of data mining also tend to have other kinds of data that are nontextual.
With problem definition text	 With this problem definition we can now look at the landscape of the topics in text mining and analytics.
	 12.
mining tasks In highlevel landscape covered Part III Figure 12.	3 Landscape of text mining tasks In this section we provide a highlevel description of the landscape of various text mining tasks which also serves as a roadmap for the topics to be covered in the subsequent chapters in Part III  Figure 12.
4 generating data human world	 Figure 12 4 shows the process of generating text data in more detail  Specifically a human sensor or human observer would look at the world from some perspective.
process observer look Different looking world different angles pay	4 shows the process of generating text data in more detail  Specifically a human sensor or human observer would look at the world from some perspective  Different people would be looking at the world from different angles and they’ll pay attention to different things.
people angles things The aspects world.	 Different people would be looking at the world from different angles and they’ll pay attention to different things  The same person at different times might also pay attention to different aspects of the observed world.
person times aspects observed	 The same person at different times might also pay attention to different aspects of the observed world.
world	 Each human—a sensor— would then form their own view of the world  This would be different from the real world because the perspective that the person has taken can often be biased.
This different world	 This would be different from the real world because the perspective that the person has taken can often be biased.
The world represented entityrelation This mind	 The observed world can be represented as for example entityrelation graphs or using a knowledge representation language  This is basically what a person has in mind about the world.
The language result text mixed languages text data world observed	 The human expresses what is observed using a natural language such as English the result is text data  In some cases we might have text data of mixed languages or different languages  The main goal of text mining is to reverse this process of generating text data and uncover various knowledge about the real world as it was observed by the human sensor.
In text data mixed languages different	 In some cases we might have text data of mixed languages or different languages.
mining tasks knowledge natural language.	4 we can distinguish four types of text mining tasks  Mining knowledge about natural language.
natural Since text potentially usage natural For written usages synonyms colloquialisms.	 Mining knowledge about natural language  Since the observed text is written in a particular language by mining the text data we can potentially mine knowledge about the usage of the natural language itself  For example if the text is written in English we may be able to discover knowledge about English such as usages collocations synonyms and colloquialisms.
Since observed text written particular language potentially For example knowledge English usages collocations synonyms Mining	 Since the observed text is written in a particular language by mining the text data we can potentially mine knowledge about the usage of the natural language itself  For example if the text is written in English we may be able to discover knowledge about English such as usages collocations synonyms and colloquialisms  Mining knowledge about the observed world.
For example written English discover knowledge synonyms colloquialisms knowledge This content text extracting major statements text data turn text quality information	 For example if the text is written in English we may be able to discover knowledge about English such as usages collocations synonyms and colloquialisms  Mining knowledge about the observed world  This has much to do with mining the content of text data focusing on extracting the major statements in the text data and turn text data into high quality information about a particular aspect of the world that we’re interested in.
This focusing statements text turn data quality interested	 This has much to do with mining the content of text data focusing on extracting the major statements in the text data and turn text data into high quality information about a particular aspect of the world that we’re interested in.
Chapter Overview Analysis This regarded author’s Mining observers text text humans subjective unique particular observer	 248 Chapter 12 Overview of Text Data Analysis This can be regarded as mining content to describe the observed world in the author’s mind  Mining knowledge about the observers text producers  Since humans are sub jective sensors the text data expressed by humans often contain subjective statements and opinions that may be unique to the particular human observer text producers.
jective expressed humans opinions unique observer proper ties authors mood issue.	 Since humans are sub jective sensors the text data expressed by humans often contain subjective statements and opinions that may be unique to the particular human observer text producers  Thus we can potentially mine text data to infer some proper ties of the authors that produced the text data such as the mood or sentiment of the person toward an issue.
potentially text data text sentiment issue.	 Thus we can potentially mine text data to infer some proper ties of the authors that produced the text data such as the mood or sentiment of the person toward an issue.
left figure text allow infer interesting world variables correlation text	 On the left side of the figure we illustrate that text mining can also allow us to infer values of interesting real world variables by leveraging the correlation of the values of such variables and the content in text data.
company stock price leveraged perform forecasting use text basis variables data e.	 a positive earnings report of a company may be correlated with the increase of the stock price of the company  Such correlations can be leveraged to perform textbased forecasting where we use text data as a basis for prediction of other variables that may only be remotely related to text data e.
	g.
applications predictions events	 prediction of stock prices  Inference about unknown factors that affect decision making can have many applications especially if we can make predictions about future events i e.
unknown factors affect decision especially predictions events	 Inference about unknown factors that affect decision making can have many applications especially if we can make predictions about future events i.
predictive analytics infer realworld beneficial mining algorithms predictive basic features generate data.	e  textbased predictive analytics  Note that when we infer other realworld variables it is often possible and beneficial to leverage the results of all kinds of text mining algorithms to generate more effective features for use in a predictive model than the basic features we can generate directly from the original text data.
predictive analytics Note infer realworld variables possible beneficial leverage results algorithms generate effective use predictive directly original For data discover topics topics i.	 textbased predictive analytics  Note that when we infer other realworld variables it is often possible and beneficial to leverage the results of all kinds of text mining algorithms to generate more effective features for use in a predictive model than the basic features we can generate directly from the original text data  For example if we can mine text data to discover topics we would be able to use topics i.
infer realworld variables results mining generate effective use predictive features generate text For example discover able use	 Note that when we infer other realworld variables it is often possible and beneficial to leverage the results of all kinds of text mining algorithms to generate more effective features for use in a predictive model than the basic features we can generate directly from the original text data  For example if we can mine text data to discover topics we would be able to use topics i e.
semantically features Since address variations usages semantic features effective wordlevel features	 a set of semantically related words rather than individual words as features  Since topics can address the issue of word sense ambiguity and variations of word usages when discussing a topic such highlevel semantic features can be expected to be more effective than wordlevel features for prediction.
topics usages discussing topic highlevel wordlevel example user based	 Since topics can address the issue of word sense ambiguity and variations of word usages when discussing a topic such highlevel semantic features can be expected to be more effective than wordlevel features for prediction  Another example is to predict what products may be liked by a user based on what the user has said in text data e.
	g.
reviews case mining knowledge clearly	 reviews in which case the results from mining knowledge about the observer would clearly be very useful for prediction.
want prices historical stock presumably best data discussions news articles improvement additional features Nontext data context opportunities contextsensitive knowledge text data	 For example if you want to predict stock prices or changes of stock prices the historical stock price data are presumably the best data to use for prediction even though online discussions news articles or social media may also be useful for further improvement of prediction accuracy by contributing additional effective features computed based on text data which would be combined with nontext features  Nontext data can also be used for analyzing text by supplying context thus opening up many interesting opportunities to mine contextsensitive knowledge from text data i.
analyzing text supplying context interesting knowledge data	 Nontext data can also be used for analyzing text by supplying context thus opening up many interesting opportunities to mine contextsensitive knowledge from text data i e.
data data	e  associating the knowledge discovered from text data with the nontext data e g.
	g.
associating topics discovered text topics text data we’ll looking content However data context associated it.	 associating topics discovered from text with time would generate temporal trends of topics  When we look at the text data alone we’ll be mostly looking at the content or opinions expressed in the text  However text data generally also has context associated with it.
For location production useful values text	 For example the time and the location of the production of the text data are both useful “metadata” values of a text document.
angles text time availability	 This context can provide interesting angles for analyzing text data we might partition text data into different time periods because of the availability of the time.
text data that’s associated areas perspectives	 Similarly we can partition text data based on location or any other metadata that’s associated with it to form interesting comparisons in those areas  In this sense nontext data can provide interesting angles or perspectives for text data analysis.
In sense nontext perspectives analysis contextsensitive content opinions authors text	 In this sense nontext data can provide interesting angles or perspectives for text data analysis  It can help us make contextsensitive analysis of content language usage or opinions about the observer or the authors of text data.
authors text	 It can help us make contextsensitive analysis of content language usage or opinions about the observer or the authors of text data.
This general text	 This is a fairly general landscape of the topics in text mining and analytics.
In selectively representative different text	 In this book we will selectively cover some of those topics that are representative of the different kinds of text mining tasks.
natural language learning data—important steps task.	 Chapters 2 and 3 already covered natural language processing and the basics of machine learning which allow us to under stand represent and classify text data—important steps in any text mining task.
First associations Chapter 13 knowledge association text objects Chapter 14 This similar	 First we will discuss how to mine word associations from text data Chapter 13 revealing lexical knowledge about language  After word association mining we will look at clustering text objects Chapter 14  This groups similar objects together allowing exploratory analysis among many other applications.
look clustering text Chapter 15 expands machine learning Chapter	 After word association mining we will look at clustering text objects Chapter 14  This groups similar objects together allowing exploratory analysis among many other applications  Chapter 15 covers text categorization which expands on the introduction to machine learning given in Chapter 2.
expands machine Chapter We different text Chapter 16.	 Chapter 15 covers text categorization which expands on the introduction to machine learning given in Chapter 2  We also explore different methods of text summarization Chapter 16.
text 16 Next we’ll analysis This analyze content it’s tions.	 We also explore different methods of text summarization Chapter 16  Next we’ll discuss topic mining and analysis Chapter 17  This is only one way to analyze content of text but it’s very useful and used in a wide array of applica tions.
Next we’ll mining 17 This way analyze content text array	 Next we’ll discuss topic mining and analysis Chapter 17  This is only one way to analyze content of text but it’s very useful and used in a wide array of applica tions.
briefly discuss textbased prediction try predict variable text number cuttingedge research results perform analysis nontext data	 Finally we will briefly discuss textbased prediction problems where we try to predict some realworld variable based on text data and present a number of cuttingedge research results on how to perform joint analysis of text and nontext data Chapter 19.
example knowledge relations useful.	 This is an example of knowledge about the natural language that we can mine from text data  We’ll first talk about what word association is and then explain why discovering such relations is useful.
useful discuss word types called paradigmatic lation relation.	 We’ll first talk about what word association is and then explain why discovering such relations is useful  Then we’ll discuss some general ideas about how to mine word associations  In general there are two types of word relations one is called a paradigmatic re lation and the other is a syntagmatic relation.
general associations types lation relation.	 Then we’ll discuss some general ideas about how to mine word associations  In general there are two types of word relations one is called a paradigmatic re lation and the other is a syntagmatic relation.
types paradigmatic syntagmatic relation.	 In general there are two types of word relations one is called a paradigmatic re lation and the other is a syntagmatic relation.
Word wa relation substituted class syntactic	 Word wa and wb have a paradigmatic relation if they can be substituted for each other  That means the two words that have paradigmatic relation would be in the same semantic class or syntactic class.
As relation word animal cat dog sentence sentence Tuesday	 As an example the words cat and dog have a paradigmatic relation because they are in the same word class animal  If you replace cat with dog in a sentence the sentence would still be mostly comprehensible  Similarly Monday and Tuesday have a paradigmatic relation.
replace cat dog sentence	 If you replace cat with dog in a sentence the sentence would still be mostly comprehensible.
Similarly paradigmatic relation.	 Similarly Monday and Tuesday have a paradigmatic relation.
second called relation words Thus related.	 The second kind of relation is called a syntagmatic relation  In this case the two words that have this relation can be combined with each other  Thus wa and wb have a syntagmatic relation if they can be combined with each other in a grammatical sentence—meaning that these two words are semantically related.
wb related cat sit related cat usually	 In this case the two words that have this relation can be combined with each other  Thus wa and wb have a syntagmatic relation if they can be combined with each other in a grammatical sentence—meaning that these two words are semantically related  For example cat and sit are related because a cat can sit somewhere usually anywhere they please.
Thus syntagmatic relation combined grammatical cat sit related cat sit	 Thus wa and wb have a syntagmatic relation if they can be combined with each other in a grammatical sentence—meaning that these two words are semantically related  For example cat and sit are related because a cat can sit somewhere usually anywhere they please.
example cat cat sit usually car drive combined convey	 For example cat and sit are related because a cat can sit somewhere usually anywhere they please  Similarly car and drive are related semantically because they can be combined with each other to convey some meaning.
car drive related However sit sentence sentence.	 Similarly car and drive are related semantically because they can be combined with each other to convey some meaning  However we cannot replace cat with sit in a sentence or car with drive in the sentence and still have a valid sentence.
Therefore previous words syntagmatic paradigmatic relation.	 Therefore the previous pairs of words have a syntagmatic relation and not a paradigmatic relation.
They alized items wb don’t phrases entities think think	 They can be gener alized to describe relations of any items in a language that is wa and wb don’t have to be words  They could be phrases or entities  If you think about the general problem of sequence mining then we can think about any units being words.
They sequence mining think relations sentence data general.	 They could be phrases or entities  If you think about the general problem of sequence mining then we can think about any units being words  We think of paradigmatic relations as relations that are applied to units that tend to occur in a similar location in a sentence or a sequence of data elements in general.
We applied units location sentence sequence elements general.	 We think of paradigmatic relations as relations that are applied to units that tend to occur in a similar location in a sentence or a sequence of data elements in general.
So measures complimentary interested discovering data word relations	 So these two measures are complimentary and we’re interested in discovering them automatically from text data  Discovering such word relations has many applications.
relations applications.	 Discovering such word relations has many applications.
First useful NLP capture synonyms different	 First such relations can be directly useful for improving accuracy of many NLP tasks and this is because these relations capture some knowledge about language  If you know two words are synonyms for example that would help with many different tasks.
If learn rules expression	 If we learn syntagmatic relations then we would be able to know the rules for putting together a larger expression based on component expressions by learning the sentence structure.
	 Word relations can be also very useful for many applications in text retrieval and mining.
saw	 As we saw in Chapter 7 this is often called query expansion.
use words suggest queries explore information Yet application word automatically browsing associations edges user information.	 We can also use related words to suggest related queries to a user to explore the information space  Yet another application is to use word associations to automatically construct a hierarchy for browsing  We can have words as nodes and associations as edges allowing a user to navigate from one word to another to find information.
Yet application use We nodes associations allowing	 Yet another application is to use word associations to automatically construct a hierarchy for browsing  We can have words as nodes and associations as edges allowing a user to navigate from one word to another to find information.
nodes associations allowing user navigate information associations compare We understanding	 We can have words as nodes and associations as edges allowing a user to navigate from one word to another to find information  Finally such word associations can also be used to compare and summarize opinions  We might be interested in understanding positive and negative opinions about a new smartphone.
associations compare summarize opinions understanding positive opinions order vs.	 Finally such word associations can also be used to compare and summarize opinions  We might be interested in understanding positive and negative opinions about a new smartphone  In order to do that we can look at what words are most strongly associated with a feature word like battery in positive vs.
We positive negative opinions new smartphone.	 We might be interested in understanding positive and negative opinions about a new smartphone.
negative reviews.	 negative reviews.
	 13.
1 associations paradigmatic	1 General idea of word association mining So how can we discover such associations automatically Let’s first look at the paradigmatic relation.
shows dog words context paradigmatic relation.	1 shows a simple example using the words dog and cat  Generally we see the two words occur in similar context  After all that is the definition of a paradigmatic relation.
Generally words relation figure context text	 Generally we see the two words occur in similar context  After all that is the definition of a paradigmatic relation  On the right side of the figure we extracted the context of cat and dog from this small sample of text data.
paradigmatic On right extracted cat dog text data We different look	 After all that is the definition of a paradigmatic relation  On the right side of the figure we extracted the context of cat and dog from this small sample of text data  We can have different perspectives to look at the context.
On figure cat small sample text	 On the right side of the figure we extracted the context of cat and dog from this small sample of text data.
That words cat context In look words cat right case In general follow cat dog.	 That is what words occur before we see cat or dog Clearly these two words have a similar left context  In the same sense if you look at the words that occur after cat and dog the right context we see that they are also very similar in this case  In general we’ll see many other words that can follow both cat and dog.
sense look occur right similar case general dog.	 In the same sense if you look at the words that occur after cat and dog the right context we see that they are also very similar in this case  In general we’ll see many other words that can follow both cat and dog.
general words We context includes word.	 In general we’ll see many other words that can follow both cat and dog  We can even look at the general context this includes all the words in the sentence or in sentences around this word.
context words Examining context general words consider	 Even in the general context there is also similarity between the two words  Examining context is a general way of discovering paradigmatic words  Let’s consider the following questions.
context discovering	 Examining context is a general way of discovering paradigmatic words.
Let’s consider following	 Let’s consider the following questions.
cat cat context cat That case similarity value second low.	 How similar is the context of cat and dog In contrast how similar are the contexts of cat and computer Intuitively the context of cat and the context of dog would be more similar than the context of cat and computer  That means in the first case the similarity value would be high and in the second the similarity would be low.
similarity value high low.	 That means in the first case the similarity value would be high and in the second the similarity would be low.
This idea relations capture relations explore occurrences based definition syntagmatic relations Figure	 This is the basic idea of what paradigmatic relations capture  For syntagmatic relations we’re going to explore correlated occurrences again based on the definition of syntagmatic relations  Figure 13.
For syntagmatic we’re based relations text	 For syntagmatic relations we’re going to explore correlated occurrences again based on the definition of syntagmatic relations  Figure 13 2 shows the same sam ple of text as the example before.
Figure 2 example	 Figure 13 2 shows the same sam ple of text as the example before.
right taken away	 On the right side of the figure we’ve taken away the two words around eats.
words tend tend eats question words eats example help meat	 Then we ask the question what words tend to occur to the left of eats What words tend to occur to the right of eats Therefore the question here has to do with whether there are some other words that tend to cooccur with eats  For example knowing whether eats occurs in a sentence would generally help us predict whether meat also occurs.
example eats help occurs like words eats occur sentence increase chance occur.	 For example knowing whether eats occurs in a sentence would generally help us predict whether meat also occurs  This is the intuition we would like to capture  In other words if we see eats occur in the sentence that should increase the chance that meat would also occur.
In words eats occur increase chance occur In look question helpful occur rence text eats text eats sentence doesn’t help predict occurs	 This is the intuition we would like to capture  In other words if we see eats occur in the sentence that should increase the chance that meat would also occur  In contrast if you look at the question at the bottom how helpful is the occur rence of eats for predicting an occurrence of text Because eats and text are not really related knowing whether eats occurred in the sentence doesn’t really help us predict whether text also occurs in the sentence.
sentence chance helpful occur occurrence Because text related eats help predict text	 In other words if we see eats occur in the sentence that should increase the chance that meat would also occur  In contrast if you look at the question at the bottom how helpful is the occur rence of eats for predicting an occurrence of text Because eats and text are not really related knowing whether eats occurred in the sentence doesn’t really help us predict whether text also occurs in the sentence.
In contrast question occur eats related sentence Essentially words.	 In contrast if you look at the question at the bottom how helpful is the occur rence of eats for predicting an occurrence of text Because eats and text are not really related knowing whether eats occurred in the sentence doesn’t really help us predict whether text also occurs in the sentence  Essentially we need to capture the correlation between the occurrences of two words.
Essentially capture correlation occurrences summary relations consider word	 Essentially we need to capture the correlation between the occurrences of two words  In summary paradigmatic relations consider each word by its context and we can compute the context similarity.
In summary paradigmatic context compute similarity words similarity paradigmatic relation For syntagmatic times context	 In summary paradigmatic relations consider each word by its context and we can compute the context similarity  We assume the words that have high context similarity will have a high paradigmatic relation  For syntagmatic relations we will count how many times two words occur together in a context which can be a sentence a paragraph or even a document.
assume similarity relation For relations context sentence document.	 We assume the words that have high context similarity will have a high paradigmatic relation  For syntagmatic relations we will count how many times two words occur together in a context which can be a sentence a paragraph or even a document.
syntagmatic count context paragraph	 For syntagmatic relations we will count how many times two words occur together in a context which can be a sentence a paragraph or even a document.
We individual occurrences usually Note closely related related	 We compare their cooccurrences with their individual occurrences  We assume words with high cooccurrences but relatively low individual occurrences will have a syntagmatic relation because they tend to occur together and they don’t usually occur alone  Note that paradigmatic relations and syntagmatic relations are closely related in that paradigmatically related words tend to have a syntagmatic relation with the same word.
We relatively low individual syntagmatic occur occur	 We assume words with high cooccurrences but relatively low individual occurrences will have a syntagmatic relation because they tend to occur together and they don’t usually occur alone.
13 relations share context.	 13 2 Discovery of paradigmatic relations By definition two words are paradigmatically related if they share a similar context.
2 Discovery paradigmatic words related share context Naturally discovering relation context word try contexts	2 Discovery of paradigmatic relations By definition two words are paradigmatically related if they share a similar context  Naturally our idea of discovering such a relation is to look at the context of each word and then try to compute the similarity of those contexts  In Figure 13.
3 word remaining words cat cooccur We like dog.	3 we have taken the word cat out of its context  The remaining words in the sentences that contain cat are the words that tend to cooccur with it  We can do the same thing for another word like dog.
remaining contain word like capture assess cat context like dog.	 The remaining words in the sentences that contain cat are the words that tend to cooccur with it  We can do the same thing for another word like dog  In general we would like to capture such contexts and then try to assess the similarity of the context of cat and the context of a word like dog.
general like contexts try similarity cat like	 In general we would like to capture such contexts and then try to assess the similarity of the context of cat and the context of a word like dog.
question First context contains regarded document different ways context.	 The question is how to formally represent the context and define the similarity function between contexts  First we note that the context contains many words  These words can be regarded as a pseudo document but there are also different ways of looking at the context.
document ways looking	 First we note that the context contains many words  These words can be regarded as a pseudo document but there are also different ways of looking at the context.
look word context left	 For example we can look at the word that occurs before the word cat  We call this context the left context L.
context left context In case words like big on.	 We call this context the left context L  In this case we will see words like my his big a the and so on.
like eats	 Here we see words like eats ate is and has.
word left form bag words sentation define perspective	 These word contexts from the left or from the right form a bag of words repre sentation  Such a wordbased representation would actually give us a useful way to define the perspective of measuring context similarity.
Such wordbased representation useful way perspective measuring L	 Such a wordbased representation would actually give us a useful way to define the perspective of measuring context similarity  For example we can com pare only the L context the R context or both.
For com context A contain like Saturday Tuesday.	 For example we can com pare only the L context the R context or both  A context may contain adjacent words like eats and my or nonadjacent words like Saturday or Tuesday.
context contain like words flexi allows match somewhat ways We similarity based yields loosely paradig matic relations.	 A context may contain adjacent words like eats and my or nonadjacent words like Saturday or Tuesday  This flexi bility allows us to match the similarity in somewhat different ways  We might want to capture similarity based on general content which yields loosely related paradig matic relations.
This match similarity different	 This flexi bility allows us to match the similarity in somewhat different ways.
capture similarity based content yields loosely related relations.	 We might want to capture similarity based on general content which yields loosely related paradig matic relations.
discovering context words For example similarity Figure	 Thus the general idea of discovering paradigmatic relations is to compute the similarity of context of two words  For example we can measure the similarity of cat and dog based on the similarity of their context as shown in Figure 13.
For example dog similarity function combination similarities texts weights different focus particular kind context.	 For example we can measure the similarity of cat and dog based on the similarity of their context as shown in Figure 13 4  The similarity function can be a combination of similarities on different con texts and we can assign weights to these different similarities to allow us to focus more on a particular kind of context.
4.	4.
The combination texts different similarities allow applicationspecific idea discovering words	 The similarity function can be a combination of similarities on different con texts and we can assign weights to these different similarities to allow us to focus more on a particular kind of context  Naturally this would be applicationspecific but again the main idea for discovering pardigmatically related words is to com pute the similarity of their contexts.
Unsurprisingly bagofwords context context word relation discovery general pseudo document dog context d2	 Unsurprisingly we can use the vector space model on bagofwords context data to model the context of a word for paradigmatic relation discovery  In general we can represent a pseudo document or context of cat as one frequency vector d1 and another word dog would give us a different context d2  We can then measure the similarity of these two vectors.
We similarity	 We can then measure the similarity of these two vectors.
context paradigmatic relation discovery problem computing similarity The address compute	 By viewing context in the vector space model we convert the problem of paradigmatic relation discovery into the problem of computing the vectors and their similarity  The two questions that we have to address are how to compute each vector and how to compute their similarity.
similarity problem retrieval They matching vector.	 The two questions that we have to address are how to compute each vector and how to compute their similarity  There are many approaches that can be used to solve the problem and most of them are developed for information retrieval  They have been shown to work well for matching a query vector and a document vector.
problem retrieval.	 There are many approaches that can be used to solve the problem and most of them are developed for information retrieval.
They matching document ideas compute context documents purpose Figure	 They have been shown to work well for matching a query vector and a document vector  We can adapt many of the ideas to compute a similarity of context documents for our purpose  Figure 13.
We ideas similarity	 We can adapt many of the ideas to compute a similarity of context documents for our purpose  Figure 13.
shows match based overlap words word Equivalently given x xi defined normalized word pick word d1.	5 shows one plausible approach where we match the similarity of con text based on the expected overlap of words and we call this EOW  We represent a context by a word vector where each word has a weight that’s equal to the proba bility that a randomly picked word from this document vector is the current word  Equivalently given a document vector x xi is defined as the normalized account of word wi in the context and this can be interpreted as the probability that you would randomly pick this word from d1.
that’s bility picked document vector given vector normalized account wi probability randomly pick d1 The xi’s sum normalized vector probability distribution words.	 We represent a context by a word vector where each word has a weight that’s equal to the proba bility that a randomly picked word from this document vector is the current word  Equivalently given a document vector x xi is defined as the normalized account of word wi in the context and this can be interpreted as the probability that you would randomly pick this word from d1  The xi’s would sum to one because they are normalized frequencies which means the vector is a probability distribution over words.
Equivalently x defined account word wi interpreted The sum normalized vector	 Equivalently given a document vector x xi is defined as the normalized account of word wi in the context and this can be interpreted as the probability that you would randomly pick this word from d1  The xi’s would sum to one because they are normalized frequencies which means the vector is a probability distribution over words.
sum normalized means probability	 The xi’s would sum to one because they are normalized frequencies which means the vector is a probability distribution over words.
d2 way	 The vector d2 can be computed in the same way and this would give us then two probability distributions representing two contexts.
This	 This addresses the problem of how to compute the vectors.
That means try context word context question expect frequently If different chance identical contexts small This measuring similarity	 That means if we try to pick a word from one context and try to pick another word from another context we can then ask the question are they identical If the two contexts are very similar then we should expect we frequently will see the two words picked from the two contexts are identical  If they are very different then the chance of seeing identical words being picked from the two contexts would be small  This is quite intuitive for measuring similarity of contexts.
This	 This is quite intuitive for measuring similarity of contexts.
look exact formulas interpreted picked words	 Let’s look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical.
This context similarity As like assess approach work related	 This is how expected overlap of words in context similarity works  As always we would like to assess whether this approach would work well  Ul timately we have to test the approach with real data and see if it gives us really semantically related words.
As assess work	 As always we would like to assess whether this approach would work well.
formula lap contexts.	 Analytically we can also analyze this formula  Initially it does make sense because this formula will give a higher score if there is more over lap between the two contexts.
The problem favor frequent high shared contexts contributes overall It actually	 The first problem is that it might favor matching one frequent term very well over matching more distinct terms  That is because in the dot product if one element has a high value and this element is shared by both contexts it contributes a lot to the overall sum  It might indeed make the score higher than in another case where the two vectors actually have much overlap in different terms.
element value element shared It higher actually overlap different case case different saying similar context.	 That is because in the dot product if one element has a high value and this element is shared by both contexts it contributes a lot to the overall sum  It might indeed make the score higher than in another case where the two vectors actually have much overlap in different terms  In our case we should intuitively prefer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context.
higher overlap terms In intuitively match confidence words	 It might indeed make the score higher than in another case where the two vectors actually have much overlap in different terms  In our case we should intuitively prefer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context.
If highscoring term second problem treats If match word like matching word like know surprising occurs	 If you only rely on one highscoring term it may not be robust  The second problem is that it treats every word equally  If we match a word like the it will be the same as matching a word like eats although we know matching the isn’t really surprising because it occurs everywhere.
If occurs everywhere.	 If we match a word like the it will be the same as matching a word like eats although we know matching the isn’t really surprising because it occurs everywhere.
text retrieval prob lems problems like occur query docu	 This is another problem of this approach  We can introduce some heuristics used in text retrieval that solve these prob lems since problems like these also occur when we match a query with a docu ment.
We introduce heuristics text solve problems occur match docu tackle use	 We can introduce some heuristics used in text retrieval that solve these prob lems since problems like these also occur when we match a query with a docu ment  To tackle the first problem we can use a sublinear transformation of term frequency.
tackle problem term frequency That raw frequency count address problem raw	 To tackle the first problem we can use a sublinear transformation of term frequency  That is we don’t have to use the raw frequency count of the term to repre sent the context  To address this problem we can transform it into some form that wouldn’t emphasize the raw frequency so much.
That don’t frequency count sent context.	 That is we don’t have to use the raw frequency count of the term to repre sent the context.
address transform wouldn’t raw frequency address second problem word.	 To address this problem we can transform it into some form that wouldn’t emphasize the raw frequency so much  To address the second problem we can reward matching a rare word.
address problem rare A transformation term document frequency IDF we’d like Chapter	 To address the second problem we can reward matching a rare word  A sublinear transformation of term frequency and inverse document frequency IDF weighting are exactly what we’d like here we discussed these types of weighting schemes in Chapter 6.
achieve course based function.	 In order to achieve this desired weighting we will use BM25 weighting which is of course based on the BM25 retrieval function.
document containing normalized BM25 shown 13.	 For this similarity scheme we define the document vector as containing ele ments representing normalized BM25 TF values as shown in Figure 13.
6 normalization takes words order	6  The normalization function takes a sum over all the words in order to normalize the weight of each word by the sum of the weights of all the words.
normalization function takes weight word words.	 The normalization function takes a sum over all the words in order to normalize the weight of each word by the sum of the weights of all the words.
This vector word distribution	 This is to ensure all the xi’s will sum to one in this vector  This would be very similar to what we had before in that this vector approximates a word distribution since the xi’s will sum to one.
This similar vector sum similarity function xiyi similarity sublinear scaling representation problem lack IDF addressed similarity itself.	 This would be very similar to what we had before in that this vector approximates a word distribution since the xi’s will sum to one  For the IDF factor the similarity function multiplies the IDF of word wi by xiyi which is the similarity in the ith dimension  Thus the first problem sublinear scaling is addressed in the vector representation and the second problem lack of IDF is addressed in the similarity function itself.
problem scaling second problem function itself.	 Thus the first problem sublinear scaling is addressed in the vector representation and the second problem lack of IDF is addressed in the similarity function itself.
We use resent represent context term weights terms weights.	 We can also use this approach to discover syntagmatic relations  When we rep resent a term vector to represent a context with a term vector we would likely see some terms have higher weights and other terms have lower weights.
Depending words word idea	 Depending on how we assign weights to these terms we might be able to use these weights to dis cover the words that are strongly associated with a candidate word in the context  The idea is to use the converted representation of the context to see which terms are scored high.
converted representation terms If term strongly related candidate	 The idea is to use the converted representation of the context to see which terms are scored high  If a term has high weight then that term might be more strongly related to the candidate word.
high weight term related	 If a term has high weight then that term might be more strongly related to the candidate word.
xi defined normalized weight weight occurs context frequent term context candidate word common	 We have each xi defined as a normalized weight of BM25  This weight alone reflects how frequently the wi occurs in the context  We can’t simply say a frequent term in the context would be correlated with the candidate word because many common words like the will occur frequently in the context.
	 This weight alone reflects how frequently the wi occurs in the context.
However apply weighting reweight terms common IDFs.	 However if we apply IDF weighting we can then reweight these terms based on IDF  That means the words that are common like the will get penalized  Now the highestweighted terms will not be those common terms because they have lower IDFs.
means words like penalized highestweighted common lower	 That means the words that are common like the will get penalized  Now the highestweighted terms will not be those common terms because they have lower IDFs.
highestweighted common lower Instead weighted terms word.	 Now the highestweighted terms will not be those common terms because they have lower IDFs  Instead the highly weighted terms would be the terms that are frequently in the context but not frequent in the collection  Clearly these are the words that tend to occur in the context of the candidate word.
Instead highly frequently context collection tend occur context	 Instead the highly weighted terms would be the terms that are frequently in the context but not frequent in the collection  Clearly these are the words that tend to occur in the context of the candidate word.
candidate word.	 Clearly these are the words that tend to occur in the context of the candidate word.
In discover relations	 In the next section we’ll talk more about how to discover syntagmatic relations in particular.
Indeed word relations discovered joint manner This shows connec tions discovery paradigmatic	 Indeed these two word relations may be discovered in a joint manner by leveraging such associations  This also shows some interesting connec tions between the discovery of syntagmatic relations and paradigmatic relations.
This interesting discovery syntagmatic relations relations Specifically paradigmatically related syntagmatic word To computing paradigmatic collect	 This also shows some interesting connec tions between the discovery of syntagmatic relations and paradigmatic relations  Specifically words that are paradigmatically related tend to have a syntagmatic re lation with the same word  To summarize the main idea of computing paradigmatic relations is to collect the context of a candidate word to form a pseudo document which is typically represented as a bag of words.
Specifically words paradigmatically related syntagmatic word summarize collect form bag	 Specifically words that are paradigmatically related tend to have a syntagmatic re lation with the same word  To summarize the main idea of computing paradigmatic relations is to collect the context of a candidate word to form a pseudo document which is typically represented as a bag of words.
similarity corresponding documents candidate words highly similar words contexts.	 We then compute the similarity of the corresponding context documents of two candidate words highly similar word pairs have the highest paradigmatic relations i e  the words that share similar contexts.
words share There different ways implement general talked	e  the words that share similar contexts  There are many different ways to implement this general idea but we just talked about a few of the approaches.
words share ways talked	 the words that share similar contexts  There are many different ways to implement this general idea but we just talked about a few of the approaches.
syntagmatic discovered relations Relations correlated	 Finally syntagmatic relations can also be discovered as a byproduct when we discover paradigmatic relations  13 3 Discovery of Syntagmatic Relations There are strong syntagmatic relations between words that have correlated co occurrences.
13 3 Discovery Syntagmatic correlated occurrences.	 13 3 Discovery of Syntagmatic Relations There are strong syntagmatic relations between words that have correlated co occurrences.
That occur tend	 That means when we see one word occur in some context we tend to see the other word.
	7.
We ques eats words occur Looking left words eats If remove surrounded eats occur eats More specifically problem text sentence likely specific	 We can ask the ques tion whenever eats occurs what other words also tend to occur Looking at the sentences on the left we see some words that might occur together with eats like cat dog or fish  If we remove them and look at where we only show eats surrounded by two blanks can we predict what words occur to the left or to the right If these words are associated with eats they tend to occur in the context of eats  More specifically our prediction problem is to take any text segment which can be a sentence paragraph or document and determine what words are most likely to cooccur in a specific context.
remove left right associated tend context	 If we remove them and look at where we only show eats surrounded by two blanks can we predict what words occur to the left or to the right If these words are associated with eats they tend to occur in the context of eats.
More specifically sentence document cooccur particular	 More specifically our prediction problem is to take any text segment which can be a sentence paragraph or document and determine what words are most likely to cooccur in a specific context  Let’s consider a particular word w.
Let’s Is 13 8 Some words actually predict figure think easier predict If think moment tends occur everywhere.	 Let’s consider a particular word w  Is w present or absent in the segment from Figure 13 8 Some words are actually easier to predict than other words—if you take a look at the three words shown in the figure meat the and unicorn which one do you think is easier to predict If you think about it for a moment you might conclude that the is easier to predict because it tends to occur everywhere.
8 words actually look figure meat easier think moment predict tends The word unicorn unicorn	8 Some words are actually easier to predict than other words—if you take a look at the three words shown in the figure meat the and unicorn which one do you think is easier to predict If you think about it for a moment you might conclude that the is easier to predict because it tends to occur everywhere  The word unicorn is also relatively easy to predict because unicorn is rare.
unicorn relatively frequency possible segment.	 The word unicorn is also relatively easy to predict because unicorn is rare  However meat is somewhere in between in terms of frequency making it harder to predict since it’s possible that it occurs in the segment.
discussion Earlier easy predict presence We random w	 Recall our discussion of entropy from Chapter 2  Earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word  We can create a random variable Xw for a particular word w that depicts whether w occurs.
easy presence word particular word depicts occurs related	 Earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word  We can create a random variable Xw for a particular word w that depicts whether w occurs  Clearly this is related to the previous question.
variable word depicts Clearly	 We can create a random variable Xw for a particular word w that depicts whether w occurs  Clearly this is related to the previous question.
useful discovering syntagmatic scenario assume know	 Clearly this is related to the previous question  Here we will further talk about conditional entropy which is useful for discovering syntagmatic relations  Now we’ll address a different scenario where we assume that we know some thing about the random variable.
talk entropy relations Now we’ll address scenario know	 Here we will further talk about conditional entropy which is useful for discovering syntagmatic relations  Now we’ll address a different scenario where we assume that we know some thing about the random variable.
Now we’ll address scenario assume know thing	 Now we’ll address a different scenario where we assume that we know some thing about the random variable.
occurred segment How presence absence word like meat If frame question entropy mean knowing reduce uncertainty In reduce random presence absence meat Would help predict absence addressed conditional	 That is suppose we know that eats occurred in the segment  How would that help us predict the presence or absence of a word like meat If we frame this question using entropy that would mean we are inter ested in knowing whether knowing the presence of eats could reduce uncertainty about meat  In other words can we reduce the entropy of the random variable cor responding to the presence or absence of meat What if we know of the absence of eats Would that also help us predict the presence or absence of meat These questions can be addressed by using conditional entropy.
predict presence like meat frame entropy eats uncertainty reduce cor responding presence meat What eats predict presence meat These	 How would that help us predict the presence or absence of a word like meat If we frame this question using entropy that would mean we are inter ested in knowing whether knowing the presence of eats could reduce uncertainty about meat  In other words can we reduce the entropy of the random variable cor responding to the presence or absence of meat What if we know of the absence of eats Would that also help us predict the presence or absence of meat These questions can be addressed by using conditional entropy.
words reduce random variable cor What know absence Would meat questions entropy.	 In other words can we reduce the entropy of the random variable cor responding to the presence or absence of meat What if we know of the absence of eats Would that also help us predict the presence or absence of meat These questions can be addressed by using conditional entropy.
probabilities indicating doesn’t segment We entropy looks	 We have probabilities indicating whether a word occurs or doesn’t occur in the segment  We have an entropy function that looks like the one in Figure 13.
function	 We have an entropy function that looks like the one in Figure 13 9.
Xeats If probabilities corresponding entropy function we’ll presence eats essentially function	 That is we have pXmeat  Xeats  1  If we replace these probabilities with their corresponding conditional probabilities in the entropy function we’ll get the conditional entropy conditioned on the presence of eats  This is essentially the same entropy function as before except that all the probabilities now have a condition.
essentially entropy function occurs	 This is essentially the same entropy function as before except that all the probabilities now have a condition  This then tells us the entropy of meat after we have known eats occurs in the segment.
This conditional eats Now putting captures condi equal 1 0 present absent.	 This then tells us the entropy of meat after we have known eats occurs in the segment  Of course we can also define this conditional entropy for the scenario where we don’t see eats  Now putting these different scenarios together we have the complete definition of conditional entropy This formula considers both scenarios of the value of eats and captures the condi tional entropy regardless of whether eats is equal to 1 or 0 present or absent.
Of course scenario Now putting definition entropy tional equal 1 0 absent define conditional given eats expected values random variables X conditional larger upper bound	 Of course we can also define this conditional entropy for the scenario where we don’t see eats  Now putting these different scenarios together we have the complete definition of conditional entropy This formula considers both scenarios of the value of eats and captures the condi tional entropy regardless of whether eats is equal to 1 or 0 present or absent  We define the conditional entropy of meat given eats as the following expected entropy of meat for both values of eat In general for any discrete random variables X and Y  we have the conditional entropy is no larger than the entropy of the variable X that is This is an upper bound for the conditional entropy.
Now putting different scenarios scenarios value eats condi tional eats present absent conditional entropy given following meat values discrete random variables X Y entropy variable X conditional	 Now putting these different scenarios together we have the complete definition of conditional entropy This formula considers both scenarios of the value of eats and captures the condi tional entropy regardless of whether eats is equal to 1 or 0 present or absent  We define the conditional entropy of meat given eats as the following expected entropy of meat for both values of eat In general for any discrete random variables X and Y  we have the conditional entropy is no larger than the entropy of the variable X that is This is an upper bound for the conditional entropy.
define given following entropy values eat discrete conditional variable This bound The adding information makes	 We define the conditional entropy of meat given eats as the following expected entropy of meat for both values of eat In general for any discrete random variables X and Y  we have the conditional entropy is no larger than the entropy of the variable X that is This is an upper bound for the conditional entropy  The inequality states that we can only reduce uncertainty by adding more information which makes sense.
information help hurt This tells know word Before capturing relations conditional entropy HXmeat	 As we know more information it should always help us make the prediction and can’t hurt the prediction in any case  This conditional entropy gives us one way to measure the association of two words because it tells us to what extent we can predict one word given that we know the presence or absence of another word  Before we look at the intuition of conditional entropy in capturing syntagmatic relations it’s useful to think of a very special case of the conditional entropy of a word given itself HXmeat  Xmeat.
conditional gives way tells extent	 This conditional entropy gives us one way to measure the association of two words because it tells us to what extent we can predict one word given that we know the presence or absence of another word.
capturing syntagmatic think conditional entropy HXmeat means know meat sentence meat occurs sentence This zero word occurs segment we’ll answer happens conditional entropy minimum.	 Before we look at the intuition of conditional entropy in capturing syntagmatic relations it’s useful to think of a very special case of the conditional entropy of a word given itself HXmeat  Xmeat  This means we know where meat occurs in the sentence and we hope to predict whether the meat occurs in the sentence  This is zero because once we know whether the word occurs in the segment we’ll already know the answer of the prediction That also happens to be when this conditional entropy reaches the minimum.
Let’s	 Let’s look at some other cases.
HXmeat Xthe HXmeat We know means doesn’t close	 We can ask the ques tion which is smaller HXmeat  Xthe or HXmeat  Xeats We know that smaller entropy means it is easier to predict  In the first case the doesn’t really tell us much about meat knowing the oc currence of the doesn’t really help us reduce entropy that much so it stays fairly close to the original entropy of meat.
In doesn’t tell meat knowing help entropy fairly original meat absence predict entropy	 In the first case the doesn’t really tell us much about meat knowing the oc currence of the doesn’t really help us reduce entropy that much so it stays fairly close to the original entropy of meat  In the case of eats since eats is related to meat knowing presence or absence of eats would help us predict whether meat oc curs  Thus it reduces the entropy of meat.
entropy meat.	 Thus it reduces the entropy of meat.
For second term Xeats means words use conditional entropy mining syntagmatic	 For this reason we expect the second term HXmeat  Xeats to have a smaller entropy which means there is a stronger association between these two words  This suggests that when you use conditional entropy for mining syntagmatic relations the algorithm would look as follows.
This entropy mining algorithm 1.	 This suggests that when you use conditional entropy for mining syntagmatic relations the algorithm would look as follows  1.
For w1 enumerate w2 corpus	 For each word w1 enumerate all other words w2 from the corpus  2.
HXw1 Sort candidates ascending order condi 3.	 Compute HXw1  Xw2  Sort all candidates in ascending order of the condi tional entropy  3.
Sort ascending condi	 Sort all candidates in ascending order of the condi tional entropy  3.
Take topranked candidate words potential syntag matic extract number value cutoff	 Take the topranked candidate words as words that have potential syntag matic relations with w1  Note that we need to use a threshold to extract the top words this can be the number of top candidates to take or a value cutoff for the conditional entropy.
This correlated particular word help k relations entire collection In ensure words.	 This would allow us to mine the most strongly correlated words with a particular word w1  But this algorithm does not help us mine the strongest k syntagmatic relations from the entire collection  In order to do that we have to ensure that these conditional entropies are comparable across different words.
strongest k collection In entropies words.	 But this algorithm does not help us mine the strongest k syntagmatic relations from the entire collection  In order to do that we have to ensure that these conditional entropies are comparable across different words.
In comparable different	 In order to do that we have to ensure that these conditional entropies are comparable across different words.
In discovering target word like need entropies w1 different The entropy given conditional entropy measure hard predict w1.	 In this case of discovering the syntagmatic relations for a target word like w1 we only need to compare the conditional entropies for w1 given different words  The conditional entropy of w1 given w2 and the conditional entropy of w1 given w3 are comparable because they all measure how hard it is to predict the w1.
conditional given w2 conditional entropy w1 predict	 The conditional entropy of w1 given w2 and the conditional entropy of w1 given w3 are comparable because they all measure how hard it is to predict the w1.
This conditional entropies	 This means we cannot really compare conditional entropies across words.
	 13.
syntagmatic relations information issue conditional words highly correlated	3 1 Mining syntagmatic relations using mutual information The main issue with conditional entropy is that its values are not comparable across different words making it difficult to find the most highly correlated words in an entire corpus.
1 Mining relations information words difficult correlated words corpus problem use mutual information In particular mutual X Y denoted I X reduc tion entropy knowing Y	1 Mining syntagmatic relations using mutual information The main issue with conditional entropy is that its values are not comparable across different words making it difficult to find the most highly correlated words in an entire corpus  To address this problem we can use mutual information  In particular the mutual information of X and Y  denoted I X Y  is the reduc tion in entropy of X obtained from knowing Y .
mutual particular information I reduc Y	 To address this problem we can use mutual information  In particular the mutual information of X and Y  denoted I X Y  is the reduc tion in entropy of X obtained from knowing Y .
In mutual Y X tion entropy obtained knowing .	 In particular the mutual information of X and Y  denoted I X Y  is the reduc tion in entropy of X obtained from knowing Y .
13 Mutual nonnegative.	 13 3 Mutual information is always nonnegative.
3 nonnegative easy original entropy going lower reduced conditional In conditional original information help hurt X.	3 Mutual information is always nonnegative  This is easy to understand because the original entropy is always not going to be lower than the possibly reduced conditional entropy  In other words the conditional entropy will never exceed the original entropy knowing some information can always help us potentially but will not hurt us in predicting X.
This easy understand original entropy going lower possibly reduced entropy exceed original help predicting mutual symmetric I Y I X.	 This is easy to understand because the original entropy is always not going to be lower than the possibly reduced conditional entropy  In other words the conditional entropy will never exceed the original entropy knowing some information can always help us potentially but will not hurt us in predicting X  Another property is that mutual information is symmetric I X Y   I Y  X.
In conditional exceed original potentially hurt predicting property I X Y I Y	 In other words the conditional entropy will never exceed the original entropy knowing some information can always help us potentially but will not hurt us in predicting X  Another property is that mutual information is symmetric I X Y   I Y  X.
mutual symmetric I I Y X property reaches completely independent.	 Another property is that mutual information is symmetric I X Y   I Y  X  A third property is that it reaches its minimum zero if and only if the two random variables are completely independent.
A zero variables That	 A third property is that it reaches its minimum zero if and only if the two random variables are completely independent  That means knowing one of them does not tell us anything about the other.
That knowing	 That means knowing one of them does not tell us anything about the other.
When fix conditional entropy information entropy exactly allows Y That information	 When we fix X to rank different Y s using conditional entropy we would get the same order as ranking based on mutual information  Thus ranking based on mutual entropy is exactly the same as ranking based on the conditional entropy of X given Y  but the mutual information allows us to compare different pairs of X and Y   That is why mutual information is more general and more useful.
Thus based based Y information allows compare Y That information general	 Thus ranking based on mutual entropy is exactly the same as ranking based on the conditional entropy of X given Y  but the mutual information allows us to compare different pairs of X and Y   That is why mutual information is more general and more useful.
Let’s examine intuition mutual 13.	 Let’s examine the intuition of using mutual information for syntagmatic relation mining in Figure 13.
question occurs tend occur question framed question high mutual information eats compute eats For example know meat mutual	 The question we ask is whenever eats occurs what other words also tend to occur This question can be framed as a mutual information question that is which words have high mutual information with eats So we need to compute the mutual information between eats and other words  For example we know the mutual information between eats and meat which is the same as between meat and eats because the mutual information is symmetric.
example know information eats eats mutual eats help predict You mutual	 For example we know the mutual information between eats and meat which is the same as between meat and eats because the mutual information is symmetric  This is expected to be higher than the mutual information between eats and the because knowing the does not really help us predict the other word  You also can easily see that the mutual information between a word and itself is the largest which is equal to the entropy of the word.
expected mutual information eats help predict word.	 This is expected to be higher than the mutual information between eats and the because knowing the does not really help us predict the other word.
easily mutual In maximum knowing predict	 You also can easily see that the mutual information between a word and itself is the largest which is equal to the entropy of the word  In that case the reduction is maximum because knowing one allows us to predict the other completely.
allows predict In zero means mutual maximum.	 In that case the reduction is maximum because knowing one allows us to predict the other completely  In other words the conditional entropy is zero which means mutual information reaches its maximum.
conditional reaches 266 Word Association compute mathematically 13.	 In other words the conditional entropy is zero which means mutual information reaches its maximum  266 Chapter 13 Word Association Mining In order to compute mutual information we often use a different form of mutual information that we can mathematically rewrite as 13.
Association In order compute use form information rewrite context	 266 Chapter 13 Word Association Mining In order to compute mutual information we often use a different form of mutual information that we can mathematically rewrite as 13 4 which is in the context of KLdivergence see Appendix C.
quantifies difference	 KLdivergence quantifies the difference between these two distributions.
distribution expected assump be.	 That is it measures the divergence of the actual joint distribution from the expected distribution under an independence assump tion  The larger the divergence is the higher the mutual information would be.
divergence Continuing formulation mutual information summed combinations values	 The larger the divergence is the higher the mutual information would be  Continuing to inspect this formulation of mutual information we see that it is also summed over many combinations of different values of the two random variables.
joint distri	 Inside the sum we are doing a comparison between the two joint distri butions.
If independent numerator denominator independent strength association simply combinations random consideration.	 If they are indeed independent then we would expect that the numerator and denominator are the same  If the numerator is different from the denominator that would mean the two variables are not independent and their difference can measure the strength of their association  The sum is simply to take all of the combinations of the values of these two random variables into consideration.
The divergence mutual be.	 The larger this divergence is the higher the mutual information would be.
probabilities involved Figure	 Let’s further look at exactly what probabilities are involved in the mutual infor mation formula displayed in Figure 13.
probabilities corresponding presence w1 probabilities shown	11  First we have to calculate the probabilities corresponding to the presence or absence of each word  For w1 we have two probabilities shown here.
corresponding presence absence probabilities sum word absent segment similarly word probabilities presence absence word.	 First we have to calculate the probabilities corresponding to the presence or absence of each word  For w1 we have two probabilities shown here  They should sum to one because a word can either be present or absent in the segment and similarly for the second word we also have two probabilities representing presence or absence of this word.
For probabilities sum absent segment second word probabilities representing sum	 For w1 we have two probabilities shown here  They should sum to one because a word can either be present or absent in the segment and similarly for the second word we also have two probabilities representing presence or absence of this word  These all sum to one as well.
similarly second word probabilities presence absence	 They should sum to one because a word can either be present or absent in the segment and similarly for the second word we also have two probabilities representing presence or absence of this word.
sum joint probabilities cooccurrences words.	 These all sum to one as well  Finally we have a lot of joint probabilities that represent the scenarios of cooccurrences of the two words.
Finally probabilities represent	 Finally we have a lot of joint probabilities that represent the scenarios of cooccurrences of the two words.
They sce Once know information	 They also sum to one because the two words can only have the four shown possible sce narios  Once we know how to calculate these probabilities we can easily calculate the mutual information  It’s important to note that there are some constraints among these probabilities.
note constraints probabilities The The second words scenarios cooccurrence.	 It’s important to note that there are some constraints among these probabilities  The first was that the marginal probabilities of these words sum to one  The second was that the two words have these four scenarios of cooccurrence.
The marginal probabilities sum The additional listed	 The first was that the marginal probabilities of these words sum to one  The second was that the two words have these four scenarios of cooccurrence  The additional constraints are listed at the bottom of Figure 13.
second scenarios cooccurrence additional constraints listed 13	 The second was that the two words have these four scenarios of cooccurrence  The additional constraints are listed at the bottom of Figure 13 12.
The listed	 The additional constraints are listed at the bottom of Figure 13.
The new constraint add probabilities word occurs probability word new constraints interpretation These equations based	 The first new constraint means if we add up the probabilities of two words cooccurring and the probabilities when the first word occurs and the second word does not occur we get exactly the probability that the first word is observed  The other three new constraints have a similar interpretation  These equations allow us to compute some probabilities based on other probabilities and this can simplify the computation.
These equations allow probabilities simplify computation More know word present easily compute absence probability.	 These equations allow us to compute some probabilities based on other probabilities and this can simplify the computation  More specifically if we know the probability that a word is present then we can easily compute the absence probability.
More present easily absence	 More specifically if we know the probability that a word is present then we can easily compute the absence probability.
easy equations probabilities	 It is very easy to use these equations to compute the probabilities of the presence and absence of each word.
let’s look available	 Now let’s look at the joint distribution  Assume that we also have available the probability that they occurred together.
It’s actually compute rest based Figure	 It’s easy to see that we can actually compute all the rest of these probabilities based on these as shown in Figure 13.
equations compute probability second probabilities equation	13  Using the first of the four equations we can compute the probability that the first word occurred and the second word did not because we know the two probabilities in the boxes  Similarly using the third equation we can compute the probability that we observe only the second word.
compute probability know boxes.	 Using the first of the four equations we can compute the probability that the first word occurred and the second word did not because we know the two probabilities in the boxes.
Similarly compute probability shows need probabilities presence segment computed based	 Similarly using the third equation we can compute the probability that we observe only the second word  The figure shows that we only need to know how to compute the three boxed probabilities namely the presence of each word and the cooccurrence of both words in a segment  All others can be computed based on them.
them.	 All others can be computed based on them.
use empirical count estimate probabilities shown Figure 13 A commonly maximum normalize observed	 In general we can use the empirical count of events in the observed data to estimate the probabilities as shown in Figure 13 14  A commonly used technique is the maximum likelihood estimate MLE where we simply normalize the observed counts.
A likelihood estimate MLE normalize observed	14  A commonly used technique is the maximum likelihood estimate MLE where we simply normalize the observed counts.
maximum estimate MLE simply normalize observed Using MLE	 A commonly used technique is the maximum likelihood estimate MLE where we simply normalize the observed counts  Using MLE we can compute these probabilities as follows.
compute probabilities follows segment normalize segments contain	 Using MLE we can compute these probabilities as follows  For estimating the probability that we see a word occuring in a segment we simply normalize the count of segments that contain this word.
For probability occuring simply count On right Figure 13 list	 For estimating the probability that we see a word occuring in a segment we simply normalize the count of segments that contain this word  On the right side of Figure 13 14 you see a list of some segments of data.
On segments words occur indicated	 On the right side of Figure 13 14 you see a list of some segments of data  In some segments you see both words occur which is indicated as ones for both columns.
list data In columns cases	14 you see a list of some segments of data  In some segments you see both words occur which is indicated as ones for both columns  In some other cases only one will occur so only that column has a one and the other column has a zero.
column column zero need counts count number contain w2 words counts normalize counts total number giving mutual information.	 In some other cases only one will occur so only that column has a one and the other column has a zero  To estimate these probabilities we simply need to collect the three counts the count of w1 the total number of segments that contain w1 the segment count for w2 and the count when both words occur both columns have ones  Once we have these counts we can just normalize these counts by N  which is the total number of segments giving us the probabilities that we need to compute mutual information.
simply need collect w1 number contain segment w2 count words columns Once normalize total probabilities information.	 To estimate these probabilities we simply need to collect the three counts the count of w1 the total number of segments that contain w1 the segment count for w2 and the count when both words occur both columns have ones  Once we have these counts we can just normalize these counts by N  which is the total number of segments giving us the probabilities that we need to compute mutual information.
Once counts number segments There small counts In probability use	 Once we have these counts we can just normalize these counts by N  which is the total number of segments giving us the probabilities that we need to compute mutual information  There is a small problem when we have zero counts sometimes  In this case we don’t want a zero probability so we use smoothing as discussed previously in this book.
There small counts In don’t want probability smoothing previously	 There is a small problem when we have zero counts sometimes  In this case we don’t want a zero probability so we use smoothing as discussed previously in this book.
zero smoothing previously To constant probability case 13.	 In this case we don’t want a zero probability so we use smoothing as discussed previously in this book  To smooth we will add a small constant to these counts so that we don’t get zero probability in any case  Smoothing for this application is displayed in Figure 13.
To small constant counts zero probability case Smoothing Figure 13.	 To smooth we will add a small constant to these counts so that we don’t get zero probability in any case  Smoothing for this application is displayed in Figure 13.
Smoothing 13 pseudosegments contribute additional counts words event zero	 Smoothing for this application is displayed in Figure 13 15  We pretend to observe pseudosegments that would contribute additional counts of these words so that no event will have zero probability.
	15.
pretend contribute additional event	 We pretend to observe pseudosegments that would contribute additional counts of these words so that no event will have zero probability.
introduce	 In particular for this exam ple we introduce four pseudosegments.
weighted 14 represent Each combination nonzero count pseudosegment segments it’s okay observed	 Each is weighted at 14  These represent the four different combinations of occurrences of the two words  Each combination will have at least a nonzero count from a pseudosegment thus in the actual segments that we’ll observe it’s okay if we haven’t observed all of the combinations.
occurrences words.	 These represent the four different combinations of occurrences of the two words.
nonzero it’s observed specifically coming ones weighted	 Each combination will have at least a nonzero count from a pseudosegment thus in the actual segments that we’ll observe it’s okay if we haven’t observed all of the combinations  More specifically you can see the 0 5 coming from the two ones in the two pseudosegments because each is weighted at one quarter.
specifically 0 5 quarter If	 More specifically you can see the 0 5 coming from the two ones in the two pseudosegments because each is weighted at one quarter  If we add them up we get 0.
coming add	5 coming from the two ones in the two pseudosegments because each is weighted at one quarter  If we add them up we get 0.
	 If we add them up we get 0 5.
Similar 25 comes single words occur In add total number case pseudosegments.	 Similar to this 0 25 comes from one single pseudosegment that indicates the two words occur together  In the denominator we add the total number of pseudosegments which in this case is four pseudosegments.
25 words add total case pseudosegments.	25 comes from one single pseudosegment that indicates the two words occur together  In the denominator we add the total number of pseudosegments which in this case is four pseudosegments.
pseudosegments case To summarize correlations words.	 In the denominator we add the total number of pseudosegments which in this case is four pseudosegments  To summarize syntagmatic relations can generally be discovered by measuring correlations between occurrences of two words.
syntagmatic discovered measuring occurrences information theory uncertainty entropy entropy X mutual X Y entropy Y knowing useful well.	 To summarize syntagmatic relations can generally be discovered by measuring correlations between occurrences of two words  We’ve used three concepts from information theory entropy which measures the uncertainty of a random variable X conditional entropy which measures the entropy of X given we know Y  and mutual information of X and Y  which matches the entropy reduction of X due to knowing Y  or entropy reduction of Y due to knowing X  These three concepts are actually very useful for other applications as well.
actually useful	 These three concepts are actually very useful for other applications as well.
allows different pairs allowing discover strongest collection documents relation discovery relation We BM25 weight terms context candidates	 Mutual information allows us to have values computed on different pairs of words that are comparable allowing us to rank these pairs and discover the strongest syntagmatic relations from a collection of documents  Note that there is some relation between syntagmatic relation discovery and paradigmatic relation discovery  We already discussed the possibility of using BM25 to weight terms in the context and suggest candidates that have syntagmatic re lations with the target word.
relation discovery We discussed BM25 suggest candidates lations target	 Note that there is some relation between syntagmatic relation discovery and paradigmatic relation discovery  We already discussed the possibility of using BM25 to weight terms in the context and suggest candidates that have syntagmatic re lations with the target word.
weight terms context suggest target Here information relations represent informa weights way	 We already discussed the possibility of using BM25 to weight terms in the context and suggest candidates that have syntagmatic re lations with the target word  Here once we use mutual information to discover syntagmatic relations we can also represent the context with this mutual informa tion as weights  This would give us another way to represent the context of a word.
Here discover relations represent mutual tion weights word And words based	 Here once we use mutual information to discover syntagmatic relations we can also represent the context with this mutual informa tion as weights  This would give us another way to represent the context of a word  And if we do the same for all the words then we can cluster these words or compute the similarity between these words based on their context similarity.
way And words cluster compute similarity words based context	 This would give us another way to represent the context of a word  And if we do the same for all the words then we can cluster these words or compute the similarity between these words based on their context similarity.
	 This provides yet another way to do term weighting for paradigmatic relation discovery.
word association basic	 To summarize this chapter about word association mining we introduced two basic associations called paradigmatic and a syntagmatic relations.
We discovering mainly showing pure kinds And well.	 We introduced multiple statistical approaches for discovering them mainly showing that pure statistical approaches are viable for discovering both kinds of relations  And they can be combined to perform joint analysis as well.
use context segment lead variations For context narrow like word maybe	 We can also use different ways to define context and segment and this would lead us to some interesting variations of applications  For example the context can be very narrow like a few words around a word a sentence or maybe paragraphs.
differing contexts allow flavors relations Of text mining Discovery word associations closely related term Chapter 14 potentially	 Using differing contexts would allow discovery of different flavors of paradigmatic relations  Of course these associations can support many other applications in both information retrieval and text data mining  Discovery of word associations is closely related to term clustering a topic that will be discussed in detail in Chapter 14 where some advanced techniques that can be potentially used for word association discovery will also be briefly discussed.
Of support applications data mining.	 Of course these associations can support many other applications in both information retrieval and text data mining.
word associations closely topic 14 techniques potentially discussed	 Discovery of word associations is closely related to term clustering a topic that will be discussed in detail in Chapter 14 where some advanced techniques that can be potentially used for word association discovery will also be briefly discussed  13.
4 Word Mining Word fundamental step	 13 4 Evaluation of Word Association Mining Word association mining is a fundamental technique in that it is often used as a first step in many other tasks.
Evaluation Word Association Mining Word fundamental chapter gave word mining query expansion The use word	4 Evaluation of Word Association Mining Word association mining is a fundamental technique in that it is often used as a first step in many other tasks  In this chapter we gave one example of using word association mining for query expansion  The best way to convince an application developer that they should use word as sociation mining is to show how it can improve their application.
gave word expansion application use sociation question adding expansion mining improve MAP We know perform Chapter 9.	 In this chapter we gave one example of using word association mining for query expansion  The best way to convince an application developer that they should use word as sociation mining is to show how it can improve their application  If the application is search the question becomes Does adding query expansion via word association mining improve MAP at a statisticallysignificant level We know how to perform this type of evaluation from Chapter 9.
search Does query expansion MAP We evaluation 9 The perform query To query expansion baseline	 If the application is search the question becomes Does adding query expansion via word association mining improve MAP at a statisticallysignificant level We know how to perform this type of evaluation from Chapter 9  The variable we control here between the two experiments is whether we perform query expansion or not  To be more thorough we can compare query expansion with word association mining to query expansion with for example Rocchio feedback as a baseline model.
The variable experiments expansion feedback baseline model.	 The variable we control here between the two experiments is whether we perform query expansion or not  To be more thorough we can compare query expansion with word association mining to query expansion with for example Rocchio feedback as a baseline model.
thorough compare expansion mining query baseline model.	 To be more thorough we can compare query expansion with word association mining to query expansion with for example Rocchio feedback as a baseline model.
evaluate word isolation set gold	 To evaluate word association mining in isolation we would need some set of gold standard data.
don’t human effort judge acceptable Let’s consider loss syntagmatic mining.	 If we don’t have such data we would need to use human manual effort to judge whether the associations found are acceptable  Let’s first consider the case where we have goldstandard data  Without loss of generality assume we wish to evaluate syntagmatic association mining.
consider case goldstandard	 Let’s first consider the case where we have goldstandard data.
Without syntagmatic Given word task rank words vocabulary word Thus average precision MAP metric	 Without loss of generality assume we wish to evaluate syntagmatic association mining  Given a word the task may be to rank all other words in the vocabulary according to how similar they are to the target word  Thus we could compute average precision for each word and use MAP as a summary metric over each word that we evaluate.
word vocabulary compute average MAP summary metric evaluate lists numerical NDCG NDCG.	 Given a word the task may be to rank all other words in the vocabulary according to how similar they are to the target word  Thus we could compute average precision for each word and use MAP as a summary metric over each word that we evaluate  Of course if such ranked lists contained numerical relevance scores we could instead use NDCG and average NDCG.
fact described evaluation topic et	 A humanbased evaluation metric would be intrusion detection  In fact this is one measure described in the evaluation of topic models Chang et al  2009 which we discuss further in Chapter 17.
fact measure evaluation models et	 In fact this is one measure described in the evaluation of topic models Chang et al.
If “intruder” added similar intrusion presented	 2009 which we discuss further in Chapter 17  If the word associations are good it should be fairly easy to find an “intruder” that has been added into the top k similar words  For example consider the following two examples of intrusion detection presented in Chang et al.
good similar For example consider presented et 2009.	 If the word associations are good it should be fairly easy to find an “intruder” that has been added into the top k similar words  For example consider the following two examples of intrusion detection presented in Chang et al  2009.
2009 We k	 2009  We have two lists with k  1  6 items.
We lists The items chosen vocabulary word added.	 We have two lists with k  1  6 items  The top k  5 items are chosen for some word in the vocabulary and an additional random word from the vocabulary is also added.
platypus agile it’s intruder k form coherent good group.	 L1  dog  cat horse apple pig  cow L2  car  teacher  platypus agile blue Zaire The idea here is that if it’s easy to spot the intruder the top k words form a coherent group meaning that they are a very good word association group.
In L1 doesn’t	 In L1 it’s quite obvious that apple is the intruder since it doesn’t fit in with the other words in the list.
remaining word association can’t word meaning word create k candidates L2 Performing type words good strictly evaluate word associations.	 Thus the remaining words form a good word association list  In L2 we can’t really tell which word is the intruder meaning the word association algorithm used to create the k candidates in L2 is not as good as the one used to generate L1  Performing this type of experiment over many different words in the vocabulary is a good yet expensive way to strictly evaluate the word associations.
Performing vocabulary expensive evaluate associations We requires Finally important consider tool	 Performing this type of experiment over many different words in the vocabulary is a good yet expensive way to strictly evaluate the word associations  We say this method is expensive since it requires many human judgements  Finally it’s important to consider the timeaccuracy tradeoff of using such a tool in word association mining.
We method requires Finally it’s consider timeaccuracy tradeoff tool word scenario baseline	 We say this method is expensive since it requires many human judgements  Finally it’s important to consider the timeaccuracy tradeoff of using such a tool in word association mining  Imagine the scenario where we have a baseline system with a MAP of 0.
Finally important tradeoff tool association	 Finally it’s important to consider the timeaccuracy tradeoff of using such a tool in word association mining  Imagine the scenario where we have a baseline system with a MAP of 0.
	89 on some dataset.
statistically higher MAP	 If we use query expansion via word association mining we can get a statistically significantly higher MAP of 0.
90 preprocessing mining query word takes place nonnegligible	90  However this doesn’t take into account the preprocessing time of mining the word associations  In this example the query time is not affected because the word association mining takes place beforehand offline but it still is a nonnegligible cost.
However doesn’t account	 However this doesn’t take into account the preprocessing time of mining the word associations.
time affected association mining takes offline nonnegligible cost The manager decide MAP 0 01 worth maintaining query expansion	 In this example the query time is not affected because the word association mining takes place beforehand offline but it still is a nonnegligible cost  The application manager would have to decide whether an increase in MAP of 0 01 is worth the effort of implementing running and maintaining the query expansion program.
job feature vectors Then term return	 As a data scientist it is often part of the job to feature vectors for each term ID  Then given a query term return the most similar terms.
Clustering natural text In	 14Text Clustering Clustering is a natural problem in exploratory text analysis  In its most basic sense clustering i.
	e.
grouping lets discover structure collecting objects words.	 grouping objects together lets us discover some inherent structure in our corpus by collecting similar objects  These objects could be documents sentences or words.
We cluster search search results data When collection text data natural multiple	 We could cluster search engine queries search engine results and even users themselves  Clustering is a general data mining technique very useful for exploring large data sets  When facing a large collection of text data clustering can reveal natural se mantic structures in the data in the form of multiple clusters i.
	e.
clustering directly	 The clustering results can sometimes be regarded as knowledge directly useful in an application.
clustering emails major customer complaints In overview useful understanding zooming specific subset	 For example clustering customer emails can reveal major customer complaints about a product  In general the clustering results are useful for providing an overview of the data which is often very useful for understanding a data set at a highlevel before zooming into any specific subset of the data for fo cused analysis.
In data useful data set clustering data structures facilitate linking In text analysis widespread clustering algorithms unsupervised manual data	 In general the clustering results are useful for providing an overview of the data which is often very useful for understanding a data set at a highlevel before zooming into any specific subset of the data for fo cused analysis  The clustering results can also support navigation into the relevant subsets of the data since the structures can facilitate linking of objects inside a clus ter and linking of related clusters  In general clustering methods are very useful for text mining and exploratory text analysis with widespread applications espe cially due to the fact that clustering algorithms are mostly unsupervised without requiring any manual effort and can thus be applied to any text data set.
The clustering results subsets data	 The clustering results can also support navigation into the relevant subsets of the data since the structures can facilitate linking of objects inside a clus ter and linking of related clusters.
In general methods useful text analysis espe clustering algorithms unsupervised text set.	 In general clustering methods are very useful for text mining and exploratory text analysis with widespread applications espe cially due to the fact that clustering algorithms are mostly unsupervised without requiring any manual effort and can thus be applied to any text data set.
cluster different leads interesting applications example clustering retrieval	 The object types that we cluster necessitate different tasks and this variation leads to many interesting applications  For example clustering of retrieval results can be used as a result summary or as a way to remove redundant documents.
documents entire common underlying understanding type data contains Term concepts	 Clustering the documents in our entire corpus lets us find common underlying themes and can give us a better understanding of the type of data it contains  Term clustering is a powerful way to find concepts or create a thesaurus.
way formally define In actually mean object Intuitively 14.	 Term clustering is a powerful way to find concepts or create a thesaurus  However how do we formally define the problem of clustering In particular what does it actually mean for an object to be in a particular cluster Intuitively we Figure 14.
problem clustering In actually mean Intuitively Figure bias.	 However how do we formally define the problem of clustering In particular what does it actually mean for an object to be in a particular cluster Intuitively we Figure 14 1 Illustration of clustering bias.
Illustration bias.	1 Illustration of clustering bias.
The results similarity defined based	 The figure in the middle shows the clustering results when similarity is defined based on the shape of an object.
The shows clustering results objects similarity based size.	 The figure on the right shows the clustering results of the same set of objects when similarity is defined based on size.
However definition exactly measure appropriate clearly lead result illustration	 However such a definition of clustering is strictly speaking not well defined as we did not make it clear how exactly we should measure similarity  Indeed an appropriate definition of similarity is quite crucial for clustering as a different definition would clearly lead to a different clustering result  Consider the illustration in Figure 14.
different clearly lead	 Indeed an appropriate definition of similarity is quite crucial for clustering as a different definition would clearly lead to a different clustering result.
illustration 1 How objects left look like Clearly answered perspective measuring clearly i.	 Consider the illustration in Figure 14 1  How should we cluster the objects shown in the figure on the left side What would an ideal clustering result look like Clearly these questions cannot be answered until we define the perspective for measuring similarity very clearly i.
figure ideal result look questions answered clearly	 How should we cluster the objects shown in the figure on the left side What would an ideal clustering result look like Clearly these questions cannot be answered until we define the perspective for measuring similarity very clearly i e.
important state measuring “clustering bias evaluating	 Thus when we define a clustering task it is important to state the desired perspective of measuring similarity which we refer to as a “clustering bias ” This bias will also be the basis for evaluating clustering results.
bias basis	” This bias will also be the basis for evaluating clustering results.
ambiguity similarity exists artificial example “horse” A clearly similar	 The ambiguity of perspective for similarity not only exists in such an artificial example but also exists everywhere  Take words for example are “car” and “horse” similar A car and a horse are clearly not similar physically.
example “horse” car physically look functions transportation clustering bias determined specific	 Take words for example are “car” and “horse” similar A car and a horse are clearly not similar physically  However if we look at them from the perspective of their functions we may say that they are similar since they can both be used as transportation tools  The “right” clustering bias clearly has to be determined by the specific application.
However perspective functions determined clustering ways.	 However if we look at them from the perspective of their functions we may say that they are similar since they can both be used as transportation tools  The “right” clustering bias clearly has to be determined by the specific application  In different algorithms the clustering bias is injected in different ways.
algorithms select similarity clustering method It example documents chosen algorithm	 For some clustering algorithms it is up to the user to define or select explicitly a similarity algorithm for the clustering method to use  It will put for example documents that are all similar according to the chosen similarity algorithm in the same cluster.
chosen similarity cluster Other clustering typically function model	 It will put for example documents that are all similar according to the chosen similarity algorithm in the same cluster  Other clustering algorithms are modelbased typically based on generative probabilistic models where the objective function of the model for the data e g.
algorithms models objective model e	 Other clustering algorithms are modelbased typically based on generative probabilistic models where the objective function of the model for the data e g.
function case create bias	 the likelihood function in the case of a generative probabilistic model is to create an indirect bias on how similarity is defined.
With modelbased it’s assigned probability distribution clusters meaning assignment” similaritybased clustering model clustering This particular chapter focuses topic Chapter example model based clustering.	 With these modelbased methods it’s often the case that an object is assigned a probability distribution over all the clusters meaning there is no “hard cluster assignment” as in the similaritybased methods  We explore both similaritybased clustering and model based clustering in this book  This particular chapter focuses on similaritybased clustering and the topic analysis chapter Chapter 17 is a fine example of model based clustering.
We based book This particular chapter clustering Chapter 17 fine based clustering.	 We explore both similaritybased clustering and model based clustering in this book  This particular chapter focuses on similaritybased clustering and the topic analysis chapter Chapter 17 is a fine example of model based clustering.
clustering techniques words ments.	 In this chapter we examine clustering techniques for both words and docu ments.
discuss hierarchical divisive	 Next we discuss similaritybased clustering via two common methods hierarchical and divisive methods.
Then semanticrelatedness information	 Then we introduce term clustering via both semanticrelatedness and pointwise mutual information before mention ing two more advanced topics.
end evaluation clusters.	 We end with clustering evaluation  As mentioned previously document clustering groups documents together into clusters.
We document	 We can categorize document clustering methods into two categories.
These clustering need func work.	 These clustering algorithms need a similarity func tion to work.
similar depending viewed user similarity clustering “bottom up”	 Any two objects have the potential to be similar depending on how they are viewed  Therefore a user must define the similarity in some way  Agglomerative clustering is a “bottom up” approach also called hierarchi cal clustering.
user way “bottom approach hierarchi clustering In approach gradually merge objects generate	 Therefore a user must define the similarity in some way  Agglomerative clustering is a “bottom up” approach also called hierarchi cal clustering  In this approach we gradually merge similar objects to generate clusters.
Agglomerative clustering In approach gradually merge clusters.	 Agglomerative clustering is a “bottom up” approach also called hierarchi cal clustering  In this approach we gradually merge similar objects to generate clusters.
approach merge similar objects “top down”	 In this approach we gradually merge similar objects to generate clusters  Divisive clustering is a “top down” approach.
clustering gradu smaller methods document belong	 Divisive clustering is a “top down” approach  In this approach we gradu ally divide the whole set of objects into smaller clusters  For both above methods each document can only belong to one cluster.
set smaller clusters.	 In this approach we gradu ally divide the whole set of objects into smaller clusters.
For belong cluster.	 For both above methods each document can only belong to one cluster.
Modelbased latent struc	 Modelbased techniques design a probabilistic model to capture the latent struc ture of data i e.
features fit data obtain	e  features in documents and fit the model to data to obtain clusters.
fit model data obtain clustering object multiple	 features in documents and fit the model to data to obtain clusters  Typically this is an example of soft clustering since one object can be in multiple clusters with a certain probability.
multiple There discussion chapter applications query expansion.	 Typically this is an example of soft clustering since one object can be in multiple clusters with a certain probability  There will be much more discussion on this in the topic analysis chapter  Term clustering has applications in query expansion.
terms added increasing possible allows features thousands conceptual discuss term clustering topic models Chapter 2.	 It allows similar terms to be added to the query increasing the possible number of documents matched from the index  It also allows humans to understand advanced features more easily if there are many hundreds or thousands of them or if they are hard to conceptual ize  Later we will first discuss term clustering using semantic relatedness via topic language models mentioned in Chapter 2.
It allows understand easily hundreds hard	 It also allows humans to understand advanced features more easily if there are many hundreds or thousands of them or if they are hard to conceptual ize.
explore probabilistic mutual information mention clustering ity document probabilistic nature methods.	 Then we explore a simple probabilistic technique called pointwise mutual information  We briefly mention a hierarchical technique for term clustering called Brown clustering  This technique has similar ity to the agglomerative document clustering along with the probabilistic nature of modelbased methods.
We briefly mention term clustering Brown clustering technique ity probabilistic nature	 We briefly mention a hierarchical technique for term clustering called Brown clustering  This technique has similar ity to the agglomerative document clustering along with the probabilistic nature of modelbased methods.
technique document modelbased We word vectors representation remind infor mation problem As Chapter 17 topic words	 This technique has similar ity to the agglomerative document clustering along with the probabilistic nature of modelbased methods  We finish term clustering with an explanation of word vectors a contextbased word representation that should remind you of the infor mation retrieval problem setup  As we will see in Chapter 17 output from a modelbased topic analysis addition ally gives us groups of similar words in fact these are the “topics”.
finish explanation word representation infor mation retrieval problem	 We finish term clustering with an explanation of word vectors a contextbased word representation that should remind you of the infor mation retrieval problem setup.
analysis clusters	 Thus topic analysis delivers both term and document clusters to the user.
unsupervised generally provide supervision requiring cluster useful like clustering users way.	 Although with an unsupervised clustering algorithm we generally do not provide any prior expectations as to what our clusters may contain it is also possible to provide some supervision in the form of requiring two objects to be in the same cluster or not to be in the same cluster  Such supervision is useful when we have some knowledge about the clustering problem that we would like to incorporate into a clustering algorithm and allows users to “steer” the clustering in a flexible way.
A number clusters user algorithm objects number optimized based statistical data explained	 A user can also control the number of clusters by setting the number of clusters desired beforehand or the user may leave it to the algorithm to determine what a natural breakdown of our objects is in which case the number of clusters is usually optimized based on some statistical measures such as how well the data can be explained by a certain number of clusters.
clustering labels examine groups documents mentally approaches automate label text cluster label phrase multiple phrases et	 Most clustering output does not give labels for the clusters found it’s up to the user to examine the groups of terms or documents and mentally assign a label such as “biology” or “architecture ” However there are also approaches to automate assignment of a label to a text cluster where a label is often a phrase or multiple phrases Mei et al  2007b.
label phrase phrases et al 2007b.	” However there are also approaches to automate assignment of a label to a text cluster where a label is often a phrase or multiple phrases Mei et al  2007b.
2007b labeling form text summarization discuss Chapter 16 implementation clustering algorithms.	 2007b  This labeling task can be regarded as a form of text summarization which we will further discuss in Chapter 16  Finally a brief note on the implementation of clustering algorithms.
book retrieval techniques II useful text analysis including clustering document documents according feature Leveraging supporting search especially unified software data access text	 As with the rest of the chapters in this part of the book we will see that the information retrieval techniques that we discussed in Part II are often also very useful for implementing many other algorithms for text analysis including clustering  For example in the case of document clustering we may assume we already have a forward index of tokenized documents according to some feature representation  Leveraging the data structures already in place for supporting search is especially desirable in a unified software system for supporting both text data access and text analysis.
example case index according representation.	 For example in the case of document clustering we may assume we already have a forward index of tokenized documents according to some feature representation.
	g.
words trigram POStags syntactic features algorithms need term IDs algorithms probabilities represent.	 unigram words bigram words trigram POStags or syntactic tree features  All the clustering algorithms need are a term vocabulary represented as term IDs  The clustering algorithms only care about term occurrences and probabilities not what they actually represent.
clustering need term IDs clustering algorithms care term occurrences algorithm—we cluster documents stylistic represented	 All the clustering algorithms need are a term vocabulary represented as term IDs  The clustering algorithms only care about term occurrences and probabilities not what they actually represent  Thus—with the same clustering algorithm—we can cluster documents by their word usage or by similar stylistic patterns represented as grammatical parse tree segments.
clustering term represent Thus—with algorithm—we cluster word usage represented	 The clustering algorithms only care about term occurrences and probabilities not what they actually represent  Thus—with the same clustering algorithm—we can cluster documents by their word usage or by similar stylistic patterns represented as grammatical parse tree segments.
Thus—with clustering similar patterns tree	 Thus—with the same clustering algorithm—we can cluster documents by their word usage or by similar stylistic patterns represented as grammatical parse tree segments.
For clustering term	 For term clustering we may not use an index but we do also assume that each sentence or document is tokenized and term IDs are assigned.
Document Clustering In similaritybased clustering As similaritybased methods required case suggest consult	2 Document Clustering In this section we examine similaritybased document clustering through two methods agglomerative clustering and divisive clustering  As these are both similaritybased clustering methods a similarity measure is required  In case a refresh of similarity measures is required we suggest the reader consult Chapter 6.
methods measure required similarity required consult In particular similarity algorithms need symmet ric simd1 simd2	 As these are both similaritybased clustering methods a similarity measure is required  In case a refresh of similarity measures is required we suggest the reader consult Chapter 6  In particular the similarity algorithms we use for clustering need to be symmet ric that is simd1 d2 must be equal to simd2 d1.
case required suggest algorithms simd1 d2 equal Furthermore normalized	 In case a refresh of similarity measures is required we suggest the reader consult Chapter 6  In particular the similarity algorithms we use for clustering need to be symmet ric that is simd1 d2 must be equal to simd2 d1  Furthermore our similarity algorithm must be normalized on some range.
In particular similarity clustering equal d1 range.	 In particular the similarity algorithms we use for clustering need to be symmet ric that is simd1 d2 must be equal to simd2 d1  Furthermore our similarity algorithm must be normalized on some range.
similarity algorithm range.	 Furthermore our similarity algorithm must be normalized on some range.
These constraints ensure fairly pairs	 Usually this range is 0 1  These constraints ensure that we can fairly compare similarity scores of different pairs of objects.
constraints ensure fairly compare similarity Most retrieval formulas length query likelihood methods—are asymmetric differently document scored.	 These constraints ensure that we can fairly compare similarity scores of different pairs of objects  Most retrieval formulas we have seen—such as BM25 pivoted length nor malization and query likelihood methods—are asymmetric since they treat the query differently from the current document being scored.
length methods—are asymmetric treat differently	 Most retrieval formulas we have seen—such as BM25 pivoted length nor malization and query likelihood methods—are asymmetric since they treat the query differently from the current document being scored.
explore popular	 Whissell and Clarke 2013 explore symmetric versions of popular retrieval formulas and they show that they are quite effective.
measures clustering use example BM25 term doc ument vectors similarity similarity.	 Despite the fact that default querydocument similarity measures are not used for clustering it is possible to use for example Okapi BM25 term weighting in doc ument vectors which are then scored with a simple symmetric similarity algorithm like cosine similarity.
term user imagine The similar angle plotted highdimensional space larger	 As mentioned the term weights may be raw counts TFIDF or anything else the user could imagine  The cosine similar ity captures the cosine of the angle between the two document vectors plotted in their highdimensional space the larger the angle the more dissimilar the docu ments are.
X ∪ Y	 It is defined as follows simJaccardX Y   X ∩ Y X ∪ Y   14.
study similarity measures sug Huang For measure cosine Jac card In particular optimal	 For a more indepth study of similarity measures and their effectiveness we sug gest that the reader consult Huang 2008  For the rest of this chapter it is sufficient to assume that the base documentdocument similarity measure is cosine or Jac card similarity  In any event the goal of a particular similarity algorithm is to find an optimal partitioning of data to simultaneously maximize intragroup similarity and minimize intergroup similarity.
For chapter base documentdocument cosine similarity.	 For the rest of this chapter it is sufficient to assume that the base documentdocument similarity measure is cosine or Jac card similarity.
goal particular similarity partitioning simultaneously	 In any event the goal of a particular similarity algorithm is to find an optimal partitioning of data to simultaneously maximize intragroup similarity and minimize intergroup similarity  14.
Agglomerative Hierarchical We ready discuss general clustering strategy This method generate bottomup approach gradually groups similar doc documents clusters cluster left.	1 Agglomerative Hierarchical Clustering We are now ready to discuss our first general clustering strategy  This method progressively constructs clusters to generate a hierarchy of merged groups  This bottomup agglomerative approach gradually groups similar objects single doc uments or groups of documents into larger and larger clusters until there is only one cluster left.
This method clusters generate	 This method progressively constructs clusters to generate a hierarchy of merged groups.
This approach groups uments groups documents larger larger clusters cluster The tree needed.	 This bottomup agglomerative approach gradually groups similar objects single doc uments or groups of documents into larger and larger clusters until there is only one cluster left  The tree may then be segmented as needed.
number dendrogram represented Figure	 The tree may then be segmented as needed  Alternatively the merg ing may be stopped when the desired number of clusters is found  This series of merges forms a dendrogram represented in Figure 14.
Alternatively number clusters forms represented Figure 2.	 Alternatively the merg ing may be stopped when the desired number of clusters is found  This series of merges forms a dendrogram represented in Figure 14 2.
series merges forms represented Figure 14 2.	 This series of merges forms a dendrogram represented in Figure 14 2.
	2.
In original documents numbered row	 In the figure the original documents are numbered one through eleven and comprise the bottom row of the dendrogram.
document lines documents form clustering ter present cluster clus ter	 Circles represent clusters of more than one document and lines represent which documents or clusters were merged together to form the next larger cluster  The clustering algorithm is straightforward while there is more than one clus ter find the two most similar clusters and merge them  This does present an issue though when we need to compare the similarity of a cluster with a cluster or a clus ter with a single document.
clustering straightforward clus ter present similarity single document.	 The clustering algorithm is straightforward while there is more than one clus ter find the two most similar clusters and merge them  This does present an issue though when we need to compare the similarity of a cluster with a cluster or a clus ter with a single document.
The cluster similarity measures use documentdocument measures outline measures illustrate 14 3.	 The cluster similarity measures we define make use of the documentdocument similarity measures presented previously  Below we outline three cluster similarity measures and illustrate them in Fig ure 14 3.
Below cluster similarity measures	 Below we outline three cluster similarity measures and illustrate them in Fig ure 14.
3.	3.
Singlelink clusters smallest minimum distance results “looser” clusters individually elements cluster perform merge Completelink merges clusters smallest elements.	 Singlelink merges the two clusters with the smallest minimum distance  This results in “looser” clusters since we only need to find two individually close elements in each cluster in order to perform the merge  Completelink merges the two clusters with the smallest maximum distance be tween elements.
Completelink clusters smallest tween elements.	 Completelink merges the two clusters with the smallest maximum distance be tween elements.
clusters kept small	 This results in very “tight” and “compact” clusters since the cluster diameter is kept small i e.
e elements low Algorithm 14.	e  the distance between all elements low  Algorithm 14.
14.	 the distance between all elements low  Algorithm 14.
	 Ex.
end Averagelink previous measures.	 end while Averagelink is a compromise between the two previous measures.
takes average Both singlelink completelink similarity pair documents.	 As its name implies it takes the smallest average distance between two clusters  Both singlelink and completelink are sensitive to outliers since they rely on only the similarity of one pair of documents.
singlelink sensitive rely similarity Averagelink making outliers.	 Both singlelink and completelink are sensitive to outliers since they rely on only the similarity of one pair of documents  Averagelink is essentially a group decision making it less sensitive to outliers.
essentially making sensitive outliers Of book application determine useful try measures dataset	 Averagelink is essentially a group decision making it less sensitive to outliers  Of course as with most methods discussed in this book the specific application will determine which method is preferred  In fact it may even be useful to try out different documentdocument similarity measures combined with different clustercluster similarity measures to see how the dataset is partitioned.
methods discussed	 Of course as with most methods discussed in this book the specific application will determine which method is preferred.
In fact different documentdocument combined clustercluster	 In fact it may even be useful to try out different documentdocument similarity measures combined with different clustercluster similarity measures to see how the dataset is partitioned  14 2.
	 14.
2 method hierarchical divisive approach apply clustering algorithm data smaller smaller	2 2 Kmeans A complementary clustering method to our hierarchical algorithm is a topdown divisive approach  In this approach we repeatedly apply a flat clustering algorithm to partition the data into smaller and smaller clusters.
A complementary hierarchical topdown approach approach smaller	2 Kmeans A complementary clustering method to our hierarchical algorithm is a topdown divisive approach  In this approach we repeatedly apply a flat clustering algorithm to partition the data into smaller and smaller clusters.
In approach apply flat clustering smaller clusters.	 In this approach we repeatedly apply a flat clustering algorithm to partition the data into smaller and smaller clusters.
In clustering We clustering improve reach criterion represents cluster values.	 In flat clustering We will start with an initial tentative clustering and iteratively improve it until we reach some stopping criterion  Here we represent a cluster with a centroid a centroid is a special document that represents all other documents in its cluster usually as an average of all its members’ values.
algorithm1 sets centroids reassigns cluster below.	 The Kmeans algorithm1 sets K centroids and iteratively reassigns documents to each one until the change in cluster assignment is small or nonexistent  This technique is described in the algorithm below.
described	 This technique is described in the algorithm below.
chosen Ex.	 be the chosen document document similarity measure  The two steps in Kmeans are marked as the expectation step Ex.
steps Kmeans expectation maximization step Max.	 The two steps in Kmeans are marked as the expectation step Ex  and the maximization step Max.
step	 and the maximization step Max.
algorithm instantiation ExpectationMaximization algorithm commonly EM We return related Chapter	 this algorithm is one instantiation of the widely found ExpectationMaximization algorithm commonly called just EM  We will return to this powerful algorithmic paradigm in much more detail in Chapter 17 on topic 1  Kmeans is not at all related to the classification algorithm kNN see Chapter 15.
We powerful paradigm 17 1.	 We will return to this powerful algorithmic paradigm in much more detail in Chapter 17 on topic 1.
classification algorithm Chapter 15 Figure 14.	 Kmeans is not at all related to the classification algorithm kNN see Chapter 15  Figure 14.
4 small set data shown initial chosen shown	4 Steps in the Kmeans clustering algorithm for a small set of data points to be clustered shown in a  First three initial starting centroids are randomly chosen shown in b.
starting randomly chosen b Then data points clusters based centroid decision boundaries shown lines c assignments lead compute centroid shown stars	 First three initial starting centroids are randomly chosen shown in b  Then all the data points are each assigned to one of the three clusters based on their distances to each centroid the decision boundaries are shown as lines in c  The assignments lead to three tentative clusters each of which can then be used to compute a new centroid to better represent the cluster shown as three stars in new locations in d.
new shown generating data set PLSA algorithm chapter Kmeans particular hard EM.	 The new boundaries are shown in e which are easily seen to be already very close to the optimal centroids for generating three clusters from this data set  analysis through the PLSA algorithm  For this chapter it is sufficient to realize that Kmeans is a particular manifestation of hard cluster assignment via EM.
PLSA algorithm For sufficient realize particular assignment EM	 analysis through the PLSA algorithm  For this chapter it is sufficient to realize that Kmeans is a particular manifestation of hard cluster assignment via EM  Figure 14.
Figure Kmeans	 Figure 14 4 shows the Kmeans algorithm in action.
Kmeans Frame shows initial setup clustered.	4 shows the Kmeans algorithm in action  Frame a shows our initial setup with the data points to be clustered.
points clustered data distinct corresponding shapes crosses circles b shows chosen	 Frame a shows our initial setup with the data points to be clustered  Here we visualize the data points with different shapes to suggest that there are three distinct clusters corresponding to three shapes crosses circles and triangles  Frame b shows how three random centroids are chosen K  3.
Here distinct corresponding crosses circles	 Here we visualize the data points with different shapes to suggest that there are three distinct clusters corresponding to three shapes crosses circles and triangles.
Frame 3 black lines documents respective centroid.	 Frame b shows how three random centroids are chosen K  3  In frame c the black lines show the partition of documents in their respective centroid.
In frame black lines documents	 In frame c the black lines show the partition of documents in their respective centroid.
connect centroids perpendicular segments centroids marked	 These lines can be found by first drawing a line to connect each pair of centroids and then finding the perpendicular bisectors of the segments connecting two centroids  This step is marked Ex.
docode cluster centroids improve positions.	 This step is marked Ex  in the pseu docode  Then once the cluster assignments are determined frame d shows how the centroids are recomputed to improve the centroids’ positions.
cluster assignments determined frame d shows centroids centroids’	 Then once the cluster assignments are determined frame d shows how the centroids are recomputed to improve the centroids’ positions.
This step marked Max pseudocode Thus d iteration leads	 This centroid reassignment step is marked as Max  in the pseudocode  Thus frames c and d represent one iteration of the algorithm which leads to improved centroids.
pseudocode Thus frames c d iteration algorithm leads	 in the pseudocode  Thus frames c and d represent one iteration of the algorithm which leads to improved centroids.
improved centroids continue obtain boundaries lead centroids discussed 6 Euclidean distance Kmeans function average cluster centroid	 Thus frames c and d represent one iteration of the algorithm which leads to improved centroids  Frames e further shows how the algorithm can continue to obtain improved boundaries which in turn would lead to further improved centroids  When a document is represented as a term vector as discussed in Chapter 6 and a Euclidean distance function is used the Kmeans algorithm can be shown to minimize an objective function that computes the average distances of all the data points in a cluster to the centroid of the cluster.
continue obtain improved improved centroids term Chapter Euclidean function Kmeans objective function distances data points cluster The algorithm known converge guaranteed global minimum.	 Frames e further shows how the algorithm can continue to obtain improved boundaries which in turn would lead to further improved centroids  When a document is represented as a term vector as discussed in Chapter 6 and a Euclidean distance function is used the Kmeans algorithm can be shown to minimize an objective function that computes the average distances of all the data points in a cluster to the centroid of the cluster  The algorithm is also known to converge to a local minimum but not guaranteed to converge to a global minimum.
document Euclidean minimize function computes algorithm known converge global minimum.	 When a document is represented as a term vector as discussed in Chapter 6 and a Euclidean distance function is used the Kmeans algorithm can be shown to minimize an objective function that computes the average distances of all the data points in a cluster to the centroid of the cluster  The algorithm is also known to converge to a local minimum but not guaranteed to converge to a global minimum.
The algorithm known global Thus generally needed order obtain	 The algorithm is also known to converge to a local minimum but not guaranteed to converge to a global minimum  Thus multiple trials are generally needed in order to obtain a good local minimum.
The applied divide data smaller hierarchical clustering Thus clustering sense Kmeans dividing data strategy agglomerative hierarchical hierarchy incrementally data points bottomup agglomerative clustering generates binary tree adapted groups iteration.	 The Kmeans algorithm can be repeatedly applied to divide the data set gradually into smaller and smaller clusters thus creating a hierarchy of clusters similar to what we can achieve with the agglomerative hierarchical clustering algorithm  Thus both agglomerative hierarchical clustering and Kmeans can be used for hierarchical clustering they complement each other in the sense that Kmeans constructs the hierarchy by incrementally dividing the whole set of data a topdown strategy while agglomerative hierarchical clustering constructs the hierarchy by incrementally merging data points a bottomup strategy  Note that although in its basic form agglomerative hierarchical clustering generates a binary tree it can easily adapted to generate more than two branches by merging more than two groups into a cluster at each iteration.
agglomerative clustering sense Kmeans hierarchy dividing set topdown strategy hierarchy incrementally merging form agglomerative clustering binary easily adapted merging cluster iteration allow algorithm	 Thus both agglomerative hierarchical clustering and Kmeans can be used for hierarchical clustering they complement each other in the sense that Kmeans constructs the hierarchy by incrementally dividing the whole set of data a topdown strategy while agglomerative hierarchical clustering constructs the hierarchy by incrementally merging data points a bottomup strategy  Note that although in its basic form agglomerative hierarchical clustering generates a binary tree it can easily adapted to generate more than two branches by merging more than two groups into a cluster at each iteration  Similarly if we only allow a binary tree then we also do not have to set K in the Kmeans algorithm for creating a hierarchy.
Note basic clustering generates easily generate merging iteration set Kmeans algorithm 14.	 Note that although in its basic form agglomerative hierarchical clustering generates a binary tree it can easily adapted to generate more than two branches by merging more than two groups into a cluster at each iteration  Similarly if we only allow a binary tree then we also do not have to set K in the Kmeans algorithm for creating a hierarchy  14.
“related” similar	 By “related” we usually mean words that have a similar semantic meaning.
assessment related synonyms.	 For example soccer and basketball are related in the sense that they are both sports  Similarly evaluation and assessment are related since they are synonyms.
evaluation assessment refer mind don’t necessarily commonly example	 Similarly evaluation and assessment are related since they are synonyms  In this section we will refer to “terms” and “words” interchangeably though keep in mind we don’t necessarily only have to cluster words  We commonly use this example since it is quite straightforward to imagine.
In refer “terms” “words” interchangeably don’t cluster words commonly use straightforward The generally tree fragments.	 In this section we will refer to “terms” and “words” interchangeably though keep in mind we don’t necessarily only have to cluster words  We commonly use this example since it is quite straightforward to imagine  The techniques we describe in this section will generally work for any sequence of features whether they are words or parse tree fragments.
straightforward imagine.	 We commonly use this example since it is quite straightforward to imagine.
It mind designed use cases.	 It is important to keep in mind however that the algorithms we discuss were designed for use on words in most cases.
It’s important forms score words scores normalized cluster set terms pairwise As clustering definition	 It’s also important to note that in some forms of term “clustering” we only receive a pairwise score between two words w1 and w2  If these scores are normalized we can still cluster the entire set of terms by using the pairwise scores  As in all clustering problems definition of similarity is important.
scores cluster pairwise	 If these scores are normalized we can still cluster the entire set of terms by using the pairwise scores.
As clustering similarity important.	 As in all clustering problems definition of similarity is important.
In case question easy relations words candidates serving basis define lead clusters “location” context syntagmatic relations clusters tend contexts different “locations.	 In the case of term clustering the question is how we should define the similarity between two terms  It is easy to see that the paradigmatic relations and syntagmatic relations between words or terms are both natural candidates for serving as a basis to define similarity  The paradigmatic relation similarity would lead to clusters of terms that tend to occur in very similar contexts with the same relative “location” in the context whereas the syntagmatic relations would lead to clusters of terms that are semantically related and also tend to cooccur in similar contexts but in different “locations.
easy syntagmatic natural candidates serving	 It is easy to see that the paradigmatic relations and syntagmatic relations between words or terms are both natural candidates for serving as a basis to define similarity.
lead clusters terms tend occur contexts relative “location” syntagmatic terms cooccur similar contexts different “locations.	 The paradigmatic relation similarity would lead to clusters of terms that tend to occur in very similar contexts with the same relative “location” in the context whereas the syntagmatic relations would lead to clusters of terms that are semantically related and also tend to cooccur in similar contexts but in different “locations.
” revisit method finding semantically related	” In the rest we will first revisit a method for finding semantically related words from earlier in this book.
introduce terms.	 Then we introduce the concept of pointwise mutual information and show how it can also be used to find related terms.
introduction advanced clustering	 We end with an introduction to more advanced term clustering methods  14.
14 3 3.	 14 3 1 Semantically Related Terms Recall from Section 3.
3 1 Recall 3 computer.	3 1 Semantically Related Terms Recall from Section 3 4 where we found which words were semantically related with the term computer.
1 Recall 4 term	1 Semantically Related Terms Recall from Section 3 4 where we found which words were semantically related with the term computer.
4 semantically related 14.	4 where we found which words were semantically related with the term computer  Figure 14.
4.	4.
likelihood θ̂ θ̂ case topic model documents containing pw θ̂ cw	 Also recall that we used the maximum likelihood estimate of a unigram language model to find pw  θ̂  where θ̂ in our case is the topic language model associated with documents containing the term computer  That is pw  θ̂   cw D D .
θ̂ cw D .	 That is pw  θ̂   cw D D .
score related word model	4 The score indicates how related a word is to our topic language model term computer.
maximum C cw cw D cw .	 Using maximum likelihood estimation this becomes scorew  pw  computer pw  C cw D D cw C cw D   C cw .
C cw	 C cw .
5 D set term entire likely context greater denominator	5 where D is the set of documents containing the term computer and C is the entire collection of documents  We see that words that are more likely to appear in the context of computer will have a greater numerator than denominator thus increasing the score.
appear score Words occur denominator numerator resulting score	 Words such as the that appear about equally regardless of the context will have a score close to one  Words that usually do not occur in the context of computer will have a denominator less than the numerator resulting in a score less than one.
As mentioned Section	 As mentioned in Section 3.
4 slight formula example assume artichoke mentioned Using artichoke know true.	4 there is a slight issue with this normalization formula  For example assume the word artichoke appears only once in the corpus and it happens to be in a document where computer is mentioned  Using the above formula will have artichoke and computer very highly related even though we know this is not true.
artichoke problem likelihood observed extra count word words.	 Using the above formula will have artichoke and computer very highly related even though we know this is not true  One way to solve this problem is to smooth a maximum likelihood estimator by pretending that we have observed an extra pseudo count of every word including unseen words.
likelihood pretending pseudo word words Thus computing language model pw C cw V	 One way to solve this problem is to smooth a maximum likelihood estimator by pretending that we have observed an extra pseudo count of every word including unseen words  Thus the formula for computing a smoothed background language model would be pw  C  cw C  1 C  V   14.
total count V complete vocabulary Note variable V pseudo added words vocabulary.	6 where C is the total count of all the words in collection C and V  is the size of the complete vocabulary set  Note that the variable V  in the denominator is the total number of pseudo counts we have added to all the words in the vocabulary.
Note total pseudo counts words	 Note that the variable V  in the denominator is the total number of pseudo counts we have added to all the words in the vocabulary.
14 287 formula unseen word zero probability estimated accurate scoring smoothed version.	 14 3 Term Clustering 287 With such a formula an unseen word would not have a zero probability and the estimated probability is in general more accurate  We can replace pw  C in the previous scoring function with our smoothed version.
function smoothed version score seen	 We can replace pw  C in the previous scoring function with our smoothed version  In the example this brings the score for artichoke much lower since we “pretend” to have seen a count of it in the background.
In brings lower “pretend” count background semantically related	 In the example this brings the score for artichoke much lower since we “pretend” to have seen a count of it in the background  Words that actually are semantically related i e.
actually	 Words that actually are semantically related i e.
	e.
frequently affected smoothing “rise up” From smoothing method probability mass	 that occur much more frequently in the context of computer would not be affected by this smoothing and instead would “rise up” as the unrelated words are shifted downwards in the list of sorted scores  From Chapter 6 we learned that this Add1 smoothing may not be the best smoothing method as it applies too much probability mass to unseen words.
Chapter Add1 best smoothing applies words.	 From Chapter 6 we learned that this Add1 smoothing may not be the best smoothing method as it applies too much probability mass to unseen words.
In improved scoring smoothing methods interpolation.	 In an even more improved scoring function we could use other smoothing methods such as Dirichlet prior or JelinekMercer interpolation.
event semantic wish term applications.	 In any event this semantic relatedness is what we wish to capture in our term clustering applications.
infeasible run vocabulary section cluster	 However you can probably see that it would be infeasible to run this calculation for every term in our vocabulary  Thus in the next section we will examine a more efficient method to cluster terms together.
method terms idea problem	 Thus in the next section we will examine a more efficient method to cluster terms together  The basic idea of the problem is exactly the same.
idea 14.	 The basic idea of the problem is exactly the same  14.
Information Mutual Information word random variables probability context words.	 14 3 2 Pointwise Mutual Information Pointwise Mutual Information PMI treats word occurrences as random variables and quantifies the probability of their cooccurrence within some context of a window of words.
2 Pointwise Mutual Pointwise PMI treats random quantifies context words For words n look words	2 Pointwise Mutual Information Pointwise Mutual Information PMI treats word occurrences as random variables and quantifies the probability of their cooccurrence within some context of a window of words  For example to find words that cooccur with wi using a window of size n we look at the words wi−n   .
wi−1 .	      wi−1 wi  wi1 .
wi−1 wi .	    wi−1 wi  wi1 .
wi−1 wi	  wi−1 wi  wi1     .
	 .
.	 .
	 14.
Note pwipwj	7 Note that if wi and wj are independent then pwipwj  pwi  wj.
yields PMI zero information	 This forces us to take a logarithm of 1 which yields a PMI of zero there is no measure of information transferred by two independent words.
words	 If however the probability of observing the two words occurring together i.
expected	 pwi  wj is substantially larger than their expected probability of cooccurrence if there were independent i e.
PMI size n document	e  pwipwj then the PMI would be high as we would expect  Depending on our application we can define the context as the aforementioned window of size n a sentence a document and so on.
PMI expect aforementioned n context modifies PMI—for context n significantly context document corpus.	 pwipwj then the PMI would be high as we would expect  Depending on our application we can define the context as the aforementioned window of size n a sentence a document and so on  Changing the context modifies the interpretation of PMI—for example if we only considered a context to be of size n  1 we will get significantly different results than if we set the context to be an entire document from the corpus.
Depending application define context aforementioned window	 Depending on our application we can define the context as the aforementioned window of size n a sentence a document and so on.
PMI—for size significantly results entire document corpus In PMI depends context.	 Changing the context modifies the interpretation of PMI—for example if we only considered a context to be of size n  1 we will get significantly different results than if we set the context to be an entire document from the corpus  In order to have comparable PMI scores we also need to ensure that our PMI measure is symmetric this again depends on our definition of context.
order need ensure depends definition context wi” pmiwi wi required terms.	 In order to have comparable PMI scores we also need to ensure that our PMI measure is symmetric this again depends on our definition of context  If we define context to be “wj follows wi” then pmiwi  wj  pmiwj  wi which is required to cluster terms.
If follows wi” terms.	 If we define context to be “wj follows wi” then pmiwi  wj  pmiwj  wi which is required to cluster terms.
It normalize 0 wj − log wj 14.	 It is possible to normalize the PMI score in the range 0 1 npmiwi  wj  pmiwi  wj − log pwi  wj 14.
8 different word pairs However normal ization fix issue	8 making comparisons between different word pairs possible  However this normal ization doesn’t fix a major issue in the PMI formula itself.
ization fix word occurs context common pair highly related connection	 However this normal ization doesn’t fix a major issue in the PMI formula itself  Imagine that we have a rare word that always occurs in the context of another perhaps very common word  It would seem that this word pair is very highly related but in fact our data is just too sparse to model the connection appropriately.
rare occurs context	 Imagine that we have a rare word that always occurs in the context of another perhaps very common word  It would seem that this word pair is very highly related but in fact our data is just too sparse to model the connection appropriately.
It word data appropriately.	 It would seem that this word pair is very highly related but in fact our data is just too sparse to model the connection appropriately.
This Chapter 13 ers case rare word	 This problem can be alleviated by using the mutual information measure introduced in Chapter 13 which consid ers not just the case when the rare word is observed but also the case when it is not observed.
methods allowing idea capture cooccurrence.	 Despite their drawbacks however PMI and nPMI are often used in practice and are also useful building blocks for more advanced methods as well as allowing us to understand the basic idea behind information capture in word cooccurrence.
We book.	 We thus included a discussion in this book.
methods	 The windowing technique employed here is critical in both of the following advanced methods  14.
3 Methods In section introduce methods term	 14 3 3 Advanced Methods In this section we introduce two advanced methods for term clustering.
Advanced In term clustering 14 3.	3 Advanced Methods In this section we introduce two advanced methods for term clustering  14 3.
3.	 14 3.
3	3 3.
Class Brown et al.	1 Ngram Class Language Models Brown clustering Brown et al.
term algorithm constructs clusters called However optimization actual process term clusters gradually Brown clustering derived function.	 1992 is a modelbased term clustering algorithm that constructs term clusters called word classes to maximize the likelihood of an ngram class language model  However since the optimization problem is in tractable to solve computationally the actual process of constructing term clusters is actually similar to hierarchical agglomerative clustering where single words are merged gradually but the criterion for merging in Brown clustering is based on a similarity function derived from the likelihood function.
optimization hierarchical agglomerative single merged criterion merging based similarity derived	 However since the optimization problem is in tractable to solve computationally the actual process of constructing term clusters is actually similar to hierarchical agglomerative clustering where single words are merged gradually but the criterion for merging in Brown clustering is based on a similarity function derived from the likelihood function.
mization equivalent maximization information adjacent word merging words merging words words respective adjacent words vocabulary classes word − 1 wn	 Specifically the maxi mization of the likelihood function is shown to be equivalent to maximization of the mutual information of adjacent word classes thus when merging two words the algorithm would favor merging two words that are distributed very similarly since when such words are replaced by their respective classes it would minimize the decrease of mutual information between adjacent classes  Mathematically assuming that we partition all the words in the vocabulary into C classes the ngram class language model defines the probability of observing a word wn given that we have already n − 1 words preceeding wn i e.
assuming partition words C classes ngram class word wn 1 words i.	 Mathematically assuming that we partition all the words in the vocabulary into C classes the ngram class language model defines the probability of observing a word wn given that we have already n − 1 words preceeding wn i.
	e.
cnpcn cn−1	  w1  pwn  cnpcn  cn−1   .
.	 .
class word wi.	    c1 where ci is the class of word wi.
class	  c1 where ci is the class of word wi.
word class ngram class fewer parameters regular language	 every word is in its own class the ngram class language model always has fewer parameters than the regular ngram language model.
As model looking previous words	 As a generative model we would generate a word by first looking up the classes of the previous words i e.
cn−1 .	e  cn−1 .
sample class nth cn pcn .	      c1 then sample a class for the nth position cn using pcn  cn−1 .
c1 sample class pcn cn−1	  c1 then sample a class for the nth position cn using pcn  cn−1 .
word	  c1 and finally sample a word at the nth position by using pw  cn.
The frequently observe latent class	 The distribution pw  cn captures how frequently we will observe word w when the latent class cn is used.
If given words maximum likelihood hard simply words classes pcn .	 If we are given the partitioning of words into C classses then the maximum likelihood estimation is not hard as we can simply replace the words with their corresponding classes to estimate pcn  cn−1 .
.	 .
estimating ngram model word pw easily pooling observations normalizing counts estimate data However computationally	    c1 in the same way as we would for estimating a regular ngram language model and the probability of a word given a particular class pw  c can also be easily estimated by pooling together all the observations of words in the data belonging to the class c and normalizing their counts which gives an estimate of pw  c essentially based on the count of word w in the whole data set  However finding the best partitioning of words is computationally intractable.
estimating regular ngram model probability particular pw estimated observations words data c c count w greedy algorithm classes agglomerative hierarchical clustering i.	  c1 in the same way as we would for estimating a regular ngram language model and the probability of a word given a particular class pw  c can also be easily estimated by pooling together all the observations of words in the data belonging to the class c and normalizing their counts which gives an estimate of pw  c essentially based on the count of word w in the whole data set  However finding the best partitioning of words is computationally intractable  Fortunately we can use a greedy algorithm to construct word classes in very much the same way as agglomerative hierarchical clustering i.
However finding best partitioning words greedy word i.	 However finding the best partitioning of words is computationally intractable  Fortunately we can use a greedy algorithm to construct word classes in very much the same way as agglomerative hierarchical clustering i.
way clustering	 Fortunately we can use a greedy algorithm to construct word classes in very much the same way as agglomerative hierarchical clustering i e.
merging lihood neat maximization likelihood alent mutual information	 gradually merging words to form classes by keeping track of the objective of maximizing the like lihood  A neat theoretical result is that the maximization of the likelihood is equiv alent to maximization of the mutual information between all the adjacent classes in the case of bigram model.
neat maximization maximization adjacent Thus tend similar e	 A neat theoretical result is that the maximization of the likelihood is equiv alent to maximization of the mutual information between all the adjacent classes in the case of bigram model  Thus the best pairs of words to merge would tend to be those that are distributed in very similar contexts e g.
pairs merge tend distributed contexts e g putting words class class word	 Thus the best pairs of words to merge would tend to be those that are distributed in very similar contexts e g  Tuesday and Wednesday since by putting such words in the same class the prediction power of the class would be about the same as that of the original word allowing to minimize the loss of mutual information.
words prediction original allowing loss mutual information Computationwise simply hierarchi clustering based function function merging words Due complexity n Brown	 Tuesday and Wednesday since by putting such words in the same class the prediction power of the class would be about the same as that of the original word allowing to minimize the loss of mutual information  Computationwise we simply do agglomerative hierarchi cal clustering and measure the “distance” of two words based on a derived function based on the likelihood function that can capture the loss of mutual information due to merging the two words  Due to the complexity of the model only bigrams n  2 were originally investigated Brown et al.
bigrams investigated et al.	 Due to the complexity of the model only bigrams n  2 were originally investigated Brown et al.
Empirically bigram model work generate highquality words paradigmatic relation	 1992  Empirically the bigram class language model has been shown to work very well and can generate very highquality paradigmatic word associations directly by treating words in the same class as having paradigmatic relation  Figure 14.
class work highquality word having paradigmatic	 Empirically the bigram class language model has been shown to work very well and can generate very highquality paradigmatic word associations directly by treating words in the same class as having paradigmatic relation.
shows clusters Brown relations The generate essentially mutual words occur different positions.	6 shows some sample word clusters taken from Brown et al  1992 they clearly capture paradigmatic relations well  The model can also be used to generate syntagmatic associations by essentially computing the pointwise mutual information between words that occur in different positions.
1992 capture relations generate syntagmatic essentially occur positions cooccurrences	 1992 they clearly capture paradigmatic relations well  The model can also be used to generate syntagmatic associations by essentially computing the pointwise mutual information between words that occur in different positions  When the window of cooccurrences is restricted to two words i.
associations computing pointwise occur different restricted words	 The model can also be used to generate syntagmatic associations by essentially computing the pointwise mutual information between words that occur in different positions  When the window of cooccurrences is restricted to two words i.
cooccurrences words i.	 When the window of cooccurrences is restricted to two words i.
e adjacent discover “sticky Figure sample results noncompositional words.	e  adjacent cooccurrences the model can discover “sticky phrases” see Figure 14 7 for sample results which are noncompositional phrases whose meaning is not a direct composition of the meanings of individual words.
g.	g.
Zhai Lin 1999 14	 Zhai 1997 Lin 1999  14 3.
3 3 2 language model word In 13 discussed length context occurs based representations.	3 3 2 Neural language model word embedding In Chapter 13 we discussed in length how to represent a term as a term vector based on the words in the context where the term occurs and compute term similarity based on the similarity of their vector representations.
2 language model 13 discussed term vector compute based view term representation relations support use cluster ing viewing “document” represented	3 2 Neural language model word embedding In Chapter 13 we discussed in length how to represent a term as a term vector based on the words in the context where the term occurs and compute term similarity based on the similarity of their vector representations  Such a contextual view of term representation can not only be used for discovering paradigmatic relations but also support term clustering in general since we can use any document cluster ing algorithm by viewing a term as a “document” represented by a vector.
view term paradigmatic term general document term “document” represented disambiguation ambiguous word sense tends “attract” words different representation.	 Such a contextual view of term representation can not only be used for discovering paradigmatic relations but also support term clustering in general since we can use any document cluster ing algorithm by viewing a term as a “document” represented by a vector  It can also help word sense disambiguation since when an ambiguous word takes a different sense it tends to “attract” different words in its surrounding text thus would have a different context representation.
help sense disambiguation different words text different	 It can also help word sense disambiguation since when an ambiguous word takes a different sense it tends to “attract” different words in its surrounding text thus would have a different context representation.
Adding features means expanding require finding richer ability downstream grammatical statistical translation obtain vector representation 13 obtain term weights.	 Adding these additional features means expanding the word vector from V  to whatever size we require  Additionally aside from finding semantically related terms using this richer word representation has the ability to improve downstream tasks such as grammatical parsing or statistical machine translation  However the heuristic way to obtain vector representation discussed in Chap ter 13 has the disadvantage that we need to make many ad hoc choices especially in how to obtain the term weights.
aside finding semantically richer downstream grammatical parsing machine However heuristic representation ter 13 disadvantage especially term weights.	 Additionally aside from finding semantically related terms using this richer word representation has the ability to improve downstream tasks such as grammatical parsing or statistical machine translation  However the heuristic way to obtain vector representation discussed in Chap ter 13 has the disadvantage that we need to make many ad hoc choices especially in how to obtain the term weights.
spans space words processing alternative use neural language et	 Another deficiency is that the vector spans the entire space of words in the vocabulary increasing the complexity of any further processing applied to the vectors  As an alternative we can use a neural language model Mikolov et al.
2010 tematically word meaningful function approach embedding refers mapping vector representation idea methods assume vector latent space model based involved parameters language vector	 2010 to sys tematically learn a vector representation for each word by optimizing a meaningful objective function  Such an approach is also called word embedding which refers to the mapping of a word into a vector representation in a lowdimensional space  The general idea of these methods is to assume that each word corresponds to a vector in an unknown latent lowdimensional space and define a language model solely based on the vector representations of the involved words so that the parameters for such a language model would be the vector representations of words.
result model set vector These neural models network.	 As a result by fitting the model to a specific data set we can learn the vector representations for all the words  These language models are called neural language models because they can be represented as a neural network.
	 .
	 .
w1 neural	  w1 the neural network would have wn−1   .
	 .
w1 output models layer neural network connected word vector represen word elements weights	    w1 as input and wn as the output  In some neural language models the hidden layer in the neural network connected to a word can be interpreted as a vector represen tation of the word with the elements being the weights on the edges connected to the word.
2013 word defined window word given exp vi .	 2013 the objective function is to use each word to predict all other words in its context as defined by a window around the word and the probability of predicting word w1 given word w2 is given by pw1  w2  exp v1  v2∑ wi∈V exp vi .
exp vi v2 word	 v2∑ wi∈V exp vi   v2 where vi is the corresponding vector representation of word wi.
words probability dot product vectors corresponding w2 vector representation words probability predict words word.	 v2 where vi is the corresponding vector representation of word wi  In words such a model says that the probability pw1  w2 is proportional to the dot product of the vectors corresponding to the two words w1 and w2  With such a model we can then try to find the vector representation for all the words that would maximize the probability of using each word to predict all other words in a small window of words surrounding the word.
says probability proportional dot product corresponding w1 With words maximize window words word.	 In words such a model says that the probability pw1  w2 is proportional to the dot product of the vectors corresponding to the two words w1 and w2  With such a model we can then try to find the vector representation for all the words that would maximize the probability of using each word to predict all other words in a small window of words surrounding the word.
model word words small words	 With such a model we can then try to find the vector representation for all the words that would maximize the probability of using each word to predict all other words in a small window of words surrounding the word.
skipgram word2vec Mikolov al.	 Google’s implementation of skipgram called word2vec Mikolov et al.
2013 wellknown They showed results vectors Germany vector	 2013 is perhaps the most wellknown software in this area  They showed that performing vector addition on terms in vector space yielded interesting results  For example adding the vectors for Germany and capital resulted in a vector very close to the vector Berlin.
vector For adding vectors capital resulted close vector	 They showed that performing vector addition on terms in vector space yielded interesting results  For example adding the vectors for Germany and capital resulted in a vector very close to the vector Berlin.
vectors Germany capital resulted Berlin.	 For example adding the vectors for Germany and capital resulted in a vector very close to the vector Berlin.
sim obtained methods class language promising tially open new flexibility functions optimized fact vector learned optimizing explicitly objective function.	 Although sim ilar results can also be obtained by using heuristic paradigmatic relation discovery e g  using the methods we described in Chapter 13 and the ngram class language model word embedding provides a very promising new alternative that can poten tially open up many interesting new applications of text mining due to its flexibility in formulating the objective functions to be optimized and the fact that the vector representation is systematically learned through optimizing an explicitly defined objective function.
g methods Chapter 13 language word embedding tially interesting new applications fact vector optimizing explicitly objective	g  using the methods we described in Chapter 13 and the ngram class language model word embedding provides a very promising new alternative that can poten tially open up many interesting new applications of text mining due to its flexibility in formulating the objective functions to be optimized and the fact that the vector representation is systematically learned through optimizing an explicitly defined objective function.
disadvantage current form vector word meaning easily interpreted As vectors	 One disadvantage of word embedding at least in its current form is that the elements in the vector representation of a word are not meaning ful and cannot be easily interpreted intuitively  As a result the utility of these word vectors has so far been mostly limited to computation of word similarities which can also obtained by using many other methods.
measure similarity clustering approach followed pointwise introduced modelbased approaches language language	 In summary we have shown several methods to measure term similarity which can then be used for term clustering  We started with a unigram language modeling approach followed by pointwise mutual information  We then briefly introduced two modelbased approaches one based on ngram language models and one based on neural language models for word embedding.
ngram language language methods improve text	 We then briefly introduced two modelbased approaches one based on ngram language models and one based on neural language models for word embedding  These term clustering methods can be leveraged to improve the computation of similarity between documents or other text objects by allowing inexact matching of terms e.
These leveraged computation text objects inexact	 These term clustering methods can be leveraged to improve the computation of similarity between documents or other text objects by allowing inexact matching of terms e.
high similarity	g  allowing words in the same cluster or with high similarity to “match” with each other.
14 clustering attempt maximize measures.	 14 4 Evaluation of Text Clustering All clustering methods attempt to maximize the following measures.
clustering attempt following	4 Evaluation of Text Clustering All clustering methods attempt to maximize the following measures.
cluster far	 Coherence  How similar are objects in the same cluster Separation  How far away are objects in different clusters Utility.
similar cluster Separation far clusters useful discovered application evaluate broad evaluation predefined measures.	 How similar are objects in the same cluster Separation  How far away are objects in different clusters Utility  How useful are the discovered clusters for an application As with most text mining and many other tasks we can evaluate in one of two broad strategies manual evaluation using humans or automatic evaluation using predefined measures.
How away objects clusters application As evaluate broad strategies predefined measures Of automatically sim ilarity mutual	 How far away are objects in different clusters Utility  How useful are the discovered clusters for an application As with most text mining and many other tasks we can evaluate in one of two broad strategies manual evaluation using humans or automatic evaluation using predefined measures  Of the three criteria mentioned above coherence and separation can be measured automatically with measures such as vector sim ilarity purity or mutual information.
How useful application evaluate broad strategies evaluation humans automatic Of criteria mentioned separation purity information.	 How useful are the discovered clusters for an application As with most text mining and many other tasks we can evaluate in one of two broad strategies manual evaluation using humans or automatic evaluation using predefined measures  Of the three criteria mentioned above coherence and separation can be measured automatically with measures such as vector sim ilarity purity or mutual information.
purity information challenge evaluating similarity algorithms obvious We word wish use	 Of the three criteria mentioned above coherence and separation can be measured automatically with measures such as vector sim ilarity purity or mutual information  There is a slight challenge when evaluating term clustering since wordtoword similarity algorithms may not be as obvious as documenttodocument similarities  We may choose to encode terms as word vec tors and use the document similarity measures or we may wish to use some other concept of semantic similarity as defined by preexisting ontologies like WordNet.
There clustering similarity obvious similarities tors similarity use concept similarity preexisting like WordNet slightly measured quantitatively.	 There is a slight challenge when evaluating term clustering since wordtoword similarity algorithms may not be as obvious as documenttodocument similarities  We may choose to encode terms as word vec tors and use the document similarity measures or we may wish to use some other concept of semantic similarity as defined by preexisting ontologies like WordNet 2 Although slightly more challenging the concept of utility can also be captured if the final system output can be measured quantitatively.
choose word vec use similarity wish use concept semantic like	 We may choose to encode terms as word vec tors and use the document similarity measures or we may wish to use some other concept of semantic similarity as defined by preexisting ontologies like WordNet.
For clustering component search different clustering MAP NCDG Chapter 9 need	 For example if clustering is used as a component in search we can see if using a different clustering algorithm improves F1 MAP or NCDG see Chapter 9  All clustering methods need some notion of similarity or bias.
methods need notion words book	 All clustering methods need some notion of similarity or bias  After all we wish to find groups of objects that are similar to one another in some way  We mainly discussed unigram words representations though in this book we have elaborated on many different feature types.
words representations feature	 We mainly discussed unigram words representations though in this book we have elaborated on many different feature types.
engineering nent implementing algorithm text mining Choosing representation allows important differences items cause clusters Even algorithm performs spectacularly example intracluster similarity clusters acceptable representation able capture crucial concept	 Indeed feature engineering is an important compo nent of implementing a clustering algorithm and in fact any text mining algorithm in general  Choosing the right representation for your text allows you to quantify the important differences between items that cause them to end up in either the same or different clusters  Even if your clustering algorithm performs spectacularly in terms of for example intracluster similarity the clusters may not be acceptable from a human viewpoint unless an adequate feature representation was used it’s possible that the feature representation is not able to capture a crucial concept and needs to be reexamined.
right representation allows quantify important differences items cause	 Choosing the right representation for your text allows you to quantify the important differences between items that cause them to end up in either the same or different clusters.
good overview In chapter categorization discuss right	 Chapter 4 gives a good overview of many different textual features supported by META  In the next chapter on text categorization Chapter 15 we also discuss how choosing the right features plays an important role in the over all classification accuracy.
As chapter explicitly function intracluster minimize intracluster	 As we saw in this chapter similaritybased algorithms explicitly encode a simi larity function in their implementation  Ideally this similarity between objects is optimized to maximize intracluster coherence and minimize intracluster separa tion.
intracluster coherence separa methods 17 inherently model notion object terms “similar” objects.	 Ideally this similarity between objects is optimized to maximize intracluster coherence and minimize intracluster separa tion  In modelbased methods which will be discussed in Chapter 17 similarity functions are not inherently part of the model instead the notion of object sim ilarity is most often captured by probabilistically high cooccurring terms within “similar” objects.
discussed Chapter instead notion high “similar” Measuring coherence separation potentially accom plished leveraging categorization set belongs product descriptions retailer product belongs books grocery	 In modelbased methods which will be discussed in Chapter 17 similarity functions are not inherently part of the model instead the notion of object sim ilarity is most often captured by probabilistically high cooccurring terms within “similar” objects  Measuring coherence and separation automatically can potentially be accom plished by leveraging a categorization data set such a corpus has predefined clus ters where each document belongs to a particular category  For example a text categorization corpus could be product descriptions from an online retailer and each product belongs in a product category such as kitchenware books grocery and so on.
potentially accom plished leveraging belongs category text product descriptions online product books on.	 Measuring coherence and separation automatically can potentially be accom plished by leveraging a categorization data set such a corpus has predefined clus ters where each document belongs to a particular category  For example a text categorization corpus could be product descriptions from an online retailer and each product belongs in a product category such as kitchenware books grocery and so on.
categorization product online belongs kitchenware based text simple application consider output predefined categories dominates population.	 For example a text categorization corpus could be product descriptions from an online retailer and each product belongs in a product category such as kitchenware books grocery and so on  A clustering algorithm would be effective if it was able to partition the products based on their text into categories that roughly matched the prede fined ones  A simple measure to evaluate this application would be to consider each output cluster and see if one of the predefined categories dominates the clus ter population.
A clustering products based categories A application categories dominates	 A clustering algorithm would be effective if it was able to partition the products based on their text into categories that roughly matched the prede fined ones  A simple measure to evaluate this application would be to consider each output cluster and see if one of the predefined categories dominates the clus ter population.
evaluate application consider output dominates population In Ci	 A simple measure to evaluate this application would be to consider each output cluster and see if one of the predefined categories dominates the clus ter population  In other words take each cluster Ci and calculate the percentage of each predefined class in it.
algorithm origi dataset Of parameter number classes In deciding optimal meth For example depend initial starting positions.	 Effectively the clustering algorithm recreated the class assignments in the origi nal dataset without any supervision  Of course however we have to be careful if this is a parameter to set the final number of clusters to match the number of classes  In fact deciding the optimal number of clusters is a hard problem for all meth ods For example in Kmeans the final clusters depend on the initial random starting positions.
parameter match optimal number problem example depend initial starting	 Of course however we have to be careful if this is a parameter to set the final number of clusters to match the number of classes  In fact deciding the optimal number of clusters is a hard problem for all meth ods For example in Kmeans the final clusters depend on the initial random starting positions.
In number hard ods For example depend starting positions.	 In fact deciding the optimal number of clusters is a hard problem for all meth ods For example in Kmeans the final clusters depend on the initial random starting positions.
Thus run algorithm results Hamerly 2003 way splitting clusters cluster	 Thus it’s quite common to run the algorithm several times and manually inspect the results  The algorithm Gmeans Hamerly and Elkan 2003 reruns Kmeans in a more principled way splitting clusters if the data assigned to each cluster is not normallydistributed.
The Hamerly reruns Kmeans principled way clusters data assigned cluster normallydistributed deciding clusters model inaccurate.	 The algorithm Gmeans Hamerly and Elkan 2003 reruns Kmeans in a more principled way splitting clusters if the data assigned to each cluster is not normallydistributed  Modelbased methods may have some ad vantages in terms of deciding the optimal number of clusters but the model itself is often inaccurate.
methods deciding clusters model inaccurate empirically set number fixed number application needs knowledge Which best bias definition reflects sumptions	 Modelbased methods may have some ad vantages in terms of deciding the optimal number of clusters but the model itself is often inaccurate  In practice we may empirically set the number of clusters to a fixed number based on application needs or domain knowledge  Which method works the best highly depends on whether the bias definition of similarity reflects our perspective for clustering accurately and whether the as sumptions made by an approach hold for the problem and applications.
empirically set application knowledge Which method works best definition reflects perspective clustering sumptions approach hold	 In practice we may empirically set the number of clusters to a fixed number based on application needs or domain knowledge  Which method works the best highly depends on whether the bias definition of similarity reflects our perspective for clustering accurately and whether the as sumptions made by an approach hold for the problem and applications.
general potential constraints model.	 In general modelbased approaches have more potential for doing “complex clustering” by encoding more constraints into the probabilistic model.
Aggarwal clustering methods document al.	 There is also a chapter on text clustering in Aggarwal and Zhai 2012 where many text clustering methods are reviewed  An empirical comparison of some document clustering techniques can be found in Steinbach et al.
	 2000.
Term clustering word covered theoretical tering satisfy small properties	 Term clustering is related to word associ ation discovery a topic covered in Chapter 13  An interesting theoretical work on clustering is Kleinberg 2002 where it is shown that there does not exist any clus tering that can satisfy a small number of desirable properties i.
An clustering exist tering desirable properties e.	 An interesting theoretical work on clustering is Kleinberg 2002 where it is shown that there does not exist any clus tering that can satisfy a small number of desirable properties i e.
theorem clustering.	e  an impossibility theorem about clustering.
impossibility clustering 15Text In previous chapter discussed clustering—grouping similar	 an impossibility theorem about clustering  15Text Categorization 15 1 Introduction In the previous chapter we discussed how to perform text clustering—grouping documents together based on similar features.
1 Introduction chapter text clustering—grouping documents based features.	1 Introduction In the previous chapter we discussed how to perform text clustering—grouping documents together based on similar features.
Clustering unsuper vised manual effort humans text However way predefined categories news agency interested classifying articles technology sports	 Clustering techniques are unsuper vised which has the advantage of not requiring any manual effort from humans and being applicable to any text data  However we often want to group text ob jects in a particular way according to a set of predefined categories  For example a news agency may be interested in classifying news articles into one or more topical categories such as technology sports politics or entertainment etc.
However want jects particular set categories For news interested classifying articles topical categories technology sports etc.	 However we often want to group text ob jects in a particular way according to a set of predefined categories  For example a news agency may be interested in classifying news articles into one or more topical categories such as technology sports politics or entertainment etc.
For news classifying news articles topical technology sports techniques clusters categories news agency To problem use	 For example a news agency may be interested in classifying news articles into one or more topical categories such as technology sports politics or entertainment etc  If we are to use clustering techniques to solve this problem we may obtain coherent topical clus ters but these clusters do not necessarily correspond to the categories the news agency has designed for their application purpose  To solve such a problem we can use text categorization techniques which have widespread applications.
clustering coherent ters clusters news agency designed	 If we are to use clustering techniques to solve this problem we may obtain coherent topical clus ters but these clusters do not necessarily correspond to the categories the news agency has designed for their application purpose.
task categorization objects illustrated 15.	e  text objects with known labels of categories the task of text categorization is to label unseen text objects with one or more categories  This is illustrated in Figure 15.
known labels categories unseen text objects categories This 1.	 text objects with known labels of categories the task of text categorization is to label unseen text objects with one or more categories  This is illustrated in Figure 15 1.
Figure	 This is illustrated in Figure 15.
1 At usually help applications	1  At the very high level text categorization is usually to help achieve two goals of applications  1.
level help achieve goals applications To representation	 At the very high level text categorization is usually to help achieve two goals of applications  1  To enrich text representation i.
To	 1  To enrich text representation i e.
e.	 To enrich text representation i e.
understanding text text categorization keywords categories.	e  achieving more understanding of text with text categorization we would be able to represent text in multiple levels keywords  categories.
understanding text categorization levels	 achieving more understanding of text with text categorization we would be able to represent text in multiple levels keywords  categories.
semantic spam text meaningful example sentiment enable aggregation positivenegative meaningful	 In such an application we also call text categorization text annotation  For example semantic categories assigned to text can be directly useful for an application as in the case of spam detection  Semantic categories assigned to text data can also facilitate aggregation of text content in a more meaningful way for example sentiment classification would enable aggregation of all positivenegative opinions about a product so as to give a more meaningful overall assessment of opinions.
semantic categories text directly detection facilitate meaningful example sentiment aggregation positivenegative opinions opinions.	 For example semantic categories assigned to text can be directly useful for an application as in the case of spam detection  Semantic categories assigned to text data can also facilitate aggregation of text content in a more meaningful way for example sentiment classification would enable aggregation of all positivenegative opinions about a product so as to give a more meaningful overall assessment of opinions.
categories aggregation way enable aggregation meaningful	 Semantic categories assigned to text data can also facilitate aggregation of text content in a more meaningful way for example sentiment classification would enable aggregation of all positivenegative opinions about a product so as to give a more meaningful overall assessment of opinions.
properties data	 2  To infer properties of entities associated with text data i.
To infer properties entities associated knowledge data data associated entities.	 To infer properties of entities associated with text data i e  discovery of knowledge about the world as long as an entity can be associated with text data in some way it is always potentially possible to use the text data to help categorize the associated entities.
e discovery long entity text use text data associated use text data written person nonnative	e  discovery of knowledge about the world as long as an entity can be associated with text data in some way it is always potentially possible to use the text data to help categorize the associated entities  For example we can use the English text data written by a person to predict whether the person is a nonnative speaker of English.
associated data way potentially data help	 discovery of knowledge about the world as long as an entity can be associated with text data in some way it is always potentially possible to use the text data to help categorize the associated entities.
English data predict speaker English party affiliation Naturally categorization category text content	 For example we can use the English text data written by a person to predict whether the person is a nonnative speaker of English  Prediction of party affiliation based on a political speech is another example  Naturally in such a case the task of text categorization is much harder as the “gap” between the category and text content is large.
	 Prediction of party affiliation based on a political speech is another example.
Naturally case task text categorization harder Indeed text categorization	 Naturally in such a case the task of text categorization is much harder as the “gap” between the category and text content is large  Indeed in such an application text categorization should really be called textbased prediction.
somewhat different goals distinguished difference categories	 These two somewhat different goals can also be distinguished based on the difference in the categories in each case.
For enriching resentation categories tend categories text	 For the purpose of enriching text rep resentation the categories tend to be “internal” categories that characterize a text object e.
properties associated entities “external” categories characterize entity text object e.	g  topical categories sentiment categories  For the purpose of inferring properties of associated entities with text data the categories tend to be “external” categories that characterize an entity associated with the text object e.
purpose properties entities data tend entity object e	 topical categories sentiment categories  For the purpose of inferring properties of associated entities with text data the categories tend to be “external” categories that characterize an entity associated with the text object e g.
purpose properties associated data categories characterize associated text	 For the purpose of inferring properties of associated entities with text data the categories tend to be “external” categories that characterize an entity associated with the text object e g.
g author meaningful associated similar text object multiple categories.	g  author at tribution or any other meaningful categories associated with text data potentially through indirect links  Computationally however these variations are all similar in that the input is a text object and the output is one or multiple categories.
distinguish different The text categorization text classified	 We thus do not further distinguish these different variations  The landscape of applications of text categorization is further enriched due to the variation we have in the text objects to be classified which can include e g.
landscape applications variation text classified e	 The landscape of applications of text categorization is further enriched due to the variation we have in the text objects to be classified which can include e g.
g sentences passages collections text.	g  documents sentences passages or collections of text.
documents passages collections text	 documents sentences passages or collections of text  15.
15.	 15.
text explic manually heuristic For example rule occurs → assign pol Obviously designing	 text data with known categories explic itly labelled we often have to manually create heuristic rules to solve the problem of categorization  For example the rule if the word “governor” occurs → assign pol itics label  Obviously designing effective rules requires a significant amount of knowledge about the specific problem of categorization.
occurs itics designing rules requires knowledge problem	 For example the rule if the word “governor” occurs → assign pol itics label  Obviously designing effective rules requires a significant amount of knowledge about the specific problem of categorization.
Obviously effective requires problem Such approach clearly relatively distinguished based	 Obviously designing effective rules requires a significant amount of knowledge about the specific problem of categorization  Such a rulebased manual approach would work well if 1 the categories are very clearly defined usually means that the categories are relatively simple 2 the categories are easily distinguished based on surface features in text e g.
1 clearly usually categories relatively simple 2 distinguished based surface features e suggest effective	 Such a rulebased manual approach would work well if 1 the categories are very clearly defined usually means that the categories are relatively simple 2 the categories are easily distinguished based on surface features in text e g  particular words only occur in a particular category of documents and 3 sufficient domain knowledge is available to suggest many effective rules.
	g.
particular effective rules approach	 particular words only occur in a particular category of documents and 3 sufficient domain knowledge is available to suggest many effective rules  However the manual approach has some significant disadvantages.
manual significant laborintensive new category new new rules.	 However the manual approach has some significant disadvantages  The first is that it is laborintensive thus it does not scale up well both to the number of categories since a new category requires new rules and to the growth of data since new data may also need new rules.
The scale categories new new rules The second possible come completely reliable rules hard rules other.	 The first is that it is laborintensive thus it does not scale up well both to the number of categories since a new category requires new rules and to the growth of data since new data may also need new rules  The second is that it may not be possible to come up with completely reliable rules and it is hard to handle the uncertainty in the rules  Finally the rules may not be all consistent with each other.
The come rules hard uncertainty rules.	 The second is that it may not be possible to come up with completely reliable rules and it is hard to handle the uncertainty in the rules.
Finally rules As categorization results depend different rules These problems help machine labeling examples correct categories i.	 Finally the rules may not be all consistent with each other  As a result the categorization results may depend on the order of application of different rules  These problems with the rulebased manual approach can mostly be addressed by using machine learning where humans would help the machine by labeling some examples with the correct categories i.
These problems manual approach machine examples	 These problems with the rulebased manual approach can mostly be addressed by using machine learning where humans would help the machine by labeling some examples with the correct categories i e.
supervised machine ap creating available happen The rules learning based minimize errors training data learned.	 Note that although in such a supervised machine learning approach cateorization ap pears to be “automatic” it does require human effort in creating the training data unless the training data is naturally available to us which sometimes does happen  The humancreated rules if any can also be used as features in such a learning based approach and they will be combined in a weighted manner to minimize the classification errors on the training data with the weights automatically learned.
The construct soft based tures humans Quinlan interpreted “rulebased” classifier paths root	 The machine may also automatically construct soft rules based on primitive fea tures provided by humans as in the case of decision trees Quinlan 1986 which can be easily interpreted as a “rulebased” classifier but the paths from the root to the leaves i.
learning.	e  the rules are inducted automatically by using machine learning.
automatically learning Once trained categorize text data.	 the rules are inducted automatically by using machine learning  Once a classifier categorizer is trained it can be used to categorize any unseen text data.
learningbased categorization methods discrimina features objects distinguish manner weights learned e.	 In general all these learningbased categorization methods rely on discrimina tive features of text objects to distinguish categories and they would combine mul tiple features in a weighted manner where the weights are automatically learned i e.
minimize categorization training Different methods vary measuring errors training	e  adjusted to minimize errors of categorization on the training data  Different methods tend to vary in their way of measuring the errors on the training data i.
methods vary way errors training	 Different methods tend to vary in their way of measuring the errors on the training data i e.
	e.
optimize features g	 they may optimize a different objective function also called a losscost function and their way of combining features e g  linear vs.
linear	g  linear vs  nonlinear.
	 linear vs.
nonlinear learningbased	 nonlinear  In the rest of the chapter we will further discuss learningbased approaches in more detail.
discuss learningbased These methods generally fall categories labels explicitly new	 In the rest of the chapter we will further discuss learningbased approaches in more detail  These automatic categorization methods generally fall into three categories  Lazy learners or instancebased classifiers do not model the class labels explicitly but compare the new instances with instances seen before usually with a similarity measure.
learners labels explicitly compare seen similarity	 Lazy learners or instancebased classifiers do not model the class labels explicitly but compare the new instances with instances seen before usually with a similarity measure.
These lack generalization training calculation performed time.	 These models are called “lazy” due to their lack of explicit generalization or training step most calculation is performed at testing time.
object provide object combine parameters minimizing	 Discriminative classifiers compute features of a text object that can provide a clue about which category the object should be in and combine them with parameters to control their weights  Parameters are optimized by minimizing categorization errors on training data.
optimized minimizing categorization	 Parameters are optimized by minimizing categorization errors on training data.
clustering leverage we’ve cussed chapters classifiers algorithms data seen	 As with clustering we will be able to leverage many of the techniques we’ve dis cussed in previous chapters to create classifiers the algorithms that assign labels to unseen data based on seen labeled data.
Next examine types features classification investigate common algorithms indexes After evaluation search engine	 Next we examine what types of features text representation are often used for classification  Then we investigate a few common learning algorithms that we can implement with our forward and inverted indexes  After that we see how evaluation for classification is performed since the problem is inherently different from search engine evaluation.
algorithms inverted	 Then we investigate a few common learning algorithms that we can implement with our forward and inverted indexes.
classification engine 15.	 After that we see how evaluation for classification is performed since the problem is inherently different from search engine evaluation  15.
15 Problem Let’s intuitive understanding categorizing mathematical form Let doc uments forward index Chapter 8.	 15 3 Text Categorization Problem Let’s take our intuitive understanding of categorizing documents and rewrite the example from Chapter 2 into a more mathematical form  Let our collection of doc uments be X perhaps they are stored in a forward index see Chapter 8.
uments stored index Chapter	 Let our collection of doc uments be X perhaps they are stored in a forward index see Chapter 8.
vector Chapter 8 example V 8.	 Our vector from Chapter 8 is an example of such an xi with a very small vocabulary of size V   8.
1 quill 1 1 1 long	 mr  1 quill 1 ’s 1 book 1 is 1 very 2 long 1   1.
quill 1 ’s 2 long xj consisted long long book	 1 quill 1 ’s 1 book 1 is 1 very 2 long 1   1  Recall that if a document xj consisted of the text long long book it would be mr.
xj consisted text mr ’s book 0 0.	 Recall that if a document xj consisted of the text long long book it would be mr  0 quill 0 ’s 0 book 1 is 0 very 0 long 2   0.
’s 1 0 0 long 2	 0 quill 0 ’s 0 book 1 is 0 very 0 long 2 .
xi 1 1 1 1 0 1 kth	 In our forward index we’d store xi  1 1 1 1 1 2 1 1 xj  0 0 0 1 0 0 2 0 so xik is the kth term in the ith document.
vector Thus news article yj A function f .	 We also have Y which is a vector of labels for each document  Thus yi may be sports in our news article classification setup and yj could be politics  A classifier is a function f .
sports news article setup function document input outputs predicted Y.	 Thus yi may be sports in our news article classification setup and yj could be politics  A classifier is a function f   that takes a document vector as input and outputs a predicted label ŷ ∈ Y.
classifier f predicted ŷ Y.	 A classifier is a function f   that takes a document vector as input and outputs a predicted label ŷ ∈ Y.
predicted ŷ ∈ sports case ŷ sports true y sports classifier correct	 that takes a document vector as input and outputs a predicted label ŷ ∈ Y  Thus we could have f xi  sports  In this case ŷ  sports and the true y is also sports the classifier was correct in its prediction.
xi sports ŷ classifier classification algorithm	 Thus we could have f xi  sports  In this case ŷ  sports and the true y is also sports the classifier was correct in its prediction  Notice how we can only evaluate a classification algorithm if we know the true labels of the data.
In fact learn good function unseen classify	 In fact we will have to use the true labels in order to learn a good function f   to take unseen document vectors and classify them.
unseen classify them.	 to take unseen document vectors and classify them.
For X parts testing evaluate	 For this reason we often split our corpus X into two parts training data and testing data  The training portion is used to build the classifier and the testing portion is used to evaluate the performance e.
training seeing labels predicted.	 The training portion is used to build the classifier and the testing portion is used to evaluate the performance e g  seeing how many correct labels were predicted.
g correct	g  seeing how many correct labels were predicted  But what does the function f .
But .	 But what does the function f .
determines article sentiment positive greater 1 negative	 actually do Consider this very simple example that determines whether a news article has positive or negative sentiment i e  Y  positive negative f x positive if x’s count for the term good is greater than 1 negative otherwise.
Y f positive count term good 1 otherwise.	e  Y  positive negative f x positive if x’s count for the term good is greater than 1 negative otherwise.
f good greater negative Of course simplified demonstrate idea classifier takes label data sen timent contain term encoded function.	 Y  positive negative f x positive if x’s count for the term good is greater than 1 negative otherwise  Of course this example is overly simplified but it does demonstrate the basic idea of a classifier it takes a document vector as input and outputs a class label  Based on the training data the classifier may have determined that positive sen timent articles contain the term good more than once therefore this knowledge is encoded in the function.
example simplified idea classifier takes document input outputs class	 Of course this example is overly simplified but it does demonstrate the basic idea of a classifier it takes a document vector as input and outputs a class label.
Based training classifier timent good knowledge encoded	 Based on the training data the classifier may have determined that positive sen timent articles contain the term good more than once therefore this knowledge is encoded in the function.
Later creating f data.	 Later in this chapter we will investigate some specific algorithms for creating the function f   based on the training data.
training learning come ent categories.	 based on the training data  It’s also important to note that these learning algorithms come in several differ ent flavors  In binary classification there are only two categories.
note learning ent flavors support	 It’s also important to note that these learning algorithms come in several differ ent flavors  In binary classification there are only two categories  Depending on the type of classifier it may only support distinguishing between two different classes.
classification support arbitrary number labels combine multiple classifiers multiclass	 Multiclass classification can support an arbitrary number of labels  As we will see it’s possible to combine multiple binary classifiers to create a multiclass classifier.
Regression related assigns example problem rainfall years.	 Regression is a very related problem to classification it assigns realvalued scores on some range as opposed to discrete labels  For example a regression problem could be to predict the amount of rainfall for a particular day given rainfall data for previous years.
predict rainfall years output number ≥ 0 representing	 For example a regression problem could be to predict the amount of rainfall for a particular day given rainfall data for previous years  The output ŷ would be a number ≥ 0 perhaps representing rainfall in inches.
number 0 representing inches On hand predict yes	 The output ŷ would be a number ≥ 0 perhaps representing rainfall in inches  On the other hand the classification variant could predict whether there would be rainfall or not Y  yes no  15.
On classification Features importance retrieval	 On the other hand the classification variant could predict whether there would be rainfall or not Y  yes no  15 4 Features for Text Categorization In Chapter 6 we emphasized the importance of the document representation in retrieval performance.
	 15.
In Chapter emphasized importance feature general case same—if text tion positive negative sentiment.	 In Chapter 2 we emphasized the importance of the feature representation in general  The case is the same—if not greater—in text categoriza tion  Suppose we wish to determine whether a document has positive or negative sentiment.
same—if greater—in Suppose wish positive sentiment text method length.	 The case is the same—if not greater—in text categoriza tion  Suppose we wish to determine whether a document has positive or negative sentiment  Clearly a bad text representation method could be the average sentence length.
Suppose wish sentiment bad representation	 Suppose we wish to determine whether a document has positive or negative sentiment  Clearly a bad text representation method could be the average sentence length.
bad method	 Clearly a bad text representation method could be the average sentence length.
term vector sentence indicator sen timent.	 That is the document term vector is a histogram of sentence lengths for each document  Intuitively sentence length would not be a good indicator of sen timent.
Intuitively sentence good indicator timent able distinguish negative documents	 Intuitively sentence length would not be a good indicator of sen timent  Even the best learning algorithm would not be able to distinguish between positive and negative documents based only on sentence lengths.
essay quality.	 In this case sentence length may indeed be some indicator of essay quality.
While classifier trained documents represented classification predicting 2 realistic	 While not perfect we can imagine that a classifier trained on documents represented as sentence lengths would get a higher accuracy than a similar classification setup predicting sentiment 2 As a slightly more realistic example we return to the sentiment analysis problem  Instead of using sentence length we decide to use the standard unigram words representation.
2 slightly problem.	2 As a slightly more realistic example we return to the sentiment analysis problem.
Usually bulk smaller	 Usually most features are not useful and the bulk of the decision is based on a smaller subset of features.
subset feature Although best sentation.	 Determining this smaller subset is the definition of feature selection but we do not discuss this in depth at this point  Although most likely effective even unigram words may not be the best repre sentation.
Although effective words sentation Consider classifier example section.	 Although most likely effective even unigram words may not be the best repre sentation  Consider the terms good and bad as mentioned in the classifier example in the previous section.
terms mentioned example section important good movie	 Consider the terms good and bad as mentioned in the classifier example in the previous section  In this scenario context is very important I thought the movie was good  I thought the movie was not bad.
scenario important I thought	 In this scenario context is very important I thought the movie was good.
I movie Alternatively	 I thought the movie was not bad  Alternatively I thought the movie was not good  I thought the movie was bad.
movie good I thought movie bad.	 Alternatively I thought the movie was not good  I thought the movie was bad.
assume As create document tokenizer length decent sentiment classification	 This we assume  As an exercise create a document tokenizer for META that uses sentence length as a feature  Can you get a decent sentiment classification accuracy 2.
Can classification accuracy 2 try experiment sentencelength tokenizer.	 Can you get a decent sentiment classification accuracy 2  Again try this experiment in META using the same sentencelength tokenizer.
bigram words better perfor bigram miss single words overhyped.	 Again try this experiment in META using the same sentencelength tokenizer  Clearly a bigram words representation would most likely give better perfor mance since we can capture not good and not bad as well as was good and was bad  As a counterexample using only bigram words leads us to miss out on rarer infor mative single words such as overhyped.
mance good bad bigram miss single	 Clearly a bigram words representation would most likely give better perfor mance since we can capture not good and not bad as well as was good and was bad  As a counterexample using only bigram words leads us to miss out on rarer infor mative single words such as overhyped.
counterexample bigram words single words This	 As a counterexample using only bigram words leads us to miss out on rarer infor mative single words such as overhyped  This term is now captured in bigrams such as overhyped period and very overhyped.
word different term polarity phe nomenon common sets unigram	 If we see the same rarer informative word in a different context—such as was overhyped—this is now an outofvocabulary term and can’t be used in determining the sentence polarity  Due to this phe nomenon it is very common to combine multiple feature sets together  In this case we can tokenize documents with both unigram and bigram words.
Due combine multiple feature documents bigram	 Due to this phe nomenon it is very common to combine multiple feature sets together  In this case we can tokenize documents with both unigram and bigram words.
documents	 In this case we can tokenize documents with both unigram and bigram words.
A wellknown discussed 2009 lowlevel features combined syntactic performance classifier.	 A wellknown strategy discussed in Stamatatos 2009 shows that lowlevel lexical features combined with highlevel syntactic features give the best performance in a classifier.
These types orthogonal capturing different text feature space.	 These two types of features are more orthogonal thus capturing different perspectives of the text to enrich the feature space.
wide range space decision boundary different class highlevel features Massung al 2013.	 Having many different types of features allows the classifier a wide range of space on which to create a decision boundary between different class labels  An example of very highlevel features can be found in Massung et al  2013.
An example highlevel features et Con sider grammatical discussed Figure	 An example of very highlevel features can be found in Massung et al  2013  Con sider the grammatical parse tree discussed in Chapter 4 reproduced in Figure 15.
	 2013.
sider parse Chapter 15	 Con sider the grammatical parse tree discussed in Chapter 4 reproduced in Figure 15 2.
2 Here versions “highlevel”	2  Here we see three versions of increasingly “highlevel” syntactic features.
Here “highlevel” syntactic	 Here we see three versions of increasingly “highlevel” syntactic features.
bot rewrite sentence containing syntactic node example S sentence NP followed phrase period	 The bot tom left square shows rewrite rules these are the grammatical productions found in the sentence containing the syntactic node categories  For example the S repre sents sentence which is composed of a noun phrase NP followed by a verb phrase VP ending with a period  The middle square in Figure 15.
For example sents composed noun phrase NP VP square Figure subtrees.	 For example the S repre sents sentence which is composed of a noun phrase NP followed by a verb phrase VP ending with a period  The middle square in Figure 15 2 omits all node labels except the roots of all subtrees.
The Figure 15 2 omits node labels This abstract produc rules focused structure.	 The middle square in Figure 15 2 omits all node labels except the roots of all subtrees  This captures a more abstract view of the produc tion rules focused more on structure.
Lastly square abstracted structural feature set syntactic category	 Lastly the right square is a fully abstracted structural feature set with no syntactic category labels at all.
e unigram words classification interesting generation method described 2015	e  unigram words improved the classification accuracy over using only one feature type  Another interesting feature generation method is described in Massung and Zhai 2015 and called SYNTACTICDIFF.
unigram improved classification type.	 unigram words improved the classification accuracy over using only one feature type.
method described Massung	 Another interesting feature generation method is described in Massung and Zhai 2015 and called SYNTACTICDIFF.
SYNTACTICDIFF edit insert word substitute transform given	 The idea of SYNTACTICDIFF is to define three basic and therefore general edit operations insert a word remove a word and substitute one word for another  These edits are used to transform a given sentence.
text sce Massung writers English S R correction.	 In the nonnative text analysis sce nario Massung and Zhai 2016 we operate on text from writers who are not native English speakers  Thus transforming S with respect to R is a form of grammatical error correction.
Thus transforming respect R form error edits sentence sentences	 Thus transforming S with respect to R is a form of grammatical error correction  While this itself is a specific application task the series of edits performed on each sentence can also be used to represent the sentences themselves.
application edits represent themselves.	 While this itself is a specific application task the series of edits performed on each sentence can also be used to represent the sentences themselves.
For example 3 .	 For example insertthe  3 substitutea → an  1     .
	   .
2 particular sentence.	  removeof   2 could be a feature vector for a particular sentence.
effectiveness effective classifier	 The effectiveness of these “edit features” determines how effective the classifier can be in learning a model to separate different classes.
it’s important machine algorithms employed terms accuracy	 Again it’s important to em phasize that almost all machine learning algorithms are not affected by the type of features employed in terms of operation of course the accuracy may be affected.
Since learning algorithms ID edit features NLP discussed Chapter effective feature sentations Usually words default method advanced techniques	 Since internally the machine learning algorithms will simply refer to each feature as an ID the algorithm may never even know if it’s operating on a parse tree a word bigram POS tags or edit features  The NLP pipeline discussed in Chapter 3 and the tokenization schemes dis cussed in Chapter 4 give good examples of the options for effective feature repre sentations  Usually unigram words will be the default method and more advanced techniques are added as necessary in order to improve accuracy.
The Chapter 3 cussed Chapter options sentations.	 The NLP pipeline discussed in Chapter 3 and the tokenization schemes dis cussed in Chapter 4 give good examples of the options for effective feature repre sentations.
Running partofspeech running coreference requires grammat ical trees.	 Running a parser requires the sentence to be partofspeech tagged and running coreference resolution requires grammat ical parse trees.
Classification In	 15 5 Classification Algorithms In this section we will look into how the function f .
Classification look f	5 Classification Algorithms In this section we will look into how the function f .
We sentiment analysis classifying positive negative label split cor pus testing	 We will continue to use the sentiment analysis example classifying new text into either the positive or negative label  Let’s also assume we split our cor pus into a training partition and testing partition.
assume pus training partition testing partition The training documents build f evaluate	 Let’s also assume we split our cor pus into a training partition and testing partition  The training documents will be used to build f   and we will be able to evaluate its performance on each testing document.
evaluate document Remember additionally Y ments true training	 and we will be able to evaluate its performance on each testing document  Remember that we additionally have the metadata information Y for all docu ments so we know the true labels of all the testing and training data.
docu data When know confidence based testing mimics real world closer data wild greater classifier’s accuracy.	 Remember that we additionally have the metadata information Y for all docu ments so we know the true labels of all the testing and training data  When used in production we will not know the true label unless a human assigns one but we can have some confidence of the algorithm’s prediction based on its performance on the testing data which mimics the unseen data from the real world  The closer the testing data is to the data you expect to see in the wild the greater is your belief in the classifier’s accuracy.
When production know assigns confidence algorithm’s testing unseen closer wild accuracy	 When used in production we will not know the true label unless a human assigns one but we can have some confidence of the algorithm’s prediction based on its performance on the testing data which mimics the unseen data from the real world  The closer the testing data is to the data you expect to see in the wild the greater is your belief in the classifier’s accuracy  15.
wild classifier’s	 The closer the testing data is to the data you expect to see in the wild the greater is your belief in the classifier’s accuracy.
5 directly	5 1 kNearest Neighbors kNN is a learning algorithm that directly uses our inverted index and search engine.
kNearest Neighbors kNN learning inverted index search	1 kNearest Neighbors kNN is a learning algorithm that directly uses our inverted index and search engine.
explicit need instancebased	 Unlike the next two algorithms we will discuss there is no explicit training step all we need to do is index the training documents  This makes kNN a lazy learner or instancebased classifier.
This makes instancebased As shown training algorithms idea kNN similar query use common similar documents class label.	 This makes kNN a lazy learner or instancebased classifier  As shown in the training and testing algorithms the basic idea behind kNN is to find the most similar documents to the query document and use the most common class label of the similar documents  The assumption is that similar documents will have the same class label.
As idea kNN similar query use class label similar The class label.	 As shown in the training and testing algorithms the basic idea behind kNN is to find the most similar documents to the query document and use the most common class label of the similar documents  The assumption is that similar documents will have the same class label.
The similar label 15.	 The assumption is that similar documents will have the same class label  Figure 15.
Figure kNN action vector space different colors plotted space.	 Figure 15 3 shows an example of kNN in action in the document vector space  Here there are three different classes represented as different colors plotted in the vector space.
3 action document different classes different 1 query k 4 assign	3 shows an example of kNN in action in the document vector space  Here there are three different classes represented as different colors plotted in the vector space  If k  1 we would assign the red label to the query if k  4 we would assign the blue label since three out of the top four similar documents are blue.
label assign blue similar documents blue.	 If k  1 we would assign the red label to the query if k  4 we would assign the blue label since three out of the top four similar documents are blue.
class tie kNN distance measure representation slight classification existing inverted	 In the case of a tie the highest ranking document of the class with a tie would be chosen  kNN can be applied to any distance measure and any document representation  With only some slight modifications we can directly use this classification method with an existing inverted index.
kNN document representation directly method	 kNN can be applied to any distance measure and any document representation  With only some slight modifications we can directly use this classification method with an existing inverted index  A forward index is not required.
With slight modifications index A index	 With only some slight modifications we can directly use this classification method with an existing inverted index  A forward index is not required.
Despite downsides finding nearest engine	 A forward index is not required  Despite these ad vantages there are some downsides as well  For one finding the nearest neighbors requires performing a search engine query for each testing instance.
Despite vantages downsides For performing query	 Despite these ad vantages there are some downsides as well  For one finding the nearest neighbors requires performing a search engine query for each testing instance.
significantly slower algorithms As vector query vector querying	 While this is a heavily optimized operation it will still be significantly slower than other machine learning algorithms in test time  As we’ll see the other two algorithms perform simple vector operations on the query vector as opposed to querying the inverted index.
we’ll perform simple operations vector querying index However algorithms longer training time k tradeoff One important label kNN highly k algo rithms examples	 As we’ll see the other two algorithms perform simple vector operations on the query vector as opposed to querying the inverted index  However these other algorithms have a much longer training time than k NN—this is the tradeoff  One more important point is the chosen label for kNN is highly dependant on only the k neighbors on the other hand the other two algo rithms take all training examples in account.
However longer k NN—this tradeoff label k hand examples account way sensitive structure occupy.	 However these other algorithms have a much longer training time than k NN—this is the tradeoff  One more important point is the chosen label for kNN is highly dependant on only the k neighbors on the other hand the other two algo rithms take all training examples in account  In this way kNN is sensitive to the local structure of the feature space that the top k documents occupy.
One k account way kNN local occupy.	 One more important point is the chosen label for kNN is highly dependant on only the k neighbors on the other hand the other two algo rithms take all training examples in account  In this way kNN is sensitive to the local structure of the feature space that the top k documents occupy.
way kNN sensitive feature k	 In this way kNN is sensitive to the local structure of the feature space that the top k documents occupy.
based	 For one we can weight the votes of the neighbors based on distance to the query in weighted knearest neighbors.
A simple scheme multiply neighbor’s impact predicted	 That is a closer neighbor to the query would have more influence or a higherweighted vote  A simple weighting scheme would be to multiply each neighbor’s vote by 1 d where d is the distance to the query  Thus more distant neighbors have less of an impact on the predicted label.
A simple multiply neighbor’s vote d distance distant impact predicted	 A simple weighting scheme would be to multiply each neighbor’s vote by 1 d where d is the distance to the query  Thus more distant neighbors have less of an impact on the predicted label.
label nearestcentroid	 Thus more distant neighbors have less of an impact on the predicted label  Another variant is the nearestcentroid classifier.
Another instead individual neighbors centroid clustering Here n closest	 Another variant is the nearestcentroid classifier  In this algorithm instead of using individual documents as neighbors we consider the centroid of each class label see Chapter 14 for more information on centroids and clustering  Here if we have n classes we simply see which of the n is closest to the query.
algorithm individual consider class label 14 clustering classes The thought prototype ideal document	 In this algorithm instead of using individual documents as neighbors we consider the centroid of each class label see Chapter 14 for more information on centroids and clustering  Here if we have n classes we simply see which of the n is closest to the query  The centroid of each class label may be thought of as a prototype or ideal representation of a document from that class.
The thought representation	 The centroid of each class label may be thought of as a prototype or ideal representation of a document from that class.
5 Bayes Naive Bayes classifier.	 15 5 2 Naive Bayes Naive Bayes is an example of a generative classifier.
5 2 Bayes Naive example classifier.	5 2 Naive Bayes Naive Bayes is an example of a generative classifier.
creates distribu class model	 It creates a probability distribu tion of features over each class label in addition to a distribution of the class labels themselves  This is very similar to language model topic probability calculation.
similar probability distribution new text object existing language it.	 This is very similar to language model topic probability calculation  With the language model we create a distribution for each topic  When we see a new text object we use our existing topics to find topic language model θ̂ that is most likely to have generated it.
language model topic.	 With the language model we create a distribution for each topic.
When new text topic language generated Our look similar.	 When we see a new text object we use our existing topics to find topic language model θ̂ that is most likely to have generated it  Recall from Chapter 2 that Our Naive Bayes classifier will look very similar.
Recall Chapter Bayes classifier Essentially feature y xi	 Recall from Chapter 2 that Our Naive Bayes classifier will look very similar  Essentially we will have a feature distribution pxi  y for each class label y where xi is a feature.
Given calculate py y ∈	 Given an unseen document we will calculate the most likely class distribution that it is generated from  That is we wish to calculate py  x for each label y ∈ Y.
That wish y Let’s knowledge Chapter rewrite form programmatically document	 That is we wish to calculate py  x for each label y ∈ Y  Let’s use our knowledge of Bayes’ rule from Chapter 2 to rewrite this into a form we can use programmatically given a document x.
Let’s use Bayes’ rule form use given document x.	 Let’s use our knowledge of Bayes’ rule from Chapter 2 to rewrite this into a form we can use programmatically given a document x.
Notice Bayes’ Rule arg result.	 Notice that we eliminate the denominator produced by Bayes’ Rule since it does not change the arg max result.
The simplification depend multiply joint reason Bayes called naive.	 The final simplification is the independence assump tion that none of the features depend on one another letting us simply multiply all the probabilities together when finding the joint probability  It is for this reason that Naive Bayes is called naive.
estimate py y This language estimation That inference maximum likelihood number occurs total number	 This means we need to estimate the following distributions py for all classes and pxi  y for each feature in each class  This estimation is done in the exact same way as our unigram language model estimation  That is an easy inference method is maximum likelihood estimation where we count the number of times a feature occurs in a class divided by its total number of occurrences.
unigram estimation.	 This estimation is done in the exact same way as our unigram language model estimation.
easy maximum feature total occurrences As 2 lead issues words sparse	 That is an easy inference method is maximum likelihood estimation where we count the number of times a feature occurs in a class divided by its total number of occurrences  As discussed in Chapter 2 this may lead to some issues with unseen words or sparse data.
discussed Chapter 2 issues unseen words data.	 As discussed in Chapter 2 this may lead to some issues with unseen words or sparse data.
In case probabilities we’d like discussed We’ve interpolation	 In this case we can smooth the estimated probabilities using any smoothing method we’d like as discussed in Chapter 6  We’ve covered Dirichlet prior smoothing and Jelinek Mercer interpolation among others.
We’ve covered prior Jelinek need probability label.	 We’ve covered Dirichlet prior smoothing and Jelinek Mercer interpolation among others  Finally we need to calculate py which is just the probability of each class label.
calculate class label This labels don’t label occurs training rate predict majority	 Finally we need to calculate py which is just the probability of each class label  This parameter is essential when the class labels are unbalanced that is we don’t want to predict a label that occurs only a few times in the training data at the same rate that we predict the majority label.
parameter labels unbalanced predict rate label.	 This parameter is essential when the class labels are unbalanced that is we don’t want to predict a label that occurs only a few times in the training data at the same rate that we predict the majority label.
Whereas spent time testing Bayes spends training estimating model	 Whereas kNN spent most of its calculation time in testing Naive Bayes spends its time in training while estimating the model parameters.
testing tions performed label.	 In testing Y calcula tions are performed to find the most likely label.
learning forward label attribute features counts document class label inverted usage required OY .	 When learning the parameters a forward index is used so it is known which class label to attribute features to that is look up the counts in each document and update the parameter for that document’s true class label  An inverted index is not necessary for this usage  Mem ory aside from the forward index is required to store the parameters which can be represented as OY  V  .
Y floating	 Y floating point numbers.
strong independence assumptions Naive Bayes outperformed discussed section 15	 Due to its simplicity and strong independence assumptions Naive Bayes is often outperformed by more sophisticated classification methods many of which are based on the linear classifiers discussed in the next section  15 5.
inherently	5 3 Linear Classifiers Linear classifiers are inherently binary classifiers.
unseen document weights vector x .	 It takes the dot product between the unseen document vector and the weights vector w where w  x  V .
Training setting values w dotting document vector produces value instance negative	 Training a linear classifier is learning setting the values in the weights vector w such that dotting it with a document vector produces a value greater than 0 for a positive instance and less than zero for a negative instance.
Figure 15 4 shows creates twodimensional example possible decision boundaries vertical barely classes	 Figure 15 4 shows how the dot product combination creates a decision boundary between two label groups plotted in a simple twodimensional example  Two possible decision boundaries are shown the almost vertical line barely separates the two classes while the other line has a wide margin between the two classes.
4 dot combination creates plotted	4 shows how the dot product combination creates a decision boundary between two label groups plotted in a simple twodimensional example.
paragraph attempts margin decision boundary classes classified correctly boundary close class.	 The SVM algorithm mentioned in the previous paragraph attempts to maximize the margin between the decision boundary and the two classes thus leaving more “room” for new examples to be classified correctly as they will fall on the side of the decision boundary close to the examples of the same class.
shown linear classifier plotted ignores corpus lines meth ods trick linear classifiers refer reader learning tails Bishop 2006.	 Naive Bayes can also be shown to be a linear classifier  This is in contrast to kNN—since it only considers a local subspace in which the query is plotted it ignores the rest of the corpus and no lines are drawn  Some more advanced meth ods such as the kernel trick may change linear classifiers into nonlinear classifiers but we refer the reader to a text more focused on machine learning for the de tails Bishop 2006.
This kNN—since considers subspace ignores rest drawn Some ods change focused tails 2006.	 This is in contrast to kNN—since it only considers a local subspace in which the query is plotted it ignores the rest of the corpus and no lines are drawn  Some more advanced meth ods such as the kernel trick may change linear classifiers into nonlinear classifiers but we refer the reader to a text more focused on machine learning for the de tails Bishop 2006.
examine relatively simple perceptron classifier linear We need specify algorithm maximum for.	 In this book we choose to examine the relatively simple perceptron classifier on which many other linear classifiers are based  We need to specify several parameters which are used in the training algorithm  Let T represent the maximum number of iterations to run training for.
algorithm Let iterations run training 0 learning rate.	 We need to specify several parameters which are used in the training algorithm  Let T represent the maximum number of iterations to run training for  Let α  0 be the learning rate.
Let T iterations	 Let T represent the maximum number of iterations to run training for.
Let α learning rate adjust weights w measured current weights weights.	 Let α  0 be the learning rate  The learning rate controls by how much we adjust the weights at each step  We may terminate training early if the change in w is small this is usually measured by comparing the norm of the current iteration’s weights to the norm of the previous iteration’s weights.
The weights step terminate early measured comparing norm previous discussions discuss	 The learning rate controls by how much we adjust the weights at each step  We may terminate training early if the change in w is small this is usually measured by comparing the norm of the current iteration’s weights to the norm of the previous iteration’s weights  There are many discussions about the choice of learning rate convergence criteria and more but we do not discuss these in this book.
familiarize reader spirit refer reader 2006 implementation theoretical perceptron consists contin updating vector performance classifying known case ŷ correctly weights	 Instead we hope to familiarize the reader with the general spirit of the algorithm and again refer the reader to Bishop 2006 for many more details on algorithm implementation and theoretical properties  As the algorithm shows training of the perceptron classifier consists of contin uously updating the weights vector based on its performance in classifying known examples  In the case where yi and ŷ have the same sign classified correctly the weights are unchanged.
training classifier uously based performance classifying In weights ŷ clas subtracted active index w.	 As the algorithm shows training of the perceptron classifier consists of contin uously updating the weights vector based on its performance in classifying known examples  In the case where yi and ŷ have the same sign classified correctly the weights are unchanged  In the case where yi  ŷ the object should have been clas sified as −1 so weight is subtracted from each active feature index in w.
case yi sign	 In the case where yi and ŷ have the same sign classified correctly the weights are unchanged.
What support Not methods multiple binary create multiclass OVA trains classifier classifiers.	 What if we need to support multiclass classification Not all classification prob lems fit nicely into two categories  Fortunately there are two common methods for using multiple binary classifiers to create one multiclass categorization method on k classes  Onevsall OVA trains one classifier per class for k total classifiers.
classifiers create multiclass categorization	 Fortunately there are two common methods for using multiple binary classifiers to create one multiclass categorization method on k classes.
Onevsall trains class sifier respective class −1	 Onevsall OVA trains one classifier per class for k total classifiers  Each clas sifier is trained to predict 1 for its respective class and −1 for all other classes.
clas class −1	 Each clas sifier is trained to predict 1 for its respective class and −1 for all other classes.
score A confidence score 588 1.	 Because of this linear classifiers that are able to give a confidence score as a prediction are used  A confidence score such as 0 588 or 1.
represents 1 “more 1 chosen.	588 or 1 045 represents the 1 label but the latter is “more confident” than the former so the class that the algorithm predicting 1 045 would be chosen.
045 represents 1 class 045 AVA trains kk−1 2 classifiers pairs classes.	045 represents the 1 label but the latter is “more confident” than the former so the class that the algorithm predicting 1 045 would be chosen  Allvsall AVA trains kk−1 2 classifiers to distinguish between all pairs of k classes.
045	045 would be chosen.
Allvsall 2 k The predictions final confidencebased add votes totals label.	 Allvsall AVA trains kk−1 2 classifiers to distinguish between all pairs of k classes  The class with the most 1 predictions is chosen as the final answer  Again confidencebased scoring may be used to add votes into totals for each class label.
predictions Again confidencebased scoring totals class label 15.	 The class with the most 1 predictions is chosen as the final answer  Again confidencebased scoring may be used to add votes into totals for each class label  15.
Again confidencebased scoring votes totals class	 Again confidencebased scoring may be used to add votes into totals for each class label.
15.	 15.
splits partition development parameter tuning.	 Training and testing splits were mentioned in the previous sections but another partition of the total corpus is also sometimes used this is the development set used for parameter tuning.
corpus development 10 testing.	 Typically a corpus is split into about 80 training 10 development and 10 testing.
For consider determining value	 For example consider the problem of determining a good k value for kNN.
index e.	 An index is created over the training documents for e.
development documents This repeated	 k  5  The accuracy is determined using the development documents  This is repeated for k  10 15 20 25.
This repeated 20 25 testing The purpose prevent overfitting algorithm losing	 This is repeated for k  10 15 20 25  The bestperforming kvalue is then finally run on the testing set to find the overall accuracy  The purpose of the development set is to prevent overfitting or tailoring the learning algorithm too much to a particular corpus subset and losing generality.
The kvalue	 The bestperforming kvalue is then finally run on the testing set to find the overall accuracy.
set algorithm corpus subset losing A trained model robust Another paradigm nfold	 The purpose of the development set is to prevent overfitting or tailoring the learning algorithm too much to a particular corpus subset and losing generality  A trained model is robust if it is not prone to overfitting  Another evaluation paradigm is nfold cross validation.
prone	 A trained model is robust if it is not prone to overfitting.
Another evaluation nfold	 Another evaluation paradigm is nfold cross validation.
This corpus partitions.	 This splits the corpus into n partitions.
final accuracy F1 score The folds hint overfitting potential algorithm If high similar	 The final accuracy F1 score or any other evaluation metric is then averaged over the n folds  The variance in scores between the folds can be a hint at the overfitting potential of your algorithm  If the variance is high it means that the accuracies are not very similar between folds.
	 The variance in scores between the folds can be a hint at the overfitting potential of your algorithm.
Having fold high trained algorithm separate accuracy low particular	 Having one fold with a very high accuracy suggests that your learning algorithm may have overfit during that training stage when using that trained algorithm on a separate corpus it’s likely that the accuracy would be very low since it modeled noise or other uninformative features from that particular split i e  it overfit.
overfit concept baseline	 it overfit  Another important concept is baseline accuracy.
3000 consisting classes case guessing correct 3 time.	 Say there are 3000 documents consisting of three classes each with 1000 documents  In this case random guessing would give you about 33 accuracy since you’d be correct approximately 1 3 of the time.
accuracy In example 3000 documents classes distribution documents picking label correct 2	 Your classifier would have to do better than 33 accuracy in order to make it useful In another example consider the 3000 documents and three classes but with an uneven class distribution one class has 2000 documents and the other two classes have 500 each  In this case the baseline accuracy is 66 since picking the majority class label will result in correct predictions 2 3 of the time.
case baseline class result time it’s class consideration A way examine performance	 In this case the baseline accuracy is 66 since picking the majority class label will result in correct predictions 2 3 of the time  Thus it’s important to take class imbalances into consideration when evaluating a classifier  A confusion matrix is a way to examine a classifier’s performance at a perlabel level.
it’s important class consideration evaluating classifier A confusion examine	 Thus it’s important to take class imbalances into consideration when evaluating a classifier  A confusion matrix is a way to examine a classifier’s performance at a perlabel level.
matrix way examine classifier’s	 A confusion matrix is a way to examine a classifier’s performance at a perlabel level.
Figure 5 output determine language author English text row column index shows column.	 Consider Figure 15 5 the output from running META on a threeclass classi fication problem to determine the native language of the author of English text  Each row column index in the table shows the fraction of times that row was classified as column.
threeclass problem author English index table shows sum	5 the output from running META on a threeclass classi fication problem to determine the native language of the author of English text  Each row column index in the table shows the fraction of times that row was classified as column  Therefore the rows all sum to one.
index table fraction Therefore rows The diagonal indicating	 Each row column index in the table shows the fraction of times that row was classified as column  Therefore the rows all sum to one  The diagonal represents the true positive rate and hopefully most of the probability mass lies here indicating a good classifier.
The lies indicating predicting 80 2 native Japanese	 The diagonal represents the true positive rate and hopefully most of the probability mass lies here indicating a good classifier  Based on the matrix we see that predicting Chinese was 80 2 accurate with native English and Japanese as 80.
99	7 and 99 1 respectively.
1 English Chinese difficulty classifier classifier prediction label Japanese	1 respectively  This shows that while English and Chinese had relatively the same difficulty Japanese was very easy for the classifier to distinguish  We also see that if the classifier was wrong in a prediction on a Chinese or English true label it almost always chose Japanese as the answer.
classifier answer Based matrix classifier label	 We also see that if the classifier was wrong in a prediction on a Chinese or English true label it almost always chose Japanese as the answer  Based on the matrix the classifier seems to default to the label “Japanese”.
The tell hypotheses based parameters feature analysis Further Text categorization extensively Manning	 The table doesn’t tell us why this is but we can make some hypotheses based on our dataset  Based on this observation we may want to tweak our classifier’s parameters or do a more thorough feature analysis  Bibliographic Notes and Further Reading Text categorization has been extensively studied and is covered in Manning et al.
Based observation feature analysis Notes Further categorization covered et al.	 Based on this observation we may want to tweak our classifier’s parameters or do a more thorough feature analysis  Bibliographic Notes and Further Reading Text categorization has been extensively studied and is covered in Manning et al.
Notes Further Reading Text categorization Manning et al 2008.	 Bibliographic Notes and Further Reading Text categorization has been extensively studied and is covered in Manning et al  2008.
	 2008.
An Sebastiani cent 2012 topic.	 An early survey of the topic can be found in Sebastiani 2002 a more re cent one can be found in Aggarwal and Zhai 2012 where one chapter is devoted to this topic.
com monly text discussion evaluation.	 Yang 1999 includes a systematic empirical evaluation of multiple com monly used text categorization methods and a discussion of text categorization evaluation.
text super vised machine	 Moreover since text categorization is often performed by using super vised machine learning any book on machine learning is relevant e g.
	g.
1997 Text refers task text form obviously important text help users main text read	 Mitchell 1997  16Text Summarization Text summarization refers to the task of compressing a relatively large amount of text data or a long text article into a more concise form for easy digestion  It is obviously very important for text data access where it can help users see the main content or points in the text data without having to read all the text.
16Text Summarization Text summarization refers relatively large text article easy digestion.	 16Text Summarization Text summarization refers to the task of compressing a relatively large amount of text data or a long text article into a more concise form for easy digestion.
However text processed	 However summarization can also be useful for text data analysis as it can help reduce the amount of text to be processed thus improving the efficiency of any analysis algorithm  However summarization is a nontrivial task.
large points sentences And “important” recognize good define In summarization semantic convey space language.	 Given a large document how can we convey the important points in only a few sentences And what do we mean by “document” and “important” Although it is easy for a human to recognize a good summary it is not as straightforward to define the process  In short for any text summarization application we’d like a semantic compression of text that is we would like to convey essentially the same amount of information in less space  The output should be fluent readable language.
short like convey essentially information space The readable language In define	 In short for any text summarization application we’d like a semantic compression of text that is we would like to convey essentially the same amount of information in less space  The output should be fluent readable language  In general we need a purpose for summarization although it is often hard to define one.
general purpose hard formulate approach task problem little evaluate concrete consider summary.	 In general we need a purpose for summarization although it is often hard to define one  Once we know a purpose we can start to formulate how to approach the task and the problem itself becomes a little easier to evaluate  In one concrete example consider a news summary.
start approach easier evaluate.	 Once we know a purpose we can start to formulate how to approach the task and the problem itself becomes a little easier to evaluate.
concrete example	 In one concrete example consider a news summary.
If input day potentially output	 If our input is a collection of news articles from one day a potentially valid output is a list of headlines.
Each task require different	 Each task will require a different solution.
	 Summarizing retrieval results is also of particular interest.
On result user relevant link short summarize display results decides read it.	 On a search engine result page how can we help the user click on a relevant link A common strategy is to highlight words matching the query in a short snippet  An alternative approach would be to take a few sentences to summarize each result and display the short summaries on the results page  Using summaries in this way could give the user a better idea of what information the document contains before he or she decides to read it.
alternative short summaries results Using way user idea decides read summarization	 An alternative approach would be to take a few sentences to summarize each result and display the short summaries on the results page  Using summaries in this way could give the user a better idea of what information the document contains before he or she decides to read it  Opinion summarization is useful for both businesses and shoppers.
way information document	 Using summaries in this way could give the user a better idea of what information the document contains before he or she decides to read it.
Opinion useful businesses reviews product know why.	 Opinion summarization is useful for both businesses and shoppers  Summariz ing all reviews of a product lets the business know whether the buyers are satisfied and why.
summaries reviews approach Wang et 2010 al.	 Reviews can be further broken down into summaries of positive reviews and summaries of negative reviews  An even more granular approach described in Wang et al  2010 and Wang et al.
described Wang	 An even more granular approach described in Wang et al.
Wang 2011 18 topic product aspects.	 2010 and Wang et al  2011 and further discussed in Chapter 18 uses topic models to summarize product reviews relating to different aspects.
discussed Chapter models product relating different For hotel reviews correspond service price human summary reviews	 2011 and further discussed in Chapter 18 uses topic models to summarize product reviews relating to different aspects  For hotel reviews this could correspond to service location price and value  Although the output in these two works is not a human readable summary we could imagine a system that is able to summarize all the hotel reviews in English or any other language for the user.
hotel service location Although output human readable summary imagine able reviews English user.	 For hotel reviews this could correspond to service location price and value  Although the output in these two works is not a human readable summary we could imagine a system that is able to summarize all the hotel reviews in English or any other language for the user.
paradigms investigate	 In this chapter we overview two main paradigms of summarization techniques and investigate their different applications.
16 1 Text methods text summarization.	 16 1 Overview of Text Summarization Techniques There are two main methods in text summarization.
Text Techniques There methods text summarization extractive	1 Overview of Text Summarization Techniques There are two main methods in text summarization  The first is selectionbased or extractive summarization.
The extractive summarization.	 The first is selectionbased or extractive summarization.
With method consists documents new summary	 With this method a summary consists of a sequence of sentences selected from the original documents  No new sentences are written hence the summary is extracted.
No new sentences written summary extracted The method stractive summarization summary contain sentences documents.	 No new sentences are written hence the summary is extracted  The second method is generationbased or ab stractive summarization  Here a summary may contain new sentences not in any of the original documents.
The method ab stractive new sentences documents One model.	 The second method is generationbased or ab stractive summarization  Here a summary may contain new sentences not in any of the original documents  One method that we explore here is using a language model.
Here contain sentences	 Here a summary may contain new sentences not in any of the original documents.
language we’ve language models calculate lihood use language model	 One method that we explore here is using a language model  Previously in this book we’ve used language models to calculate the like lihood of some text in this chapter we will show how to use a language model in reverse to generate sentences.
like language generate sentences We briefly field natural generation	 Previously in this book we’ve used language models to calculate the like lihood of some text in this chapter we will show how to use a language model in reverse to generate sentences  We also briefly touch on the field of natural language generation in our discussion of abstractive techniques.
field natural discussion Following previous text	 We also briefly touch on the field of natural language generation in our discussion of abstractive techniques  Following the pattern of previous chapters we then move on to evaluation of text summarization.
g.	g.
abstractive summary algorithm applications text realworld systems Text broad field concepts	 an abstractive evaluation metric on a summary generated by an extractive algorithm  Finally we look into some applications of text summarization and see how they are implemented in realworld systems  Text summarization is a broad field and we only touch on the core concepts in this chapter.
core concepts Das Martins systematic chapter form.	 Text summarization is a broad field and we only touch on the core concepts in this chapter  For further reading we recommend that the reader start with Das and Martins 2007 which provides a systematic overview of the field and contains much of the content from this chapter in an expanded form.
reading start Martins systematic overview form 16 2 Text Information use sentence vectors similarity order	 For further reading we recommend that the reader start with Das and Martins 2007 which provides a systematic overview of the field and contains much of the content from this chapter in an expanded form  16 2 Extractive Text Summarization Information retrievalbased techniques use the notion of sentence vectors and similarity functions in order to create a summarization text.
16.	 16.
outline summarization system.	 Below we will outline a basic information retrievalbased summarization system.
Split document summarized	 Split the document to be summarized into sections or passages  2.
2.	 2.
	1.
sentences order normalized measure Chapter adjacent pairs sentences plot righthand figure change	 The sentences in the document are traversed in order and a normalized symmetric similarity measure see Chapter 14 is applied on adjacent pairs of sentences  The plot on the righthand side of the figure shows the change in similarity between the sentences.
occurs.	e  a shift in topic occurs.
task subfield	 This rudimentary partitioning strategy is a task in discourse analysis a subfield of NLP.
deals sequences opposed	 Discourse analysis deals with sequences of sentences as opposed to only one sentence.
output select sentence passage.	2 shows the output of the algorithm when we only select one sentence from each passage.
chunk document	 R is a partitioned chunk of sentences in the document we wish to summarize.
profile p determines “relevance Originally MMR returned retrieval term reranking.	 The profile p determines what is exactly meant by “relevance ” Originally the MMR formula was applied to documents returned from an information retrieval system hence the term reranking.
sentence si 3 Abstractive Text Summarization R notation read “R set i.	 According to marginal relevance the next sentence si to be added into the selected list S is defined as 16 3 Abstractive Text Summarization 321 The R  S notation may be read as “R set minus S” i.
3 Text Summarization 321 The S e elements S.	3 Abstractive Text Summarization 321 The R  S notation may be read as “R set minus S” i e  all the elements in R that are not in S.
R S ∈ 0 1 control positive score redundancy	 all the elements in R that are not in S  The MMR formulation uses λ ∈ 0 1 to control relevance versus re dundancy the positive relevance score is discounted by the amount of redundancy similarity to the alreadyselected sentences  Again the two similarity metrics may be any normalized symmetric measures.
MMR formulation 0 1 control score discounted normalized sim Goldstein 1998.	 The MMR formulation uses λ ∈ 0 1 to control relevance versus re dundancy the positive relevance score is discounted by the amount of redundancy similarity to the alreadyselected sentences  Again the two similarity metrics may be any normalized symmetric measures  The simplest instantiation for the sim ilarity metric would be cosine similarity and this is in fact the measure used in Carbonell and Goldstein 1998.
algorithm words sentences sim1s Furthermore way include sentence function away candidate subtract similarity score.	 The algorithm may be terminated once an appropriate number of words or sentences is in S or if the score sim1s  p is below some threshold  Furthermore the similarity functions may be tweaked as well  Could you think of a way to include sentence position in the similarity function That is if a sentence is far away dissimilar from the candidate sentence we could subtract from the similarity score.
Furthermore similarity tweaked Could position similarity function far sentence	 Furthermore the similarity functions may be tweaked as well  Could you think of a way to include sentence position in the similarity function That is if a sentence is far away dissimilar from the candidate sentence we could subtract from the similarity score.
think sentence position sentence away dissimilar candidate Even interpolate ∈ 0 1 measure .	 Could you think of a way to include sentence position in the similarity function That is if a sentence is far away dissimilar from the candidate sentence we could subtract from the similarity score  Even better we could interpolate the two values into a new similarity score such as where α ∈ 0 1 controls the weight between the regular cosine similarity and the distance measure and d  .
Even better interpolate new similarity α 1 weight similarity d eters.	 Even better we could interpolate the two values into a new similarity score such as where α ∈ 0 1 controls the weight between the regular cosine similarity and the distance measure and d    is the number of sentences between the two param eters.
sentences eters.	 is the number of sentences between the two param eters.
Of course MMR In Das suggests λ 0.	 Of course λ in the MMR formula is also able to be set  In fact for multidocument summarization Das and Martins 2007 suggests starting out with λ  0.
fact summarization λ 0 3 slowly increasing λ 0.	 In fact for multidocument summarization Das and Martins 2007 suggests starting out with λ  0 3 and then slowly increasing to λ  0.
λ 0 reasoning emphasize default	3 and then slowly increasing to λ  0 7  The reasoning behind this is to first emphasize novelty and then default to relevance.
emphasize exploration	 The reasoning behind this is to first emphasize novelty and then default to relevance  This should remind you of the exploration exploitation tradeoff discussed in Chapter 11.
remind exploitation discussed Abstractive An summary sentences ment documents.	 This should remind you of the exploration exploitation tradeoff discussed in Chapter 11  16 3 Abstractive Text Summarization An abstractive summary creates sentences that did not exist in the original docu ment or documents.
Instead vector use language	 Instead of a document vector we will use a language model to represent the original text.
	 Imagine we tokenized our document with unigram words.
To text draw words distribution language model estimated summarize draw w2	 To create our own text we will draw words from this probability distribution  Say we have the unigram language model θ estimated on a document we wish to summarize  We wish to draw words w1 w2 w3 .
Say language model θ wish	 Say we have the unigram language model θ estimated on a document we wish to summarize.
draw words .	 We wish to draw words w1 w2 w3   .
	   .
	   from θ that will comprise our summary.
want summary occurred original document—this text document 3 depicts accomplish task.	 We want the word wi to occur in our summary with about the same probability it occurred in the original document—this is how our generated text will approximate the longer document  Figure 16 3 depicts how we can accomplish this task.
Figure	 Figure 16.
We	 We output the term and repeat the process.
values 0	 In the example imagine we have the following values pcat 0 010 pcat  pdog 0.
pcat pdog 018 038	010 pcat  pdog 0 018 pcat  pdog  pa 0 038 .
018 pcat	018 pcat  pdog  pa 0 038 .
.	038 .
.	   .
pcat pa	 pcat  pdog  pa    .
.	   .
.	 .
generate random number x1 This denoted x1	  pzap 1 0 Say we generate a random number x1 using a uniform distribution on 0 1  This is denoted as x1 ∼ U0 1.
generate random distribution	0 Say we generate a random number x1 using a uniform distribution on 0 1.
∼ imagine x1 0.	 This is denoted as x1 ∼ U0 1  Now imagine that x1  0.
imagine	 Now imagine that x1  0 032.
	032.
output “a”.	032 in our distribution and output “a”.
If fluent required use n independently generated new 1 work way word wi wi1 language	 If more fluent language is required we can use an ngram language model where n  1  Instead of each word being independently generated the new word will depend on the previous n − 1 words  The generation will work the same way as in the unigram case say we have the word wi and wish to generate wi1 with a bigram language model.
new depend work way unigram case wi generate wi1 bigram language	 Instead of each word being independently generated the new word will depend on the previous n − 1 words  The generation will work the same way as in the unigram case say we have the word wi and wish to generate wi1 with a bigram language model.
generation way unigram case word generate wi1 bigram words occur wi way	 The generation will work the same way as in the unigram case say we have the word wi and wish to generate wi1 with a bigram language model  Our bigram language model gives us a distribution of words that occur after wi and we draw the next word from there in the same way depicted in Figure 16.
model words occur draw depicted Figure 3.	 Our bigram language model gives us a distribution of words that occur after wi and we draw the next word from there in the same way depicted in Figure 16 3.
	3.
	g  The.
	 The.
	 Then pick from the distribution pw  The using the cu mulative sum technique.
use pw “sum	 The next selected word could be cat  Then we use the distribution pw  cat to find the next w and so on  While the unigram model only had one “sum table” Figure 16.
Then pw cat on.	 Then we use the distribution pw  cat to find the next w and so on.
model “sum table” Figure 16.	 While the unigram model only had one “sum table” Figure 16.
case tables w′ nvalue original	3 the bigram case needs V tables one for each w′ in pw  w′  Typically the nvalue will be around three to five depending on how much original data there is.
nvalue original saw happened small jumble together.	 Typically the nvalue will be around three to five depending on how much original data there is  We saw what happened when n is too small we get a jumble of words that don’t make sense together.
small jumble problem n large extreme case n 20.	 We saw what happened when n is too small we get a jumble of words that don’t make sense together  But we have another problem if n is too large  Consider the extreme case where n  20.
problem n large Consider n	 But we have another problem if n is too large  Consider the extreme case where n  20.
words wish generate 20gram language unlikely 19 words occurred document.	 Consider the extreme case where n  20  Then given 19 words we wish to generate the next one using our 20gram language model  It’s very unlikely that those 19 words occurred more than once in our original document.
Then 20gram words occurred	 Then given 19 words we wish to generate the next one using our 20gram language model  It’s very unlikely that those 19 words occurred more than once in our original document.
unlikely words	 It’s very unlikely that those 19 words occurred more than once in our original document.
That choice	 That means there would only be one choice for the 20th word.
In like ngram disadvantage abstractive summarization Due	 In practice we would like to choose an ngram value that is large enough to produce coherent text yet small enough to not simply reproduce the corpus  There is one major disadvantage to this abstractive summarization method  Due to its nature a given word only depends on the n surrounding words.
nature given n surrounding That dependencies consider following trigram virtually seeds planted	 Due to its nature a given word only depends on the n surrounding words  That is there will be no longrange dependencies in our generated text  For example consider the following sentence generated from a trigram language model They imposed a gradual smoking ban on virtually all corn seeds planted are hybrids.
example sentence language model imposed gradual hybrids groups writer smoking hybrid midsentence special summary words summarizing highly strategy ef fective shown Ganesan al.	 For example consider the following sentence generated from a trigram language model They imposed a gradual smoking ban on virtually all corn seeds planted are hybrids  All groups of three words make sense but as a whole the sentence is incompre hensible it seems the writer changed the topic from a smoking ban to hybrid crops midsentence  In special cases when we restrict the length of a summary to a few words when summarizing highly redundant text such a strategy appears to be ef fective as shown in the micropinion summarization method described in Ganesan et al.
All groups words sentence hensible changed crops cases restrict strategy fective shown described Ganesan 2012.	 All groups of three words make sense but as a whole the sentence is incompre hensible it seems the writer changed the topic from a smoking ban to hybrid crops midsentence  In special cases when we restrict the length of a summary to a few words when summarizing highly redundant text such a strategy appears to be ef fective as shown in the micropinion summarization method described in Ganesan et al  2012.
length appears method et al 16.	 In special cases when we restrict the length of a summary to a few words when summarizing highly redundant text such a strategy appears to be ef fective as shown in the micropinion summarization method described in Ganesan et al  2012  16.
2012 16	 2012  16 3.
Abstractive rely language process build model document	 16 3 1 Advanced Abstractive Methods Some advanced abstractive methods rely more heavily on natural language process ing to build a model of the document to summarize.
	3.
Once	 Once these actors and roles are discovered they are stored in some internal representation.
Such systems finegrained control ated text basic model generator structure exist 2→conclusion chosen spot.	 Such realization systems have much more finegrained control over the gener ated text than the basic abstractive language model generator described above  A templated document structure may exist such as intro→paragraph 1→paragraph 2→conclusion and the structures are chosen to fill each spot.
templated 2→conclusion structures This control layout easilyreadable flow.	 A templated document structure may exist such as intro→paragraph 1→paragraph 2→conclusion and the structures are chosen to fill each spot  This control over text summarization and layout enables an easilyreadable summary since it has a natural topical flow.
summary entity names fell	 To make the summary sound even more natural pronouns can be used instead of entity names if the entity name has already been mentioned  Below are examples of these two operations Gold prices fell today.
examples operations prices fell today Silver	 Below are examples of these two operations Gold prices fell today  Silver prices fell today.
Silver prices fell today prices fell A	 Silver prices fell today  → Gold and silver prices fell today  Company A lost 9.
43 Company A biggest	43 today  Company A was the biggest mover.
biggest → A 9.	 Company A was the biggest mover  → Company A lost 9.
biggest mover Even today’s	 It was the biggest mover  Even better would be Company A was today’s biggest mover losing 9 43.
Even better today’s biggest losing 9	 Even better would be Company A was today’s biggest mover losing 9 43.
43 These entities structured format For generation Reiter 2000 implementation.	43  These operations are possible since the entities are stored in a structured format  For more on advanced natural language generation we suggest Reiter and Dale 2000 which has a focus on practicality and implementation.
These entities structured For advanced 2000 implementation.	 These operations are possible since the entities are stored in a structured format  For more on advanced natural language generation we suggest Reiter and Dale 2000 which has a focus on practicality and implementation.
Evaluation summarization representative text This information retrieval critical issue MMR it.	4 Evaluation of Text Summarization In extractive summarization representative sentences were selected from passages in the text and output as a summary  This solution is modeled as an information retrieval problem and we can evaluate it as such  Redundancy is a critical issue and the MMR technique we discussed attempts to alleviate it.
IR evaluation metrics Although generated ranked passage ranked list composed multiple	 For full output scoring we should prefer IR evaluation metrics that do not take into account result position  Although our summary is generated by ranked sentences per passage the entire output is not a ranked list since the original document is composed of multiple passages.
Text 325 rank scoring retrieval position NDCG final feasible need evaluate passage both.	 16 5 Applications of Text Summarization 325 It is possible to rank the passage scoring retrieval function using position dependent metrics such as average precision or NDCG but with the final output this is not feasible  Thus we need to decide whether to evaluate the passage scoring or the entire output or both.
Thus need evaluate scoring entire output	 Thus we need to decide whether to evaluate the passage scoring or the entire output or both.
Entire ac tual passage scoring useful finetune	 Entire output scoring is likely more useful for ac tual users while passage scoring could be useful for researchers to finetune their methods.
In summarization use IR don’t fixed set sentences.	 In abstractive summarization we can’t use the IR measures since we don’t have a fixed set of candidate sentences.
recall number relevant average precision don’t know complete set correct sentences.	 How can we compute recall if we don’t know the total number of relevant sentences There is also no intermediate ranking stage so we also can’t use average precision or NDCG and again we don’t even know the complete set of correct sentences.
A laborious accurate human annotators gold	 A laborious yet accurate evaluation would have human annotators create a gold standard summary.
g ROUGE	g  ROUGE would be used to quantify the difference.
An alternative means summary generated ngram level interpretable result probability distributions	 An alternative means would be to learn an ngram language model over the gold standard summary and then calculate the loglikelihood of the generated summary  This can ensure a basic level of fluency at the ngram level while also producing an interpretable result  Other comparisons between two probability distributions would also be applicable such as KLdivergence.
This level interpretable result comparisons probability distributions	 This can ensure a basic level of fluency at the ngram level while also producing an interpretable result  Other comparisons between two probability distributions would also be applicable such as KLdivergence.
distributions	 Other comparisons between two probability distributions would also be applicable such as KLdivergence.
Using model extractive summary biased contains phrases original text high likelihood.	 Using a language model to score an extractive summary vs  an abstractive one would likely be biased towards the extractive one since this method contains phrases directly from the original text giving it a very high likelihood.
abstractive extractive method text high likelihood 16 Text mentioned retrieval results summa rization.	 an abstractive one would likely be biased towards the extractive one since this method contains phrases directly from the original text giving it a very high likelihood  16 5 Applications of Text Summarization At the beginning of the chapter we’ve already touched on a few summarization applications we mentioned news articles retrieval results and opinion summa rization.
” opinion segments portions user reviews We analysis collect large comments	” The aspect opinion analysis mentioned earlier segments portions of user reviews into speaking about a particular topic  We can use this topic analysis to collect passages of text into a large group of comments on one aspect.
aspect sorted words topic generating output methods aspects union structured unstructured data	 Instead of describing this aspect with sorted unigram words we could run a summarizer on each topic generating readable text as output  These two methods complement each other since the first step finds what aspects the users are interested in while the second step conveys the information  A theme in this book is the union of both structured and unstructured data mentioned much more in detail in Chapter 19.
These methods second conveys union data	 These two methods complement each other since the first step finds what aspects the users are interested in while the second step conveys the information  A theme in this book is the union of both structured and unstructured data mentioned much more in detail in Chapter 19.
A book structured data Chapter application.	 A theme in this book is the union of both structured and unstructured data mentioned much more in detail in Chapter 19  Summarization is an excellent example of this application.
Summarization example application consider financial summarizer reports Commission stock market data.	 Summarization is an excellent example of this application  For example consider a financial summarizer with text reports from the Securities and Exchange Commission SEC as well as raw stock market data.
For consider reports Commission raw market Summarizing sources valuable	 For example consider a financial summarizer with text reports from the Securities and Exchange Commission SEC as well as raw stock market data  Summarizing both these data sources in one location would be very valuable for e.
data valuable e.	 Summarizing both these data sources in one location would be very valuable for e.
	g.
mutual fund financial	 mutual fund managers or other financial workers.
electronic discovery finding relevant court	 Ediscovery electronic discovery is the process of finding relevant information in litigation lawsuits and court cases.
ediscovery amounts textual The field.	 Lawyers rely on ediscovery to sift through vast amounts of textual information to build their case  The Enron email dataset1 is a wellknown corpus in this field.
Summarizing people department quickly decide approach.	 Summarizing email correspondence between two people or a department lets investigators quickly decide whether they’d like to dig deeper in a particular area or try another approach.
summarize research given Given main common approaches compared previous conferences When paper introduction paper.	 Perhaps of more interest to those reading this book is the ability to summarize research from a given field  Given proceedings from a conference could we have a summarizer explain the main trends and common approaches What was most novel compared to previous conferences When writing your own paper can you write everything except the introduction and related work The introduction is an overview summary of your paper.
conference explain common approaches conferences paper write related The summary papers Notes Reading As chapter Das Martins survey	 Given proceedings from a conference could we have a summarizer explain the main trends and common approaches What was most novel compared to previous conferences When writing your own paper can you write everything except the introduction and related work The introduction is an overview summary of your paper  Related work is mostly a summary of papers similar to yours  Bibliographic Notes and Further Reading As mentioned in this chapter Das and Martins 2007 is a comprehensive survey on summarization techniques.
Related papers similar yours.	 Related work is mostly a summary of papers similar to yours.
Bibliographic comprehensive survey summarization techniques Nenkova valuable read latent Wang al.	 Bibliographic Notes and Further Reading As mentioned in this chapter Das and Martins 2007 is a comprehensive survey on summarization techniques  Additionally Nenkova and McKeown 2012 is a valuable read  For applications latent aspect rating analysis Wang et al.
Additionally Nenkova 2012 read For applications latent rating et al.	 Additionally Nenkova and McKeown 2012 is a valuable read  For applications latent aspect rating analysis Wang et al.
	 2011 is a form of summarization applied to product reviews.
summarizer	 We mention this particular application in more detail in Chapter 18  A typical extractive summarizer is presented in Radev et al.
Steinberger The originally Goldstein	 2010 and evaluation suggestions are presented in Steinberger and Jezek 2009  The MMR algorithm was originally described in Carbonell and Goldstein 1998.
The Carbonell	 The MMR algorithm was originally described in Carbonell and Goldstein 1998.
17Topic Analysis This mining analysis covering text called probabilistic text A topic understand actually easy formally	 17Topic Analysis This chapter is about topic mining and analysis covering a family of unsupervised text mining techniques called probabilistic topic models that can be used to dis cover latent topics in text data  A topic is something that we all understand intuitively but it’s actually not easy to formally define it.
A intuitively easy define Roughly main text regarded topic different	 A topic is something that we all understand intuitively but it’s actually not easy to formally define it  Roughly speaking a topic is the main idea discussed in text data which may also be regarded as a theme or subject of a discussion or conversation  A topic can have different granularities.
Roughly main theme different	 Roughly speaking a topic is the main idea discussed in text data which may also be regarded as a theme or subject of a discussion or conversation  A topic can have different granularities.
applications There analysis topics text For example interested users talking	 Different granularities of topics have different applications  There are many applications that require discovery and analysis of topics in text  For example we might be interested in knowing about what Twitter users are talking about today.
applications text interested knowing Twitter users Are sports international knowing research interested research data years	 There are many applications that require discovery and analysis of topics in text  For example we might be interested in knowing about what Twitter users are talking about today  Are they talking about NBA sports international events or another topic We may also be interested in knowing about research topics one might be interested in knowing the current research topics in data mining and how they are different from those five years ago.
sports events knowing interested knowing topics data mining different years ago questions literature including today’s	 Are they talking about NBA sports international events or another topic We may also be interested in knowing about research topics one might be interested in knowing the current research topics in data mining and how they are different from those five years ago  To answer such questions we need to discover topics in the data mining literature including specifically topics in today’s literature and those in the past so that we can make a comparison.
To need mining specifically today’s past comparison.	 To answer such questions we need to discover topics in the data mining literature including specifically topics in today’s literature and those in the past so that we can make a comparison.
people smartphones This topics positive reviews reviews.	 We might also be interested in knowing what people like about some products such as smartphones  This requires discovering topics in both positive reviews and negative reviews.
This reviews reviews knowing election All topics analyzing them.	 This requires discovering topics in both positive reviews and negative reviews  Or perhaps we’re interested in knowing what the major topics debated in a presidential election are  All these have to do with discovering topics in text and analyzing them.
knowing major election discovering text	 Or perhaps we’re interested in knowing what the major topics debated in a presidential election are  All these have to do with discovering topics in text and analyzing them.
topics text analyzing How topic view knowledge world Figure	 All these have to do with discovering topics in text and analyzing them  How to do this is a main topic of this chapter  We can view a topic as describing some knowledge about the world as shown in Figure 17.
How main chapter.	 How to do this is a main topic of this chapter.
view describing 17 1 From text data number provide	 We can view a topic as describing some knowledge about the world as shown in Figure 17 1  From text data we want to discover a number of topics which can provide a description about the world.
1.	1.
From data number description	 From text data we want to discover a number of topics which can provide a description about the world  That is a topic tells us something about the world e.
That	 That is a topic tells us something about the world e g.
All topics discover use analyze For looking topics there’s topic Similarly topics help insights people’s	 All such metadata or context variables can be associated with the topics that we discover and we can then use these context variables to analyze topic patterns  For example looking at topics over time we would be able to discover whether there’s a trending topic or some topics might be fading away  Similarly looking at topics in different locations might help reveal insights about people’s opinions in different locations.
looking different reveal people’s different	 Similarly looking at topics in different locations might help reveal insights about people’s opinions in different locations.
tasks mining As	 Let’s look at the tasks of topic mining and analysis  As shown in Figure 17.
As shown 17.	 As shown in Figure 17.
topic In case We like topics	2 topic analysis first involves discovering a number of topics  In this case there are k topics  We also would like to know which topics are covered in which documents and to what extent.
know topics documents extent.	 We also would like to know which topics are covered in which documents and to what extent.
Topic extent Thus different tasks topics task documents cover	 It also covers Topic k to some extent  Thus there are generally two different tasks or sub tasks the first is to discover the k topics from a collection of text the second task is to figure out which documents cover which topics to what extent.
generally collection More formally	 Thus there are generally two different tasks or sub tasks the first is to discover the k topics from a collection of text the second task is to figure out which documents cover which topics to what extent  More formally we can define the problem as shown in Figure 17 3.
	3.
input text collection text di input number potentially	 First we have as input a collection of N text documents  Here we can denote the text collection as C and denote a text article as di  We also need to have as input the number of topics k though this number may be potentially set automatically based on data characteristics.
need input number set chapter need number The denoted .	 We also need to have as input the number of topics k though this number may be potentially set automatically based on data characteristics  However in the techniques that we will discuss in this chapter we need to specify a number of topics  The output includes the k topics that we would like to discover denoted by θ1 .
The includes k θ1 .	 The output includes the k topics that we would like to discover denoted by θ1     .
θk topics denoted topic θj	  θk and the coverage of topics in each document of di which is denoted by πij   πij is the probability of document di covering topic θj .
probability topic θj	 πij is the probability of document di covering topic θj .
set values document assume means topics discovered defined define exactly	 For each document we have a set of such π values to indicate to what extent the document covers each topic  We assume that these probabilities sum to one which means that we assume a document won’t be able to cover other topics outside of the topics we discovered  Now the next question is how do we define a topic θi Our task has not been completely defined until we define exactly what θ is.
assume document cover outside Now question define topic defined exactly In discuss simplest	 We assume that these probabilities sum to one which means that we assume a document won’t be able to cover other topics outside of the topics we discovered  Now the next question is how do we define a topic θi Our task has not been completely defined until we define exactly what θ is  In the next section we will first discuss the simplest way to define a topic as a term.
Now defined define exactly is.	 Now the next question is how do we define a topic θi Our task has not been completely defined until we define exactly what θ is.
In simplest way define topic term.	 In the next section we will first discuss the simplest way to define a topic as a term.
term	 A term can be a word or a phrase.
example separate data Figure 17 If topic analyze topics based topical	 For example we may have terms like sports travel or science to denote three separate topics covered in text data as shown in Figure 17 4  If we define a topic in this way we can then analyze the coverage of such topics in each document based on the occurrences of these topical terms.
4 30 1 12 We 2 So coverage zero.	4 30 of the content of Doc 1 is about sports and 12 is about travel etc  We might also discover Doc 2 does not cover sports at all  So the coverage π21 is zero.
Doc	 We might also discover Doc 2 does not cover sports at all.
So π21 tasks.	 So the coverage π21 is zero  Recall that we have two tasks.
One topics solve problem need	 One is to discover the topics and the other is to analyze coverage  To solve the first problem we need to mine k topical terms from a collection  There are many different ways to do that.
To There different	 To solve the first problem we need to mine k topical terms from a collection  There are many different ways to do that.
different	 There are many different ways to do that.
The need design quantify term topic.	 The simplest case is to just take each word as a term  These words then become candidate topics  Next we will need to design a scoring function to quantify how good each term is as a topic.
need scoring function There designing function basis statistics	 Next we will need to design a scoring function to quantify how good each term is as a topic  There are many things that we can consider when designing such a function with a main basis being the statistics of terms.
consider main terms.	 There are many things that we can consider when designing such a function with a main basis being the statistics of terms.
like representative meaning represent lot content want term simply frequency function highest terms words a.	 Intuitively we would like to favor representative terms meaning terms that can represent a lot of content in the collection  That would mean we want to favor a frequent term  However if we simply use the frequency to design the scoring function then the highest scored terms would be general terms or function words like the or a.
That term.	 That would mean we want to favor a frequent term.
simply use design scoring terms terms words Those occur want having terms	 However if we simply use the frequency to design the scoring function then the highest scored terms would be general terms or function words like the or a  Those terms occur very frequently in English so we also want to avoid having such words on the top  That is we would like to favor terms that are fairly frequent but not too frequent.
Those frequently English want having top.	 Those terms occur very frequently in English so we also want to avoid having such words on the top.
like fairly	 That is we would like to favor terms that are fairly frequent but not too frequent.
A approach achieving goal TFIDF weighting previous retrieval models word	 A specific approach to achieving our goal is to use TFIDF weighting discussed in some previous chapters of the book on retrieval models and word association discovery.
course apply particular problem try domainspecific example title title article.	 Of course when we apply such an approach to a particular problem we should always try to leverage some domainspecific heuristics  For example in news we might favor title words because the authors tend to use the title to describe the topic of an article.
If we’re dealing tweets favor invented denote After discover k simply scores We encounter situation highest scored terms	 If we’re dealing with tweets we could also favor hashtags which are invented to denote topics  After we have designed the scoring function we can discover the k topical terms by simply picking the k terms with the highest scores  We might encounter a situation where the highest scored terms are all very similar.
After designed k picking terms highest scores encounter situation highest scored similar related synonyms.	 After we have designed the scoring function we can discover the k topical terms by simply picking the k terms with the highest scores  We might encounter a situation where the highest scored terms are all very similar  That is they are semantically similar or closely related or even synonyms.
terms similar similar closely related synonyms.	 We might encounter a situation where the highest scored terms are all very similar  That is they are semantically similar or closely related or even synonyms.
good remove redundancy One way called Marginal Relevance reranking.	 This is not desirable since we also want to have a good coverage over all the content in the collection meaning that we would like to remove redundancy  One way to do that is to use a greedy algorithm called Maximal Marginal Relevance MMR reranking.
One way algorithm Marginal MMR scoring function k terms.	 One way to do that is to use a greedy algorithm called Maximal Marginal Relevance MMR reranking  The idea is to go down the list based on our scoring function and select k topical terms.
The idea list function When look picked try similar.	 The idea is to go down the list based on our scoring function and select k topical terms  The first term of course will be picked  When we pick the next term we will look at what terms have already been picked and try to avoid picking a term that’s too similar.
pick picked similar.	 When we pick the next term we will look at what terms have already been picked and try to avoid picking a term that’s too similar.
balance removal picking terms scores The MMR technique k terms denote topic πij	 With appropriate thresholding we can then get a balance of redundancy removal and picking terms with high scores  The MMR technique is described in more detail in Chapter 16  After we obtain k topical terms to denote our topics the next question is how to compute the coverage of each topic in each document πij .
The technique Chapter 16.	 The MMR technique is described in more detail in Chapter 16.
After terms question topic πij	 After we obtain k topical terms to denote our topics the next question is how to compute the coverage of each topic in each document πij .
One simply count occurrences Figure 17 example occurred di occurred	 One solution is to simply count occurrences of each topical term as shown in Figure 17 5  So for example sports might have occurred four times in document di and travel occurred twice.
example occurred times document di twice.	5  So for example sports might have occurred four times in document di and travel occurred twice.
example sports document di twice.	 So for example sports might have occurred four times in document di and travel occurred twice.
The normalization ensure add forming	 The normalization is to ensure that the coverage of each topic in the document would add to one thus forming a distribution over the topics for each document to characterize coverage.
As think solving ask following questions solution way actual works analyze	 As always when we think about an idea for solving a problem we have to ask the following questions how effective is the solution Is this the best way of solving problem In general we have to do some empirical evaluation by using actual data sets and to see how well it works  However it is often also instructive to analyze some specific examples.
6 Here text	6  Here we have a text document that’s about an NBA basketball game.
In it’s sports count topics sports occur article content sports.	 In terms of the content it’s about sports but if we simply count these words that represent our topics we will find that the word sports actually did not occur in the article even though the content is about sports.
coverage We term science document intuitively science.	 Since the count of sports is zero the coverage of sports would be estimated as zero  We may note that the term science also did not occur in the document and so its estimate is also zero which is intuitively what we want since the document is not about science.
We estimate science However sports certainly content	 We may note that the term science also did not occur in the document and so its estimate is also zero which is intuitively what we want since the document is not about science  However giving a zero probability to sports certainly is a problem because we know the content is about sports.
However giving content sports What’s worse topic travel count estimated	 However giving a zero probability to sports certainly is a problem because we know the content is about sports  What’s worse the term travel actually occurred in the document so when we estimate the coverage of the topic travel we would have a nonzero count higher than the estimated coverage of sports.
What’s actually occurred coverage nonzero sports obviously analysis simple example reveals	 What’s worse the term travel actually occurred in the document so when we estimate the coverage of the topic travel we would have a nonzero count higher than the estimated coverage of sports  This obviously is also not desirable  Our analysis of this simple example thus reveals a few problems of this approach.
This Our analysis problems approach First topic need consider related words.	 This obviously is also not desirable  Our analysis of this simple example thus reveals a few problems of this approach  First when we count what words belong to a topic we also need to consider related words.
Our simple example approach First count words consider related words.	 Our analysis of this simple example thus reveals a few problems of this approach  First when we count what words belong to a topic we also need to consider related words.
count words consider words.	 First when we count what words belong to a topic we also need to consider related words.
simply count extracted e g sports	 We cannot simply just count the extracted term denoting a topic e g  sports which may not occur at all in a document about the topic.
The second word like star basketball star mean need uncertainty word Finally restriction approach term	 The second problem is that a word like star is ambiguous  While in this article it means a basketball star it might also mean a star on the sky in another context so we need to consider the uncertainty of an ambiguous word  Finally a main restriction of this approach is that we have only one term to describe the topic so it cannot really describe complicated topics.
While article means consider	 While in this article it means a basketball star it might also mean a star on the sky in another context so we need to consider the uncertainty of an ambiguous word.
term complicated topics specialized topic sports word	 Finally a main restriction of this approach is that we have only one term to describe the topic so it cannot really describe complicated topics  For example a very specialized topic in sports would be harder to describe by using just a word or one phrase.
key simple example general simple approach defining lacks	 A key takeaway point from analyzing this simple example is that there are three general problems with our simple approach of defining a topic as a single term first it lacks expressive power.
topics represent complicated	 It can only represent the simple general topics but cannot represent the complicated topics that might require more words to describe.
Second vocabulary meaning topic represented term It suggest related topic making impossible estimate contribution words topic problem words.	 Second it’s incomplete in vocabulary coverage meaning that the topic itself is only represented as one term  It does not suggest what other terms are related to the topic making it impossible to estimate the contribution of related words to the coverage of a topic  Finally there is a problem due to ambiguity of words.
related contribution problem related	 It does not suggest what other terms are related to the topic making it impossible to estimate the contribution of related words to the coverage of a topic  Finally there is a problem due to ambiguity of words  A topical term or related term can be ambiguous.
Finally topical related ambiguous.	 Finally there is a problem due to ambiguity of words  A topical term or related term can be ambiguous.
ambiguous section improved address problems	 A topical term or related term can be ambiguous  In the next section we will discuss an improved representation of a topic as a distribution over words that can address these problems  17.
Topics single term topic immediately expressive power.	 17 2 Topics as Word Distributions A natural idea to address the problems of using one single term to denote a topic is to use more words to describe the topic which would immediately address the first problem of lack of expressive power.
2 Word A single denote topic use words topic immediately address expressive When words use topic complicated second problem related need	2 Topics as Word Distributions A natural idea to address the problems of using one single term to denote a topic is to use more words to describe the topic which would immediately address the first problem of lack of expressive power  When we have more words that we can use to describe the topic we would be able to describe complicated topics  To address the second problem of how to involve related words we need to introduce weights on words.
unigram model denote shown Figure 17	 a unigram language model to denote a topic as shown in Figure 17 7.
7.	7.
vocabulary.	 Here you see that for every topic we have a word distribution over all the words in the vocabulary.
These likelihood topic Note chance topic probabilities sum topic words.	 These are all intuitively sportsrelated terms whose occurrences should contribute to the likelihood of covering the topic “sports” in an article  Note that in general the distribution may give all the words a nonzero probability since there is always a very very small chance that even a word not so related to the topic would be mentioned in an article about the topic  Note also that these probabilities for all the words always sum to one for each topic thus forming a probability distribution over all the words.
distribution nonzero probability word topic probabilities words sum topic forming probability	 Note that in general the distribution may give all the words a nonzero probability since there is always a very very small chance that even a word not so related to the topic would be mentioned in an article about the topic  Note also that these probabilities for all the words always sum to one for each topic thus forming a probability distribution over all the words.
Note words sum probability distribution topic words tend words interesting note special concentrated entirely e.	 Note also that these probabilities for all the words always sum to one for each topic thus forming a probability distribution over all the words  Such a word distribution represents a topic in that if we sample words from the distribution we tend to see words that are related to the topic  It is also interesting to note that as a very special case if the probability of the mass is concentrated entirely on just one word e.
word represents words related It interesting note concentrated e	 Such a word distribution represents a topic in that if we sample words from the distribution we tend to see words that are related to the topic  It is also interesting to note that as a very special case if the probability of the mass is concentrated entirely on just one word e g.
word representation simplest single discussed sense natural However words involve words differences	 sports then the word distribution representation of a topic would degenerate to the simplest representation of a topic as just one single word discussed before  In this sense the word distribution representation is a natural generalization and extension of the singleterm representation  However representing a topic by a distribution over words can involve many words to describe a topic and model subtle differences of topics.
topic involve words subtle topics.	 However representing a topic by a distribution over words can involve many words to describe a topic and model subtle differences of topics.
adjusting probabilities different words topic particular sports basketball expect basketball basketball.	 Through adjusting probabilities of different words we may model variations of the general “sports” topic to focus more on a particular kind of sports such as basketball where we would expect basketball to have a very high probability or football where football would have a much higher probability than basketball.
Similarly words like	 Similarly in the distribution for “travel” we see top words like attraction trip flight and so on.
It doesn’t necessarily zero topic lower probabilities.	 It is important to note that it doesn’t mean sportsrelated terms will necessarily have zero probabilities in a distribution representing the topic “science” but they generally have much lower probabilities.
Note reasonably high probabilities	 Note that there are some words that are shared by these topics meaning that they have reasonably high probabilities for all these topics.
word occurred topics probability “travel” topic 0.	 For example the word travel occurred in the topword lists for all the three topics but with different probabili ties  It has the highest probability for the “travel” topic 0.
“sports”	05 but with much smaller probabilities for “sports” and “science” which makes sense.
star “science” probabilities We seen representing word addresses problems	 Similarly you can see star also occurred in “sports” and “science” with reasonably high probabilities be cause the word is actually related to both topics due to its ambiguous nature  We have thus seen that representing a topic by a word distribution effectively addresses all the three problems of a topic as a single term mentioned earlier.
topic word topic single term	 We have thus seen that representing a topic by a word distribution effectively addresses all the three problems of a topic as a single term mentioned earlier.
.	 .
.	 .
weights semantics related topics words topic coverage topic.	 It assigns weights to terms enabling the modeling of subtle differences of semantics in related topics  We can also easily bring in related words together to model a topic and estimate the coverage of the topic.
We easily related coverage	 We can also easily bring in related words together to model a topic and estimate the coverage of the topic  .
word accommodate multiple word Next discovering way.	   Because we have probabilities for the same word in different topics we can accommodate multiple senses of a word addressing the issue of word ambi guity  Next we examine the task of discovering topics represented in this way.
topics accommodate multiple senses word issue ambi guity way.	 Because we have probabilities for the same word in different topics we can accommodate multiple senses of a word addressing the issue of word ambi guity  Next we examine the task of discovering topics represented in this way.
task discovering represented natural use discovering probabilistic topic modeling When analysis Figure 17.	 Next we examine the task of discovering topics represented in this way  Since the representation is a probability distribution it is natural to use probabilistic models for discovering such word distributions which is referred to as probabilistic topic modeling  When using a word distribution to denote a topic our task of topic analysis can be further refined based on the formal definition in Figure 17.
word distribution problem input collection know number k data As know determines units basic	 That is each θi is now a word distribution and we have As a computation problem our input is text data a collection of documents C and we assume that we know the number of topics k or hypothesize that there are k topics in the text data  As part of our input we also know the vocabulary V which determines what units would be treated as the basic units i e.
words cases simply units generalize identify words.	e  words for analysis  In most cases we will use words as the basis for analysis simply because they are the most natural units but it is easy to generalize such an approach to use phrases or any other units that we can identify in text as the basic units and treat them as if they were words.
words	 words for analysis.
analysis natural words Our output consists	 In most cases we will use words as the basis for analysis simply because they are the most natural units but it is easy to generalize such an approach to use phrases or any other units that we can identify in text as the basic units and treat them as if they were words  Our output consists of two families of probability distributions.
consists	 Our output consists of two families of probability distributions.
potentially ways introduce general solving problem called generative	 There are potentially many different ways to do this but here we introduce a general way of solving this problem called a generative model.
e model generated model probability likely The data necessarily unlikely assuming data generated particular way according characterize data discovery.	e  a probabilistic model to model how the data are generated or a model that can allow us to compute the probability of how likely we will observe the data we have  The actual data aren’t necessarily indeed often unlikely generated this way but by assuming the data to be generated in a particular way according to a particular model we can have a formal way to characterize our data which further facilitates topic discovery.
generated probability data actual aren’t generated generated model formal characterize In general parameters model kind high low probabilities.	 a probabilistic model to model how the data are generated or a model that can allow us to compute the probability of how likely we will observe the data we have  The actual data aren’t necessarily indeed often unlikely generated this way but by assuming the data to be generated in a particular way according to a particular model we can have a formal way to characterize our data which further facilitates topic discovery  In general our model will have some parameters which can be denoted by they control the behavior of the model by controlling what kind of data would have high or low probabilities.
The actual necessarily assuming data particular model data In model behavior model controlling data set differently points	 The actual data aren’t necessarily indeed often unlikely generated this way but by assuming the data to be generated in a particular way according to a particular model we can have a formal way to characterize our data which further facilitates topic discovery  In general our model will have some parameters which can be denoted by they control the behavior of the model by controlling what kind of data would have high or low probabilities  If you set these parameters to different values the model would behave differently that is it would tend to give different data points high or low probabilities.
set model differently tend data low	 If you set these parameters to different values the model would behave differently that is it would tend to give different data points high or low probabilities.
model way encode estimate data parameters observed output designed like discover model data problem statistics different ways Chapter 2.	 We design the model in such a way that its parameters would encode the knowl edge we would like to discover  Then we attempt to estimate these parameters based on the data or infer the values of parameters based on the observed data so as to generate the desired output in the form of parameter values which we have designed to denote the knowledge we would like to discover  How exactly we should fit the model to the data or infer the parameter values based on the data is often a standard problem in statistics and there are many different ways to do this as we discussed briefly in Chapter 2.
fit values data problem statistics briefly Chapter Following generative model specific problem topics data needs contain k distributions output compute problem setup parameters	 How exactly we should fit the model to the data or infer the parameter values based on the data is often a standard problem in statistics and there are many different ways to do this as we discussed briefly in Chapter 2  Following the idea of using a generative model to solve the specific problem of discovering topics and topic coverages from text data we see that our generative model needs to contain all the k word distributions representing the topics and the topic coverage distributions for all the documents which is all the output we intend to compute in our problem setup  Thus there will be many parameters in the model.
model solve discovering topics topic coverages text data generative model needs contain k distributions documents intend compute problem setup Thus V parameters probabilities words total	 Following the idea of using a generative model to solve the specific problem of discovering topics and topic coverages from text data we see that our generative model needs to contain all the k word distributions representing the topics and the topic coverage distributions for all the documents which is all the output we intend to compute in our problem setup  Thus there will be many parameters in the model  First we have V  parameters for the probabilities of words in each word distribution so we have in total V k word probability parameters.
model First parameters words word total k word Second document π probability parameters.	 Thus there will be many parameters in the model  First we have V  parameters for the probabilities of words in each word distribution so we have in total V k word probability parameters  Second for each document we have k values of π  so we have in total Nk topic coverage probability parameters.
Second document π coverage	 Second for each document we have k values of π  so we have in total Nk topic coverage probability parameters.
total V k Nk	 Thus we have in total V k  Nk parameters.
smaller V − distribution specify V probabilities specify	 Given that we have constraints on both θ and π  however the number of free parameters is smaller at V  − 1k  Nk − 1 in each word distribution we only need to specify V  − 1 probabilities and for each document we only need to specify k − 1 probabilities.
That estimate parameters data set maximum probability.	 That means we can estimate the parameters or infer the parameters based on the data  In other words we would like to adjust these parameter values until we give our data set maximum probability.
depending parameter values probabilities	 Like we just mentioned depending on the parameter values some data points will have higher probabilities than others.
What we’re interested parameter probability Figure	 What we’re interested in is what parameter values will give our data the highest probability  In Figure 17.
parameters It’s oversimplification suffices	 In Figure 17 8 we illustrate  the parameters as a onedimensional variable  It’s oversimplification obviously but it suffices to show the idea.
8 parameters It’s oversimplification obviously	8 we illustrate  the parameters as a onedimensional variable  It’s oversimplification obviously but it suffices to show the idea.
It’s oversimplification y probability This obviously setting varies parameter maximize	 It’s oversimplification obviously but it suffices to show the idea  The y axis shows the probability of the data  This probability obviously depends on this setting of so that’s why it varies as you change ’s value in order to find ∗ the parameter settings that maximize the probability of the observed data.
shows probability	 The y axis shows the probability of the data.
search estimate	 Such a search yields our estimate of the model parameters.
general model text mining model	 This is the general idea of using a generative model for text mining  We design a model with some parameter values to describe the data as well as we can  After we have fit the data we learn parameter values.
We text data 3 Mining Topic Text simplest generative modeling text data assume text goal topic.	 We treat the learned parameters as the discovered knowledge from text data  17 3 Mining One Topic from Text In this section we discuss the simplest instantiation of a generative model for modeling text data where we assume that there is just one single topic covered in the text and our goal is to discover this topic.
Topic Text In discuss generative model text assume topic covered text topic More 17.	3 Mining One Topic from Text In this section we discuss the simplest instantiation of a generative model for modeling text data where we assume that there is just one single topic covered in the text and our goal is to discover this topic  More specifically as illustrated in Figure 17.
interested document	9 we are interested in analyzing each document and discovering a single topic covered in the document.
This simplest case model input longer k topics specify independently loss generality assume document.	 This is the simplest case of a topic model  Our input now no longer has k topics because we know or rather specify that there is only one topic  Since each document can be mined independently without loss of generality we further assume that the collection has only one document.
input know topic document assume collection	 Our input now no longer has k topics because we know or rather specify that there is only one topic  Since each document can be mined independently without loss of generality we further assume that the collection has only one document.
In output longer assumed document coverage	 In the output we also no longer have coverage because we assumed that the document has a 100 coverage of the topic we would like to discover.
Thus distribution ing topic probabilities Figure 17 9.	 Thus the output to compute is the word distribution represent ing this single topic or probabilities of all words in the vocabulary given by this distribution as illustrated in Figure 17 9.
9 Mining One 341	9  17 3 Mining One Topic from Text 341 17.
17.	3 Mining One Topic from Text 341 17.
Topic Model Unigram Language use generative start kind need Our differently use different	3 1 The Simplest Topic Model Unigram Language Model When we use a generative model to solve a problem we start with thinking about what kind of data we need to model and from what perspective  Our data would “look” differently if we use a different perspective.
data “look” different	 Our data would “look” differently if we use a different perspective.
example view simply words frequencies discussed vector Such representation generative model view document words care frequencies words analysis subtle word frequencies.	 For example we may view a document simply as a set of words without considering the frequencies of words which would lead to a bit vector representation as we discussed in the context of the vector space retrieval model  Such a representation would need a different generative model than if we view the document as a bag of words where we care about the different frequencies of words  In topic analysis the frequencies of words can help distinguish subtle semantic variations so we generally should retain the word frequencies.
different model words different words In semantic	 Such a representation would need a different generative model than if we view the document as a bag of words where we care about the different frequencies of words  In topic analysis the frequencies of words can help distinguish subtle semantic variations so we generally should retain the word frequencies.
In analysis frequencies subtle variations	 In topic analysis the frequencies of words can help distinguish subtle semantic variations so we generally should retain the word frequencies.
decide view data specific i.	 Once we decide on a perspective to view the data we will design a specific model for generating the data from the desired perspective i.
	e.
model reflecting	 model the data based on the representation of the data reflecting the desired perspective.
target knowledge parameters denote knowledge interesting parameters Here discovering represented distribution choice model unigram	 The target knowledge would determine what parameters we would include in the model since we want our parameters to denote the knowledge interesting to us after we estimate the values of these parameters  Here we are interested in discovering a topic represented as a word distribution so a natural choice of model would be a unigram language model as in Section 3.
discovering word unigram language Section 3 likelihood i.	 Here we are interested in discovering a topic represented as a word distribution so a natural choice of model would be a unigram language model as in Section 3 4  After we specify the model we can formally write down the likelihood function i.
	4.
data assumed	 the probability of the data given the assumed model.
This generally parameters function values parameters.	 This is generally a function of the unknown parameters and the value of the function would vary according to the values of the parameters.
likelihood obtaining Maximum Estimate MLE	 Thus we can attempt to find the parameter values that would maximize the value of this function the likelihood given data from the model  Such a way of obtaining parameters is called the Maximum Likelihood Estimate MLE as we’ve discussed previously.
desirable additional belief parameters parameters compromise data consistent impose.	 Sometimes it is desirable to also incorporate some additional knowledge a prior belief about the parameters that we may have about a particular application  We can do this by using Bayesian estimation of parameters which seeks a compromise of maximizing the probability of the observed data maximum likelihood and being consistent with the prior belief that we impose.
We parameters compromise observed maximum likelihood impose In generative model obtain parameter best	 We can do this by using Bayesian estimation of parameters which seeks a compromise of maximizing the probability of the observed data maximum likelihood and being consistent with the prior belief that we impose  In any case once we have a generative model we would be able to fit such a model to our data and obtain the parameter values that can best explain the data.
generative fit data values best data.	 In any case once we have a generative model we would be able to fit such a model to our data and obtain the parameter values that can best explain the data.
parameter values output Let’s follow topic topic examine	 These parameter values can then be taken as the output of our mining process  Let’s follow these steps to design the simplest topic model for discovering a topic from one document we will examine many more complicated cases later.
steps design document The model	 Let’s follow these steps to design the simplest topic model for discovering a topic from one document we will examine many more complicated cases later  The model is shown in Figure 17.
shown	 The model is shown in Figure 17.
10 decided words word denoted unigram model	10 where we see that we have decided to view a document as a sequence of words  Each word here is denoted by xi  Our model is a unigram language model i.
Each xi Our	 Each word here is denoted by xi  Our model is a unigram language model i e.
model	 Our model is a unigram language model i e.
	e.
denotes latent number vocabulary	 a word distribution that denotes the latent topic that we hope to discover  Clearly the model has as many parameters as the number of words in our vocabulary which is M in this case.
model number	 Clearly the model has as many parameters as the number of words in our vocabulary which is M in this case.
For convenience use θi probability wi sum probability generating according model In unigram language model assume independence generating document equals probability line likelihood	 For convenience we will use θi to denote the probability of word wi  According to our model the probabilities of all the words must sum to one Next we see that our likelihood function is the probability of generating this whole document according to our model  In a unigram language model we assume independence in generating each word so the probability of the document equals the product of the probability of each word in the document the first line of the equation for the likelihood function.
probabilities function according	 According to our model the probabilities of all the words must sum to one Next we see that our likelihood function is the probability of generating this whole document according to our model.
language assume independence probability equals document We rewrite product form terms corresponding positions shown second line likelihood function words occurrences	 In a unigram language model we assume independence in generating each word so the probability of the document equals the product of the probability of each word in the document the first line of the equation for the likelihood function  We can rewrite this product into a slightly different form by grouping the terms corresponding to the same word together so that the product would be over all the distinct words in the vocabulary instead of over all the positions of words in the document which is shown in the second line of the equation for the likelihood function  Since some words might have repeated occurrences when we use a product over the unique words we must also incorporate the count of a word wi in document d which is denoted by cwi  d.
rewrite product slightly different grouping words shown line function Since occurrences words incorporate count wi d cwi Although taken entire vocabulary word occur zero 0 essentially words occurred	 We can rewrite this product into a slightly different form by grouping the terms corresponding to the same word together so that the product would be over all the distinct words in the vocabulary instead of over all the positions of words in the document which is shown in the second line of the equation for the likelihood function  Since some words might have repeated occurrences when we use a product over the unique words we must also incorporate the count of a word wi in document d which is denoted by cwi  d  Although the product is taken over the entire vocabulary it is clear that if a word did not occur in the document it would have a zero count cwi  d  0 and that corresponding term would be essentially absent in the formula thus the product is still essentially over the words that actually occurred in the document.
Since product words d cwi occur document corresponding words actually document.	 Since some words might have repeated occurrences when we use a product over the unique words we must also incorporate the count of a word wi in document d which is denoted by cwi  d  Although the product is taken over the entire vocabulary it is clear that if a word did not occur in the document it would have a zero count cwi  d  0 and that corresponding term would be essentially absent in the formula thus the product is still essentially over the words that actually occurred in the document.
product clear document count 0 formula essentially	 Although the product is taken over the entire vocabulary it is clear that if a word did not occur in the document it would have a zero count cwi  d  0 and that corresponding term would be essentially absent in the formula thus the product is still essentially over the words that actually occurred in the document.
We product vocabulary convenient formulas estimation.	 We often prefer such a form of the likelihood function where the product is over the entire vocabulary because it is convenient for deriving formulas for parameter estimation.
likelihood values i.	 Now that we have a well defined likelihood function we will attempt to find the parameter values i.
word probabilities func Let’s estimation problem	e  word probabilities that maximize this likelihood func tion  Let’s take a look at the maximum likelihood estimation problem more closely in Figure 17.
tion maximum	 word probabilities that maximize this likelihood func tion  Let’s take a look at the maximum likelihood estimation problem more closely in Figure 17.
Let’s look Figure	 Let’s take a look at the maximum likelihood estimation problem more closely in Figure 17 11.
original finding maximum likelihood problem	11  The first line is the original optimization problem of finding the maximum likelihood estimate  The next line shows an equivalent optimization problem with the loglikelihood.
original problem maximum estimate shows function results problem.	 The first line is the original optimization problem of finding the maximum likelihood estimate  The next line shows an equivalent optimization problem with the loglikelihood  The equivalence is due to the fact that the log arithm function results in a monotonic transformation of the original likelihood function and thus does not affect the solution of the optimization problem.
fact log results function solution optimization	 The equivalence is due to the fact that the log arithm function results in a monotonic transformation of the original likelihood function and thus does not affect the solution of the optimization problem.
transformation mathematical loga function sum instead optimal solution function.	 Such a transformation is purely for mathematical convenience because after the loga rithm transformation our function will become a sum instead of product the sum makes it easier to take the derivative which is often needed for finding the optimal solution of this function.
function reflects general loglikelihood models .	 Although simple this loglikelihood function reflects some general characteris tics of a loglikelihood function of some more complex generative models  .
The sum words vocabulary.	 The sum is over all the unique data points the words in the vocabulary.
	 .
multiplied logarithm probability 17 At point problem mathematical optimization prob optimal solution The function probabilities	 the count of each word in the observed data which is multiplied by the logarithm of the probability of the particular unique data point  344 Chapter 17 Topic Analysis At this point our problem is a welldefined mathematical optimization prob lem where the goal is to find the optimal solution of a constrained maximization problem  The objective function is the loglikelihood function and the constraint is that all the word probabilities must sum to one.
344 17 Analysis lem goal problem objective loglikelihood function word sum one.	 344 Chapter 17 Topic Analysis At this point our problem is a welldefined mathematical optimization prob lem where the goal is to find the optimal solution of a constrained maximization problem  The objective function is the loglikelihood function and the constraint is that all the word probabilities must sum to one.
The constraint word sum How solve book case obtain solution Lagrange This commonly approach provide	 The objective function is the loglikelihood function and the constraint is that all the word probabilities must sum to one  How to solve such an optimization problem is beyond the scope of this book but in this case we can obtain a simple analytical solution by using the Lagrange multiplier approach  This is a commonly used approach so we provide some detail on how it works in Figure 17.
How optimization problem scope case solution multiplier approach provide Figure	 How to solve such an optimization problem is beyond the scope of this book but in this case we can obtain a simple analytical solution by using the Lagrange multiplier approach  This is a commonly used approach so we provide some detail on how it works in Figure 17.
commonly provide 17 We Lagrange combines jective encodes Lagrange multiplier denoted	 This is a commonly used approach so we provide some detail on how it works in Figure 17 11  We will first construct a Lagrange function which combines our original ob jective function with another term that encodes our constraint with the Lagrange multiplier denoted by λ introducing an additional parameter.
solution original constrained new Lagrange straightforward problem taking obtaining equations M word λ.	 It can be shown that the solution to the original constrained optimization problem is the same as the solution to the new unconstrained Lagrange function  Since there is no constraint involved any more it is straightforward to solve this optimization problem by taking partial derivatives with respect to all the parame ters and setting all of them to zero obtaining an equation for each parameter 1 We thus have in total M  1 linear equations corresponding to the M word probability parameters and λ.
involved straightforward problem partial derivatives setting parameter total M λ multiplier λ	 Since there is no constraint involved any more it is straightforward to solve this optimization problem by taking partial derivatives with respect to all the parame ters and setting all of them to zero obtaining an equation for each parameter 1 We thus have in total M  1 linear equations corresponding to the M word probability parameters and λ  Note that the equation for the Lagrange multiplier λ is precisely our original constraint.
Note easily equations obtain Maximum language θ̂ cwi cwj d d	 Note that the equation for the Lagrange multiplier λ is precisely our original constraint  We can easily solve this system of linear equations to obtain the Maximum Likelihood estimate of the unigram language model as pwi  θ̂   cwi  d∑M j1 cwj  d cwi  d d .
equations obtain Likelihood j1 cwj d 3 estimated probability word normalized document sum words	 We can easily solve this system of linear equations to obtain the Maximum Likelihood estimate of the unigram language model as pwi  θ̂   cwi  d∑M j1 cwj  d cwi  d d   17 3 This has a very meaningful interpretation the estimated probability of a word is the count of each word normalized by the document length which is also a sum of all the counts of words in the document.
meaningful estimated probability word length sum words matches intuition observed “deserve” higher observed nonzero probabilities unseen words zero	3 This has a very meaningful interpretation the estimated probability of a word is the count of each word normalized by the document length which is also a sum of all the counts of words in the document  This estimate mostly matches our intuition in order to maximize the likelihood words observed more often “deserve” higher probabilities and only words observed are “allowed” to have nonzero probabilities unseen words should have a zero probability.
This estimate matches words observed higher words observed “allowed” probabilities unseen words zero probability general maximum estimation tends result corresponding event observed higher bility events	 This estimate mostly matches our intuition in order to maximize the likelihood words observed more often “deserve” higher probabilities and only words observed are “allowed” to have nonzero probabilities unseen words should have a zero probability  In general maximum likelihood estimation tends to result in a probability estimated as normalized counts of the corresponding event so that the events observed often would have a higher proba bility and the events not observed would have zero probability.
general maximum likelihood tends result probability counts event events observed probability While solution simple analytical impossible.	 In general maximum likelihood estimation tends to result in a probability estimated as normalized counts of the corresponding event so that the events observed often would have a higher proba bility and the events not observed would have zero probability  While we have obtained an analytical solution to the maximum likelihood esti mate in this simple case such an analytical solution is not always possible indeed it is often impossible.
While analytical maximum esti mate analytical solution optimization problem MLE solve problem.	 While we have obtained an analytical solution to the maximum likelihood esti mate in this simple case such an analytical solution is not always possible indeed it is often impossible  The optimization problem of the MLE can often be very complicated and numerical optimization algorithms would generally be needed to solve the problem.
The MLE optimization solve problem.	 The optimization problem of the MLE can often be very complicated and numerical optimization algorithms would generally be needed to solve the problem.
	12.
high probability words common words function words English.	 On the top you will see the high probability words tend to be those very common words often function words in English.
This content characterize topic mining small related topic mentioned	 This will be followed by some content words that really characterize the topic well like text and mining  In the end you also see there is a small probability of words that are not really related to the topic but might happen to be mentioned in the document.
In end words topic mentioned topic distribution high prob	 In the end you also see there is a small probability of words that are not really related to the topic but might happen to be mentioned in the document  As a topic representation such a distribution is not ideal because the high prob ability words are function words which do not characterize the topic.
model word distribution introduce distribution model distribution Such model called mixture model multiple models “mixed” section.	 How can we improve our generative model to downweight such common words in the es timated word distribution for our topic The answer is that we can introduce a second background word distribution into the generative model so that the com mon words can be generated from this background model and thus the topic word distribution would only need to generate the contentcarrying topical words  Such a model is called a mixture model because multiple component models are “mixed” together to generate data  We discuss it in detail in the next section.
Such model mixture model multiple models data We discuss	 Such a model is called a mixture model because multiple component models are “mixed” together to generate data  We discuss it in detail in the next section.
We	 We discuss it in detail in the next section.
2 Adding Language order solve common words 346 Chapter Analysis think	3 2 Adding a Background Language Model In order to solve the problem of assigning highest probabilities to common words in the estimated unigram language model based on one document it would be 346 Chapter 17 Topic Analysis useful to think about why we end up having this problem.
It hard reasons common words frequent likelihood tend high Second assumes words unigram	 It is not hard to see that the problem is due to two reasons  First these common words are very frequent in our data thus any maximum likelihood estimator would tend to give them high probabilities  Second our generative model assumes all the words are generated from one single unigram language model.
frequent likelihood tend probabilities Second assumes words single The assign high probabilities	 First these common words are very frequent in our data thus any maximum likelihood estimator would tend to give them high probabilities  Second our generative model assumes all the words are generated from one single unigram language model  The ML estimate thus has no choice but to assign high probabilities to such common words in order to maximize the likelihood.
generative words generated single unigram model.	 Second our generative model assumes all the words are generated from one single unigram language model.
The assign words maximize	 The ML estimate thus has no choice but to assign high probabilities to such common words in order to maximize the likelihood.
words design different gen model explain words text	 Thus in order to get rid of the common words we must design a different gen erative model where the unigram language model representing the topic doesn’t have to explain all the words in the text data.
language model generate words.	 Specifically our target topic unigram language model should not have to generate the common words.
distribution common complete generative	 This further sug gests that we must introduce another distribution to generate these common words so that we can have a complete generative model for all the words in the document.
intend distribution common words natu ral unigram model model unigram language unknown language probabilities common	 Since we intend for this second distribution to explain the common words a natu ral choice for this distribution is the background unigram language model  We thus have a mixture model with two component unigram language models one being the unknown topic that we would like to discover and one being a background language model that is fixed to assign high probabilities to common words.
In 17.	 In Figure 17.
generate data common contentbearing	13 we see that the two distributions can be mixed together to generate the text data with the background model generates common words while the topic language model to generate contentbearing words in the document.
Thus expect language high probabilities contentbearing words English The assumed process word slightly simplest unigram	 Thus we can expect the discovered learned topic unigram language model to assign high probabilities to such contentbearing words rather than the common function words in English  The assumed process for generating a word with such a mixture model is just slightly different from the generation process of our simplest unigram language.
process generating mixture model different process language.	 The assumed process for generating a word with such a mixture model is just slightly different from the generation process of our simplest unigram language.
Since decide generate word sample text data way	 Since we now have two distributions we have to decide which distribution to use when we generate the word but each word will still be a sample from one of the two distributions  The text data are still generated in the same way by generating one word at a time.
data way generating word More specifically generate word	 The text data are still generated in the same way by generating one word at a time  More specifically when we generate a word we first decide which of the two distributions to use.
This probability models choices case including specifically probability θd unknown topic model probability θB known background model.	 This is controlled by a new probability distribution over the choices of the component models to use two choices in our case including specifically the probability of θd using the unknown topic model and the probability of θB using the known background model.
pθB figure pθd	 Thus pθd  pθB  1  In the figure we see that both pθd and pθB are set to 0.
5.	5.
This imagine fair general equal likely generating	 This means that we can imagine flipping a fair coin to decide which distribution to use although in general these probabilities don’t have to be equal one topic could be more likely than another  The process of generating a word from such a mixture model is as follows.
	 If the coin shows up as heads we would use θd  otherwise θB.
We use word means use pw θB illustrated 13.	 We will use the chosen word distribution to generate a word  This means if we are to use θd we would sample a word using pw  θd otherwise using pw  θB as illustrated in Figure 17 13.
This θd sample θd pw	 This means if we are to use θd we would sample a word using pw  θd otherwise using pw  θB as illustrated in Figure 17 13.
We generative model uncertainty associated use word distribution generate treat model box similarly use distribution sequences	 We now have a generative model that has some uncertainty associated with the use of which word distribution to generate a word  If we treat the whole generative model as a black box the model would behave very similarly to our simplest topic model where we only use one word distribution in that the model would specify a distribution over sequences of words.
treat generative model box model behave topic model model We observing mixture model prob words.	 If we treat the whole generative model as a black box the model would behave very similarly to our simplest topic model where we only use one word distribution in that the model would specify a distribution over sequences of words  We can thus examine the probability of observing any particular word from such a mixture model and compute the prob ability of observing a sequence of words.
consider specific words text.	13 and consider two specific words the and text.
model Note probability observing case probability observing background order generated chosen model sampling background pw	 What’s the probability of observing a word like the from the mixture model Note that there are two ways to generate the so the probability is intuitively a sum of the probability of observing the in each case  What’s the probability of observing the being generated using the background model In order for the to be generated in this way we must have first chosen to use the background model and then obtained the word the when sampling a word from the background language model pw  θB.
probability generated model In order way chosen use model word language model θB.	 What’s the probability of observing the being generated using the background model In order for the to be generated in this way we must have first chosen to use the background model and then obtained the word the when sampling a word from the background language model pw  θB.
Thus probability model pθBpthe model distribution θd shown 17.	 Thus the probability of observing the from the background model is pθBpthe  θB and the probability of observing the from the mixture model regardless of which distribution we use would be pθBpthe  θB  pθdpthe  θd as shown in Figure 17.
It compute word mixture pθBpw pw θdpθd	 It is not hard to generalize the calculation to compute the probability of observ ing any word w from such a mixture model which would be pw  pθBpw  θB  pw  θdpθd  17.
17.	 17.
Each sum probability word distributions.	 Each term in the sum captures the probability of observing the word from one of the two distributions.
For example observing w background The product fact w cided pθB 2 obtained pw	 For example pθBpw  θB gives the probability of observing word w from the background language model  The product is due to the fact that in order to observe word w we must have 1 de cided to use the background distribution which has the probability of pθB and 2 obtained word w from the distribution θB which has the probability of pw  θB.
The fact order observe word 1 cided use word probability θB observe background multiply observing w distribution θd probability w topic	 The product is due to the fact that in order to observe word w we must have 1 de cided to use the background distribution which has the probability of pθB and 2 obtained word w from the distribution θB which has the probability of pw  θB  Both events must happen in order to observe word w from the background distri bution thus we multiply their probabilities to obtain the probability of observing w from the background distribution  Similarly pθdpw  θd gives the probability of observing word w from the topic word distribution.
Both observe word bution multiply probabilities probability w background	 Both events must happen in order to observe word w from the background distri bution thus we multiply their probabilities to obtain the probability of observing w from the background distribution.
gives probability observing w topic distribution.	 Similarly pθdpw  θd gives the probability of observing word w from the topic word distribution.
probability observing Such form actually model.	 Adding them together gives us the total probability of observing w regardless which distribution has actually been used to generate the word  Such a form of likelihood actually reflects some general characteristics of the likelihood function of any mixture model.
term sum probabilities component corresponding probability actually observing data selected model Their gives probability point generated corresponding component model point generate data point As seen later topic tend generating form component	 Second each term in the sum is a product of two probabilities one is the probability of selecting the component model corresponding to the term while the other is the probability of actually observing the data point from that selected component model  Their product gives the probability of observing the data point when it is generated using the corresponding component model which is why the sum would give the total probability of observing the data point regardless which component model has been used to generate the data point  As will be seen later more sophisticated topic models tend to use more than two components and their probability of generating a word would be of the same form as we see here except that there are more than two products in the sum more precisely as many products as the number of component models.
product point corresponding sum total observing model generate point.	 Their product gives the probability of observing the data point when it is generated using the corresponding component model which is why the sum would give the total probability of observing the data point regardless which component model has been used to generate the data point.
likelihood easy mixture single defined way That gives distribution	 Once we write down the likelihood function for one word it is very easy to see that as a whole the mixture model can be regarded as a single word distribution defined in a somewhat complicated way  That is it also gives us a probability distribution over words as defined above.
mixture model generating independently language defines distribution word.	 Thus conceptually the mixture model is yet another generative model that also generates a sequence of words by generating each word independently  This is the same as the case of a simple unigram language model which defines a distribution over words by explicitly specifying the probability of each word.
simple defines words specifying probability	 This is the same as the case of a simple unigram language model which defines a distribution over words by explicitly specifying the probability of each word.
main Figure draw box single model When viewing box generative model	 The main idea of a mixture model is to group multiple distributions together as one model as shown in Figure 17 15 where we draw a box to “encapsulate” the two distributions to form a single generative model  When viewing the whole box as one model we can easily see that it’s just like any other generative model that would give us the probability of each word.
15 “encapsulate” form single generative When easily it’s like	15 where we draw a box to “encapsulate” the two distributions to form a single generative model  When viewing the whole box as one model we can easily see that it’s just like any other generative model that would give us the probability of each word  However how this probability is determined in such a mixture model is quite different from when we have just one unigram language model.
In case examine component zero case term corresponding background model special distribution characterizing discovered.	 In this case we can examine what would happen if we set the probability of choosing the background component model to zero  It is easy to see that in such a case the term corresponding to the background model would disappear from the sum and the mixture model would degenerate to the special case of just one distribution characterizing the topic to be discovered.
It case corresponding model disappear degenerate characterizing discovered.	 It is easy to see that in such a case the term corresponding to the background model would disappear from the sum and the mixture model would degenerate to the special case of just one distribution characterizing the topic to be discovered.
In previous model covered case.	 In this sense the mixture model is more general than the previous model where we have just one distribution which can be covered as a special case.
probability language explain words allow topic concentrated	 Naturally our reason for using a mixture model is to enforce a nonzero probability of choosing the background language model so that it can help explain the common words in the data and allow our topic word distribution to be more concentrated on content words.
case	 As in the case of the single unigram language model we can use any method e.
	g.
parameters twocomponent unigram language models estimation First data model	 What parameters do we have in such a twocomponent mixture model In Fig ure 17 16 we summarize the mixture of two unigram language models list all the parameters and illustrate the parameter estimation problem  First our data is just one document d and the model is a mixture model with two components.
mixture language parameters model components.	16 we summarize the mixture of two unigram language models list all the parameters and illustrate the parameter estimation problem  First our data is just one document d and the model is a mixture model with two components.
data d mixture model parameters include unigram	 First our data is just one document d and the model is a mixture model with two components  Second the parameters include two unigram language models and a distribution mixing weight over the two language models.
Mathematically denotes doc ument word distribution fixed word distribution probabilities denote collectively Can parameters	 Mathematically θd denotes the topic of doc ument d while θB represents the background word distribution which we can set to a fixed word distribution with high probabilities on common words  We denote all the parameters collectively by   Can you see how many parameters exactly we have in total The figure also shows the derivation of the likelihood function.
We The figure shows likelihood function.	 We denote all the parameters collectively by   Can you see how many parameters exactly we have in total The figure also shows the derivation of the likelihood function.
total The likelihood words exactly simple language model.	 Can you see how many parameters exactly we have in total The figure also shows the derivation of the likelihood function  The likelihood function is seen to be a product over all the words in the document which is exactly the same as in the case of a simple unigram language model.
likelihood words document exactly simple it’s instead language mixture model generate	 The likelihood function is seen to be a product over all the words in the document which is exactly the same as in the case of a simple unigram language model  The only difference is that inside the product it’s now a sum instead of just one probability as in the simple unigram language model  We have this sum due to the mixture model where we have an uncertainty in using which model to generate a data point.
product probability simple We sum mixture uncertainty likelihood function denote probability	 The only difference is that inside the product it’s now a sum instead of just one probability as in the simple unigram language model  We have this sum due to the mixture model where we have an uncertainty in using which model to generate a data point  Because of this uncertainty our likelihood function also contains a parameter to denote the probability of choosing each particular component distribution.
likelihood choosing component	 Because of this uncertainty our likelihood function also contains a parameter to denote the probability of choosing each particular component distribution.
The function unique equation We types word distributions constraint	 The second line of the equation for the likelihood function is just another way of writing the product which is now a product over all the unique words in our vocabulary instead of over all the positions in the document as in the first line of the equation  We have two types of constraints one is that all the word distributions must sum to one and the other constraint is that the probabilities of choosing each topic must sum to one.
We word distributions probabilities sum The problem optimization satisfy	 We have two types of constraints one is that all the word distributions must sum to one and the other constraint is that the probabilities of choosing each topic must sum to one  The maximum likelihood estimation problem can now be seen as a constrained optimization problem where we seek parameter values that can maximize the likelihood function and satisfy all the constraints.
likelihood estimation optimization problem seek likelihood function constraints.	 The maximum likelihood estimation problem can now be seen as a constrained optimization problem where we seek parameter values that can maximize the likelihood function and satisfy all the constraints.
	 17.
discuss model start completely background word distribution goal estimate hope mon high	3 Estimation of a mixture model In this section we will discuss how to estimate the parameters of a mixture model  We will start with the simplest scenario where one component the background is already completely known and the topic choice distribution has an equal prob ability of choosing either the background or the topic word distribution  Our goal is to estimate the unknown topic word distribution where we hope to not see com mon words with high probabilities.
background known topic equal prob background topic word	 We will start with the simplest scenario where one component the background is already completely known and the topic choice distribution has an equal prob ability of choosing either the background or the topic word distribution.
topic com high probabilities A main assumption words background model discriminative words generated unknown distribution illustrated 17	 Our goal is to estimate the unknown topic word distribution where we hope to not see com mon words with high probabilities  A main assumption is that those common words are generated using the background model while the more discriminative contentbearing words are generated using the unknown topic word distribution as illustrated in Figure 17 17.
A common background contentbearing words generated unknown illustrated mixture model.	 A main assumption is that those common words are generated using the background model while the more discriminative contentbearing words are generated using the unknown topic word distribution as illustrated in Figure 17 17  This is also the scenario that we used to motivate the use of the mixture model.
	17.
In parameters unknown distribution pw θd Thus exactly number single model probabilistic embed variable parts	 In this scenario the only parameters unknown would be the topic word distribution pw  θd  Thus we have exactly the same number of parameters to estimate as in the case of a single unigram language model  Note that this is an example of customizing a general probabilistic model so that we can embed an unknown variable that we are interested in computing while simplifying other parts of the model based on certain assumptions that we can make about them.
Setting background model fixed distribution large sample English designing generative factor Feeding known background technique words parameter set based percentage common common words removed word	 Setting the background model to a fixed word distribution based on the maximum likelihood estimate of a unigram language model of a large sample of English text is not only feasible but also desirable since our goal of designing such a generative model is to factor out the common words from the topic word distribution to be estimated  Feeding the model with a known background word distribution is a powerful technique to inject our knowledge about what words are counted as noise stop words in this case  Similarly the parameter pθB can also be set based on our desired percentage of common words to factor out the larger pθB is set the more common words would be removed from the topic word distribution.
model word inject words case.	 Feeding the model with a known background word distribution is a powerful technique to inject our knowledge about what words are counted as noise stop words in this case.
assumed case guarantee probabilities application	 Note that we could have assumed that both θB and θd are unknown and we can also estimate both by using the maximum likelihood estimation but in such a case we would no longer be able to guarantee that we will obtain a distribution θB that assigns high probabilities to common words  For our application scenario i.
For application e.	 For our application scenario i e.
e.	e.
factoring words appropriate background bias allocating words distribution distribution explain view model	 factoring out common words it is more appropriate to preset the background word distribution to bias the model toward allocating the common words to the background word distribution and thus allow the topic word distribution to focus more on the content words as we will further explain  If we view the mixture model in Figure 17.
18 black actually exactly single unigram ture gives different likelihood function intuitively requires θB explain served document together” given model θd require explain section.	18 as a black box we would notice that it actually now has exactly the same number of parameters indeed the same parameters as the simplest single unigram language model  However the mix ture model gives us a different likelihood function which intuitively requires θd to work together optimally with the fixed background model θB to best explain the ob served document  It might not be obvious why the constraint of “working together” with the given background model would have the effect of factoring out the com mon words from θd as it would require understanding the behavior of parameter estimation in the case of a mixture model which we explain in the next section.
ture model requires work optimally background best explain It background model effect factoring mon words θd case section.	 However the mix ture model gives us a different likelihood function which intuitively requires θd to work together optimally with the fixed background model θB to best explain the ob served document  It might not be obvious why the constraint of “working together” with the given background model would have the effect of factoring out the com mon words from θd as it would require understanding the behavior of parameter estimation in the case of a mixture model which we explain in the next section.
17 Behavior Model order interesting behaviors look simple illustrated Figure 17.	 17 3 4 Behavior of a Mixture Model In order to understand some interesting behaviors of mixture models we take a look at a very simple case as illustrated in Figure 17.
4 Mixture order understand interesting look illustrated 19.	4 Behavior of a Mixture Model In order to understand some interesting behaviors of mixture models we take a look at a very simple case as illustrated in Figure 17 19.
	19.
Although patterns actually	 Although the example is very simple the observed patterns here actually are applicable to mixture models in general.
coin decide use Furthermore text oversimplification text useful ex behavior	 That is we will flip a fair coin to decide which model to use  Furthermore we will assume that there are precisely two words in our vocabulary the and text  Obviously this is a naive oversimplification of the actual text but it’s useful to ex amine the behavior in such a special case.
Furthermore assume vocabulary Obviously naive actual text useful ex case.	 Furthermore we will assume that there are precisely two words in our vocabulary the and text  Obviously this is a naive oversimplification of the actual text but it’s useful to ex amine the behavior in such a special case.
	9 to the and 0.
probability product probability generating Since θd function unknown θd ptext estimate function reach	 The probability of the twoword document is simply the product of the probability of each word which is itself a sum of the probability of generating the word with each of the two distributions  Since we already know all the parameters except for the θd  the likelihood function has just two unknown variables pthe  θd and ptext  θd  Our goal of computing the maximum likelihood estimate is to find out for what values of these two probabilities the likelihood function would reach its maximum.
θd variables pthe Our goal computing maximum values maximum.	 Since we already know all the parameters except for the θd  the likelihood function has just two unknown variables pthe  θd and ptext  θd  Our goal of computing the maximum likelihood estimate is to find out for what values of these two probabilities the likelihood function would reach its maximum.
Our goal computing likelihood values probabilities likelihood function maximum problem optimize simple expression 17 20.	 Our goal of computing the maximum likelihood estimate is to find out for what values of these two probabilities the likelihood function would reach its maximum  Now the problem has become one to optimize a very simple expression with two variables as shown in Figure 17 20.
simple expression shown	 Now the problem has become one to optimize a very simple expression with two variables as shown in Figure 17.
However can’t words probability	0 to maximize the likelihood expression  However we can’t do this because we can’t give both words a probability of one or otherwise they would sum to 2.
	0.
probability mass clearly value likelihood function allocation e.	 How should we allocate the probability between the two words As we shift probability mass from one word to the other it would clearly affect the value of the likelihood function  Imagine we start with an even allocation between the and text i e.
	e  each would have a probability of 0.
	5.
We probability mass text vice	 We can then imagine we could gradually move some probability mass from the to text or vice versa.
How change function value intuitively somewhat intuition mathematical variables constant product reach variables	 How would such a change affect the likelihood function value If you examine the formula carefully you might intuitively feel that we want to set the probability of text to be somewhat larger than the and this intuition can indeed be supported by a mathematical fact when the sum of two variables is a constant their product would reach the maximum when the two variables have the same value.
In case sum product 5	 In our case the sum of the two terms in the product is 0 5 .
5	5 .
ptext θd	 ptext  θd  0.
	1  0 5   pthe  θd  0.
pthe	5   pthe  θd  0.
5 0.	5   0.
0.	 0.
9 0 product reaches 0	9  1 0 so their product reaches maximum when 0 5 .
5 ptext	5   ptext  θd  0.
0.	 ptext  θd  0.
0.	1  0.
5 .	5 .
pthe θd 0 0.	 pthe  θd  0 5   0.
0	 0 9.
9 constraint ptext pthe θd 1 easily ptext θd θd	9  Plugging in the constraint ptext  θd  pthe  θd  1 we can easily obtain the solution ptext  θd  0 9 and pthe  θd  0.
constraint ptext θd θd 1 θd pthe 1.	 Plugging in the constraint ptext  θd  pthe  θd  1 we can easily obtain the solution ptext  θd  0 9 and pthe  θd  0 1.
1 Therefore larger probability effectively	1  Therefore the probability of text is indeed much larger than the probability of the effectively factoring out this common word.
Note case higher probability text reducing probability use model low probability text Looking process text θB background given probability solution higher probability overall models working the.	 Note that this is not the case when we have just one distribution where the has a much higher probability than text  The effect of reducing the estimated probability of the is clearly due to the use of the background model which assigned very high probability to the and low probability to text  Looking into the process of reaching this solution we see that the reason why text has a higher probability than the is because its corresponding probability by the background model ptext  θB is smaller than that of the had the background model given the a smaller probability than text our solution would give the a higher probability than text in order to ensure that the overall probability given by the two models working together is the same for text and the.
background high low process higher probability ptext smaller smaller text order ensure overall probability models text the.	 The effect of reducing the estimated probability of the is clearly due to the use of the background model which assigned very high probability to the and low probability to text  Looking into the process of reaching this solution we see that the reason why text has a higher probability than the is because its corresponding probability by the background model ptext  θB is smaller than that of the had the background model given the a smaller probability than text our solution would give the a higher probability than text in order to ensure that the overall probability given by the two models working together is the same for text and the.
	 In other words the two distributions tend to give high probabilities to different words as if they try to avoid giving the high probability to the same word.
probability product likelihood higher word probability given background θB The observed mixture model probability word tend discourage distributions	 In order to make their combined probability equal so as to maximize the product in the likelihood function the probability assigned by θd must be higher for a word that has a smaller probability given by the background model θB  The general behavior we have observed here about a mixture model is that if one distribution assigns a higher probability to one word than another the other distribution would tend to do the opposite it would discourage other distributions to do the same.
general observed distribution opposite discourage distributions	 The general behavior we have observed here about a mixture model is that if one distribution assigns a higher probability to one word than another the other distribution would tend to do the opposite it would discourage other distributions to do the same.
This means background model high common stop topical word assign probabilities words Let’s look Figure	 This also means that by using a background model that is fixed to assigning high probabilities to common stop words we can indeed encourage the unknown topical word distribution to assign smaller probabilities for such common words so as to put more probability mass on those content words that cannot be explained well by the background model  Let’s look at another behavior of the mixture model in Figure 17.
examining probabilities frequencies In 17.	21 by examining the response of the estimated probabilities to the data frequencies  In Figure 17.
In	 In Figure 17.
pw θ the’s add words document need multiply terms account additional	 What would happen to the estimated pw  θ if we keep adding more and more the’s to the document As we add more words to the document we would need to multiply the likelihood function by additional terms to account for the additional occurrences.
terms simply multiply term probability	 In this case since all the additional terms are the we simply need to multiply by the term representing the probability of the.
This obviously solution ML	 This obviously changes the likelihood function and thus also the solution of the ML estimation.
terms accounting occurrences ML solution modify solution One question word add mass word one.	 How exactly would the additional terms accounting for multiple occurrences of the change the ML estimate The solution we derived earlier ptext  θd  0 9 is no longer optimal  How should we modify this solution to make it optimal for the new likelihood function One way to address this question is to take away some probability mass from one word and add the probability mass to the other word which would ensure that they sum to one.
longer optimal likelihood One address away probability mass add probability mass ensure one.	9 is no longer optimal  How should we modify this solution to make it optimal for the new likelihood function One way to address this question is to take away some probability mass from one word and add the probability mass to the other word which would ensure that they sum to one.
solution new question word add word	 How should we modify this solution to make it optimal for the new likelihood function One way to address this question is to take away some probability mass from one word and add the probability mass to the other word which would ensure that they sum to one.
The course probability word	 The question is of course which word to have a reduced probability and which word to have a larger probability.
behavior because—after likelihood data word occurs higher overall probability phenomenon	 Such a behavior should not be surprising at all because—after all—we are maximizing the likelihood of the data so the more a word occurs the higher its overall probability should be  This is in fact a very gen eral phenomenon of all the maximum likelihood estimators.
case occurs frequently observed data θd assign	 In our special case if a word occurs more frequently in the observed text data it would also encourage the unknown distribution θd to assign a somewhat higher probability to this word.
We examine choosing We’ve far equally	 We can also use this example to examine the impact of pθB the probability of choosing the background model  We’ve been so far assuming that each model is equally likely i.
	e.
likelihood function 17 try picture probability choosing background model.	5  But you can again look at this likelihood function shown in Figure 17 21 and try to picture what would happen to the likelihood function if we increase the probability of choosing the background model.
But look likelihood 17 try happen likelihood function increase probability choosing	 But you can again look at this likelihood function shown in Figure 17 21 and try to picture what would happen to the likelihood function if we increase the probability of choosing the background model.
0.	 It is not hard to notice that if pθB  0.
large value terms larger background	5 is set to a very large value then all the terms representing the probability of the would be even larger because the background has a very high probability for the 0.
0	9 and the coefficient in front of 0 9 which was 0 5 would be even larger.
	9 which was 0 5 would be even larger.
important θd probability document overall probability large coefficient small large beneficial θd θd model θd assign high	 The consequence is that it is now less important for θd to increase the probability mass for the even when we add more and more occurrences of the to the document  This is because the overall probability of the is already very large due to the large pθB and large pthe  θB and the impact of increasing pthe  θd is regulated by the coefficient pθd which would be small if we make pθB very large  It would be more beneficial for θd to ensure ptext  θd to be high since text does not get any help from the background model and it must rely on θd to assign a high probability.
probability large pthe θB impact small pθB large It ensure ptext high help background θd high words pw θd degree document regularized	 This is because the overall probability of the is already very large due to the large pθB and large pthe  θB and the impact of increasing pthe  θd is regulated by the coefficient pθd which would be small if we make pθB very large  It would be more beneficial for θd to ensure ptext  θd to be high since text does not get any help from the background model and it must rely on θd to assign a high probability  While high frequency words tend to get higher probabilities in the estimated pw  θd the degree of increase of probability due to the increased counts of a word observed in the document is regularized by pθd or equivalently pθB.
ptext θd high model rely	 It would be more beneficial for θd to ensure ptext  θd to be high since text does not get any help from the background model and it must rely on θd to assign a high probability.
high frequency words degree observed document regularized pθB pθd important respond	 While high frequency words tend to get higher probabilities in the estimated pw  θd the degree of increase of probability due to the increased counts of a word observed in the document is regularized by pθd or equivalently pθB  The smaller pθd is the less important for θd to respond to the increase of counts of a word in the data.
likely component important assign higher probability	 In general the more likely a component is being chosen in a mixture model the more important it is for the component model to assign higher probability values to these frequent words.
First model assign high probabilities fre collaboratively maximize Second high order waste probability.	 First every component model attempts to assign high probabilities to high fre quent words in the data so as to collaboratively maximize the likelihood  Second different component models tend to bet high probabilities on different words in order to avoid the “competition” or waste of probability.
Second component models tend bet high probabilities words “competition” probability efficiently likelihood ity component collaboration	 Second different component models tend to bet high probabilities on different words in order to avoid the “competition” or waste of probability  This would allow them to collaborate more efficiently to maximize the likelihood  Third the probabil ity of choosing each component regulates the collaboration and the competition between component models.
It allow component data case background distribution timated documents simplest single language The model spe factor word making	 It would allow some component models to respond more to the change of frequency of a word in the data  We also discussed the special case of fixing one component to a background word distribution which can be es timated based on a large collection of English documents using the simplest single unigram language model to model the data  The behaviors of the ML estimate of such a mixture model ensure that the use of a fixed background model in such a spe cialized mixture model can effectively factor out common words such as the in the other topic word distribution making the discovered topic more discriminative.
discussed component word distribution timated large collection unigram data The estimate ensure model spe cialized mixture effectively topic word discovered topic view imposed strong prior model tion.	 We also discussed the special case of fixing one component to a background word distribution which can be es timated based on a large collection of English documents using the simplest single unigram language model to model the data  The behaviors of the ML estimate of such a mixture model ensure that the use of a fixed background model in such a spe cialized mixture model can effectively factor out common words such as the in the other topic word distribution making the discovered topic more discriminative  We may view our specialized mixture model as one where we have imposed a very strong prior on the model parameter and we use Bayesian parameter estima tion.
behaviors estimate mixture ensure spe model effectively words distribution discovered topic discriminative We model imposed strong prior parameter use Bayesian estima Our prior language models LM exactly predefined	 The behaviors of the ML estimate of such a mixture model ensure that the use of a fixed background model in such a spe cialized mixture model can effectively factor out common words such as the in the other topic word distribution making the discovered topic more discriminative  We may view our specialized mixture model as one where we have imposed a very strong prior on the model parameter and we use Bayesian parameter estima tion  Our prior is on one of the two unigram language models and it requires that this particular unigram LM must be exactly the same as a predefined background language model.
We specialized model strong prior model parameter parameter	 We may view our specialized mixture model as one where we have imposed a very strong prior on the model parameter and we use Bayesian parameter estima tion.
unigram unigram LM background language In estimation strong hold It useful model mix retrieval introduced	 Our prior is on one of the two unigram language models and it requires that this particular unigram LM must be exactly the same as a predefined background language model  In general Bayesian estimation would seek for a compromise between our prior and the data likelihood but in this case we can assume that our prior is infinitely strong and thus there is essentially no compromise hold ing one component model as constant the same as the provided background model  It is useful to point out that this mixture model is precisely the mix ture model for feedback in information retrieval that we introduced earlier in the book.
point mixture model precisely model feedback information retrieval introduced	 It is useful to point out that this mixture model is precisely the mix ture model for feedback in information retrieval that we introduced earlier in the book  17 3.
	 17 3.
The discussion behaviors ML estimate mixture model intuition use mixture factored background model section discuss compute ML estimate.	5 ExpectationMaximization The discussion of the behaviors of the ML estimate of the mixture model provides an intuition about why we can use a mixture model to mine one topic from a docu ment with common words factored out through the use of a background model  In this section we further discuss how we can compute such an ML estimate.
In discuss compute ML	 In this section we further discuss how we can compute such an ML estimate.
e.	e.
use numerical optimization ML	 pθd  We must use a numerical optimization algorithm to compute the ML estimate.
optimization algorithm compute In introduce specific algorithm computing ML twocomponent mixture EM algorithm.	 We must use a numerical optimization algorithm to compute the ML estimate  In this section we introduce a specific algorithm for computing the ML esti mate of the twocomponent mixture model called the ExpectationMaximization EM algorithm.
introduce algorithm ML esti mixture EM family algorithms computing likelihood mixture	 In this section we introduce a specific algorithm for computing the ML esti mate of the twocomponent mixture model called the ExpectationMaximization EM algorithm  EM is a family of useful algorithms for computing the maximum likelihood estimate of mixture models in general.
EM family mixture models general pθB “free” pw words sum illustrated Figure	 EM is a family of useful algorithms for computing the maximum likelihood estimate of mixture models in general  Recall that we have assumed both pw  θB and pθB are already given so the only “free” parameters in our model are pw  θd for all the words subject to the constraint that they sum to one  This is illustrated in Figure 17.
This illustrated 17.	 This is illustrated in Figure 17.
Intuitively ML estimate exploring possible word distribution θd values mixture data partitioned	22  Intuitively when we compute the ML estimate we would be exploring the space of all possible values for the word distribution θd until we find a set of values that would maximize the probability of the observed documents  According to our mixture model we can imagine that the words in the text data can be partitioned into two groups.
words partitioned	 According to our mixture model we can imagine that the words in the text data can be partitioned into two groups.
model explained unknown topical distribution The challenge ML word mixture	 One group will be explained generated by the background model  The other group will be explained by the unknown topical model the topic word distribution  The challenge in computing the ML estimate is that we do not know this partition due to the possibility of generating a word using either of the two distributions in the mixture model.
explained topical topic challenge computing partition word distributions If know trivial Figure	 The other group will be explained by the unknown topical model the topic word distribution  The challenge in computing the ML estimate is that we do not know this partition due to the possibility of generating a word using either of the two distributions in the mixture model  If we actually know which word is from which distribution computation of the ML estimate would be trivial as illustrated in Figure 17.
The challenge ML know possibility word distributions model If know trivial Figure	 The challenge in computing the ML estimate is that we do not know this partition due to the possibility of generating a word using either of the two distributions in the mixture model  If we actually know which word is from which distribution computation of the ML estimate would be trivial as illustrated in Figure 17.
word computation trivial Figure 17 22 d denote pseudo words d known θd ML θd pseudo document That words θd compute count word count total	 If we actually know which word is from which distribution computation of the ML estimate would be trivial as illustrated in Figure 17 22 where d ′ is used to denote the pseudo document that is composed of all the words in document d that are known to be generated by θd  and the ML estimate of θd is seen to be simply the normalized word frequency in this pseudo document d ′  That is we can simply pool together all the words generated from θd  compute the count of each word and then normalize the count by the total counts of all the words in such a pseudo document.
mixture model unigram models estimated separately based generated situation EM infer distribution parameters use partitioning improve estimate param eters hillclimbing estimate	 In such a case our mixture model is really just two independent unigram language models which can thus be estimated separately based on the data points generated by each of them  Unfortunately the real situation is such that we don’t really know which word is from which distribution  The main idea of the EM algorithm is to guess infer which word is from which distribution based on a tentative estimate of parameters and then use the inferred partitioning of words to improve the estimate of param eters which in turn enables improved inference of the partitioning leading to an iterative hillclimbing algorithm to improve the estimate of the parameters until hitting a local maximum.
don’t distribution algorithm guess word estimate use improve estimate param turn enables improved inference partitioning estimate parameters	 Unfortunately the real situation is such that we don’t really know which word is from which distribution  The main idea of the EM algorithm is to guess infer which word is from which distribution based on a tentative estimate of parameters and then use the inferred partitioning of words to improve the estimate of param eters which in turn enables improved inference of the partitioning leading to an iterative hillclimbing algorithm to improve the estimate of the parameters until hitting a local maximum.
iteration explained 3 Mining One Text 361 For assume tentative estimate parameters.	 In each iteration it would invoke an Estep followed by an Mstep which will be explained in more detail  17 3 Mining One Topic from Text 361 For now let’s assume we have a tentative estimate of all the parameters.
3 Topic 361 estimate parameters.	 17 3 Mining One Topic from Text 361 For now let’s assume we have a tentative estimate of all the parameters.
factors θd opposed θB generate word general This	 The value of pθd  text would depend on two factors    How often is θd as opposed to θB used to generate a word in general This probability is given by pθd.
How θB text	   How often is θd as opposed to θB used to generate a word in general This probability is given by pθd  If pθd is high then we’d expect pθd  text to be high.
pθd .	 If pθd is high then we’d expect pθd  text to be high  .
.	 .
θd we’d text	 If ptext  θd is high then we’d also expect pθd  text to be high.
17 inference prior distributions	 This is illustrated in Figure 17 23  The Bayesian inference involved here is a typical one where we have some prior about how likely each of these two distributions is used to generate any word i.
23 inference involved typical e.	23  The Bayesian inference involved here is a typical one where we have some prior about how likely each of these two distributions is used to generate any word i e.
Bayesian inference word	 The Bayesian inference involved here is a typical one where we have some prior about how likely each of these two distributions is used to generate any word i.
	 pθd and pθB.
Such prior updated incorporating data θd gives text	 Such a prior is then updated by incorporating the data likelihood ptext  θd and ptext  θB so that we would favor a distribution that gives text a higher probability  In the example shown in Figure 17.
prior distribution generate pw θd pw θB.	 In the example shown in Figure 17 23 our prior says that each of the two models is equally likely thus it is a noninformative prior one with no bias  As a result our inference of which distribution has been used to generate a word would solely be based on pw  θd and pw  θB.
23 prior says equally noninformative bias.	23 our prior says that each of the two models is equally likely thus it is a noninformative prior one with no bias.
result solely pw θB.	 As a result our inference of which distribution has been used to generate a word would solely be based on pw  θd and pw  θB.
ptext larger ptext conclude likely text.	 Since ptext  θd is much larger than ptext  θB we can conclude that θd is much more likely the distribution that has been used to generate text.
heavily prior For imagine says text θd ptext higher ptext strong	 Indeed a heavily biased prior can even dominate over the data likelihood to essen tially dictate the decision  For example imagine our prior says pθB  0 99999999 then our inference result would say that text is more likely generated by θB than by θd even though ptext  θd is much higher than ptext  θB due to the very strong prior.
For pθB 0.	 For example imagine our prior says pθB  0.
likely θd ptext θd prior Bayes’ Rule way	99999999 then our inference result would say that text is more likely generated by θB than by θd even though ptext  θd is much higher than ptext  θB due to the very strong prior  Bayes’ Rule provides us a principled way of combining the prior and data likelihood.
principled Figure 17 introduced latent variable	 Bayes’ Rule provides us a principled way of combining the prior and data likelihood  In Figure 17 23 we introduced a binary latent variable z here to denote whether the word is from the background or the topic.
In 23 z denote background topic.	 In Figure 17 23 we introduced a binary latent variable z here to denote whether the word is from the background or the topic.
binary variable z word background means it’s it’s it’s The posterior pz 0 guess generate word seen product prior pθd θd intuitively generate choose captured pθd word text selected θd pw	23 we introduced a binary latent variable z here to denote whether the word is from the background or the topic  When z is 0 it means it’s from the topic θd  when it’s 1 it means it’s from the background θB  The posterior probability pz  0  w  text formally captures our guess about which distribution has been used to generate the word text and it is seen to be proportional to the product of the prior pθd and the likelihood ptext  θd which is intuitively very meaningful since in order to generate text from θd  we must first choose θd as opposed to θB which is captured by pθd and then obtain word text from the selected θd  which is captured by pw  θd.
When 0 it’s topic θd it’s means it’s θB.	 When z is 0 it means it’s from the topic θd  when it’s 1 it means it’s from the background θB.
The posterior captures distribution word proportional prior pθd ptext θd intuitively order generate text choose θd opposed pθd obtain selected θd pw θd Understanding Bayesian distribution generate based tentative parameter crucial understanding EM use mixture model.	 The posterior probability pz  0  w  text formally captures our guess about which distribution has been used to generate the word text and it is seen to be proportional to the product of the prior pθd and the likelihood ptext  θd which is intuitively very meaningful since in order to generate text from θd  we must first choose θd as opposed to θB which is captured by pθd and then obtain word text from the selected θd  which is captured by pw  θd  Understanding how to make such a Bayesian inference of which distribution has been used to generate a word based on a set of tentative parameter values is very crucial for understanding the EM algorithm  This is essentially the Estep of the EM algorithm where we use Bayes’ rule to partition data and allocate all the data points among all the component models in the mixture model.
Bayesian distribution word based understanding This Estep algorithm Bayes’ model.	 Understanding how to make such a Bayesian inference of which distribution has been used to generate a word based on a set of tentative parameter values is very crucial for understanding the EM algorithm  This is essentially the Estep of the EM algorithm where we use Bayes’ rule to partition data and allocate all the data points among all the component models in the mixture model.
Bayes’ partition allocate data component	 This is essentially the Estep of the EM algorithm where we use Bayes’ rule to partition data and allocate all the data points among all the component models in the mixture model.
pz text tells count allocated contribute collect counts potentially improved estimate goal.	 That is pz  0  text tells us what percent of the count of text should be allocated to θd  and thus contribute to the estimate of θd   This way we will be able to collect all the counts allocated to θd  and renormalize them to obtain a potentially improved estimate of pw  θd which is our goal.
way renormalize obtain potentially	 This way we will be able to collect all the counts allocated to θd  and renormalize them to obtain a potentially improved estimate of pw  θd which is our goal.
This reestimating parameters based results Estep called One Topic Text EM algorithm	 This step of reestimating parameters based on the results from the Estep is called the Mstep  17 3 Mining One Topic from Text 363 With the Estep and Mstep as the basis the EM algorithm works as follows.
Topic 363 Estep basis EM works unknown parameters values randomly.	 17 3 Mining One Topic from Text 363 With the Estep and Mstep as the basis the EM algorithm works as follows  First we initialize all the unknown parameters values randomly.
unknown parameters allows specification mixture model use rule infer distribution This	 First we initialize all the unknown parameters values randomly  This allows us to have a complete specification of the mixture model which further enables us to use Bayes’ rule to infer which distribution is more likely to generate each word  This prediction i.
allows rule distribution word.	 This allows us to have a complete specification of the mixture model which further enables us to use Bayes’ rule to infer which distribution is more likely to generate each word.
prediction i.	 This prediction i.
helps separate words distributions Finally probabilistically topic word distribution probabilities serve parameters The repeated gradually improve parameter	 Estep essentially helps us probabilistically separate words generated by the two distributions  Finally we will collect all the probabilistically allocated counts of words belonging to our topic word distribution and normalize them into probabilities which serve as an improved estimate of the parameters  The process can then be repeated to gradually improve the parameter estimate until the likelihood function reaches a local maximum.
words normalize estimate	 Finally we will collect all the probabilistically allocated counts of words belonging to our topic word distribution and normalize them into probabilities which serve as an improved estimate of the parameters.
The parameter reaches maximum local maxima.	 The process can then be repeated to gradually improve the parameter estimate until the likelihood function reaches a local maximum  The EM algorithm can guarantee reaching such a local maximum but it cannot guarantee reaching a global maximum when there are multiple local maxima.
The EM 17 binary hidden z generated background model 0 illustration shows generated value	 The EM algorithm is illustrated in Figure 17 24 where we see that a binary hidden variable z has been introduced to indicate whether a word has been generated from the background model z  1 or the topic model z  0  For example the illustration shows that the is generated from background and thus the z value is 1.
binary variable introduced indicate background model z 1 topic model For illustration generated background value	24 where we see that a binary hidden variable z has been introduced to indicate whether a word has been generated from the background model z  1 or the topic model z  0  For example the illustration shows that the is generated from background and thus the z value is 1.
For example generated background value 1 topic value imagined latent word	 For example the illustration shows that the is generated from background and thus the z value is 1 0 while text is from the topic so its z value is 0  Note that we simply assumed imagined the existence of such a binary latent variable associated with each word token but we don’t really observe these z values.
0 text topic z 0 Note existence binary latent don’t observe z hidden variable.	0 while text is from the topic so its z value is 0  Note that we simply assumed imagined the existence of such a binary latent variable associated with each word token but we don’t really observe these z values  This is why we referred to such a variable as a hidden variable.
This hidden variable.	 This is why we referred to such a variable as a hidden variable.
A variables simplify computa estimate knowing variables estimate compute pool normalize z values potentially help simplify EM exploits fact alternating Estep estimate	 A main idea of EM is to leverage such hidden variables to simplify the computa tion of the ML estimate since knowing the values of these hidden variables makes the ML estimate trivial to compute we can pool together all the words whose z val ues are 0 and normalize their counts  Knowing z values can potentially help simplify the task of computing the ML estimate and EM exploits this fact by alternating the Estep and Mstep in each iteration so as to improve the parameter estimate in a hillclimbing manner.
e.	e.
w expected count event word w generated θd	 probability that the word w is indeed from θd so as to obtain a discounted count cw dpz 0  w which can be interpreted as the expected count of the event that word w is generated from θd .
5 word w distributions.	5 showing that all the counts of word w have been split between the two distributions.
Mstep pz 1 compute based words generated Figure EM	 Note that in the Mstep if pz  0  w  1 for all words we would simply compute the simple single unigram language model based on all the observed words which makes sense since the Estep would have told us that there is no chance that any word has been generated from the background  In Figure 17 25 we further illustrate in detail what happens in each iteration of the EM algorithm.
In illustrate happens EM	 In Figure 17 25 we further illustrate in detail what happens in each iteration of the EM algorithm.
	e.
pn1w assume component models θB fixed table.	 pn1w  θd  Second we assume the two component models θd and θB have equal probabilities we also assume that the background model word distribution is known fixed as shown in the third column of the table.
The starts preparation word counts observed text data EM algorithm parameters estimated.	 The computation of EM starts with preparation of relevant word counts  Here we assume that we have just four words and their counts in the observed text data are shown in the second column of the table  The EM algorithm then initializes all the parameters to be estimated.
assume counts The algorithm In set	 Here we assume that we have just four words and their counts in the observed text data are shown in the second column of the table  The EM algorithm then initializes all the parameters to be estimated  In our case we set all the probabilities to 0.
The algorithm initializes In	 The EM algorithm then initializes all the parameters to be estimated  In our case we set all the probabilities to 0.
set probabilities 0 25 column In EM infer distributions word i.	 In our case we set all the probabilities to 0 25 in the fourth column of the table  In the first iteration of the EM algorithm we will apply the Estep to infer which of the two distributions has been used to generate each word i.
	25 in the fourth column of the table.
	e.
showed pz 0 needed Mstep 1 pz 0 w Clearly w values prob initialized θd .	 We only showed pz  0  w which is needed in our Mstep pz  1  w  1 − pz  0  w  Clearly pz  0  w has different values for different words and this is because these words have different prob abilities in the background model and the initialized θd .
model θd	 Clearly pz  0  w has different values for different words and this is because these words have different prob abilities in the background model and the initialized θd .
Thus equally initial values θd uniform pz 0 w words probabilities pw θB pz z adjust counts corresponding words.	 Thus even though the two distributions are equally likely by our prior and our initial values for pw  θd form a uniform distribution the inferred pz  0  w would tend to give words with smaller probabilities if pw  θB give them a higher probability  For example pz  0  text  pz  0  the  Once we have the probabilities of all these z values we can perform the Mstep where these probabilities would be used to adjust the counts of the corresponding words.
text pz	 For example pz  0  text  pz  0  the.
Once z values probabilities adjust corresponding For example pz 0 0 33 count 0.	 Once we have the probabilities of all these z values we can perform the Mstep where these probabilities would be used to adjust the counts of the corresponding words  For example the count of the is 4 but since pz  0  the  0 33 we would obtain a discounted count of the 4 × 0.
example count 4 0 33 count Mstep.	 For example the count of the is 4 but since pz  0  the  0 33 we would obtain a discounted count of the 4 × 0 33 when estimating pthe  θd in the Mstep.
estimating Mstep.	33 when estimating pthe  θd in the Mstep.
count 0.	 Similarly the adjusted count for text would be 4 × 0.
	71.
Mstep ptext higher shown Iteration believed come topic word θd	 After the Mstep ptext  θd would be much higher than pthe  θd as shown in the table shown in the first column under Iteration 2  Those words that are believed to have come from the topic word distribution θd according to the Estep would have a higher probability.
adjust latent variable new generation ties fed estimate θd	 This new generation of parameters would allow us to further adjust the inferred latent variable or hidden variable values leading to a new generation of probabili ties for the z values which can be fed into another Mstep to generate yet another generation of potentially improved estimate of θd .
row table iteration.	 In the last row of the table we show the loglikelihood after each iteration.
loglikelihood negative transformation.	 These loglikelihood val ues are all negative because the probability is between 0 and 1 which becomes a negative value after the logarithm transformation.
value increasing improving values manner.	 We see that after each iteration the loglikelihood value is increasing showing that the EM algorithm is iteratively improving the estimated parameter values in a hillclimbing manner.
convergence meaningful useful	e  pw  θd the inferred pz  0  w after convergence is also meaningful and may sometimes be a useful byproduct.
pw θd pz convergence meaningful probabilities word come topic add obtain estimate extent	 pw  θd the inferred pz  0  w after convergence is also meaningful and may sometimes be a useful byproduct  Specifically these are the probabilities that a word is believed to have come from the topic distribution and we can add them up to obtain an estimate of to what extent the document has cov ered background vs.
content extent document “typical” This score document compare documents subsets e	 content or to what extent the content of the document deviates from a “typical” background document  This would give us a single numerical score for each document so we can then use the score to compare different documents or different subsets of documents e g.
numerical use documents e g.	 This would give us a single numerical score for each document so we can then use the score to compare different documents or different subsets of documents e g.
g associated different sources.	g  those associated with different authors or from different sources.
different sources.	 those associated with different authors or from different sources.
discover topic document useful measure “typicality” Next provide EM algorithm converge local	 Thus our simple twocomponent mixture model can not only help us discover a single topic from the document but also provide a useful measure of “typicality” of a document which may be useful in some applications  Next we provide some intuitive explanation why the EM algorithm will converge to a local maximum in Figure 17.
26 θd Yaxis likelihood	26  Here we show the parameter θd on the Xaxis and the Yaxis denotes the likelihood function value.
θd onedimensional view makes understand algorithm general like local	 This is an oversimplification since θd is an Mdimensional vector but the onedimensional view makes it much easier to understand the EM algorithm  We see that in general the original like lihood function as a function of θd may have multiple local maxima.
θd value makes function reach global	 the θd value that makes the likelihood function reach it global maximum.
The It random guess optimal iteratively picture scenario iteration n 1.	 The EM algorithm is a hillclimbing algorithm  It starts with an initial random guess of the optimal parameter value and then iteratively improves it  The picture shows the scenario of going from iteration n to iteration n  1.
guess parameter value The picture scenario iteration iteration n At n guess value	 It starts with an initial random guess of the optimal parameter value and then iteratively improves it  The picture shows the scenario of going from iteration n to iteration n  1  At iteration n the current guess of the parameter value is pnw  θd and it is seen to be nonoptimal in the picture.
picture shows going n iteration n At n guess parameter value θd seen nonoptimal picture In Estep function lower original likelihood	 The picture shows the scenario of going from iteration n to iteration n  1  At iteration n the current guess of the parameter value is pnw  θd and it is seen to be nonoptimal in the picture  In the Estep the EM algorithm conceptually computes an auxiliary function which lower bounds the original likelihood function.
At iteration n current guess parameter θd seen computes function lower bounds original function Lower means θd value auxiliary function original likelihood	 At iteration n the current guess of the parameter value is pnw  θd and it is seen to be nonoptimal in the picture  In the Estep the EM algorithm conceptually computes an auxiliary function which lower bounds the original likelihood function  Lower bounding means that for any given value of θd  the value of this auxiliary function would be no larger than that of the original likelihood function.
In original	 In the Estep the EM algorithm conceptually computes an auxiliary function which lower bounds the original likelihood function.
means given value original likelihood function Mstep finds parameter auxiliary value improved estimate	 Lower bounding means that for any given value of θd  the value of this auxiliary function would be no larger than that of the original likelihood function  In the Mstep the EM algorithm finds an optimal parameter value that would maximize the auxiliary function and treat this parameter value as our improved estimate pn1w  θd.
EM parameter function parameter bound function maximizing auxiliary new value according local case local maximum	 In the Mstep the EM algorithm finds an optimal parameter value that would maximize the auxiliary function and treat this parameter value as our improved estimate pn1w  θd  Since the auxiliary function is a lower bound of the original likelihood function maximizing the auxiliary function ensures the new parameter to also have a higher value according to the original likelihood function unless it has already reached a local maximum in which case the optimal value maximizing the auxiliary function is also a local maximum of the original likelihood function.
Since auxiliary bound function ensures higher value original likelihood function reached local maximum auxiliary function local function This explains EM maximum wonder don’t improved function.	 Since the auxiliary function is a lower bound of the original likelihood function maximizing the auxiliary function ensures the new parameter to also have a higher value according to the original likelihood function unless it has already reached a local maximum in which case the optimal value maximizing the auxiliary function is also a local maximum of the original likelihood function  This explains why the EM algorithm is guaranteed to converge to a local maximum  You might wonder why we don’t work on finding an improved parameter value directly on the original likelihood function.
explains EM converge local wonder don’t work parameter original Indeed EM optimize original function problem	 This explains why the EM algorithm is guaranteed to converge to a local maximum  You might wonder why we don’t work on finding an improved parameter value directly on the original likelihood function  Indeed it is possible to do that but in the EM algorithm the auxiliary function is usually much easier to optimize than the original likelihood function so in this sense it reduces the problem into a somewhat simpler one.
wonder work improved parameter likelihood function easier likelihood sense simpler	 You might wonder why we don’t work on finding an improved parameter value directly on the original likelihood function  Indeed it is possible to do that but in the EM algorithm the auxiliary function is usually much easier to optimize than the original likelihood function so in this sense it reduces the problem into a somewhat simpler one.
Indeed possible EM likelihood function problem simpler one.	 Indeed it is possible to do that but in the EM algorithm the auxiliary function is usually much easier to optimize than the original likelihood function so in this sense it reduces the problem into a somewhat simpler one.
auxiliary function analytical means maximization function require embedded EM	 Although the auxiliary function is generally easier to optimize it does not always have an analytical solution which means that the maximization of the auxiliary function may itself require another iterative process which would be embedded in the overall iterative process of the EM algorithm.
Thus word generated distributions solution solution values obtained Estep.	 Thus in the Estep we only computed a key component in the auxiliary function which is the probability that a word has been generated from each of the two distributions and our Mstep directly gives us an analytical solution to the problem of optimizing the auxiliary function and the solution directly uses the values obtained from the Estep.
EM applications For general timation models EM algorithm.	 The EM algorithm has many applications  For example in general parameter es timation of all mixture models can be done by using the EM algorithm.
example general mixture	 For example in general parameter es timation of all mixture models can be done by using the EM algorithm.
variables model component model data point Thus values distribution estima parameters.	 The hidden variables introduced in a mixture model often indicate which component model has been used to generate a data point  Thus once we know the values of these hidden variables we would be able to partition data and identify the data points that are likely generated from any particular distribution thus facilitating estima tion of component model parameters.
The EM	 The EM algorithm would then work as follows.
randomly initialize parameters estimated Second Estep attempt current generation hidden possible values variables	 First it would randomly initialize all the parameters to be estimated  Second in the Estep it would attempt to infer the values of the hidden variables based on the current generation of parameters and obtain a probability distribution of hidden variables over all possible values of these hidden variables  Intuitively this is to take a good guess of the values of the hidden variables.
parameters distribution variables values hidden variables good guess Mstep use inferred hidden variable values estimate parameter	 Second in the Estep it would attempt to infer the values of the hidden variables based on the current generation of parameters and obtain a probability distribution of hidden variables over all possible values of these hidden variables  Intuitively this is to take a good guess of the values of the hidden variables  Third in the Mstep it would use the inferred hidden variable values to compute an improved estimate of the parameter values.
Intuitively good values Third inferred variable compute This local	 Intuitively this is to take a good guess of the values of the hidden variables  Third in the Mstep it would use the inferred hidden variable values to compute an improved estimate of the parameter values  This process is repeated until convergence to a local maximum of the likelihood function.
Mstep inferred values improved local	 Third in the Mstep it would use the inferred hidden variable values to compute an improved estimate of the parameter values  This process is repeated until convergence to a local maximum of the likelihood function.
This process convergence Note likelihood guaran maximum guarantee parameters estimated convergence particular set values	 This process is repeated until convergence to a local maximum of the likelihood function  Note that although the likelihood function is guaran teed to converge to a local maximum there is no guarantee that the parameters to be estimated always have a stable convergence to a particular set of values  That is the parameters may oscillate even though the likelihood is increasing.
Note likelihood parameters estimated stable set likelihood	 Note that although the likelihood function is guaran teed to converge to a local maximum there is no guarantee that the parameters to be estimated always have a stable convergence to a particular set of values  That is the parameters may oscillate even though the likelihood is increasing.
Semantic Analysis section probabilistic analysis topic applications In short PLSA earlier chapter discover text data Thus understood component	4 Probabilistic Latent Semantic Analysis In this section we introduce probabilistic latent semantic analysis PLSA the most basic topic model with many applications  In short PLSA is simply a generalization of the twocomponent mixture model that we discussed earlier in this chapter to discover more than one topic from text data  Thus if you have understood the two component mixture model it would be straightforward to understand how PLSA works.
short mixture text understood component mixture model straightforward understand PLSA works.	 In short PLSA is simply a generalization of the twocomponent mixture model that we discussed earlier in this chapter to discover more than one topic from text data  Thus if you have understood the two component mixture model it would be straightforward to understand how PLSA works.
component understand works.	 Thus if you have understood the two component mixture model it would be straightforward to understand how PLSA works.
mentioned general task topic analysis multiple text coverage precisely designed perform task As	 As we mentioned earlier the general task of topic analysis is to mine multiple topics from text documents and compute the coverage of each topic in each doc ument  PLSA is precisely designed to perform this task  As in all topic models we make two key assumptions.
models key	 As in all topic models we make two key assumptions.
First assume represented generally term Second text document sample drawn	 First we assume that a topic can be represented as a word distribution or more generally a term distribution  Second we assume that a text document is a sample of words drawn from a probabilistic model.
assume text illustrate assumptions Figure	 Second we assume that a text document is a sample of words drawn from a probabilistic model  We illustrate these two assumptions in Figure 17.
27 blog Hurricane imagined	27 where we see a blog article about Hurricane Katrina and some imagined topics each represented by a word distribution including e.
	g.
topic response The article words	 a topic on government response θ1 a topic on the flood of the city of New Orleans θ2 a topic on donation θk and a background topic θB  The article is seen to contain words from all these distributions.
criticism government response excerpt discussion flooding city sen tence mixed main goal decode topics segmenting obtain text coverage topics document.	 Specif ically we see there is a criticism of government response at the beginning of this excerpt which is followed by discussion of flooding of the city and then a sen tence about donation  We also see background words mixed in throughout the article  The main goal of topic analysis is to try to decode these topics behind the text by segmenting them and figure out which words are from which distribution so that we can obtain both characterizations of all the topics in the text data and the coverage of topics in each document.
We mixed	 We also see background words mixed in throughout the article.
The mining illustrated Fig 17.	 The formal definition of mining multiple topics from text is illustrated in Fig ure 17.
	28.
text ulary set The output	 The input is a collection of text data the number of topics and a vocab ulary set  The output is of two types.
output types One topic topic word distribution πij refers probability document covers .	 The output is of two types  One is topic characterization where each topic is represented by θi which is a word distribution  The other is the topic coverage for each document πij which refers to the probability that document di covers topic θj .
One topic represented θi distribution document covers Such problem PLSA generalization simple component model	 One is topic characterization where each topic is represented by θi which is a word distribution  The other is the topic coverage for each document πij which refers to the probability that document di covers topic θj   Such a problem can be solved by using PLSA a generalization of the simple two component mixture model to more than two components.
coverage πij covers topic	 The other is the topic coverage for each document πij which refers to the probability that document di covers topic θj .
problem generalization simple components.	 Such a problem can be solved by using PLSA a generalization of the simple two component mixture model to more than two components.
generative 17 29 retain model topic.	 Such a more generative model is illustrated in Figure 17 29 where we also retain the background model used in the twocomponent mixture model which if you recall was designed to discover just one topic.
retain model model discover topic.	29 where we also retain the background model used in the twocomponent mixture model which if you recall was designed to discover just one topic.
Different model discussed earlier model includes distinct word background model 1	 Different from the simple mixture model discussed earlier the model here includes k component models each of which represents a distinct topic and can be used to generate a word in the observed text data  Adding the background model θB we thus have a total of k  1 component unigram language models in PLSA 2 2.
θB unigram	 Adding the background model θB we thus have a total of k  1 component unigram language models in PLSA.
2 The PLSA 1999 model gives common high probabilities learned words As mixture generating	2 2  The original PLSA Hofmann 1999 did not include a background language model thus it gives common words high probabilities in the learned topics if such common words are not removed in the preprocessing stage  As in the case of the simple mixture model the process of generating a word still consists of two steps.
decision λB background model πd denoting topic decided	 The first is to choose a component model to use this decision is controlled by both a parameter λB denoting the probability of choosing the background model and a set of πd  i denoting the probability of choosing topic θi if we decided not to use the background model.
background topics constraint∑k πd 1 background λB 1	 If we do not use the background model we must choose one from the k topics which has the constraint∑k i1 πd  i  1  Thus the probability of choosing the background model is λB while the probability of choosing topic θi is 1 − λBπd  i.
probability choosing model topic θi 1 Once generation simply draw word model.	 Thus the probability of choosing the background model is λB while the probability of choosing topic θi is 1 − λBπd  i  Once we decide which component word distribution to use the second step in the generation process is simply to draw a word from the selected distribution exactly the same as in the simple mixture model.
Once decide word distribution use step process simply distribution exactly generative likelihood function ask mixture mixture model sum total 1 models k terms term probability distribution particular model observing particular word distribution.	 Once we decide which component word distribution to use the second step in the generation process is simply to draw a word from the selected distribution exactly the same as in the simple mixture model  As usual once we design the generative model the next step is to write down the likelihood function  We ask the question what’s the probability of observing a word from such a mixture model As in the simple mixture model this probability is a sum over all the different ways to generate the word we have a total of k  1 different component models thus it is a sum of k  1 terms where each term captures the probability of observing the word from the corresponding component word distribution which can be further written as the product of the probability of selecting the particular component model and the probability of observing the particular word from the particular selected word distribution.
usual write word As model different word k 1 models sum terms probability observing corresponding word product selecting particular model observing particular selected distribution	 As usual once we design the generative model the next step is to write down the likelihood function  We ask the question what’s the probability of observing a word from such a mixture model As in the simple mixture model this probability is a sum over all the different ways to generate the word we have a total of k  1 different component models thus it is a sum of k  1 terms where each term captures the probability of observing the word from the corresponding component word distribution which can be further written as the product of the probability of selecting the particular component model and the probability of observing the particular word from the particular selected word distribution  The likelihood function is as illustrated in Figure 17.
question what’s probability word model probability word total k 1 1 terms term probability component word written product model word word likelihood function illustrated	 We ask the question what’s the probability of observing a word from such a mixture model As in the simple mixture model this probability is a sum over all the different ways to generate the word we have a total of k  1 different component models thus it is a sum of k  1 terms where each term captures the probability of observing the word from the corresponding component word distribution which can be further written as the product of the probability of selecting the particular component model and the probability of observing the particular word from the particular selected word distribution  The likelihood function is as illustrated in Figure 17.
The function illustrated Figure θB probability observing topic 1 λBπd	 The likelihood function is as illustrated in Figure 17 30  Specifically the prob ability of observing a word from the background distribution is λBpw  θB while the probability of observing a word from a topic θj is 1 − λBπd jpw  θj.
30 Specifically observing θB probability observing 1 − λBπd jpw θj The pdw sum	30  Specifically the prob ability of observing a word from the background distribution is λBpw  θB while the probability of observing a word from a topic θj is 1 − λBπd jpw  θj  The probability of observing the word regardless of which distribution is used pdw is just a sum of all these cases.
probability distribution sum Assuming words document generated follows d second equation	 The probability of observing the word regardless of which distribution is used pdw is just a sum of all these cases  Assuming that the words in a document are generated independently it follows that the likelihood function for document d is the second equation in Figure 17.
Assuming generated follows function document 17 30 entire represents background factor	 Assuming that the words in a document are generated independently it follows that the likelihood function for document d is the second equation in Figure 17 30 and that the likelihood function for the entire collection C is given by the third equation  What are the parameters in PLSA First we see λB which represents the percent age of background words that we believe exist in the text data and that we would like to factor out.
empirically desired discrim ination discovered models Second background pw large text available estimate e.	 This parameter can be set empirically to control the desired discrim ination of the discovered topic models  Second we see the background language model pw  θB which we also assume is known  We can use any large collection of text or use all the text that we have available in collection C to estimate pw  θB e.
Second background language θB large collection pw	 Second we see the background language model pw  θB which we also assume is known  We can use any large collection of text or use all the text that we have available in collection C to estimate pw  θB e g.
large collection estimate pw	 We can use any large collection of text or use all the text that we have available in collection C to estimate pw  θB e.
text generated set pw θB count word w πd j indicates θj	g  assuming all the text data are generated from θB we can use the ML estimate to set pw  θB to the normalized count of word w in the data  Third we see πd j which indicates the coverage of topic θj in document d.
data set normalized count w data coverage topic document d parameter knowledge hope	 assuming all the text data are generated from θB we can use the ML estimate to set pw  θB to the normalized count of word w in the data  Third we see πd j which indicates the coverage of topic θj in document d  This parameter encodes the knowledge we hope to discover from text.
encodes knowledge discover k representing	 This parameter encodes the knowledge we hope to discover from text  Finally we see the k word distributions each representing a topic pw  θj.
k word distributions representing pw encodes like parameters model exercise understand outputs text	 Finally we see the k word distributions each representing a topic pw  θj  This parameter also encodes the knowledge we would like to discover from the text data  Can you figure out how many unknown parameters are there in such a PLSA model This would be a useful exercise to do which helps us understand what exactly are the outputs that we would generate by using PLSA to analyze text data.
This encodes discover unknown model helps exactly outputs	 This parameter also encodes the knowledge we would like to discover from the text data  Can you figure out how many unknown parameters are there in such a PLSA model This would be a useful exercise to do which helps us understand what exactly are the outputs that we would generate by using PLSA to analyze text data.
use Maximum shown 17.	 As usual we can use the Maximum Likelihood estimator as shown in Figure 17.
31 optimization problem case mixture	31 where we see that the problem is essentially a constrained optimization problem as in the case of the simple mixture model except that   we now have a collection of text articles instead of just one document .
constraint parameters.	 we have more parameters to estimate and   we have more constraint equations which is a consequence of having more parameters.
Despite constraints two.	 Despite the third point the kinds of constraints are essentially the same as before namely there are two.
ensures topic coverage document ensures	 One ensures the topic coverage probabilities sum to one for each document over all the possible topics and the other ensures that the probabilities of all the words in each topic sum to one.
simple mixture models algorithm In hidden	 As in the case of simple mixture models we can also use the EM algorithm to compute the ML estimate for PLSA  In the Estep we have to introduce more hidden variables because we have more topics.
In topics Our variable indicator word 1 1	 In the Estep we have to introduce more hidden variables because we have more topics  Our hidden variable z which is a topic indicator for a word now would take k  1 values 1 2 .
hidden indicator 1 values	 Our hidden variable z which is a topic indicator for a word now would take k  1 values 1 2   .
	 .
k topic Bayes’ Rule probability z 17.	    k  B corresponding to the k topics and the extra background topic  The Estep uses Bayes’ Rule to infer the probability of each value for z as shown in Figure 17.
Estep infer probability value	 The Estep uses Bayes’ Rule to infer the probability of each value for z as shown in Figure 17.
	32.
A Estep twocomponent mixture similar	 A comparison between these equations as the Estep for the simple twocomponent mixture model would reveal immediately that the equations are essentially similar only now we have more topics.
k Estep simple pzd w probability generated topic θj probability conditioned model generating word Bpzd j case topic background model θj	 Indeed if we assume there is just one topic k  1 then we would recover the Estep equation of the simple mixture model with just one small difference pzd w  j is not quite the probability that the word is generated from topic θj  but rather this probability conditioned on having not chosen the background model  In other words the probability of generating a word using θj is 1 − pzd w  Bpzd w  j  In the case of having just one topic other than the background model we would have pzd w  j  1 only for θj .
pzd Bpzd w j In case topic w j 1	 In other words the probability of generating a word using θj is 1 − pzd w  Bpzd w  j  In the case of having just one topic other than the background model we would have pzd w  j  1 only for θj .
In having topic background model j θj Note d w.	 In the case of having just one topic other than the background model we would have pzd w  j  1 only for θj   Note that we use document d here to index the word w.
word w generated depends document Indeed j potentially topic coverage assumption reasonable documents different topics.	 Note that we use document d here to index the word w  In our model whether w has been generated from a particular topic actually depends on the document Indeed the parameter πd j is tied to each document and thus each document can have a potentially different topic coverage distribution  Such an assumption is reasonable as different documents generally have a different emphasis on specific topics.
generated particular Indeed parameter πd j tied document different topic	 In our model whether w has been generated from a particular topic actually depends on the document Indeed the parameter πd j is tied to each document and thus each document can have a potentially different topic coverage distribution.
Such reasonable documents This means word different generally πd j values similar	 Such an assumption is reasonable as different documents generally have a different emphasis on specific topics  This means that in the Estep the inferred probability of topics for the same word can be potentially very different for different documents since different documents generally have different πd j values  The Mstep is also similar to that in the simple mixture model.
means Estep probability topics generally πd values Mstep similar model.	 This means that in the Estep the inferred probability of topics for the same word can be potentially very different for different documents since different documents generally have different πd j values  The Mstep is also similar to that in the simple mixture model.
Mstep mixture	 The Mstep is also similar to that in the simple mixture model.
equations 17.	 We show the equations in Figure 17.
pw θ respectively pzd w j counts use distribution values Estep split counts w distributions.	 We see that a key component in the two equations for reestimating π and pw  θ respectively is cw d1 − pzd w  Bpzd w  j which can be interpreted as the allocated counts of w to topic θj   Intuitively we use the inferred distribution of z values from the Estep to split the counts of w among all the distributions.
Intuitively use inferred values split distributions The θj based inferred θj	 Intuitively we use the inferred distribution of z values from the Estep to split the counts of w among all the distributions  The amount of split counts of w that θj can get is determined based on the inferred likelihood that w is generated by topic θj .
33 d split counts words document d counts split counts word documents counts	33  To reestimate πd j  the probability that document d covers topic θj we would simply collect all the split counts of words in document d that belong to each θj  and then normalize these counts among all the k topics  Similarly to reestimate pw  θj we would collect the split counts of a word toward θj from all the documents in the collection and then normalize these counts among all the words.
To θj collect document belong counts k Similarly θj split collection normalize counts	 To reestimate πd j  the probability that document d covers topic θj we would simply collect all the split counts of words in document d that belong to each θj  and then normalize these counts among all the k topics  Similarly to reestimate pw  θj we would collect the split counts of a word toward θj from all the documents in the collection and then normalize these counts among all the words.
reestimation π values normalizer document.	 In the case of reestimation of π  the constraint is that the π values must sum to one for each document thus our normalizer has been chosen to ensure that the reestimated values of π indeed sum to one for each document.
The true pw normalizer allows distribution topic actually generally true distribution hidden variables compute normalized reestimate parameters.	 The same is true for the re estimation of pw  θ where our normalizer allows us to obtain a word distribution for each topic  What we observed here is actually generally true when using the EM algorithm  That is the distribution of the hidden variables computed in the Estep can be used to compute the expected counts of an event which can then be aggregated and normalized appropriately to obtain a reestimate of the parameters.
What actually true	 What we observed here is actually generally true when using the EM algorithm.
In EM algorithm counts events obtain reestimates	 In the implementation of the EM algorithm we can thus just keep the counts of various events and then normalize them appropriately to obtain reestimates for various parameters.
In Figure PLSA We cov distribution πd j document d distribution pw	 In Figure 17 34 we show the computation of the EM algorithm for PLSA in more detail  We first initialize all the unknown parameters randomly including the cov erage distribution πd j for each document d and the word distribution for each topic pw  θj.
We initialize unknown parameters including πd document d After initialization step EM algorithm How know track likelihood current likelihood previous iteration	 We first initialize all the unknown parameters randomly including the cov erage distribution πd j for each document d and the word distribution for each topic pw  θj  After the initialization step the EM algorithm would go through a loop until the likelihood converges  How do we know when the likelihood con verges We can keep track of the likelihood values in each iteration and compare the current likelihood with the likelihood from the previous iteration or the average of the likelihood from a few previous iterations.
step algorithm loop How verges track likelihood values iteration current likelihood likelihood iteration likelihood iterations likelihood ilar previous stop	 After the initialization step the EM algorithm would go through a loop until the likelihood converges  How do we know when the likelihood con verges We can keep track of the likelihood values in each iteration and compare the current likelihood with the likelihood from the previous iteration or the average of the likelihood from a few previous iterations  If the current likelihood is very sim ilar to the previous one judged by a threshold we can assume that the likelihood has converged and can stop the algorithm.
know likelihood iteration current previous If assume likelihood algorithm.	 How do we know when the likelihood con verges We can keep track of the likelihood values in each iteration and compare the current likelihood with the likelihood from the previous iteration or the average of the likelihood from a few previous iterations  If the current likelihood is very sim ilar to the previous one judged by a threshold we can assume that the likelihood has converged and can stop the algorithm.
If sim previous threshold assume stop algorithm.	 If the current likelihood is very sim ilar to the previous one judged by a threshold we can assume that the likelihood has converged and can stop the algorithm.
augment predicting variables In case variable word topic k from.	 In the Estep it would augment the data by predicting the hidden variables  In this case the hidden variable zd w indicates whether word w in d is from a “real” topic or the background  If it’s from a real topic it determines which of the k topics it is from.
variable w indicates word topic real determines k topics	 In this case the hidden variable zd w indicates whether word w in d is from a “real” topic or the background  If it’s from a real topic it determines which of the k topics it is from.
k 17.	 If it’s from a real topic it determines which of the k topics it is from  From Figure 17.
From 17 Estep probability z unique document.	 From Figure 17 34 we see that in the Estep we need to compute the probability of z values for every unique word in each document.
Thus iterate compute corresponding	 Thus we can iterate over all the documents and for each document iterate over all the unique words in the document to compute the corresponding pzd w.
This computation probability selected	 This computation involves computing the product of the probability of selecting a topic and the probability of word w given by the selected distribution.
based ∑k j 1 In	 We can then normalize these products based on the constraints we have to ensure ∑k j1 pzd w  j  1  In this case the normalization is among all the topics.
topics Mstep relevant normalize priately obtain parameters We use distribution w document d	 In this case the normalization is among all the topics  In the Mstep we will also collect the relevant counts and then normalize appro priately to obtain reestimates of various parameters  We would use the estimated probability distribution pzd w to split the count of word w in document d among all the topics.
collect relevant priately parameters We estimated probability distribution split word topics generally different documents.	 In the Mstep we will also collect the relevant counts and then normalize appro priately to obtain reestimates of various parameters  We would use the estimated probability distribution pzd w to split the count of word w in document d among all the topics  Note that the same word would generally be split in different ways in different documents.
different different counts aggregate split counts coverage θj relevant counts words d topic counts topics normalization	 Note that the same word would generally be split in different ways in different documents  Once we split the counts for all the words in this way we can aggregate the split counts and normalize them  For example to reestimate πd j coverage of topic θj in document d the relevant counts would be the counts of words in d that have been allocated to topic θj  and the normalizer would be the sum of all such counts over all the topics so that after normalization we would ob tain a probability distribution over all the topics.
Once words counts reestimate πd coverage topic counts d θj normalizer normalization	 Once we split the counts for all the words in this way we can aggregate the split counts and normalize them  For example to reestimate πd j coverage of topic θj in document d the relevant counts would be the counts of words in d that have been allocated to topic θj  and the normalizer would be the sum of all such counts over all the topics so that after normalization we would ob tain a probability distribution over all the topics.
reestimate j topic counts counts d allocated θj topics normalization ob tain Similarly θj split counts doc	 For example to reestimate πd j coverage of topic θj in document d the relevant counts would be the counts of words in d that have been allocated to topic θj  and the normalizer would be the sum of all such counts over all the topics so that after normalization we would ob tain a probability distribution over all the topics  Similarly to reestimate pw  θj the relevant counts are the sum of all the split counts of word w in all the doc uments.
counts normalized sum vocabulary normalization obtain If complete starting lot memory Estep interleave collect needed Mstep compute	 These aggregated counts would then be normalized by the sum of such aggregated counts over all the words in the vocabulary so that after normalization we again would obtain a distribution this time over all the words rather than all the topics  If we complete all the computation of the Estep before starting the Mstep we would have to allocate a lot of memory to keep track of all the results from the Estep  However it is possible to interleave the Estep and Mstep so that we can collect and aggregate relevant counts needed for the Mstep while we compute the Estep.
However interleave collect needed compute	 However it is possible to interleave the Estep and Mstep so that we can collect and aggregate relevant counts needed for the Mstep while we compute the Estep.
This eliminate need 17 5 Latent Dirichlet Allocation unsupervised text data manual effort.	 This would eliminate the need for storing many intermediate values unnecessarily  17 5 Extension of PLSA and Latent Dirichlet Allocation PLSA works well as a completely unsupervised method for analyzing topics in text data thus it does not require any manual effort.
	 17.
5 works analyzing text data require	5 Extension of PLSA and Latent Dirichlet Allocation PLSA works well as a completely unsupervised method for analyzing topics in text data thus it does not require any manual effort.
sense minimizing effort topics data characteristics extra knowledge coverage set extra imposes analyzed beneficial impose knowledge parameters text parameters.	 While this is an advantage in the sense of minimizing human effort the discovery of topics is solely driven by the data characteristics with no consideration of any extra knowledge about the topics and their coverage in the data set  Since we often have such extra knowledge or our application imposes a particular preference for the topics to be analyzed it is beneficial or even necessary to impose some prior knowledge about the parameters to be estimated so that the estimated parameters would not only explain the text data well but also be consistent with our prior knowledge  Prior knowledge or preferences may be available for all the parameters.
Since knowledge application preference parameters estimated data consistent knowledge knowledge user topics text data define topic distributions.	 Since we often have such extra knowledge or our application imposes a particular preference for the topics to be analyzed it is beneficial or even necessary to impose some prior knowledge about the parameters to be estimated so that the estimated parameters would not only explain the text data well but also be consistent with our prior knowledge  Prior knowledge or preferences may be available for all the parameters  First a user may have some expectations about which topics to analyze in the text data and such knowledge can be used to define a prior on the topic word distributions.
parameters.	 Prior knowledge or preferences may be available for all the parameters.
First expectations text data define word distributions.	 First a user may have some expectations about which topics to analyze in the text data and such knowledge can be used to define a prior on the topic word distributions.
topics covered document For example tags assigned documents users regard tags covered prior coverage generated assigned	 Second users may have knowledge about what topics are or are not covered in a document  For example if we have topical tags assigned to documents by users we may regard the tags assigned to a document as knowledge about what topics are covered in the document  Thus we can define a prior on the topic coverage to ensure that a document can only be generated using topics corresponding to the tags assigned to it.
define generated tags This essentially document learning cooccuring context topic data pure induce topic PLSA Maximum Estimation Maximum	 Thus we can define a prior on the topic coverage to ensure that a document can only be generated using topics corresponding to the tags assigned to it  This essentially gives us a constraint on what topics can be used to generate words in a document which can be useful for learning cooccuring words in the context of a topic when the data are sparse and pure cooccurrence statistics are insufficient to induce a meaningful topic  All such prior knowledge can be incorporated into PLSA by using Maximum A Posteriori Estimation MAP instead of Maximum Likelihood estimation.
This essentially constraint generate learning context pure meaningful	 This essentially gives us a constraint on what topics can be used to generate words in a document which can be useful for learning cooccuring words in the context of a topic when the data are sparse and pure cooccurrence statistics are insufficient to induce a meaningful topic.
Such distribu possible topic coverage defined knowledge inject model With prior follows ∗ ppData 17.	 Such a prior distribu tion would technically include a distribution over all possible word distributions for topic characterization and all possible coverage distributions of topics in a document for topic coverage and can be defined based on whatever knowledge or preferences we would like to inject into the model  With such a prior we can then estimate parameters by using MAP as follows ∗  arg max ppData   17.
With prior MAP ∗ arg max	 With such a prior we can then estimate parameters by using MAP as follows ∗  arg max ppData   17.
	 There are potentially many different ways to define p.
ularly convenient prior density pData tion functions single	 However it is partic ularly convenient to use a conjugate prior distribution in which the prior density function p is of the same form as the likelihood function pData   as a func tion of the parameter   Due to the same form of the two functions we can generally merge the two to derive a single function again of the same form.
posterior parameter mization probability similar likelihood	 In other words our posterior distribution is written as a function of the parameter so the maxi mization of the posterior probability would be similar to the maximization of the likelihood function.
prior earlier pseudo data added incorporate define conjugate prior word distributions representing topics θj EM MAP shown 17.	 When using such a conjugate prior the computation of MAP can be done by using a slightly modified version of the EM algorithm that we introduced earlier for PLSA where appropriate counts of pseudo data are added to incorporate the prior  As a specific example if we define a conjugate prior on the word distributions representing the topics pw  θj then the EM algorithm for computing the MAP is shown in Figure 17.
As example prior representing topics shown Figure 17 We adding count Mstep probability	 As a specific example if we define a conjugate prior on the word distributions representing the topics pw  θj then the EM algorithm for computing the MAP is shown in Figure 17 35  We see that the difference is adding an additional pseudo count for word w in the Mstep which is proportional to the probability of the word in the prior.
35 adding pseudo count word Mstep proportional word adding sum estimated one.	35  We see that the difference is adding an additional pseudo count for word w in the Mstep which is proportional to the probability of the word in the prior  The denominator needs to be adjusted accordingly adding μ which is the sum of all the pseudo counts for all the words to ensure the estimated word probabilities for a topic sum to one.
denominator needs adjusted pseudo counts word probabilities Here μ 0 μ 0 original algorithm	 The denominator needs to be adjusted accordingly adding μ which is the sum of all the pseudo counts for all the words to ensure the estimated word probabilities for a topic sum to one  Here μ ∈ 0 ∞ is a parameter encoding the strength of our prior  If μ  0 we recover the original EM algorithm for PLSA i.
encoding strength	 Here μ ∈ 0 ∞ is a parameter encoding the strength of our prior.
influence ∞ case Mstep estimated probability i.	e  with no prior influence  A more interesting case is when μ  ∞ in such a case the Mstep is simply to set the estimated probability of a word pw  θj to the prior i.
	 with no prior influence.
A interesting simply word	 A more interesting case is when μ  ∞ in such a case the Mstep is simply to set the estimated probability of a word pw  θj to the prior i e  the word distribution is fixed to the prior.
interpret heuristic distribution simply topics Bayesian prior prior data able data estimate overriding completely infinitely data.	 This is why we can interpret our heuristic inclusion of a background word distribution as a topic in PLSA as simply imposing such an infinitely strong prior on one of the topics  Intuitively in Bayesian inference this means that if the prior is infinitely strong then no matter how much data we collect we will not be able to override the prior  In general however as we increase the amount of data we will be able to let the data dominate the estimate eventually overriding the prior completely as we collect infinitely more data.
coverage way j parameter higher reducing	 A prior on the coverage distribution π can be added in a similar way to the updating formula for πd j to force the updated parameter value to give some topics higher probabilities by reducing the probabilities of others.
prior zero generative modeling document generative documents document model	 In the extreme it is also possible to achieve the effect of setting the probability of a topic to zero by using an infinitely strong prior that gives such a topic a zero probability  PLSA is a generative model for modeling the words in a given document but it is not a generative model for documents since it cannot give a probability of a new unseen document it cannot give a distribution over all the possible documents  However we sometimes would like to have a generative model for documents.
generative example estimate documents category use model probability document generative model assigning document generative model gives probability unseen probability topic tied available model coverage ics unseen order new	 However we sometimes would like to have a generative model for documents  For example if we can estimate such a model for documents in each topic category then we would be able to use the model for text categorization by comparing the probability of observing a document from the generative model of each category and assigning the document to the category whose generative model gives the high est probability to the document  The difficulty in giving a new unseen document a probability using PLSA is that the topic coverage parameter in PLSA is tied to an observed document and we do not have available in the model the coverage of top ics in a new unseen document which is needed in order to generate words in a new document.
model topic model probability document category document model probability	 For example if we can estimate such a model for documents in each topic category then we would be able to use the model for text categorization by comparing the probability of observing a document from the generative model of each category and assigning the document to the category whose generative model gives the high est probability to the document.
The difficulty giving document PLSA topic coverage tied new document order generate	 The difficulty in giving a new unseen document a probability using PLSA is that the topic coverage parameter in PLSA is tied to an observed document and we do not have available in the model the coverage of top ics in a new unseen document which is needed in order to generate words in a new document.
Although way solve Bayesian version model.	 Although it is possible to use a heuristic approach to estimate the topic coverage in an unseen document a more principled way to solve the prob lem is to add priors on the parameters of PLSA and make a Bayesian version of the model.
LDA Specifically topic multinomial distribution drawn defines entire space parameters nomial e.	 This has led to the development of the Latent Dirichlet Allocation LDA model  Specifically in LDA the topic coverage distribution a multinomial distribution for each document is assumed to be drawn from a prior Dirichlet distribution which defines a distribution over the entire space of the parameters of a multi nomial distribution i e.
Specifically LDA topic distribution distribution assumed drawn prior defines distribution parameters probabilities topics.	 Specifically in LDA the topic coverage distribution a multinomial distribution for each document is assumed to be drawn from a prior Dirichlet distribution which defines a distribution over the entire space of the parameters of a multi nomial distribution i e  a vector of probabilities of topics.
vector topics.	e  a vector of probabilities of topics.
probabilities	 a vector of probabilities of topics.
word assumed distribution topic coverage distribution distributions assumed parameters	 Similarly all the word distributions representing the latent topics in a collection of text are also assumed to be drawn from another Dirichlet distribution  In PLSA both the topic coverage distribution and the word distributions are assumed to be unknown parameters in the model.
LDA longer parameters Dirichlet prior	 In LDA they are no longer parameters of the model since they are assumed to be drawn from the corresponding Dirichlet prior distributions.
characterize Dirichlet Once parameters behavior fixed entire topics distribution document generating words PLSA.	 Thus LDA only has parameters to characterize these two kinds of Dirichlet distributions  Once these parameters are fixed the behavior of these two Dirichlet distributions would be fixed and thus the behavior of the entire generative model would also be fixed  Once we have sampled all the word distributions for the whole collection which shares these topics and the topic coverage distribution for a document the rest of the process of generating words in the document is exactly the same as in PLSA.
Once parameters behavior generative model word shares topics coverage generating	 Once these parameters are fixed the behavior of these two Dirichlet distributions would be fixed and thus the behavior of the entire generative model would also be fixed  Once we have sampled all the word distributions for the whole collection which shares these topics and the topic coverage distribution for a document the rest of the process of generating words in the document is exactly the same as in PLSA.
Once word distributions collection topics process generating generalization imposing Dirichlet illustrated 17 36 distribution topic coverage parameters	 Once we have sampled all the word distributions for the whole collection which shares these topics and the topic coverage distribution for a document the rest of the process of generating words in the document is exactly the same as in PLSA  The generalization of PLSA to LDA by imposing Dirichlet priors is illustrated in Figure 17 36 where we see that the Dirichlet distribution governing the topic coverage has k parameters α1 .
PLSA imposing Dirichlet illustrated Figure 17 36 Dirichlet .	 The generalization of PLSA to LDA by imposing Dirichlet priors is illustrated in Figure 17 36 where we see that the Dirichlet distribution governing the topic coverage has k parameters α1 .
36 distribution topic k parameters	36 where we see that the Dirichlet distribution governing the topic coverage has k parameters α1   .
	   .
αk M parameters	  αk and the Dirichlet distribution governing the topic word distributions has M parameters β1   .
.	    βM .
count θi according βi count corresponding	  βM   Each αi can be interpreted as the pseudo count of the corresponding topic θi according to our prior while each βi can be interpreted as the pseudo count of the corresponding word wi according to our prior.
The likelihood 37 PLSA LDA PLSA LDA common model component probability document model k distributions	 The likelihood function of LDA is given in Figure 17 37 where we also make a comparison between the likelihood of PLSA and that of LDA  The comparison allows us to see that both PLSA and LDA share the common generative model component to define the probability of observing a word w in document d from a mixture model involving k word distributions θ1 .
comparison likelihood PLSA The comparison allows word w distributions .	37 where we also make a comparison between the likelihood of PLSA and that of LDA  The comparison allows us to see that both PLSA and LDA share the common generative model component to define the probability of observing a word w in document d from a mixture model involving k word distributions θ1 .
allows w distributions	 The comparison allows us to see that both PLSA and LDA share the common generative model component to define the probability of observing a word w in document d from a mixture model involving k word distributions θ1     .
mixture documents multiple document like lihood document entire LDA adding form	 Indeed such a mixture of unigram language models is the common component in most topic models and is key for modeling documents with multiple topics covered in the same document  However the like lihood function for a document and the entire collection C is clearly different with LDA adding the uncertainty of the topic coverage distribution and the uncertainty of all the word distributions in the form of an integral.
k word distribution j document tunately usually happens inference inference.	e  the k word distributions θi characterizing all the topics in a collection and the topic coverage distribution πd j for each document is unfor tunately no longer immediately available to us after we estimate all the parameters  Indeed as usually happens in Bayesian inference to obtain values of such latent variables in LDA we must rely on posterior inference.
θi topics coverage distribution j tunately longer available estimate	 the k word distributions θi characterizing all the topics in a collection and the topic coverage distribution πd j for each document is unfor tunately no longer immediately available to us after we estimate all the parameters.
Indeed happens Bayesian LDA rely posterior	 Indeed as usually happens in Bayesian inference to obtain values of such latent variables in LDA we must rely on posterior inference.
This posterior distribution interesting variables point compute properties depend computation complicated integrals involved	 This gives us a posterior distribution over all the possible values of these interesting variables from which we can then further obtain a point estimate or compute other interesting properties that depend on the distribution  The computation process is once again complicated due to the integrals involved in some of the probabilities.
process complicated involved probabilities Many approach collapsed similar way PLSA.	 The computation process is once again complicated due to the integrals involved in some of the probabilities  Many different inference algorithms have been proposed  A very popular and efficient approach is collapsed Gibbs sampling which works in a very similar way to the EM algorithm of PLSA.
Many proposed.	 Many different inference algorithms have been proposed.
A popular efficient approach sampling similar EM PLSA Empirically work tasks representation j document space distributions similar.	 A very popular and efficient approach is collapsed Gibbs sampling which works in a very similar way to the EM algorithm of PLSA  Empirically LDA and PLSA have been shown to work similarly on various tasks when using such a model to learn a lowdimensional semantic representation of documents by using πd j to represent a document in the kdimensional space  The learned word distributions also tend to look very similar.
Empirically PLSA represent The word tend look 17.	 Empirically LDA and PLSA have been shown to work similarly on various tasks when using such a model to learn a lowdimensional semantic representation of documents by using πd j to represent a document in the kdimensional space  The learned word distributions also tend to look very similar  17.
tend look similar.	 The learned word distributions also tend to look very similar.
Evaluating similar difficulties retrieval evalua	 17 6 Evaluating Topic Analysis Topic analysis evaluation has similar difficulties to information retrieval evalua tion.
true human topic We issue models seemingly straightforward question complicates task measures analysis	 In both cases there is usually not one true answer and evaluation metrics heavily depend on the human issuing judgements  What defines a topic We ad dressed this issue the best we could when defining the models but the challenging nature of such a seemingly straightforward question complicates the eventual eval uation task  Loglikelihood and model perplexity are two common evaluation measures used by language models and they can be applied for topic analysis in the same way.
What topic We issue straightforward question model perplexity meaning heldout presented applied calculating likelihood.	 What defines a topic We ad dressed this issue the best we could when defining the models but the challenging nature of such a seemingly straightforward question complicates the eventual eval uation task  Loglikelihood and model perplexity are two common evaluation measures used by language models and they can be applied for topic analysis in the same way  Both are predictive measures meaning that heldout data is presented to the model and the model is applied to this new information calculating its likelihood.
common evaluation measures language way predictive presented calculating	 Loglikelihood and model perplexity are two common evaluation measures used by language models and they can be applied for topic analysis in the same way  Both are predictive measures meaning that heldout data is presented to the model and the model is applied to this new information calculating its likelihood.
new likelihood al.	 If the model generalizes well to this new data by assigning it a high likelihood or low perplexity then the model is assumed to be sufficient  In Chapter 13 we mentioned Chang et al.
Chapter 13 Chang et al 2009.	 In Chapter 13 we mentioned Chang et al  2009.
Human judges responded intrusion measure topicword distribu	 2009  Human judges responded to intrusion detection scenarios to measure the coherency of the topicword distribu tions.
test previously setup given topics θ2	 This test can measure the coherency of top ics discovered from documents through the previously used intrusion test  The setup is as follows given a document d from the collection the top three topics are chosen call these most likely topics θ1 θ2 and θ3.
The setup chosen topics θ2	 The setup is as follows given a document d from the collection the top three topics are chosen call these most likely topics θ1 θ2 and θ3  An additional low probability topic θu is also selected and displayed along with the top three topics.
additional selected displayed short snippet highprobability words judge determine θ θu.	 An additional low probability topic θu is also selected and displayed along with the top three topics  The title and a short snippet is shown from d along with the top few highprobability words from each topic  The human judge must determine which θ is θu.
short snippet d topic human θ θu.	 The title and a short snippet is shown from d along with the top few highprobability words from each topic  The human judge must determine which θ is θu.
human θ	 The human judge must determine which θ is θu.
As word judge easy document title snippet discern θu adequate d Of repeated different	 As with the word intrusion test the human judge should have a fairly easy task if the top three topics make sense together and with the document title and snippet  If it’s hard to discern θu then the top topics must not be an adequate representation of d  Of course this process is repeated for many different documents in the collection.
course documents	 Of course this process is repeated for many different documents in the collection.
al 2009	 Directly from Chang et al  2009  .
	 2009 .
	 we demonstrated that traditional metrics do not capture whether topics are coherent or not.
demonstrated traditional metrics Traditional measures topic data erative	we demonstrated that traditional metrics do not capture whether topics are coherent or not  Traditional metrics are indeed negatively correlated with the measures of topic quality  “Traditional metrics” refers to loglikelihood of heldout data in the case of gen erative models.
Traditional negatively measures quality.	 Traditional metrics are indeed negatively correlated with the measures of topic quality.
refers heldout data case gen models.	 “Traditional metrics” refers to loglikelihood of heldout data in the case of gen erative models.
This misalignment pressing research models task effective	 This misalignment of results is certainly a pressing issue though most recent research still relies on the traditional measures to evaluate new models  Downstream task improvement is perhaps the most effective and transparent evaluation metric.
effective transparent evaluation	 Downstream task improvement is perhaps the most effective and transparent evaluation metric.
loglikelihood heldout data coherency concern classification model interpretability humandistinguishable.	 In such a case loglikelihood of heldout data and even topic coherency is not a concern if the classification accuracy improves—although model interpretability may be compromised if topics are not humandistinguishable.
Summary summary topic We represent topic deficiency	 17 7 Summary of Topic Models In summary we introduced techniques for topic analysis in this chapter  We started with the simple idea of using one term to represent a topic and discussed the deficiency of such an approach.
Summary Topic Models In summary analysis chapter started simple term represent topic	7 Summary of Topic Models In summary we introduced techniques for topic analysis in this chapter  We started with the simple idea of using one term to represent a topic and discussed the deficiency of such an approach.
distribution unigram PLSA mixture model language k topics background language discover discriminative background language help common computed algorithm estimate	 We then introduced the idea of representing a topic with a word distribution or a unigram language model and introduced the PLSA model which is a mixture model with k unigram language models representing k topics  We also added a prespecified background language model to help discover discriminative topics because this background language model can help attract the common terms  We used the maximum likelihood estimator computed using the EM algorithm to estimate the parameters of PLSA.
added model discover discriminative language model common We maximum likelihood algorithm PLSA The values enabled discover k distributions topic proportion topic	 We also added a prespecified background language model to help discover discriminative topics because this background language model can help attract the common terms  We used the maximum likelihood estimator computed using the EM algorithm to estimate the parameters of PLSA  The estimated parameter values enabled us to discover two things one is k word distributions with each one representing a topic and the other is the proportion of each topic in each document.
We likelihood estimate parameters PLSA The estimated values things distributions topic	 We used the maximum likelihood estimator computed using the EM algorithm to estimate the parameters of PLSA  The estimated parameter values enabled us to discover two things one is k word distributions with each one representing a topic and the other is the proportion of each topic in each document.
Notes Reading 385 ple aggregate documents particular particular period This allow temporal topics We aggregate topics covered particular reveal author.	 For exam Bibliographic Notes and Further Reading 385 ple we can aggregate the documents in a particular time period to assess the coverage of a particular topic in the time period  This would allow us to gener ate a temporal trend of topics  We can also aggregate topics covered in documents associated with a particular author to reveal the expertise areas of the author.
This gener topics topics reveal expertise Fur cluster terms cluster	 This would allow us to gener ate a temporal trend of topics  We can also aggregate topics covered in documents associated with a particular author to reveal the expertise areas of the author  Fur thermore we can also cluster terms and cluster documents.
author areas author thermore cluster	 We can also aggregate topics covered in documents associated with a particular author to reveal the expertise areas of the author  Fur thermore we can also cluster terms and cluster documents.
Fur cluster documents.	 Fur thermore we can also cluster terms and cluster documents.
In fact word distribution eas obtained selecting words generate PLSA Documents way document covered document.	 In fact each topic word distribution can be regarded as a cluster for example the cluster can be eas ily obtained by selecting the top N words with the highest probabilities  So we can generate term clusters easily based on the output from PLSA  Documents can also be clustered in the same way we can assign a document to the topic cluster that’s covered most in the document.
clusters easily	 So we can generate term clusters easily based on the output from PLSA.
Documents way assign topic covered document.	 Documents can also be clustered in the same way we can assign a document to the topic cluster that’s covered most in the document.
Recall document	 Recall that πd j indicates to what extent each topic θj is covered in document d.
document πd j Another use results way representing lowdimensional dimension corresponds topic.	 We can thus assign the document to the top ical cluster that has the highest πd j   Another use of the results from PLSA is to treat the inferred topic coverage distribution in a document as an alternative way of representing the document in a lowdimensional semantic space where each dimension corresponds to a topic.
use treat inferred topic lowdimensional topic ofwords representation enhance inexact matching words generally beneficial e.	 Another use of the results from PLSA is to treat the inferred topic coverage distribution in a document as an alternative way of representing the document in a lowdimensional semantic space where each dimension corresponds to a topic  Such a representation can supplement the bag ofwords representation to enhance inexact matching of words in the same topic which can generally be beneficial e.
Such representation bag enhance inexact generally	 Such a representation can supplement the bag ofwords representation to enhance inexact matching of words in the same topic which can generally be beneficial e.
	 for information retrieval text clustering and text categorization.
variant allocation extends adding priors documenttopic small topics dominate document usually document topics true mixture k adding word topic distribution words we’ve discussed previously.	 Finally a variant of PLSA called latent Dirichlet allocation LDA extends PLSA by adding priors to the documenttopic distributions and topicword distributions  These priors can force a small number of topics to dominate in each document which makes sense because usually a document is only about one or two topics as opposed to a true mixture of all k topics  Secondly adding these priors can give us sparse word distributions in each topic as well which mimics the Zipfian distribution of words we’ve discussed previously.
number topics dominate sense usually mixture k topics.	 These priors can force a small number of topics to dominate in each document which makes sense because usually a document is only about one or two topics as opposed to a true mixture of all k topics.
Secondly adding priors sparse distributions topic distribution discussed	 Secondly adding these priors can give us sparse word distributions in each topic as well which mimics the Zipfian distribution of words we’ve discussed previously.
Bibliographic Further mentioned original paper Hofmann LDA 2003.	 Bibliographic Notes and Further Reading We’ve mentioned the original PLSA paper Hofmann 1999 and its successor LDA Blei et al  2003.
	 2003.
compares inference models For evaluation referenced al 2009 showed convenient math ematical correlated measures.	 2009 compares various inference methods for topic models and concludes that they are all very similar  For evaluation we’ve referenced Chang et al  2009 in this chapter and it showed that convenient math ematical measures such as loglikelihood are not correlated with human measures.
2009 showed ematical correlated human measures For models EM scope machine learning.	 2009 in this chapter and it showed that convenient math ematical measures such as loglikelihood are not correlated with human measures  For books Koller and Friedman 2009 is a large and detailed introduction to prob abilistic graphical models  Bishop 2006 covers graphical models mixture models EM and inference in the larger scope of machine learning.
Koller large detailed 2006 models scope learning	 For books Koller and Friedman 2009 is a large and detailed introduction to prob abilistic graphical models  Bishop 2006 covers graphical models mixture models EM and inference in the larger scope of machine learning  Steyvers and Griffiths 2007 is a short summary of topic models alone.
Bishop inference larger Steyvers Griffiths models mention LDA	 Bishop 2006 covers graphical models mixture models EM and inference in the larger scope of machine learning  Steyvers and Griffiths 2007 is a short summary of topic models alone  In the exercises we mention su pervised LDA McAuliffe and Blei 2008.
Griffiths pervised 2008.	 Steyvers and Griffiths 2007 is a short summary of topic models alone  In the exercises we mention su pervised LDA McAuliffe and Blei 2008.
su Blei LDA	 In the exercises we mention su pervised LDA McAuliffe and Blei 2008  There are many other variants of LDA such as MedLDA Zhu et al.
MedLDA Zhu et Ramage incorporates	 There are many other variants of LDA such as MedLDA Zhu et al  2009 another supervised model which attempts to maxi mize the distance between classes and LabeledLDA Ramage et al  2009 which incorporates metadata tags.
classes LabeledLDA et al.	 2009 another supervised model which attempts to maxi mize the distance between classes and LabeledLDA Ramage et al.
2009 incorporates tags Mining Sentiment Analysis In chapter going mining different kind generated talk sentiment analysis.	 2009 which incorporates metadata tags  18Opinion Mining and Sentiment Analysis In this chapter we’re going to talk about mining a different kind of knowledge namely knowledge about the observer or humans that have generated the text data  In particular we’re going to talk about opinion mining and sentiment analysis.
chapter going knowledge data talk opinion	 18Opinion Mining and Sentiment Analysis In this chapter we’re going to talk about mining a different kind of knowledge namely knowledge about the observer or humans that have generated the text data  In particular we’re going to talk about opinion mining and sentiment analysis.
As humans subjective	 As we discussed earlier text data can be regarded as data generated from humans as subjective sensors.
video report happening world generate	 In contrast we have other devices such as video recorders that can report what’s happening in the real world to generate data.
main text data data rich opinions content tends subjective it’s humans	 The main difference between text data and other data like video data is that it has rich opinions and the content tends to be subjective because it’s generated from humans as shown in Figure 18.
data data offers great opinions	1  This is actually a unique advantage of text data as compared to other data because this offers us a great opportunity to understand the observers—we can mine text data to understand their opinions  Let’s start with the concept of an opinion.
start concept It’s formally define statement person believes 18 2.	 Let’s start with the concept of an opinion  It’s not that easy to formally define an opinion but for the most part we would define an opinion as a subjective statement describing what a person believes or thinks about something as shown in Figure 18 2.
easy define subjective person believes shown Figure	 It’s not that easy to formally define an opinion but for the most part we would define an opinion as a subjective statement describing what a person believes or thinks about something as shown in Figure 18 2.
2 Let’s key word figure contrast factual This key differentiating factor tends easy reflects person	2  Let’s first look at the key word subjective in the figure this is in contrast with an objective statement or factual statement  This is a key differentiating factor from opinions which tends to be not easy to prove wrong or right because it reflects what the person thinks about something.
key word objective differentiating factor tends easy prove wrong person In proved	 Let’s first look at the key word subjective in the figure this is in contrast with an objective statement or factual statement  This is a key differentiating factor from opinions which tends to be not easy to prove wrong or right because it reflects what the person thinks about something  In contrast an objective statement can usually be proved wrong or right.
factor reflects thinks In statement proved right.	 This is a key differentiating factor from opinions which tends to be not easy to prove wrong or right because it reflects what the person thinks about something  In contrast an objective statement can usually be proved wrong or right.
objective usually wrong For example screen battery	 In contrast an objective statement can usually be proved wrong or right  For example you might say a computer has a screen and a battery  Clearly that’s something you can check either it has a battery or doesn’t.
contrast sentence “This “This laptop nice screen.	 Clearly that’s something you can check either it has a battery or doesn’t  In contrast with this think about a sentence such as “This laptop has the best battery life” or “This laptop has a nice screen.
it’s prove right word person opinion When it’s	” These statements are more subjective and it’s very hard to prove whether they are wrong or right  The word person indicates an opinion holder  When we talk about an opinion it’s about an opinion held by someone.
The word opinion holder talk opinion it’s Of opinion	 The word person indicates an opinion holder  When we talk about an opinion it’s about an opinion held by someone  Of course an opinion will depend on culture background and the context in general.
talk opinion Of opinion culture	 When we talk about an opinion it’s about an opinion held by someone  Of course an opinion will depend on culture background and the context in general.
course opinion depend culture thought process shows multiple include	 Of course an opinion will depend on culture background and the context in general  This thought process shows that there are multiple elements that we need to include in order to characterize opinions.
elements need The “What’s basic opinion First specify holder	 This thought process shows that there are multiple elements that we need to include in order to characterize opinions  The next logical question is “What’s a basic opinion representation” It should include at least three elements  First it has to specify who the opinion holder is.
The logical It opinion holder	 The next logical question is “What’s a basic opinion representation” It should include at least three elements  First it has to specify who the opinion holder is.
opinion is.	 First it has to specify who the opinion holder is.
Second specify target about.	 Second it must also specify the target or what the opinion is about.
Third want content	 Third of course we want the opinion content  If you can identify these we get a basic understanding of opinions.
context opinion understand e.	 That means we also want to understand for example the context of the opinion and in what situation the opinion was expressed  We would also like to understand the opinion sentiment i e.
negative feeling.	e  whether it is a positive or negative feeling.
feeling Let’s simple product	 whether it is a positive or negative feeling  Let’s take a simple example of a product review.
case	 In this case we already know the opinion holder and the target.
review	 When the review is posted we can usually extract this information.
Additional usergenerated opinions article.	 Additional understanding by analyzing the usergenerated text adds value to mining the opinions  Figure 18 3 shows a sentence extracted from a news article.
In case holder target don’t know information harder As identify holder governor	 In this case we have an implicit holder and an implicit target since we don’t automatically know this information  This makes the task harder  As humans we can identify the opinion holder as the governor of Connecticut.
humans holder Connecticut hurricane	 As humans we can identify the opinion holder as the governor of Connecticut  We can also identify the target Hurricane Sandy but there is also another target mentioned which is the hurricane of 1938.
Hurricane Sandy target mentioned	 We can also identify the target Hurricane Sandy but there is also another target mentioned which is the hurricane of 1938.
like identify context New England.	 What’s the opinion There is negative sentiment indicated by words like bad and worst  We can also identify context which is New England.
identify context elements extracted difficult analysis opinions product	 We can also identify context which is New England  All these elements must be extracted by using NLP techniques  Analyzing the sentiment in news is still quite difficult it’s more difficult than the analysis of opinions in product reviews.
Analyzing sentiment difficult difficult analysis	 Analyzing the sentiment in news is still quite difficult it’s more difficult than the analysis of opinions in product reviews.
There	 There are also some other interesting variations.
The holder people.	 The holder could be an individual or it could be group of people.
committee Opinion targets vary person particular particular policy	 Sometimes the opinion is from a committee or from a whole country of people  Opinion targets will vary greatly as well they can be about one entity a particular person a particular product a particular policy and so on.
battery It else’s opinion person variation different forms.	 For example it could just be about the battery of a smartphone  It could even be someone else’s opinion and one person might comment on another person’s opinion  Clearly there is much variation here that will cause the problem to take different forms.
It person Clearly cause problem different forms Opinion content identify opin ion onephrase	 It could even be someone else’s opinion and one person might comment on another person’s opinion  Clearly there is much variation here that will cause the problem to take different forms  Opinion content can also vary on the surface we can identify a onesentence opin ion or a onephrase opinion.
different forms surface identify opin opinion.	 Clearly there is much variation here that will cause the problem to take different forms  Opinion content can also vary on the surface we can identify a onesentence opin ion or a onephrase opinion.
We text express opinion Furthermore emotion opinion We	 We can also have longer text to express an opinion such as a whole news article  Furthermore we can identify the variation in the sen timent or emotion of the opinion holder  We can distinguish positive vs.
identify sen emotion We	 Furthermore we can identify the variation in the sen timent or emotion of the opinion holder  We can distinguish positive vs  negative or neutral sentiment.
positive vs negative	 We can distinguish positive vs  negative or neutral sentiment.
neutral	 negative or neutral sentiment  Finally the opinion context can also vary.
Finally vary We context time	 Finally the opinion context can also vary  We can have a simple context like a different time or different locations.
background opinion expressed it’s expressed context perspective we’re interested opinions extracted text data.	 There could be also complex contexts such as some background of a topic being discussed  When an opinion is expressed in a particular discourse context it has to be interpreted in different ways than when it’s expressed in another context  From a computational perspective we’re mostly interested in what opinions can be extracted from text data.
From computational opinions don’t like clearly speaker phone.	 From a computational perspective we’re mostly interested in what opinions can be extracted from text data  One computational objective might be to determine the target of an opinion  For example “I don’t like this phone at all” is clearly an opinion by the speaker about a phone.
objective opinion For example don’t like phone all” opinion speaker phone In	 One computational objective might be to determine the target of an opinion  For example “I don’t like this phone at all” is clearly an opinion by the speaker about a phone  In contrast the text might also report opinions about others.
contrast report opinions	 In contrast the text might also report opinions about others.
observation person’s opinion opinion “I believe loves ” opinion doesn’t person loves painting.	 One could make an observation about another person’s opinion and report this opinion  For example “I believe he loves the painting ” That opinion is really expressed from another person it doesn’t mean this person loves that painting.
For loves That opinion expressed doesn’t	 For example “I believe he loves the painting ” That opinion is really expressed from another person it doesn’t mean this person loves that painting  Clearly these two kinds of opinions need to be analyzed in different ways.
Clearly opinions	” That opinion is really expressed from another person it doesn’t mean this person loves that painting  Clearly these two kinds of opinions need to be analyzed in different ways.
opinions Sometimes reviewer mention opinions friend Another indirect inferred obtained expressed necessarily opinion.	 Clearly these two kinds of opinions need to be analyzed in different ways  Sometimes a reviewer might mention opinions of his or her friend  Another complication is that there may be indirect opinions or inferred opinions that can be obtained by making inferences about what is expressed in the text that might not necessarily look like opinion.
	 Sometimes a reviewer might mention opinions of his or her friend.
indirect opinions making inferences look For “This phone battery hour.	 Another complication is that there may be indirect opinions or inferred opinions that can be obtained by making inferences about what is expressed in the text that might not necessarily look like opinion  For example one statement might be “This phone ran out of battery in only one hour.
For example hour factual it’s	 For example one statement might be “This phone ran out of battery in only one hour ” Now this is in a way a factual statement because it’s either true or false.
However negative opinions opinion battery life These need pay	 However one can also infer some negative opinions about the quality of the battery of this phone or the opinion about the battery life  These are interesting variations that we need to pay attention to when we extract opinions.
The opinion taking contextualized input gen erate set 18 identify	 The task of opinion mining can be defined as taking contextualized input to gen erate a set of opinion representations as shown in Figure 18 4  Each representation should identify the opinion holder target content and context.
representation target	 Each representation should identify the opinion holder target content and context.
Ideally opinion comment understand opinion.	 Ideally we can also infer opinion sentiment from the comment and the context to better understand the opinion.
elements saw example case product opinion identified.	 Often some elements of the representation are already known  We just saw an example in the case of a product review where the opinion holder and the opinion target are often explicitly identified.
We people’s order decision product buy service use others’ opinions for.	 We often look at other people’s opinions by reading their reviews in order to make a decision such as which product to buy or which service to use  We also would be interested in others’ opinions when we decide whom to vote for.
The understand For help preferences.	 The second application is to understand people  For example it could help understand human preferences.
preferences.	 For example it could help understand human preferences.
We product engine recommender in.	 We could optimize a product search engine or optimize a recommender system if we know what people are interested in.
It help kind people types The opinions humans assess	 It also can help with advertising we can have targeted advertising if we know what kind of people tend to like which types of products  The third kind of application is aggregating opinions from many humans at once to assess a more general opinion.
aggregating opinions	 The third kind of application is aggregating opinions from many humans at once to assess a more general opinion.
manufacturers want know products advantages winning features competitors’ Market research	 This would be very useful for business intelligence where manufacturers want to know where their products have advantages or disadvantages  What are the winning features of their products or competitors’ products Market research has to do with understanding consumer opinions.
social science research benefit group opinions If media study behavior people social networks.	 Datadriven social science research can benefit from this because they can do text mining to understand group opinions  If we aggregate opinions from social media we can study the behavior of people on social networks.
1 Classification assume task sentiment	 18 1 Sentiment Classification If we assume that most of the elements in an opinion representation are already known then our only task may be sentiment classification.
That know opinion target The decide sentiment	 That is suppose we know the opinion holder and the opinion target and also know the content and the context of the opinion  The only component remaining is to decide the opinion sentiment of the review.
review.	 The only component remaining is to decide the opinion sentiment of the review.
Sentiment classification opinionated label tag defined polarity positive	 Sentiment classification can be defined more specifically as follows the input is an opinionated text object and the output is typically a sentiment label or a sentiment tag that can be defined in two ways  One is polarity analysis where we have categories such as positive negative or neutral.
One categories positive	 One is polarity analysis where we have categories such as positive negative or neutral.
In emotion analysis ways categories.	 In emotion analysis there are also different ways to design the categories.
sad fearful essentially classification categorization task	 Some typical categories are happy sad fearful angry surprised and disgusted  Thus the task is essentially a classification task or categorization task as we’ve seen before.
If simply apply default techniques sentiment classification improvement regular categorization techniques needs sophisticated features appropriate tagging.	 If we simply apply default classification techniques the accuracy may not be good since sentiment classification requires some improvement over regular text categorization techniques  In particular it needs two kind of improvements  One is to use more sophisticated features that may be more appropriate for sentiment tagging.
In needs One use sophisticated features tagging.	 In particular it needs two kind of improvements  One is to use more sophisticated features that may be more appropriate for sentiment tagging.
One features sentiment tagging especially polarity clear order choices.	 One is to use more sophisticated features that may be more appropriate for sentiment tagging  The other is to consider the order of these categories especially in polarity analysis since there is a clear order among the choices.
consider polarity analysis clear regression range.	 The other is to consider the order of these categories especially in polarity analysis since there is a clear order among the choices  For example we could use ordinal regression to predict a value within some range.
example regression	 For example we could use ordinal regression to predict a value within some range  We’ll discuss this idea in the next section.
text egorization text e.	 For now let’s talk about some features that are often very useful for text cat egorization and text mining in general but also especially needed for sentiment analysis  The simplest feature is character ngrams i e.
simplest feature ngrams i.	 The simplest feature is character ngrams i.
e n characters treated unit This general text method	e  sequences of n adjacent characters treated as a unit  This is a very general and robust way to represent text data since we can use this method for any language.
n This data method language spelling misspell representa tion match text correctly.	 sequences of n adjacent characters treated as a unit  This is a very general and robust way to represent text data since we can use this method for any language  This is also robust to spelling errors or recognition errors if you misspell a word by one character this representa tion still allows you to match the word as well as when it occurs in the text correctly.
This general robust way text data method language robust errors recognition errors misspell match correctly.	 This is a very general and robust way to represent text data since we can use this method for any language  This is also robust to spelling errors or recognition errors if you misspell a word by one character this representa tion still allows you to match the word as well as when it occurs in the text correctly.
This robust errors misspell representa tion word correctly.	 This is also robust to spelling errors or recognition errors if you misspell a word by one character this representa tion still allows you to match the word as well as when it occurs in the text correctly.
Of representation Next We different nvalues.	 Of course such a representation would not be as discriminating as words  Next we have word ngrams a sequence of words as opposed to characters  We can have a mix of these with different nvalues.
sequence words We different	 Next we have word ngrams a sequence of words as opposed to characters  We can have a mix of these with different nvalues.
processing tasks words information humans communication unigram words task sentiment	 Unigrams are often very effective for text processing tasks it’s mostly because words are the basic unit of information used by humans for communication  However unigram words may not be suffi cient for a task like sentiment analysis.
unigram cient For sentence “It’s good” “It’s good else.	 However unigram words may not be suffi cient for a task like sentiment analysis  For example we might see a sentence “It’s not good” or “It’s not as good as something else.
suggest sample Clearly If appear making representation	” In such a case if we just take the feature good that would suggest a positive text sample  Clearly this would not be accurate  If we take a bigram n  2 representation the bigram not good would appear making our representation more accurate.
	 Clearly this would not be accurate.
bigram representation good	 If we take a bigram n  2 representation the bigram not good would appear making our representation more accurate.
gen discriminative.	 Thus longer ngrams are gen erally more discriminative.
However ngrams cause create unique associate highly particular example phrase appears training document gram associated positive sentiment.	 However long ngrams may cause overfitting because they create very unique features that machine learning programs associate as be ing highly correlated with a particular class label when in reality they are not  For example if a 7gram phrase appears only in a positive training document that 7 gram would always be associated with positive sentiment.
example document gram	 For example if a 7gram phrase appears only in a positive training document that 7 gram would always be associated with positive sentiment.
consider ngrams tags bigram	 We can consider ngrams of partofspeech tags  A bigram feature could be an adjective followed by a noun.
A mix ngrams POS	 A bigram feature could be an adjective followed by a noun  We can mix ngrams of words and ngrams of POS tags.
ngrams great feature—a hybrid useful sentiment analysis Next	 We can mix ngrams of words and ngrams of POS tags  For example the word great might be followed by a noun and this could become a feature—a hybrid feature—that could be useful for sentiment analysis  Next we can have word classes.
For word great noun hybrid feature—that useful sentiment	 For example the word great might be followed by a noun and this could become a feature—a hybrid feature—that could be useful for sentiment analysis  Next we can have word classes.
classes.	 Next we can have word classes.
We we’ve associations Chapter 13 paradigmatically words syntagmatically features	 We also can learn word clusters since we’ve talked about min ing associations of words in Chapter 13  We can have clusters of paradigmatically related words or syntagmatically related words and these clusters can be features to supplement the base word representation.
clusters syntagmatically words syntax represents word necessarily context.	 We can have clusters of paradigmatically related words or syntagmatically related words and these clusters can be features to supplement the base word representation  Furthermore we can have a frequent pattern syntax which represents a frequent word set these are words that do not necessarily occur next to each other but often occur in the same context.
This parse treebased g frequent subtrees.	 This is a problem in general and the same is true for parse treebased features e g  frequent subtrees.
	g.
discriminating likely general pattern discovery useful search large possible processing help derive enrich text representations.	 frequent subtrees  Those are even more discriminating but they’re also more likely to cause overfitting  In general pattern discovery algorithms are very useful for feature construction because they allow us to search a large space of possible features that are more complex than words and natural language processing is very important to help us derive complex features that can enrich text representations.
they’re likely overfitting In allow large space complex words language important text representations.	 Those are even more discriminating but they’re also more likely to cause overfitting  In general pattern discovery algorithms are very useful for feature construction because they allow us to search a large space of possible features that are more complex than words and natural language processing is very important to help us derive complex features that can enrich text representations.
First want problem.	 First we want to use do main knowledge that is a specialized understanding of the problem.
design features machine work on.	 With this we can design a basic feature space with many possible features for the machine learning program to work on.
Machine select	 Machine learning methods can be applied to select the most effective features or to even construct new features.
lead revision feature set These	 This can lead into feature validation that will cause a revision in the feature set  These steps are then iterated until a desired accuracy is achieved.
steps desired conclusion main designing features difficult.	 These steps are then iterated until a desired accuracy is achieved  In conclusion a main challenge in designing features is to optimize a tradeoff between exhaustivity and specificity  This tradeoff turns out to be very difficult.
challenge features optimize exhaustivity specificity tradeoff difficult.	 In conclusion a main challenge in designing features is to optimize a tradeoff between exhaustivity and specificity  This tradeoff turns out to be very difficult.
tradeoff difficult.	 This tradeoff turns out to be very difficult.
means features coverage documents In sense requires tend frequent.	 Exhaustivity means we want the features to have high coverage on many documents  In that sense we want the features to be frequent  Specificity requires the features to be discriminative so naturally the features tend to be less frequent.
Specificity discriminative naturally features	 Specificity requires the features to be discriminative so naturally the features tend to be less frequent.
sentiment engineering	 Particularly in our case of sentiment analysis feature engineering is a critical task  18.
positive negative document input want generate output rating it’s rating categorization problem finding correct categories.	 positive to negative with other labels in between  We have an opinionated text document d as input and we want to generate as output a rating in the range of 1 through k  Since it’s a discrete rating this could be treated as a categorization problem finding which is the correct of k categories.
it’s discrete problem finding correct k	 Since it’s a discrete rating this could be treated as a categorization problem finding which is the correct of k categories.
solution order Intuitively rating	 Unfortunately such a solution would not consider the order and dependency of the categories  Intuitively the features that can distinguish rating 2 from 1 may be similar to those that can distinguish k from k − 1.
Intuitively features distinguish 1 For words generally suggest higher train	 Intuitively the features that can distinguish rating 2 from 1 may be similar to those that can distinguish k from k − 1  For example positive words generally suggest a higher rating  When we train a categorization problem by treating these categories as independent we would not capture this.
example positive suggest rating train categorization independent capture One addresses issue ordinal logistic	 For example positive words generally suggest a higher rating  When we train a categorization problem by treating these categories as independent we would not capture this  One approach that addresses this issue is ordinal logistic regression.
train categories capture	 When we train a categorization problem by treating these categories as independent we would not capture this.
use regression binary categorization problem.	 Let’s first think about how we use logistic regression for binary sentiment which is a binary categorization problem.
The features log features feature value xi real number document binary variable 0 1 means means X negative.	 The predictors features are represented as X and we can output a score based on the log probability ratio There are M features all together and each feature value xi is a real number  As usual these features can be a representation of a text document  X is a binary response variable 0 or 1 where 1 means X is positive and 0 means X is negative.
category logistic regression.	 Of course this is then a standard two category categorization problem and we can apply logistic regression.
means rating	 So when Yj  1 it means the rating is j or above.
When it’s 0	 When it’s 0 that means the rating is lower than j .
predict rating k distinguish k versus others.	 If we want to predict a rating in the range of 1 to k we first have one classifier to distinguish k versus the others.
Then dis tinguish k − 1	 Then we’re going to have another classifier to dis tinguish k − 1 from the rest.
end need altogether gives − With needs different set parameters ing	 In the end we need a classifier to distinguish between 2 and 1 which altogether gives us k − 1 classifiers  With this modification each classifier needs a different set of parameters yield ing many more parameters overall.
modification classifier yield parameters	 With this modification each classifier needs a different set of parameters yield ing many more parameters overall.
index classifiers index j corresponds rating level This consistent logistic regression.	 We will index the logistic regression classifiers by an index j  which corresponds to a rating level  This is to make the notation more consistent with what we show in the ordinal logistic regression.
regular regression classifiers set approach predict ratings shown Figure	 So we now have k − 1 regular logistic regression classifiers each with its own set of parameters  With this approach we can now predict ratings as shown in Figure 18.
After logistic classifiers new instance invoke sequentially	6  After we have separately trained these k − 1 logistic regression classifiers we can take a new instance and then invoke classifiers sequentially to make the decision.
rating	 This classifier will tell us whether this object should have a rating of k or not.
according regression classifier 0.	 If the probability according to this logistic regression classifier is larger than 0.
tells k	5 we need to invoke the next classifier which tells us whether it’s at least k − 1.
We invoke hit it’s 1 Unfortunately strategy problem	 We continue to invoke the classifiers until we hit the end when we need to decide whether it’s 2 or 1  Unfortunately such a strategy is not an optimal way of solving this problem  Specifically there are two issues with this approach.
strategy optimal Specifically approach simply parameters.	 Unfortunately such a strategy is not an optimal way of solving this problem  Specifically there are two issues with this approach  The first problem is that there are simply too many parameters.
Specifically issues	 Specifically there are two issues with this approach.
For M 1 k 1 total number k − .	 The first problem is that there are simply too many parameters  For each classifier we have M  1 parameters with k − 1 classifiers all together so the total number of parameters is k − 1 .
classifier 1 k 1 total parameters k	 For each classifier we have M  1 parameters with k − 1 classifiers all together so the total number of parameters is k − 1   M  1.
M classifier general need training help parameters model The k	 M  1  When a classifier has many parameters we would in general need more training data to help us decide the optimal parameters of such a complex model  The second problem is that these k − 1 classifiers are not really independent.
training help decide complex second problem We know words positive higher classifiers	 When a classifier has many parameters we would in general need more training data to help us decide the optimal parameters of such a complex model  The second problem is that these k − 1 classifiers are not really independent  We know that in general words that are positive would make the rating higher for any of these classifiers so we should be able to take advantage of this fact.
words classifiers fact.	 We know that in general words that are positive would make the rating higher for any of these classifiers so we should be able to take advantage of this fact.
This regression − 1 regression shown Figure	 This is precisely the idea of ordinal logistic regression which is an improvement over the k − 1 independent logistic regression classifiers as shown in Figure 18 7.
	7.
tie β parameters assume values − classifiers.	 The improvement is to tie the β parameters together that means we are going to assume the β values are the same for all the k − 1 classifiers.
This encodes intuition words likely In One number parameters significantly.	 This encodes our intuition that positive words in general would make a higher rating more likely  In fact this would allow us to have two benefits  One is to reduce the number of parameters significantly.
One parameters	 One is to reduce the number of parameters significantly.
allow share training data classifiers parameters	 The other is to allow us to share the training data amongst all classifiers since the parameters are the same.
resulting similar we’ve β index corresponds levels However distinct	 The resulting formula would look very similar to what we’ve seen before only now the β parameter has just one index that corresponds to a single feature it no longer has the other indices that correspond to rating levels  However each classifier still has a distinct predicted rating value.
So rest parameters βi’s	 So αj is different since it depends on j  but the rest of the parameters the βi’s are the same  We now have M  k − 1 parameters.
We M − parameters idea parameters end Figure	 We now have M  k − 1 parameters  It turns out that with this idea of tying all the parameters we end up having a similar way to make decisions as shown in Figure 18.
It turns tying parameters end way Figure 18.	 It turns out that with this idea of tying all the parameters we end up having a similar way to make decisions as shown in Figure 18.
More probabilities equivalent score equal	8  More specifically the criteria whether the predictor probabilities are at least 0 5 or above is equivalent to whether the score of the object is larger than or equal to αk.
predictor probabilities	 More specifically the criteria whether the predictor probabilities are at least 0.
equivalent object larger equal αk The β	5 or above is equivalent to whether the score of the object is larger than or equal to αk  The scoring function is just taking a linear combination of all the features with the β values.
The features	 The scoring function is just taking a linear combination of all the features with the β values.
simply scoring function seeing we’re going object trained trained values in.	 This means now we can simply make a rating decision by looking at the value of this scoring function and seeing which bracket it falls into  In this approach we’re going to score the object by using the features and trained parameter values  This score will then be compared with a set of trained α values to see which range the score is in.
In score parameter	 In this approach we’re going to score the object by using the features and trained parameter values.
3 we’re going continue discussing opinion mining ment In particular Latent Aspect Analysis perform detailed overall ratings.	 18 3 Latent Aspect Rating Analysis In this section we’re going to continue discussing opinion mining and senti ment analysis  In particular we’re going to introduce Latent Aspect Rating Analysis LARA which allows us to perform detailed analysis of reviews with overall ratings.
In discussing opinion ment analysis we’re going introduce LARA overall Figure	3 Latent Aspect Rating Analysis In this section we’re going to continue discussing opinion mining and senti ment analysis  In particular we’re going to introduce Latent Aspect Rating Analysis LARA which allows us to perform detailed analysis of reviews with overall ratings  Figure 18.
Figure 9 hotel Both stars.	 Figure 18 9 shows two hotel reviews  Both reviewers are given five stars.
reviews.	9 shows two hotel reviews.
If score hotel location service	 If you just look at the overall score it’s not very clear whether the hotel is good for its location or for its service  It’s also unclear specifically why a reviewer liked this hotel.
specifically What want overall ratings different aspects room location service decompose ratings ratings obtain detailed reviewers’ opinions	 It’s also unclear specifically why a reviewer liked this hotel  What we want to do is to decompose this overall rating into ratings on different aspects such as value room location and service  If we can decompose the overall ratings into ratings on these different aspects we can obtain a much more detailed understanding of the reviewers’ opinions about the hotel.
This rank hotels value Using knowledge understand perspective.	 This would also allow us to rank hotels along different dimensions such as value or room quality  Using this knowledge we can better understand how the reviewers view this hotel from their own perspective.
view perspective.	 Using this knowledge we can better understand how the reviewers view this hotel from their own perspective.
Not aspect infer aspect weights.	 Not only do we want to infer these aspect ratings we also want to infer the aspect weights.
That opposed service.	 That is some reviewers may care more about value as opposed to the service.
aspects.	 Clearly different users place priority on different rating aspects.
Despite expensive If value hotel fivestar review likely price ratings different aspects accurately weights.	 Despite this it might still be very expensive  If a reviewer really cares about the value of a hotel then the fivestar review would most likely mean a competitive price  In order to interpret the ratings on different aspects accurately we also need to know these aspect weights.
reviewer cares value hotel order ratings different	 If a reviewer really cares about the value of a hotel then the fivestar review would most likely mean a competitive price  In order to interpret the ratings on different aspects accurately we also need to know these aspect weights.
weights opinion task reviews ratings input generate aspect Aspect	 When these different aspects are combined together with specific weights for each user we can have a much more detailed understanding of the overall opinion  Thus the task is to take these reviews and their overall ratings as input and generate both the aspect ratings and aspect weights as output  This is called Latent Aspect Rating Analysis LARA.
Latent Analysis More specifically given set articles topic overall hope generate	 This is called Latent Aspect Rating Analysis LARA  More specifically we are given a set of review articles about a topic with overall ratings and we hope to generate three things.
specifically review articles ratings hope major aspects commented	 More specifically we are given a set of review articles about a topic with overall ratings and we hope to generate three things  One is the major aspects commented on in the reviews.
One aspect value	 One is the major aspects commented on in the reviews  Second is ratings on each aspect such as value and room service.
aspects reviewer.	 Third is the relative weights placed on different aspects by each reviewer.
For opinion summary.	 This task has many applications  For example we can do opinionbased entity ranking or we can generate an aspectlevel opinion summary.
We analyze reviewers’ preferences compare different All enables	 We can also analyze reviewers’ preferences compare them or compare their preferences on different hotels  All this enables personalized product recommendation.
As topics won’t technique detail.	 As in other cases of these advanced topics we won’t cover the technique in detail.
present basic technique developed shown Figure 18	 Instead we will present a basic introduction to the technique developed for this problem as shown in Figure 18 10.
10.	10.
As given review aspects words talking room condition particular denoted ciw	 As input we are given a review with the overall rating  First we will segment the aspects we’re going to pick out what words are talking about location what words are talking about room condition and so on  In particular we will obtain the counts of all the words in each segment denoted by ciw d where i is a particular segment index.
seed like location room segments correlated words seed words allows partitions discussing different	 This can be done by using seed words like location room or price to retrieve the aspect label of each segment  From those segments we can further mine correlated words with these seed words which allows us to segment the text into partitions discussing different aspects  Later we will see that we can also use unsupervised models to do the segmentation.
discussing different aspects.	 From those segments we can further mine correlated words with these seed words which allows us to segment the text into partitions discussing different aspects.
second stage Latent Regression we’re going use frequencies aspects predict overall	 In the second stage Latent Rating Regression we’re going to use these words and their frequencies in different aspects to predict the overall rating.
In we’re going aspect predict aspect rating cussion like amazing high figure weight	 This pre diction happens in two stages  In the first stage we’re going to use the weights of these words in each aspect to predict the aspect rating  For example if in the dis cussion of location you see a word like amazing mentioned many times it will have a high weight in the figure it’s given a weight of 3.
In we’re use words aspect rating.	 In the first stage we’re going to use the weights of these words in each aspect to predict the aspect rating.
9.	9.
This high aspect location.	 This high weight increases the aspect rating for location.
case word like mentioned weight aspect frequencies weights senti ment	 In the case of another word like far which is mentioned many times the weight will decrease  The aspect ratings assume that it will be a weighted combination of these word frequencies where the weights are the senti ment weights of the words.
combination weights	 The aspect ratings assume that it will be a weighted combination of these word frequencies where the weights are the senti ment weights of the words.
sentiment different	 Of course these sentiment weights might be different for different aspects.
aspect sentiment word w second weighted ratings aspect αid average	 For each aspect i we have a set of term sentiment weights for word w denoted as βi w  In the second stage we assume that the overall rating is simply a weighted combination of these aspect ratings  We assume we have aspect weights αid and these will be used to take a weighted average of the aspect ratings rid.
second assume overall simply combination	 In the second stage we assume that the overall rating is simply a weighted combination of these aspect ratings.
We αid rid predict	 We assume we have aspect weights αid and these will be used to take a weighted average of the aspect ratings rid  This method assumes the overall rating is simply a weighted average of these aspect ratings which allows us to predict the overall rating based on the observable word frequencies.
left Figure right hidden information discover.	 On the left side of Figure 18 10 is all the observed information rd the overall rating and ciw d  On the right side is all the latent hidden information that we hope to discover.
interesting Then generative probability adjust parameter values probability rating	 This is a typical case of a generative model where we embed the interesting latent variables  Then we set up a generative probability for the overall rating given the observed words  We can adjust these parameter values to maximize the conditional probability of the observed rating given the document.
probability given We adjust parameter values models PLSA predict topics	 Then we set up a generative probability for the overall rating given the observed words  We can adjust these parameter values to maximize the conditional probability of the observed rating given the document  We have seen such cases before in other models such as PLSA where we predict topics in text data.
probability cases models predict	 We can adjust these parameter values to maximize the conditional probability of the observed rating given the document  We have seen such cases before in other models such as PLSA where we predict topics in text data.
Here data modeling	 Here we’re predicting the aspect ratings and other parameters  More formally the data we are modeling here is a set of review documents with overall ratings as shown in Figure 18.
formally review documents overall 18	 More formally the data we are modeling here is a set of review documents with overall ratings as shown in Figure 18 11.
Each ratings We ciw d word aspect segment i.	11  Each review document is denoted as d and the overall ratings denoted by rd   We use ciw d to denote the count of word w in aspect segment i.
denote count aspect predict rating based d we’re prd 4	 We use ciw d to denote the count of word w in aspect segment i  The model is going to predict the rating based on d so we’re interested in the rating regression problem of prd  d  This model is set 4 up as follows.
predict rating regression d This	 The model is going to predict the rating based on d so we’re interested in the rating regression problem of prd  d  This model is set 4 up as follows.
follows.	 This model is set 4 up as follows.
The aspects self Gaussian ∼ N rating going val Gaussian prior distribution α values going weighted average ratings rd	 The vector of weights α for the aspects in the overall rating is it self drawn from another multivariate Gaussian distribution αd ∼ N μ   This means when we generate our overall rating we’re going to first draw a set of α val ues from this multivariate Gaussian prior distribution  Once we get these α values we’re going to compute the weighted average of aspect ratings as the mean of the normal distribution to generate the overall rating rd .
way model aspect another.	 That gives us a way to model different aspect segments of the same word since the same word might have positive sentiment for one aspect and negative for another.
w μ As usual estimate yields maximize observed ratings Figure 18	 How can we estimate all these parameters Let’s collectively denote them as βi w  μ δ2  As usual we can use the maximum likelihood estimate which yields parameters that maximize observed ratings conditioned on their respective reviews as shown in Figure 18 12.
As parameters observed conditioned reviews shown 18 12.	 As usual we can use the maximum likelihood estimate which yields parameters that maximize observed ratings conditioned on their respective reviews as shown in Figure 18 12.
12 easily words segment w summing The sum simply sum	12  Once we estimate the parameters we can easily compute the aspect rating for a particular aspect by taking all counts of the words that occurred in segment i and multiplying them by βi w summing over all words  The sum would be zero for words that do not occur so we can simply take the sum of all the words in the vocabulary.
parameters compute rating words segment multiplying w occur sum vocabulary.	 Once we estimate the parameters we can easily compute the aspect rating for a particular aspect by taking all counts of the words that occurred in segment i and multiplying them by βi w summing over all words  The sum would be zero for words that do not occur so we can simply take the sum of all the words in the vocabulary.
compute αid values maximum posteriori.	 To compute the αid values we must use maximum a posteriori.
This prior Gaussian distribution rd pαd d βi 18 The likelihood probability overall particular value	 This means that we maximize the product of the prior of α according to our assumed multivariate Gaussian distribution and the likelihood of the rating rd αd∗  arg maxαd pαd  μ prd  d  βi w  δ2 αd  18 3 The likelihood rating is the probability of generating this observed overall rating given this particular α value and some other parameters.
For details model Wang 2010.	 For more details about this model we refer the reader to Wang et al  2010.
solve problem First different aspect	 Earlier we talked about how to solve the LARA problem in two stages  First we did segmentation of different aspects and then used a latent regression model to learn the aspect ratings and weights.
segmentation regression aspect ratings It’s develop unified	 First we did segmentation of different aspects and then used a latent regression model to learn the aspect ratings and weights  It’s also possible to develop a unified gen erative model for solving this problem.
It’s gen	 It’s also possible to develop a unified gen erative model for solving this problem.
That text model generation model.	 That is we not only model the generation of overall ratings based on text but also model the generation of the text itself  A natural solution would be to use a topic model.
solution use topic model.	 A natural solution would be to use a topic model.
Given entity aspects word	 Given an entity we can assume there are aspects that are described by word distributions i.
	 topics.
generation drawn way assumed PLSA.	 We then use a topic model to model the generation of the reviewed text  We assume words in the review text are drawn from these distributions in the same way as we assumed in PLSA.
We words way PLSA.	 We assume words in the review text are drawn from these distributions in the same way as we assumed in PLSA.
model overall To rating based generated aspect rating aspect weights overall	 Then we can plug in the latent regression model to use the text to further predict the overall rating  To predict the overall rating based on the generated text we first predict the aspect rating and then combine them with aspect weights to predict the overall rating.
generative model text text don’t model	 This gives us a unified generative model where we model both the generation of text and the overall rating conditioned on text  We don’t have space to discuss this model in detail so we refer reader to Wang et al.
refer al.	 We don’t have space to discuss this model in detail so we refer reader to Wang et al.
look First ratings hotels look overall rating can’t ence ratings hotels ratings like dimensions	 Let’s look at some applications enabled by using these kinds of generative mod els  First consider the decomposed ratings for some hotels that have the same overall rating  If you just look at the overall rating you can’t really tell much differ ence between these hotels but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some dimension like value while others might score better in other dimensions like location.
decomposed ratings hotels overall If look can’t tell differ ence hotels ratings ratings hotels dimension like better dimensions like location.	 First consider the decomposed ratings for some hotels that have the same overall rating  If you just look at the overall rating you can’t really tell much differ ence between these hotels but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some dimension like value while others might score better in other dimensions like location.
If rating differ ence decomposing hotels higher ratings like score dimensions like location This detailed opinions	 If you just look at the overall rating you can’t really tell much differ ence between these hotels but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some dimension like value while others might score better in other dimensions like location  This breakdown can give us detailed opinions at the aspect level.
At high level different dimensions This different reviewers.	 At a high level overall ratings may look the same but after decomposing the ratings you might see that they have high scores on different dimensions  This is because the model can discern differences in opinions of different reviewers.
help In 18.	 This is because the model can discern differences in opinions of different reviewers  Such a detailed understanding can help us learn about the reviewers and better incorporate their feedback  In Figure 18.
detailed reviewers incorporate In Figure 13 words weighted dimensions value room cleanliness.	 Such a detailed understanding can help us learn about the reviewers and better incorporate their feedback  In Figure 18 13 we show some highly weighted words and the negatively weighted words for each of the four aspect dimensions value room location and cleanliness.
18 13 highly words weighted words dimensions location cleanliness Thus data.	 In Figure 18 13 we show some highly weighted words and the negatively weighted words for each of the four aspect dimensions value room location and cleanliness  Thus we can also learn sentiment information directly from the data.
highly weighted weighted value location	13 we show some highly weighted words and the negatively weighted words for each of the four aspect dimensions value room location and cleanliness.
This like different If battery laptop long” positive “The laptop clearly	 This kind of lexicon is very useful because in general a word like long may have different sentiment polarities for different contexts  If we see “The battery life of this laptop is long” then that’s positive  But if we see “The rebooting time for the laptop is long” then that’s clearly not good.
battery life that’s But rebooting good reviews prod	 If we see “The battery life of this laptop is long” then that’s positive  But if we see “The rebooting time for the laptop is long” then that’s clearly not good  Even for reviews about the same prod uct i.
reviews prod uct	 Even for reviews about the same prod uct i.
laptop kind learn negative	e  a laptop the word long is ambiguous  However with this kind of lexicon we can learn whether a word is positive or negative for a particular aspect.
laptop long However kind learn word positive negative	 a laptop the word long is ambiguous  However with this kind of lexicon we can learn whether a word is positive or negative for a particular aspect.
However kind learn word negative particular	 However with this kind of lexicon we can learn whether a word is positive or negative for a particular aspect.
reviews tag comments hotels media.	 Such a lexicon can be directly used to tag other reviews about hotels or tag comments about hotels in social media.
Since unsupervised aside ratings potentially larger internet topicspecific infer reviewer service price.	 Since this is almost completely unsupervised aside from the overall ratings this can allow us to learn from a potentially larger amount of data on the internet to create a topicspecific sentiment lexicon  Recall that the model can infer whether a reviewer cares more about service or the price.
How evaluation	 How do we know whether the inferred weights are correct This poses a very difficult challenge for evaluation  Figure 18.
14 hotels cities prices reviewers ratio importance aspects.	14 shows prices of hotels in different cities  These are the prices of hotels that are favored by different groups of reviewers  Here we show the ratio of importance of value to other aspects.
prices favored different Here importance value aspects example	 These are the prices of hotels that are favored by different groups of reviewers  Here we show the ratio of importance of value to other aspects  For example we have value vs.
For value	 Here we show the ratio of importance of value to other aspects  For example we have value vs.
For value	 For example we have value vs.
	 location.
In “top ten” reviewers ratios particular	 In the figure “top ten” refers to the reviewers that have the highest ratios by a particular measure.
The higher weights people dimension care These ratios inferred	 The bottom ten refers to reviewers that have put higher weights on other aspects than value these are people who care about another dimension and don’t care so much about value at least compared to the top ten group  These ratios are computed based on the inferred weights from the model.
weights group average half cares like room tend higher average With build	 This provides some indirect way of validating the inferred weights  Looking at the average price in these three cities you can actually see the top ten group tends to have below average prices whereas the bottom half that cares about aspects like service or room condition tend to have hotels that have higher prices than average  With these results we can build many interesting applications.
tends aspects service condition prices average results interesting applications.	 Looking at the average price in these three cities you can actually see the top ten group tends to have below average prices whereas the bottom half that cares about aspects like service or room condition tend to have hotels that have higher prices than average  With these results we can build many interesting applications.
applications direct generate summary including sand This informative overall text.	 With these results we can build many interesting applications  For example a direct application would be to generate a collective summary for each aspect including the positive sand negative sentences about each aspect  This is more informative than the original review that just has an overall rating and review text.
example generate collective aspect	 For example a direct application would be to generate a collective summary for each aspect including the positive sand negative sentences about each aspect.
left like hotels expensive heavy service.	 On the left side you see the weights of viewers that like the expensive hotels  They gave the expensive hotels five stars with heavy aspect weight on service.
That expensive good surprising model	 That suggests that people like expensive hotels because of good service which is not surprising  This is another way to validate the model by the inferred weights.
ratings right reviewers cheaper If like cheaper you’ll condition	 The fivestar ratings on the right side correspond to the reviewers that like the cheaper hotels  As expected they put more weight on value  If you look at when they didn’t like cheaper hotels you’ll see that they tended to have more weights on the condition of the room cleanliness.
expected weight value look like cheaper This shows infer information hard obtain reviews.	 As expected they put more weight on value  If you look at when they didn’t like cheaper hotels you’ll see that they tended to have more weights on the condition of the room cleanliness  This shows that by using this model we can infer some information that’s very hard to obtain even if you read all the reviews.
didn’t you’ll weights cleanliness shows infer obtain reviews This patterns data.	 If you look at when they didn’t like cheaper hotels you’ll see that they tended to have more weights on the condition of the room cleanliness  This shows that by using this model we can infer some information that’s very hard to obtain even if you read all the reviews  This is a case where text mining algorithms can go beyond what humans can do to discover interesting patterns in the data.
shows that’s hard obtain reviews This text algorithms humans	 This shows that by using this model we can infer some information that’s very hard to obtain even if you read all the reviews  This is a case where text mining algorithms can go beyond what humans can do to discover interesting patterns in the data.
Of	 Of course the model is quite general so it can be applied to any reviews with an overall ratings.
Finally results applying model shown	 Finally the results of applying this model for personalized ranking or recom mendation of entities are shown in Figure 18 16.
16.	16.
Because weights dimensions allow indicate ally	 Because we can infer the reviewers’ weights on different dimensions we can allow a user to indicate what they actu ally care about.
That user cheap hotel—an emphasis value	 That is this user just cares about getting a cheap hotel—an emphasis on the value dimension.
model reviewers similar query Then use personalized specific The recommendations results higher group.	 With this model we can find the reviewers whose weights are similar to the query user’s  Then we can use those reviewers to recommend hotels this is what we call personalized or query specific recommendation  The nonpersonalized recommendations are shown on the top and you can see the top results generally have much higher price than the bottom group.
use reviewers personalized query specific The nonpersonalized results	 Then we can use those reviewers to recommend hotels this is what we call personalized or query specific recommendation  The nonpersonalized recommendations are shown on the top and you can see the top results generally have much higher price than the bottom group.
recommendations shown generally higher	 The nonpersonalized recommendations are shown on the top and you can see the top results generally have much higher price than the bottom group.
That’s value text mining serve	 That’s because the reviewers on the bottom cared more about the value  This shows that by doing text mining we can better understand and serve the users.
better understand serve analysis important topic	 This shows that by doing text mining we can better understand and serve the users  To summarize our discussion sentiment analysis is an important topic with many applications.
summarize sentiment analysis important topic Text analysis standard tend need feature representation We consider order discussion	 To summarize our discussion sentiment analysis is an important topic with many applications  Text sentiment analysis can be readily done by using just text categorization but standard techniques tend to be insufficient so we need to have an enriched feature representation  We also need to consider the order of the senti ment categories if there are more than two this is where our discussion on ordinal regression comes into play.
We order play.	 We also need to consider the order of the senti ment categories if there are more than two this is where our discussion on ordinal regression comes into play.
shown generative mining user particular generative sentiment model product making analyze.	 We have also shown that generative models are pow erful for mining latent user preferences in particular the generative model for mining latent rating regression  Here we embedded some interesting preference information and sentiment by weighting words in the model  For product reviews the opinion holder and the opinion target are clear making them easy to analyze.
preference weighting words reviews holder clear easy There course applications.	 Here we embedded some interesting preference information and sentiment by weighting words in the model  For product reviews the opinion holder and the opinion target are clear making them easy to analyze  There of course we have many practical applications.
course that’s difficult analyzing opinion opinion	 There of course we have many practical applications  Opinion mining from news and social media is also important but that’s more difficult than analyzing review data mainly because the opinion holders and opinion targets are not clearly de fined.
media important data mainly advanced language processing future suggest Pang comprehensive survey opinion analysis.	 Opinion mining from news and social media is also important but that’s more difficult than analyzing review data mainly because the opinion holders and opinion targets are not clearly de fined  This challenge calls for more advanced natural language processing  For future reading on topics presented in this chapter we suggest Pang and Lee 2008 a comprehensive survey on opinion mining and sentiment analysis.
This calls processing reading presented chapter Lee comprehensive mining analysis	 This challenge calls for more advanced natural language processing  For future reading on topics presented in this chapter we suggest Pang and Lee 2008 a comprehensive survey on opinion mining and sentiment analysis  18.
Opinion Mining Sentiment In investigated opinion analysis	 18 4 Evaluation of Opinion Mining and Sentiment Analysis In this chapter we investigated opinion mining and sentiment analysis from the viewpoint of both classification or regression and topic analysis.
Chapter 15 Chapter 17 topic use positive This learning setup use techniques determine method.	 Thus evaluation from these perspectives will be similar to those discussed in Chapter 15 catego rization and Chapter 17 topic analysis  From a classification viewpoint we can use a dataset with documents labeled as positive or negative or as ratings on a numerical scale as described in this chapter  This then becomes the standard machine learning testing setup where we can use techniques such as crossfold validation to determine the effectiveness of our method.
viewpoint use documents described chapter This standard machine learning techniques crossfold validation Additionally selection determining e.	 From a classification viewpoint we can use a dataset with documents labeled as positive or negative or as ratings on a numerical scale as described in this chapter  This then becomes the standard machine learning testing setup where we can use techniques such as crossfold validation to determine the effectiveness of our method  Additionally feature selection can show which features are the most useful for determining whether a sentence e.
features useful determining	 Additionally feature selection can show which features are the most useful for determining whether a sentence e.
negative Based try	g  is positive or negative  Based on the useful features we can either adjust the algorithm or try to finetune the feature set.
Based features adjust algorithm try From like ensure coverage	 is positive or negative  Based on the useful features we can either adjust the algorithm or try to finetune the feature set  From a topic analysis viewpoint we would like to ensure that the topics are coherent and have a believable coverage and distribution over the documents in the dataset.
try finetune	 Based on the useful features we can either adjust the algorithm or try to finetune the feature set.
mentioned corpus log likelihood test model data While et	 We mentioned that corpus log likelihood is a way to test how well the model fits to the data  While this evaluation metric doesn’t always agree with human judges Chang et al.
doesn’t human Chang et proxy opinion existing	 While this evaluation metric doesn’t always agree with human judges Chang et al  2009 it does serve as a sanity check or proxy for true usefulness  Additionally we can test the effectiveness of adding opinion mining and senti ment analysis to an existing system by the method discussed in Chapter 13.
check proxy usefulness.	 2009 it does serve as a sanity check or proxy for true usefulness.
opinion mining analysis existing 13 system’s compare performance afterwards.	 Additionally we can test the effectiveness of adding opinion mining and senti ment analysis to an existing system by the method discussed in Chapter 13  That is we can compare the system’s performance before sentiment analysis and then compare its performance afterwards.
compare system’s	 That is we can compare the system’s performance before sentiment analysis and then compare its performance afterwards.
statistically evaluation relevant task hand definitive improvement.	 If the system performs statistically signifi cantly better under some evaluation metric relevant to the task at hand then the sentiment analysis is a definitive improvement.
Further Reading analysis	 Bibliographic Notes and Further Reading Opinion mining and sentiment analysis have been extensively studied.
ex cellent general book Opinion book topic	 Two ex cellent books on this general topic are the book Opinion Mining and Sentiment Analysis Pang and Lee 2008 and the book Sentiment Analysis and Opinion Min ing Liu 2012  Multiple extensions of topic models for analyzing opinionated topics have been made e.
models topics Mei et	 Multiple extensions of topic models for analyzing opinionated topics have been made e g  topicsentiment mixture model Mei et al.
	g.
Latent mainly covered papers Wang al al.	 Techniques for Latent Aspect Rating Analysis are mainly covered in two KDD papers Wang et al  2010 Wang et al.
Text Structured chapter techniques joint analysis enriches necessary prediction structured data.	 2011  19Joint Analysis of Text and Structured Data In this chapter we discuss techniques for joint analysis of text and structured data which not only enriches text analysis but is often necessary for many prediction problems involving both structured data and unstructured text data.
Analysis Structured In chapter techniques joint text enriches structured data unstructured	 19Joint Analysis of Text and Structured Data In this chapter we discuss techniques for joint analysis of text and structured data which not only enriches text analysis but is often necessary for many prediction problems involving both structured data and unstructured text data.
methods main ideas provide support.	 Due to the complexity of many of these methods and the limited space we will only give a brief introduction to the main ideas and show sample results to provide a sense about what kind of applications these techniques can support.
	 19.
Introduction In data applications structured data structured available making.	1 Introduction In realworld big data applications we would have both structured data and un structured text data available to help us make predictions and support decision making.
analyze data jointly latent realworld variable To illustrate setup data mining loop Figure	 It is thus important to analyze both kinds of data jointly especially when the goal is to predict some latent realworld variable that is not directly observed in the data  To illustrate this problem setup and the big picture of data mining in general we show the data mining loop in Figure 19 1.
illustrate problem mining Figure 19.	 To illustrate this problem setup and the big picture of data mining in general we show the data mining loop in Figure 19.
In multiple sensors—including human report seen real world The data include nontext data	1  In this figure we see that there are multiple sensors—including human sen sors—to report what we have seen in the real world in the form of data  The data include both nontext data and text data.
data data text data Our predict values world variables matter interested	 The data include both nontext data and text data  Our goal is to see if we can predict some values of important real world variables that matter to us  For example we might be interested in predicting the condition of a bridge the weather stock prices or presidential election results.
Our goal important real variables interested bridge weather stock presidential results.	 Our goal is to see if we can predict some values of important real world variables that matter to us  For example we might be interested in predicting the condition of a bridge the weather stock prices or presidential election results.
example interested condition bridge stock prices presidential election	 For example we might be interested in predicting the condition of a bridge the weather stock prices or presidential election results.
How data mining analysis data.	 How can we get from the data to these predicted values We’ll first have to do data mining and analysis of the data.
In leverage mining data Through generate multiple predictors variables us.	 In general we should try to leverage all the data that we can collect and joint mining of nontext and text data is critical  Through analysis of all the data we can generate multiple predictors of the interesting variables to us.
general process data human role especially involvement data.	 This is the general process for making a prediction based on any data  It’s important to emphasize that a human actually plays a very important role in this process especially because of the involvement of text data.
important actually plays data user involved data.	 It’s important to emphasize that a human actually plays a very important role in this process especially because of the involvement of text data  First a human user would be involved in the mining of the data.
human user mining data.	 First a human user would be involved in the mining of the data.
user generation create	 The user can control the generation of these features and can even manually create features.
humans understand text data text data humans effectively The course enormous data humans read digest information help that’s need text	 Second humans can help understand the text data because text data are created to be consumed by humans and humans can consume and interpret text data much more effectively than a machine  The challenge of course is when there is an enormous amount of text data since it would not be feasible for humans to read and digest all the information  Thus machines must help and that’s why we need to do text data mining.
The challenge enormous data feasible information machines need text	 The challenge of course is when there is an enormous amount of text data since it would not be feasible for humans to read and digest all the information  Thus machines must help and that’s why we need to do text data mining.
Sometimes machines humans data humans involved building testing tive In particular knowledge prediction problem model.	 Sometimes machines can even “see” patterns in data that humans may not see even if they have the time to read all the data  Next humans also must be involved in building adjusting and testing a predic tive model  In particular we humans will have important domain knowledge about the prediction problem that we can build into the predictive model.
adjusting tive model domain knowledge	 Next humans also must be involved in building adjusting and testing a predic tive model  In particular we humans will have important domain knowledge about the prediction problem that we can build into the predictive model.
In humans domain predicted humans taking particular values.	 In particular we humans will have important domain knowledge about the prediction problem that we can build into the predictive model  Once we have the predicted values for the variables humans would be involved in taking actions to change the world or make decisions based on these particular values.
variables humans involved change world based particular	 Once we have the predicted values for the variables humans would be involved in taking actions to change the world or make decisions based on these particular values.
Thus collect new potentially useful data data need Machines help identify data next.	 Thus this forms a data mining loop because as we perturb the sensors they will collect additional new and potentially more useful data allowing us to improve the prediction  In this loop humans will recognize what additional data will need to be collected  Machines can help humans identify what data should be collected next.
loop help identify data general collect learning.	 In this loop humans will recognize what additional data will need to be collected  Machines can help humans identify what data should be collected next  In general we want to collect data that is most useful for learning.
useful learning study helpful machine learning referred subarea learning loop acquisition data mining values actions finally observe	 In general we want to collect data that is most useful for learning  The study of how to identify data points that would be most helpful for machine learning is often referred to as active learning which is an important subarea in machine learning  There is a loop from data acquisition to data analysis from data mining to prediction of values from actions to change the world and finally we observe what happens.
data machine learning active important There loop data data finally happens.	 The study of how to identify data points that would be most helpful for machine learning is often referred to as active learning which is an important subarea in machine learning  There is a loop from data acquisition to data analysis from data mining to prediction of values from actions to change the world and finally we observe what happens.
acquisition actions change observe happens.	 There is a loop from data acquisition to data analysis from data mining to prediction of values from actions to change the world and finally we observe what happens.
We additional data adjust sensors data need big actually important applications big	 We can then decide what additional data have to be collected and adjust the sensors accordingly  Analysis of the prediction errors can help reveal what additional data we need to acquire in order to improve the accuracy of prediction  This big picture is actually very general and can serve as a model for many important applications of big data.
Analysis help reveal data need acquire big picture general model applications big focus book consider special loop shown Figure	 Analysis of the prediction errors can help reveal what additional data we need to acquire in order to improve the accuracy of prediction  This big picture is actually very general and can serve as a model for many important applications of big data  Since the focus of the book is on text data it is useful to consider the special case of the loop shown in Figure 19.
focus text data case shown Figure 2 goal infer variables real world related text analysis	 Since the focus of the book is on text data it is useful to consider the special case of the loop shown in Figure 19 2 where the goal is to use text data to infer values of some other variables in the real world that may not be directly related to the text  Such an analysis task is different from a task such as topic mining where the goal is to directly characterize the content of text.
2 use text values real world text.	2 where the goal is to use text data to infer values of some other variables in the real world that may not be directly related to the text.
Such task different mining directly	 Such an analysis task is different from a task such as topic mining where the goal is to directly characterize the content of text.
textbased goal information	 In textbased prediction our goal can be to infer any information about the world.
This variable case text data cases g.	 This is of course only possible if there exist clues in the text data about the target variable fortunately this is often the case since people report everything in text data  In many cases e g.
cases g.	 In many cases e g.
data historical stock effective text data text provide additional prediction task.	 stock price prediction the nontext data historical stock prices are much more effective for prediction than text data though text data can generally help provide additional indicators for the prediction task.
Sometimes data useful nontext sufficient making	 Sometimes text data contain more useful indicators than nontext data and text data alone may also be sufficient for making predictions.
In	 In all cases of textbased prediction there are two important questions.
task Second text For term prediction problem inevitably However ngrams topics previous chapters.	 First what features indicators are most useful for the prediction task Second how can we generate such effective indicators from text For convenience we will use the term “feature” and “indicator” interchangeably  The first question has much to do with the specific prediction problem and is thus inevitably applicationspecific  However there are some generic features we can often start with like ngrams or topics as discussed in some previous chapters.
question prediction applicationspecific.	 The first question has much to do with the specific prediction problem and is thus inevitably applicationspecific.
generic features discussed	 However there are some generic features we can often start with like ngrams or topics as discussed in some previous chapters  Supervised learning methods can be used to learn what features are most effective.
learn effective The previous book techniques introduced useful indicators	 Supervised learning methods can be used to learn what features are most effective  The second question has been addressed to some extent in the previous chapters of the book since many techniques that we have introduced can be potentially used to obtain features from text data  For example topic mining can be very useful to generate topic based indicators or predictors that can be further fed into a predictive model.
The second question previous chapters obtain text	 The second question has been addressed to some extent in the previous chapters of the book since many techniques that we have introduced can be potentially used to obtain features from text data.
based fed predictive topicbased certain terms documents features.	 For example topic mining can be very useful to generate topic based indicators or predictors that can be further fed into a predictive model  Such topicbased features can be mixed with wordbased features to enrich the feature representation  Sizes of a certain cluster of terms or cluster of documents may also be potential features.
Such topicbased mixed wordbased enrich representation cluster set terms indicator single sentiment tags based text useful problems.	 Such topicbased features can be mixed with wordbased features to enrich the feature representation  Sizes of a certain cluster of terms or cluster of documents may also be potential features  A set of terms with paradigmatic relations may be a better indicator than any single term and sentiment tags that we may be able to generate based on text data are yet another kind of useful feature for some prediction problems.
documents	 Sizes of a certain cluster of terms or cluster of documents may also be potential features.
The joint analysis text data seen	 The benefit of joint analysis of text and nontext data can be seen from two differ ent perspectives.
Specifically nontext data mining data e	 Specifically nontext data can often provide a context for mining text data and thus enable us to partition data in different ways based on the companion nontext data e g.
time location text mining defined nontext data topics specific nontext variable patterns different contexts temporal	g  partitioning text based on time or location  This opens up possibilities of contextual text mining or mining text data in the context defined by nontext data to discover contextspecific knowledge such as topics associated with a specific nontext variable such as time or patterns across different contexts like temporal trends.
partitioning based time This opens contextual text text discover knowledge topics associated time patterns trends.	 partitioning text based on time or location  This opens up possibilities of contextual text mining or mining text data in the context defined by nontext data to discover contextspecific knowledge such as topics associated with a specific nontext variable such as time or patterns across different contexts like temporal trends.
This opens mining context defined knowledge topics different contexts like	 This opens up possibilities of contextual text mining or mining text data in the context defined by nontext data to discover contextspecific knowledge such as topics associated with a specific nontext variable such as time or patterns across different contexts like temporal trends.
text interpret patterns data For example frequent discovered data associated instances pattern difference sets text pattern offer insights interpret pattern	 Second text data can help interpret patterns discovered from nontext data  For example if a frequent pattern is discovered from nontext data we can separate the text data associated with the data instances where the pattern occurs from those associated with the instances that do not match the pattern  We can then analyze the difference be tween these two sets of text data which may be associated with the meaning of the pattern and thus can offer insights about how to interpret the pattern which would otherwise be hard to interpret by only looking at the nontext data.
example discovered data associated instances associated match pattern.	 For example if a frequent pattern is discovered from nontext data we can separate the text data associated with the data instances where the pattern occurs from those associated with the instances that do not match the pattern.
analyze difference offer insights interpret pattern hard interpret discussed al 2006.	 We can then analyze the difference be tween these two sets of text data which may be associated with the meaning of the pattern and thus can offer insights about how to interpret the pattern which would otherwise be hard to interpret by only looking at the nontext data  This technique is called pattern annotation and discussed in detail in Mei et al  2006.
2006 Mining In discuss use context text data.	 2006  19 2 Contextual Text Mining In this section we discuss how to use nontext data as context to enrich topic analysis of text data.
discuss nontext data context enrich data analysis extension reveal associated context	 19 2 Contextual Text Mining In this section we discuss how to use nontext data as context to enrich topic analysis of text data  Such analysis techniques can be regarded as an extension of topic analysis to further reveal the correlation of topics and any associated context e.
analysis reveal e g.	 Such analysis techniques can be regarded as an extension of topic analysis to further reveal the correlation of topics and any associated context e g.
time location opinions reveal	 time or location  When topics represent opinions we may also reveal contextdependent opinions.
represent reveal contextdependent	 When topics represent opinions we may also reveal contextdependent opinions.
First available location author source regarded direct	 First text data almost always have metadata available such as time location author and source of the data which can be regarded as direct context information.
text refers related	 Second text data may also have indirect context which refers to addi tional data related to the metadata.
example social author’s location.	 For example from the authors of a text article we can further obtain additional context such as the social network of the author the author’s age or the author’s location.
Such text connect them.	 Such information is not in general di rectly related to the text yet through such a propagation process we can connect all of them.
source current articles source well.	 There may also be other articles from the same source as a current arti cle and we can connect these articles from the same source and make them related as well.
related context What data text interesting partition want comparative text us.	 In general any related data can be regarded as context  What can the context of text data be used for Context can be used to partition text data in many interesting ways  It can almost allow us to partition text data in any way that we want and thus enables comparative analysis of text to be done across any context dimension that is interesting to us.
context text data partition interesting ways.	 What can the context of text data be used for Context can be used to partition text data in many interesting ways.
data way want enables comparative text dimension interesting specific context papers different Figure 19 3.	 It can almost allow us to partition text data in any way that we want and thus enables comparative analysis of text to be done across any context dimension that is interesting to us  As a specific example we show how some context variables enable partition ing of research papers in many different ways in Figure 19 3.
specific variables enable partition papers different Figure 19 3 The shows venues papers published dimension	 As a specific example we show how some context variables enable partition ing of research papers in many different ways in Figure 19 3  The horizontal di mension of the figure shows different conference venues where the papers are published and the vertical dimension shows the time of a publication.
3 mension shows conference venues papers dimension treat separate unit case paper text	3  The horizontal di mension of the figure shows different conference venues where the papers are published and the vertical dimension shows the time of a publication  We can treat each paper as a separate unit in this case a paper ID serves as the con text and each paper has its own context.
This comparison data U.	 This comparison is enabled by the availability of the nontext variable of the conference venue  Furthermore we can also partition the data to obtain the papers written by authors in the U.
partition written authors additional text	 Furthermore we can also partition the data to obtain the papers written by authors in the U S  by using additional con text of the authors.
contextual view pare papers American authors Sometimes partition involving data example obtain “text	 Such a contextual view of the data would allow us to com pare papers written by American authors with those written by authors in other countries  Sometimes we can use topics to partition the data without involving nontext data  For example we can obtain a set of papers about the topic “text mining” and compare them with the papers about another topic.
partition involving nontext data topic “text mining” topic.	 Sometimes we can use topics to partition the data without involving nontext data  For example we can obtain a set of papers about the topic “text mining” and compare them with the papers about another topic.
For example obtain papers “text mining” papers topic.	 For example we can obtain a set of papers about the topic “text mining” and compare them with the papers about another topic.
Note partitions partitions.	 Note that these partitions can be intersected with each other to generate even more complicated partitions.
So constraints nontext interesting data discovery associated contexts.	 So we may form constraints on nontext variables to create interesting contexts for partitioning text data which can then facilitate discovery of knowledge associated with different contexts.
nontext enables text data potentially different patterns For example topical Comparing contexts	 The incorporation of nontext contextual variables enables the association of topics from text data with potentially many different contexts generating interest ing and useful patterns  For example in comparing topics over time we can see topical trends  Comparing topics in different contexts can also reveal differences about the two contexts.
For comparing contexts	 For example in comparing topics over time we can see topical trends  Comparing topics in different contexts can also reveal differences about the two contexts.
There questions require text mining answer For question getting increasing research” need Is difference people different To question	 There are many interesting questions that require contextual text mining to answer  For example to answer a question such as “What topics have been getting increasing attention recently in data mining research” we would need to analyze text in the context of time  Is there any difference in the responses of people in different regions to an event To answer such a question location can be the context.
people different To answer context research interests researchers	 Is there any difference in the responses of people in different regions to an event To answer such a question location can be the context  What are the common research interests of two researchers In this case authors can be the context.
Here context affiliation location case need author.	S  and those outside Here the context would include the authors and their affiliation and location  This is a case where we need to go beyond just the authors and further look at the additional information connected to the author.
Here affiliation location case need additional connected author.	 and those outside Here the context would include the authors and their affiliation and location  This is a case where we need to go beyond just the authors and further look at the additional information connected to the author.
look connected author opinions expressed social context.	 This is a case where we need to go beyond just the authors and further look at the additional information connected to the author  Is there any difference in the opinions of all the topics expressed in one social network compared to another In this case the social network of authors and the topic can be a context.
network social topic Are data coverage sudden stock addressed series stock issues presidential campaign Here serves context.	 Is there any difference in the opinions of all the topics expressed in one social network compared to another In this case the social network of authors and the topic can be a context  Are there topics in news data whose coverage is correlated with sudden changes in stock prices Such a question can be addressed by using a time series such as stock prices as context  What issues mattered in the 2012 presidential campaign and election Here time serves again as context.
sudden changes stock Such addressed time series stock context issues presidential serves applications.	 Are there topics in news data whose coverage is correlated with sudden changes in stock prices Such a question can be addressed by using a time series such as stock prices as context  What issues mattered in the 2012 presidential campaign and election Here time serves again as context  Clearly contextual text mining can have many applications.
What issues 2012 presidential election time serves context.	 What issues mattered in the 2012 presidential campaign and election Here time serves again as context.
Clearly text mining 19 Probabilistic Latent Analysis In section technique Latent Semantic CPLSA.	 Clearly contextual text mining can have many applications  19 3 Contextual Probabilistic Latent Semantic Analysis In this section we briefly introduce a specific technique for contextual text mining called Contextual Probabilistic Latent Semantic Analysis CPLSA.
3 section briefly called Analysis CPLSA extension model topics word associated	 19 3 Contextual Probabilistic Latent Semantic Analysis In this section we briefly introduce a specific technique for contextual text mining called Contextual Probabilistic Latent Semantic Analysis CPLSA  CPLSA is an extension of PLSA to incorporate context variables into a generative model so that both the selection of topics and the topic word distributions can depend on the context associated with text.
Latent Analysis called Contextual Latent Semantic Analysis CPLSA variables generative model selection depend context text.	3 Contextual Probabilistic Latent Semantic Analysis In this section we briefly introduce a specific technique for contextual text mining called Contextual Probabilistic Latent Semantic Analysis CPLSA  CPLSA is an extension of PLSA to incorporate context variables into a generative model so that both the selection of topics and the topic word distributions can depend on the context associated with text.
context selection word distributions depend context associated	 CPLSA is an extension of PLSA to incorporate context variables into a generative model so that both the selection of topics and the topic word distributions can depend on the context associated with text.
Recall PLSA selecting word	 Recall that in PLSA the text data are generated by first selecting a topic and then generating a word from a topic.
selection e coverage document.	 The topics are shared by all the documents in the collection but the selection probability i e  coverage of topics is specific to a document.
	e  coverage of topics is specific to a document.
In CPLSA information words document specific context set collection assume	 In CPLSA the generation process is similar but since we assume that we have context information time or location about a document the generation of words in the document may be conditioned on the specific context of the document  Instead of assuming just one set of common topics for the collection we assume that there may be variations of this set of topics depending on the context.
Instead assuming topics collection variations set topics depending view topics particular context time period different different	 Instead of assuming just one set of common topics for the collection we assume that there may be variations of this set of topics depending on the context  For example we might have a particular view of all the topics imposed by a particular context such as a particular time period or a particular location so we may have multiple sets of comparable topics that represent different views of these topics associated with different contexts.
In 4 use Hurricane Katrina idea.	 In Figure 19 4 we use a collection of blog articles about Hurricane Katrina to illustrate this idea.
Katrina In imagine potential government flooding These different represented distribution.	4 we use a collection of blog articles about Hurricane Katrina to illustrate this idea  In such a collection we can imagine potential topics such as government response donation and flooding of New Orleans  These are shown as different “themes” each represented by a word distribution.
In collection potential topics government different “themes” represented distribution.	 In such a collection we can imagine potential topics such as government response donation and flooding of New Orleans  These are shown as different “themes” each represented by a word distribution.
shown different represented different contains tributions figure Texas talk presumably differ view associated context context occupation	 These are shown as different “themes” each represented by a word distribution  Besides these themes we also show three potentially different views of these three themes topics View1 is associated with a location context Texas and contains Texasspecific word dis tributions for all the three themes shown in the figure which may reflect how the authors in Texas talk about these topics which presumably would be different from how the authors in Illinois talk about them which can be represented as a differ ent view  Similarly View2 is associated with a time context July 2005 and View3 is associated with a context of author occupation a sociologist.
Besides themes View1 associated Texas word themes reflect authors Texas topics presumably different authors talk ent Similarly context July topics document	 Besides these themes we also show three potentially different views of these three themes topics View1 is associated with a location context Texas and contains Texasspecific word dis tributions for all the three themes shown in the figure which may reflect how the authors in Texas talk about these topics which presumably would be different from how the authors in Illinois talk about them which can be represented as a differ ent view  Similarly View2 is associated with a time context July 2005 and View3 is associated with a context of author occupation a sociologist  The selection of topics when generating words in a document can also be in fluenced by the context of the document.
Similarly associated context 2005 associated context The generating context document.	 Similarly View2 is associated with a time context July 2005 and View3 is associated with a context of author occupation a sociologist  The selection of topics when generating words in a document can also be in fluenced by the context of the document.
The words document For example Texas locations Similarly contexts time author pation preference certain	 The selection of topics when generating words in a document can also be in fluenced by the context of the document  For example the authors in Texas may tend to cover one particular aspect more than another while the authors in other locations may be different  Similarly contexts such as the time and author occu pation may also suggest a preference for certain topics.
author occu pation suggest certain preference document	 Similarly contexts such as the time and author occu pation may also suggest a preference for certain topics  In the standard PLSA we assume that every document has its own preference for topics this can be regarded as a special case of CPLSA where we have taken each document ID as a context for the document.
In standard assume topics special document selection preferences 19 4.	 In the standard PLSA we assume that every document has its own preference for topics this can be regarded as a special case of CPLSA where we have taken each document ID as a context for the document  The different topic selection preferences of different contexts are illustrated at the bottom of Figure 19 4.
The different different contexts 19.	 The different topic selection preferences of different contexts are illustrated at the bottom of Figure 19.
generate word view topics document view chosen Texas Texas topics	 To generate a word in document d we would first choose a particular view of all the topics based on the context of the document  For example the view being chosen may be the location Texas in such a case we would be using the Texas specific topics to generate the word.
view location Texas Texas generate The topic coverage choose July 2005.	 For example the view being chosen may be the location Texas in such a case we would be using the Texas specific topics to generate the word  The next step is to decide a topic coverage which again depends on the context  We may choose the coverage associated with the time July 2005.
step topic We coverage associated coverage documents July time context.	 The next step is to decide a topic coverage which again depends on the context  We may choose the coverage associated with the time July 2005  Note that this same coverage would be used when generating all documents with July 2005 as the time context.
We coverage generating 2005	 We may choose the coverage associated with the time July 2005  Note that this same coverage would be used when generating all documents with July 2005 as the time context.
generating documents 2005 document preference independent context introduced enables particular contribute learning coverage.	 Note that this same coverage would be used when generating all documents with July 2005 as the time context  This is very different from the PLSA where each document has its own coverage preference which is independent of each other  The dependency on context introduced here enables all the text with a particular context associated to contribute to the learning of the topic coverage.
The context text particular context topic Once topics chosen process distribution use corresponding word	 The dependency on context introduced here enables all the text with a particular context associated to contribute to the learning of the topic coverage  Once a view of topics and a topic coverage distribution have been chosen the rest of the generation process is exactly the same as in PLSA  That is we will use the topic coverage and selection distribution to sample a topic and then use the corresponding word distribution of the topic i.
use selection use topic erate word.	 That is we will use the topic coverage and selection distribution to sample a topic and then use the corresponding word distribution of the topic i e  chosen view of the topic to gen erate a word.
Note context deter choice view topic context coverage This illustrated	 Note that the context that deter mines the choice of view of a topic can be different from the context chosen to decide the topic coverage  This is illustrated in Figure 19.
words mixture models associated contexts In views topics context different topic different topics context dependency topic coverage	5 where we show that all the words in the document have now been generated by using CPLSA which is es sentially a mixture model with many component models associated with different contexts  In CPLSA both the views of topics and the topic coverage depend on context so it enables discovery of different variations of the same topic in different contexts due to the dependency of a view of topics on context and different topic cover ages in different contexts due to the dependency of topic coverage on context.
topic coverage depend enables different topic different view contexts dependency coverage contextdependent patterns PLSA easily seen single document ID context	 In CPLSA both the views of topics and the topic coverage depend on context so it enables discovery of different variations of the same topic in different contexts due to the dependency of a view of topics on context and different topic cover ages in different contexts due to the dependency of topic coverage on context  Such contextdependent topic patterns can be very useful for answering various questions as we mentioned earlier  The standard PLSA can easily be seen as a special case of CPLA when we have used just one single view of topics by using the whole collection as context and used each document ID as a context for deciding topic coverage.
Since CPLSA remains mixture use problem parameter parameters	 Since CPLSA remains a mixture model we can still use the EM algorithm to solve the problem of parameter estimation although the number of parameters to be estimated would be significantly larger than PLSA.
Thus CPLSA allow discover contextspecific topic coverage butions However inevitable sparsity restrict complexity model Once context conditional given precisely hope discover	 Thus theoretically speaking CPLSA can allow us to discover any contextspecific topics and topic coverage distri butions  However in reality due to the inevitable sparsity of data we must restrict the space of the context variables to control the complexity of the model  Once esti mated the parameters of CPLSA would naturally contain context variables includ ing particularly many conditional probabilities of topics given a certain context which are precisely what we hope to discover in contextual text mining.
However reality inevitable sparsity restrict context Once mated parameters naturally contain ing probabilities given certain context precisely hope text details readers Mei Zhai	 However in reality due to the inevitable sparsity of data we must restrict the space of the context variables to control the complexity of the model  Once esti mated the parameters of CPLSA would naturally contain context variables includ ing particularly many conditional probabilities of topics given a certain context which are precisely what we hope to discover in contextual text mining  For details of CPLSA readers can refer to Mei and Zhai 2006.
esti mated parameters CPLSA contain includ particularly conditional certain hope discover contextual For readers refer Mei	 Once esti mated the parameters of CPLSA would naturally contain context variables includ ing particularly many conditional probabilities of topics given a certain context which are precisely what we hope to discover in contextual text mining  For details of CPLSA readers can refer to Mei and Zhai 2006.
refer Mei 2006 We sample CPLSA	 For details of CPLSA readers can refer to Mei and Zhai 2006  We now show some sample results of CPLSA  First in Figure 19.
We sample results Figure sets news articles Iraq Afghanistan high probability collectionspecific topics.	 We now show some sample results of CPLSA  First in Figure 19 6 we show re sults from comparing two sets of news articles about the Iraq and Afghanistan wars respectively including high probability words in both common topics and collectionspecific topics.
Figure sets Afghanistan high probability common topics topics tioning articles CPLSA simpler crosscollection	 First in Figure 19 6 we show re sults from comparing two sets of news articles about the Iraq and Afghanistan wars respectively including high probability words in both common topics and collectionspecific topics  In this case the context is the topic and leads to a parti tioning of news articles into two sets corresponding to these two wars and CPLSA degenerates to a simpler crosscollection mixture model Zhai et al.
comparing sets wars including high probability topics	6 we show re sults from comparing two sets of news articles about the Iraq and Afghanistan wars respectively including high probability words in both common topics and collectionspecific topics.
context articles crosscollection We 30 war 26 articles Afghanistan war.	 In this case the context is the topic and leads to a parti tioning of news articles into two sets corresponding to these two wars and CPLSA degenerates to a simpler crosscollection mixture model Zhai et al  2004  We have 30 articles on the Iraq war and 26 articles on the Afghanistan war.
We 30 Afghanistan compare topics sets topics set.	 We have 30 articles on the Iraq war and 26 articles on the Afghanistan war  Our goal is to compare the two sets of articles to discover the common topics shared by the two sets and understand the variations of these topics in each set.
compare sets common	 Our goal is to compare the two sets of articles to discover the common topics shared by the two sets and understand the variations of these topics in each set.
Figure 19 discover topics.	 The results in Figure 19 6 show that CPLSA can discover meaningful topics.
CPLSA discover meaningful	6 show that CPLSA can discover meaningful topics.
column 1 common topic United sense	 The first column Cluster 1 shows a very meaningful common topic about the United Nations on the first row which intuitively makes sense given the topics of these two collections.
people result automatically What’s These Iraqspecific topic United topic respectively.	 Such a topic may not be surprising to people who know the background about these articles but the result shows that CPLSA can automatically discover it  What’s more interesting however is the two cells of word distributions shown on the second and third rows in the first column right below the cell about the United Nations  These two cells show the Iraqspecific view of the topic about the United Nations and the Afghanistan view of the same topic respectively.
These view topic United Afghanistan view	 These two cells show the Iraqspecific view of the topic about the United Nations and the Afghanistan view of the same topic respectively.
These topic topic wars understanding	 These two contextspecific views of the topic of the United Nations show different variations of the topic in the two wars and reveal a more detailed understanding of topics in a contextspecific way.
g cell examine relevant articles shared fatalities surprising wars.	g  each cell can be made clickable to enable a user to examine the relevant discussion in the news articles in detail  The second column shows another shared common topic about fatalities which again should not be surprising given that these articles are about wars.
cell enable news The second column shows shared common fatalities given able extract meaningful common	 each cell can be made clickable to enable a user to examine the relevant discussion in the news articles in detail  The second column shows another shared common topic about fatalities which again should not be surprising given that these articles are about wars  It also again confirms that CPLSA is able to extract meaningful common topics.
second shared topic articles	 The second column shows another shared common topic about fatalities which again should not be surprising given that these articles are about wars.
case collectionspecific column variations contexts.	 As in the case of the first column the collectionspecific topics in the third column also further show the variations of the topic in these two different contexts.
In temporal blog Katrina.	 In Figure 19 7 we show the temporal trends of topics discovered from blog articles about Hurricane Katrina.
plots In 7 trends time.	 The plots are enabled directly by the parameters of CPLSA where we used time and location as context  In Figure 19 7 we show a visualization of the trends of topics over time.
Figure	 In Figure 19.
One price Orleans The plot probability given particular period parameters timedependent	 One is oil price and one is about the flooding of the city of New Orleans  The plot is based on the conditional probability of a topic given a particular time period which is one of the parameters in CPLSA for capturing timedependent coverage of topics.
plot conditional particular period parameters coverage topics.	 The plot is based on the conditional probability of a topic given a particular time period which is one of the parameters in CPLSA for capturing timedependent coverage of topics.
initially curves tracked	 Here we see that initially the two curves tracked each other very well.
Interestingly later New Orleans mentioned prices	 Interestingly later the topic of New Orleans was mentioned again but oil prices was not.
The figure city New blog article authors different	 The bottom figure shows the coverage of the topic about flooding of the city New Orleans by blog article authors in different locations different states in the U S .
	S .
.	.
We initially	 We see that the topic was initially heavily covered by authors in the victim areas e g.
Louisiana picked Texas explained people Louisiana Texas topical trends revealing enable patterns events associated	g  Louisiana but the topic was then picked up by the authors in Texas which might be explained by the move of people from the state of Louisiana to Texas  Thus these topical trends not only are themselves useful for revealing the topics and their dynamics over time but also enable comparative analysis of topics across different contexts to help discover interesting patterns and potentially interesting events associated with the patterns.
topic authors explained people time analysis topics discover potentially events patterns In	 Louisiana but the topic was then picked up by the authors in Texas which might be explained by the move of people from the state of Louisiana to Texas  Thus these topical trends not only are themselves useful for revealing the topics and their dynamics over time but also enable comparative analysis of topics across different contexts to help discover interesting patterns and potentially interesting events associated with the patterns  In Figure 19.
	 In Figure 19.
government response different weeks event	8 we show spatiotemporal patterns of the coverage of the topic of government response in the same data set of blog articles about Hurricane Katrina  These visualizations show the distribution of the coverage of the topic in different weeks of the event over the states in the U S.
We initially concentrated gradually spread left distribution This explained hitting	 We see that initially the coverage is concentrated mostly in the victim states in the south but the topic gradually spread to other locations over time  In week four shown on the bottom left the coverage distribution pattern was very similar to that of the first week shown on the top left  This can again be explained by Hurricane Rita hitting the region around that time.
week week shown	 In week four shown on the bottom left the coverage distribution pattern was very similar to that of the first week shown on the top left.
Hurricane region location time topical patterns	 This can again be explained by Hurricane Rita hitting the region around that time  These results show that CPLSA can leverage location and time as context to reveal interesting topical patterns in text data.
results time context patterns	 These results show that CPLSA can leverage location and time as context to reveal interesting topical patterns in text data.
Note applied kinds reveal patterns In 19.	 Note that the CPLSA model is completely general so it can be easily applied to other kinds of text data to reveal similar spatiotemporal patterns or topical patterns  In Figure 19.
	 In Figure 19 9 we show yet another application of CPLSA for analysis of the impact of an event.
application CPLSA analysis impact	9 we show yet another application of CPLSA for analysis of the impact of an event.
The views covered assumed related	 The basic idea is to compare the views of topics covered in text before and after an event so as to reveal any difference  The difference can be assumed to be potentially related to the impact of the event.
potentially event.	 The difference can be assumed to be potentially related to the impact of the event.
discovered information focusing shown left goal	 The results shown here are the topics discovered from research articles on information retrieval particularly SIGIR papers  The topic we are focusing on is about the retrieval models shown on the left  The goal is to analyze the impact of two events.
The	 The goal is to analyze the impact of two events.
	S.
impact research retrieval.	 government which is known to have made a huge impact on the topics of research in information retrieval.
To impact use contexts	 To understand the impact of these two events we can use time periods before and after an event as different contexts and apply CPLSA.
g XML retrieval retrieval years.	g  XML retrieval email retrieval and subtopic retrieval which are connected to some tasks introduced in TREC over the years.
paper clear language parameter estimation language help potentially reveal impact event text	 The results on the bottom show that before the language modeling paper was published in 1998 the study of retrieval models focused on probabilistic logic and Boolean models but after 1998 there was a clear focus on language modeling approaches and parameter estimation which is an integral part of studies of language models for IR  Thus these results can help potentially reveal the impact of an event as reflected in the text data.
Analysis Social section discuss The context text form research form content networks social network Twitter Facebook.	4 Topic Analysis with Social Networks as Context In this section we discuss how to mine text data with a social network as context  The context of a text article can sometimes form a network the authors of research articles might form collaboration networks  Similarly authors of social media content might form social networks via a social network platform such as Twitter or Facebook.
context text form network research form collaboration networks.	 The context of a text article can sometimes form a network the authors of research articles might form collaboration networks.
authors social media form social networks social network	 Similarly authors of social media content might form social networks via a social network platform such as Twitter or Facebook.
friends.	 For example in Twitter people might follow each other whereas in Facebook people might be friends.
Such context authors network text form ical add data	 Such a network context can indirectly connect the content written by the authors involved in a network  Similarly locations associated with text can also be connected to form geograph ical networks which again add indirect connections between text data that would otherwise not be directly connected.
locations text connected ical networks In general imagine data form network relations interesting analyze	 Similarly locations associated with text can also be connected to form geograph ical networks which again add indirect connections between text data that would otherwise not be directly connected  In general you can imagine the metadata of the text data can easily form a network if we can identify some relations among the metadata  When we have network context available it offers an interesting opportunity to jointly analyze text and its associated network context.
general imagine metadata network interesting opportunity jointly analyze associated	 In general you can imagine the metadata of the text data can easily form a network if we can identify some relations among the metadata  When we have network context available it offers an interesting opportunity to jointly analyze text and its associated network context.
When interesting associated context joint	 When we have network context available it offers an interesting opportunity to jointly analyze text and its associated network context  The benefit of such a joint analysis is as follows.
The benefit joint First network	 The benefit of such a joint analysis is as follows  First we can use a network to impose some constraints on topics of text.
use impose constraints topics text it’s connected networks tend write	 First we can use a network to impose some constraints on topics of text  For example it’s reasonable to assume that authors connected in collaboration networks tend to write about similar topics.
it’s assume topics.	 For example it’s reasonable to assume that authors connected in collaboration networks tend to write about similar topics  Such heuristics can be used to guide us in analyzing topics.
Second	 Second text can also help characterize the content associated with each subnetwork.
The model Figure 19.	 The general idea of such a model is illustrated in Figure 19.
model	10  First we can view any generative model e.
defining optimization problem parameters denoted function likelihood function.	g  PLSA as defining an optimization problem where the variables are the parameters denoted by  here in the figure and the objective function is the likelihood function.
problem parameters figure function From perspective text ∗ algorithm based model.	 PLSA as defining an optimization problem where the variables are the parameters denoted by  here in the figure and the objective function is the likelihood function  From the perspective of mining text data the estimated parameter values ∗ can be regarded as the output of the mining algorithm based on the model.
context text preferences application This lead problem objective optimize.	 With this view we can then potentially use any context information of the text data to impose constraints or preferences on the parameters so as to incorporate domain knowledge or any preferences dictated by the user or application  This would lead to a modification to the original optimization problem such that the likelihood is no longer the only objective to optimize.
This lead modification objective optimize joint network context use parameters For example nodes network assumed cover similar topics.	 This would lead to a modification to the original optimization problem such that the likelihood is no longer the only objective to optimize  Following this thinking the main idea of performing joint analysis of text and associated network context is to use the network to impose some constraints on the model parameters  For example the text at adjacent nodes of the network can be assumed to cover similar topics.
For similar topics.	 For example the text at adjacent nodes of the network can be assumed to cover similar topics.
tend cover authors collaborating tend papers Such preference essentially topic adjacent distributions.	 Indeed in many cases they do tend to cover similar topics two authors collaborating with each other tend to publish papers on similar topics  Such a constraint or preference would essentially attempt to smooth the topic distributions on the graph or network so that adjacent nodes will have very similar topic distributions.
essentially attempt smooth topic nodes	 Such a constraint or preference would essentially attempt to smooth the topic distributions on the graph or network so that adjacent nodes will have very similar topic distributions.
expect distribution slight variations add regularizer objective 10.	 This means we expect them to share a common distribution on the topics or have just slight variations of the topic coverage distribution  We add a networkinduced regularizer to the likelihood objective function as shown in Figure 19 10.
10.	10.
That instead pTextData f likelihood regularizer r prefer network context optimizing new function f seek compromise maximize likelihood Thus view impact context model parameters optimization parameter values explicitly	 That is instead of optimizing the probability pTextData we will optimize another function f  which combines the likelihood function pTextData   with a regularizer r Network defined based on whatever prefer ences we can derive from the network context  When optimizing the new objective function f  we would seek a compromise between parameter values that maximize the likelihood and those that satisfy our regularization constraints or preferences  Thus we may also view the impact of the network context as imposing some prior on the model parameters if we view the new optimization problem conceptually as Bayesian inference of parameter values even though we do not have any explicitly defined prior distribution of parameters.
When optimizing objective values maximize likelihood regularization constraints network prior parameters view new conceptually Bayesian defined parameters.	 When optimizing the new objective function f  we would seek a compromise between parameter values that maximize the likelihood and those that satisfy our regularization constraints or preferences  Thus we may also view the impact of the network context as imposing some prior on the model parameters if we view the new optimization problem conceptually as Bayesian inference of parameter values even though we do not have any explicitly defined prior distribution of parameters.
The regularizer capture different particular cation multiple regularizers Finally f vary different func regularizers Another variation based	 The regularizer can also be any regularizer that we would like to use to capture different heuristics suitable for a particular appli cation it may even be a combination of multiple regularizers  Finally the function f can also vary allowing for many different ways to combine the likelihood func tion with the regularizers  Another variation is to specify separate constraints that must be satisfied based on network context making a constrained optimization problem.
allowing different ways specify constraints satisfied making constrained problem.	 Finally the function f can also vary allowing for many different ways to combine the likelihood func tion with the regularizers  Another variation is to specify separate constraints that must be satisfied based on network context making a constrained optimization problem.
idea practice challenge lies idea specific op remain tractable instantiation shown incor implementing distributions.	 Although the idea is quite general in practice the challenge often lies in how to instantiate such a general idea with specific regularizers so as to make the op timization problem remain tractable  Below we introduce a specific instantiation called NetPLSA shown in Figure 19 11 which is an extension of PLSA to incor porate network context by implementing the heuristic that the neighbors on the network must have similar topic distributions.
PLSA incor porate implementing heuristic neighbors distributions	11 which is an extension of PLSA to incor porate network context by implementing the heuristic that the neighbors on the network must have similar topic distributions  As shown in Figure 19.
new weighted sum standard PLSA function regularizer parameter λ ∈ 0 1 weight regularizer Clearly PLSA.	11 the new modified objective function is a weighted sum of the standard PLSA likelihood function and a regularizer where the parameter λ ∈ 0 1 controls the weight on the regularizer  Clearly if λ  0 the model reduces to the standard PLSA.
In constraint topic probabilities neighboring u ∑k j1pθj u pθj strongly u pθj similar values regularization term network context	 In the regularizer we see that the main constraint is the square loss defined on the difference of the topic selection probabilities of the two neighboring nodes u and v ∑k j1pθj  u − pθj  v2 which strongly prefers to give pθj  u and pθj  v similar values  In front of this regularization term we see a weight wu v which is based on our network context where the edges may be weighted.
nodes important ensure nodes topics In weighted v edge u wu v essentially edges	 This weight states that the more connected the two nodes are the more important it is to ensure the two nodes have similar topics  In the case when the edges are not weighted we may set wu v  1 if there exists an edge between u and v and wu v  0 otherwise essentially to keep only the regularizer for edges that exist on the graph.
In case weighted wu u wu Note sign want minimize loss regularizer.	 In the case when the edges are not weighted we may set wu v  1 if there exists an edge between u and v and wu v  0 otherwise essentially to keep only the regularizer for edges that exist on the graph  Note that there’s a negative sign in front of the regularizer because while we want to maximize the likelihood part we want to minimize the loss defined by the regularizer.
sign regularizer likelihood minimize defined	 Note that there’s a negative sign in front of the regularizer because while we want to maximize the likelihood part we want to minimize the loss defined by the regularizer.
Such modified problem solved EM EM algorithm maximum function finds value increase auxiliary function fact function	 Such a modified optimization problem can still be solved using a variant of the EM algorithm called General EM where in the Mstep the algorithm does not attempt to find a maximum of the auxiliary function but instead just finds a new parameter value that would increase the value of the auxiliary function thus also ensuring an increase of the likelihood function due to the fact that the auxiliary function is a lower bound of the original function see Section 17.
hill algorithm guarantee convergence local	3 5 on the EM algorithm for more explanation about this  The whole algorithm is still a hill climbing algorithm with guarantee of convergence to a local maximum.
EM	5 on the EM algorithm for more explanation about this.
algorithm guarantee local Figure 19.	 The whole algorithm is still a hill climbing algorithm with guarantee of convergence to a local maximum  In Figure 19.
19.	 In Figure 19.
topics discovered standard PLSA data DBLP consists research mining machine ML Web.	12 we show the four major topics discovered using the standard PLSA from a bibliographic database data set DBLP which consists of titles of papers from four research communities including information retrieval IR data mining DM machine learning ML and World Wide Web Web.
constructed goal successfully learn topics aligned communities PLSA.	 The data set has been constructed by pooling together papers from these research communities and our goal is to see if NetPLSA can more successfully learn topics well aligned to the communities than the standard PLSA.
	 The results in Figure 19.
The mixed statistics insufficient	 The reason was because they are all mixed together and there are many words that are shared by these communities and the cooccurrence statistics in the data are insufficient for separating them.
In contrast NetPLSA Figure 19 13 meaningful communities discover data easy communities shown table.	 In contrast the results of NetPLSA shown in Figure 19 13 are much more meaningful and the four topics correspond well to the four communities that we intend to discover from the data set  Indeed it is very easy to label them with the four communities as shown in the table.
topics correspond communities data set.	13 are much more meaningful and the four topics correspond well to the four communities that we intend to discover from the data set.
The reason separate discover network collaboration impose similar topics discovered better topics worked authors involved network topics coherent better represented	 The reason why NetPLSA can separate these communities well and discover more meaningful topics is because of the influence of the network context  Since our network is the collaboration network of authors when we impose the preference for two nodes connected in the network to have similar topics the model would further tune the discovered topics to better reflect the topics worked on by authors involved in the same collaboration network  As a result the topics would be more coherent and also better correspond to the communities represented by subnetworks of collaboration.
collaboration authors impose preference nodes connected similar tune discovered topics reflect involved network As better correspond communities collaboration.	 Since our network is the collaboration network of authors when we impose the preference for two nodes connected in the network to have similar topics the model would further tune the discovered topics to better reflect the topics worked on by authors involved in the same collaboration network  As a result the topics would be more coherent and also better correspond to the communities represented by subnetworks of collaboration.
Taking general living rich information environment That connect related data big network data For data associated network case NetPLSA discussed.	 Taking a more general view of text mining in the context of networks we can treat text data as living in a rich information network environment  That is we can connect all the related data together as a big network and associate text data with various structures in the network  For example text data can be associated with the nodes of the network such a case can be analyzed by using NetPLSA as we have just discussed.
related big text associated nodes	 That is we can connect all the related data together as a big network and associate text data with various structures in the network  For example text data can be associated with the nodes of the network such a case can be analyzed by using NetPLSA as we have just discussed.
associated analyzed NetPLSA discussed However text data edges paths discover topics comparative analysis 19.	 For example text data can be associated with the nodes of the network such a case can be analyzed by using NetPLSA as we have just discussed  However text data can also be associated with edges in a network paths or even subnetworks to help discover topics or perform comparative analysis  19.
Analysis Time Context text happened world special case text time	 19 5 Topic Analysis with Time Series Context In many applications we may be interested in mining text data to understand events that happened in the real world  As a special case we may be interested in using text mining to understand a time series.
As interested text mining series.	 As a special case we may be interested in using text mining to understand a time series.
interested relevant topics news stream fluctuation Prediction measures people’s opinions presidential cases problem topics term non rigorous refer time series	 Similarly one might also be interested in understanding what topics reported in the news stream were relevant for a presidential election and thus interested in finding topics in news stream that are correlated with the fluctuation of the Presidential Prediction Market which measures people’s opinions toward each presidential candidate  All these cases are special cases of a general problem of joint analysis of text and a time series to discover causal topics  Here we use the term causal in a non rigorous way to refer to any topic that might be related to the time series and thus can be potentially causal.
All special cases joint analysis text time series	 All these cases are special cases of a general problem of joint analysis of text and a time series to discover causal topics.
This analysis task 19.	 This analysis task is illustrated in Figure 19.
	14.
The input series text time stream.	 The input includes a time series plus text data that are produced in the same time period also known as a companion text stream.
The series regarded analyzing text The want coverage text stream strong correlations mentioned frequently text stream variable lower values.	 The time series can be regarded as a context for analyzing the text data  The output that we want to generate is the topics whose coverage in the text stream has strong correlations with the time series  That is whenever the topic is mentioned frequently in the text stream the time series variable tends to have higher or lower values.
generate topics series.	 The output that we want to generate is the topics whose coverage in the text stream has strong correlations with the time series.
output	 They can also be useful features for predicting time series  Intuitively the output is similar to what we generate by using a topic model but with an important difference.
topic regular topic modeling goal best explain content text data topics meaning coherent topic external To idea apply number topics coverage	 Intuitively the output is similar to what we generate by using a topic model but with an important difference  In regular topic modeling our goal is to discover topics that best explain the content in the text data but in our setup of discovering causal topics the topics to be discovered should not only be semantically meaning ful and coherent as in the case of regular topic modeling but also be correlated with the external time series  To solve this problem a natural idea is to apply a model such as CPLSA to our text stream so as to discover a number of topics along with their coverage over time.
idea discover series topic coverage text trends Figure	 To solve this problem a natural idea is to apply a model such as CPLSA to our text stream so as to discover a number of topics along with their coverage over time  This would allow us to obtain a time series for each topic representing its coverage in the text such as the temporal trends shown in Figure 19.
This obtain time series topic representing coverage trends Figure 19 choose set strongest correlation time series.	 This would allow us to obtain a time series for each topic representing its coverage in the text such as the temporal trends shown in Figure 19 7  We can then choose the topics from this set that have the strongest correlation with the external time series.
7 set correlation external	7  We can then choose the topics from this set that have the strongest correlation with the external time series.
series.	 We can then choose the topics from this set that have the strongest correlation with the external time series.
optimal based text data e.	 However this approach is not optimal because the content of the topics would have been discovered solely based on the text data e.
g likelihood all.	g  maximizing the likelihood function without consideration of the time series at all.
maximizing function time tend topics explain data correlated	 maximizing the likelihood function without consideration of the time series at all  Indeed the discovered topics would tend to be the major topics that explain the text data well as they should be but they are not necessarily correlated with time series.
topics topics text correlated time	 Indeed the discovered topics would tend to be the major topics that explain the text data well as they should be but they are not necessarily correlated with time series.
choose correlated topics perspective discovering causal topics improve use select time series content	 Even if we choose the best ones from them the most correlated topics might still have a low correlation and thus not be very useful from the perspective of discovering causal topics  One way to improve this simple approach is to use time series context to not only select the topics with the highest correlations with the time series but also influence the content of topics.
One simple approach use time correlations influence	 One way to improve this simple approach is to use time series context to not only select the topics with the highest correlations with the time series but also influence the content of topics.
One Causal Modeling shown	 One approach is called Iterative Causal Topic Modeling shown in Figure 19 15.
approach topics topic time series prior Specifically shown input regular number	 The idea of this approach is to do an iterative adjustment of topics discovered by topic models using time series to induce a prior  Specifically as shown in Figure 19 15 we first take the text stream as input and apply regular topic modeling to generate a number of topics four shown here.
shown 19 stream generate here.	 Specifically as shown in Figure 19 15 we first take the text stream as input and apply regular topic modeling to generate a number of topics four shown here.
Next use topic external time causality Granger correlated topic 3 earlier stopped topics 4 potential causal topics.	 Next we use the external time series to assess which topic is more causally related correlated with the external time series by using a causality measure such as Granger Test  For example in this figure topic 1 and topic 4 may be more correlated than topic 2 and topic 3  The simple approach that we discussed earlier would have just stopped here and taken topics 1 and 4 as potential causal topics.
For example figure topic 4 topic The approach discussed earlier stopped 1	 For example in this figure topic 1 and topic 4 may be more correlated than topic 2 and topic 3  The simple approach that we discussed earlier would have just stopped here and taken topics 1 and 4 as potential causal topics.
The approach taken topics 4 potential causal topics However zooming level words correlated series Specifically word words highest correlation	 The simple approach that we discussed earlier would have just stopped here and taken topics 1 and 4 as potential causal topics  However here we go further to improve them by zooming into the word level to further identify the words that are most strongly correlated with the time series  Specifically we can look into each word in the top ranked words for each topic those with highest probabilities and compute the correlation of each word with the time series.
words topic correlation	 Specifically we can look into each word in the top ranked words for each topic those with highest probabilities and compute the correlation of each word with the time series.
This allow strongly positively correlated strongly weakly related words.	 This would allow us to further separate those words into three groups strongly positively correlated words strongly negatively correlated words and weakly cor related words.
The groups define subtopics expected positively correlated series The figure potential correlated subtopics w1 w4 negative semantically.	 The first two groups can then each be regarded as seeds to define two new subtopics that can be expected to be positively and negatively correlated with the time series respectively  The figure shows a potential split of topic 1 into two such potentially more correlated subtopics one with w1 and w3 positive and one with w2 and w4 negative  However these two subtopics may not necessarily be coherent semantically.
However coherent	 However these two subtopics may not necessarily be coherent semantically.
improve coherence directly feed prior model steer model subtopics expect topic iteration time	 To improve the coherence the algorithm would not take these directly as topics but rather feed them as a prior to the topic model so as to steer the topic model toward discovering topics matching these two subtopics  Thus we can expect the topics discovered by the topic model in the next iteration to be more correlated with the time series than the original topics discovered from the previous iteration.
discover new generation process generate seed fed	 Once we discover a new generation of topics we can repeat the process to analyze the words in correlated topics and generate another set of seed topics which would then be fed into the topic model again as prior.
The process seen optimizing causal topic model topics positively negatively subtopics essentially tion illustrated Figure 19	 The whole process is seen as a heuristic way of optimizing causality and coher ence which is precisely our goal in discovery of causal topics  When applying the topic model we ensure the semantic coherence in the discovered topics but when splitting a topic into positively and negatively subtopics we improve the correlation with the time series essentially iteratively improving both coherence and correla tion causality as illustrated in Figure 19 16.
applying topics topic negatively subtopics improve time essentially correla causality 19.	 When applying the topic model we ensure the semantic coherence in the discovered topics but when splitting a topic into positively and negatively subtopics we improve the correlation with the time series essentially iteratively improving both coherence and correla tion causality as illustrated in Figure 19.
Here pure good topic scoring meaningful If generate set words time yaxis semantically Our goal scores	 Here we see that the pure topic models will be very good at maximizing topic coherence thus scoring high on the xaxis meaning the discovered topics will all be meaningful  If we only use a causality test or correlation measure then we would generate a set of words that are strongly correlated with the time series thus scor ing high on the yaxis causality but they aren’t necessarily coherent semantically  Our goal is to have a causal topic that scores high in both topic coherence and correlation.
If use test generate series yaxis aren’t necessarily coherent semantically Our scores topic coherence The approach alternate way maximize axes.	 If we only use a causality test or correlation measure then we would generate a set of words that are strongly correlated with the time series thus scor ing high on the yaxis causality but they aren’t necessarily coherent semantically  Our goal is to have a causal topic that scores high in both topic coherence and correlation  The approach discussed above can be regarded as an alternate way to maximize both axes.
Our causal topic coherence	 Our goal is to have a causal topic that scores high in both topic coherence and correlation.
discussed axes maximizing herence topic model series words time series Thus model causal	 The approach discussed above can be regarded as an alternate way to maximize both axes  When we apply the topic models we’re maximizing the co herence while when we decompose the topic model words into sets of words that are very strongly correlated with the time series we would select the most strongly correlated words with the time series  Thus we are in effect pushing the model back to the causal dimension to make it better in causal scoring.
decompose sets strongly strongly	 When we apply the topic models we’re maximizing the co herence while when we decompose the topic model words into sets of words that are very strongly correlated with the time series we would select the most strongly correlated words with the time series.
Thus pushing dimension causal	 Thus we are in effect pushing the model back to the causal dimension to make it better in causal scoring.
selected guide topic optimize coherence iterative pected compromise semantic correlation	 When we apply the selected words as a prior to guide topic models in topic discovery we again go back to optimize the coherence  Eventually such an iterative process can be ex pected to reach a compromise of semantic coherence and strong correlation with time series.
ex pected reach compromise strong correlation time series.	 Eventually such an iterative process can be ex pected to reach a compromise of semantic coherence and strong correlation with time series.
This relies components model causality measure book There ways measure causality time series.	 This general approach relies on two specific technical components a topic model and a causality measure  The former has already been introduced earlier in the book so we briefly discuss the latter  There are various ways to measure causality between two time series.
book briefly time series.	 The former has already been introduced earlier in the book so we briefly discuss the latter  There are various ways to measure causality between two time series.
The Pearson	 There are various ways to measure causality between two time series  The simplest measure is Pearson correlation.
Pearson correlation It gives value exploit quantify impact case causal relation.	 The simplest measure is Pearson correlation  Pearson correlation is one of the most common methods used to measure the correlation between two variables  It gives us a correlation value in the range of −1 1 and the sign of the output value indicates the orientation of the correlation which we will exploit to quantify the impact in the case of a causal relation.
We directly basic correlation zero lag compares	 We can also measure the significance of the correlation value  If used directly the basic Pearson correlation would have zero lag because it compares values on the same time stamp.
If directly compares However input variables correlation shift A common time	 If used directly the basic Pearson correlation would have zero lag because it compares values on the same time stamp  However we can compute a lagged correlation by shifting one of the input time series variables by the lag and measuring the Pearson correlation after the shift  A more common method for causality test on time series data is the Granger Test.
series variables shift.	 However we can compute a lagged correlation by shifting one of the input time series variables by the lag and measuring the Pearson correlation after the shift.
causality series Test test performs significance time time series.	 A more common method for causality test on time series data is the Granger Test  The Granger test performs a statistical significance test with different time lags by using autoregression to see if one time series has a causal relationship with another series.
performs significance time causal relationship series yt tested hope causality yt basic Granger a0 a1yt−1	 The Granger test performs a statistical significance test with different time lags by using autoregression to see if one time series has a causal relationship with another series  Formally let yt and xt be two time series to be tested where we hope to see if xt has Granger causality for yt with a maximum p time lag  The basic formula for the Granger test is the following yt  a0  a1yt−1  .
yt time tested xt causality yt maximum time The Granger a0 .	 Formally let yt and xt be two time series to be tested where we hope to see if xt has Granger causality for yt with a maximum p time lag  The basic formula for the Granger test is the following yt  a0  a1yt−1  .
test yt a0	 The basic formula for the Granger test is the following yt  a0  a1yt−1      .
.	    apyt−p  b1xt−1    .
b1xt−1 .	  apyt−p  b1xt−1      .
bpxt−p.	      bpxt−p.
	 19.
test test naturally gives significance value	 Because the Granger test is essentially an F test it naturally gives us a significance value of causality.
” impact values seed correlated words higher prior pass We results generated potentially sample topics discovered news set series	” The impact values can be used to assign weights to the selected seed words so that highly correlated words would have a higher probability in the prior that we pass to topic modeling  We now show some sample results generated by this approach to illustrate the applications that it can potentially support  First we show a sample of causal topics discovered from a news data set when using two different stock time series as context in Figure 19.
The data time June December 2011.	 The text data set here is the New York Times news articles in the time period of June 2000 through December 2011.
time stock American AAMRQ Apple	 The time series used is the stock prices of two companies American Airlines AAMRQ and Apple Inc.
use model data obtain neutral We topics corresponding time series	 AAPL  If we are to use a topic model to mine the news data set to discover topics we would obtain topics that are neutral to both American Airlines and Apple  We would like to see whether we can discover biased topics toward either Amer ican Airlines or Apple when we use their corresponding time series as context.
discover biased Amer ican Airlines Apple use corresponding iterative topic modeling ates topics	 We would like to see whether we can discover biased topics toward either Amer ican Airlines or Apple when we use their corresponding time series as context  The results here show that the iterative causal topic modeling approach indeed gener ates topics that are more tuned toward each stock time series.
iterative approach series relevant including g.	 The results here show that the iterative causal topic modeling approach indeed gener ates topics that are more tuned toward each stock time series  Specifically on the left column we see topics highly relevant to American Airlines including e g.
topics American Airlines including g.	 Specifically on the left column we see topics highly relevant to American Airlines including e g.
g topic terrorism September 11th attack column topics clearly including technology Internet.	g  a topic about airport and airlines and another about terrorism the topic is relevant because of the September 11th terrorist attack in 2001 which negatively impacted American Airlines though the correlation of other topics with American Airlines stock is not obvious  In contrast on the right column we see topics that are clearly more related to AAPL including a topic about computer technology and another about the Web and Internet.
airlines terrorism attack 2001 correlation topics American Airlines contrast right including topic topics use series context discovered discovery topics series.	 a topic about airport and airlines and another about terrorism the topic is relevant because of the September 11th terrorist attack in 2001 which negatively impacted American Airlines though the correlation of other topics with American Airlines stock is not obvious  In contrast on the right column we see topics that are clearly more related to AAPL including a topic about computer technology and another about the Web and Internet  While not all topics can be easily interpreted it is clear the use of the time series as context has impacted the topics discovered and enabled discovery of specific topics that are intuitively related to the time series.
In right column topics related technology topics interpreted clear use series context discovery topics intuitively time series.	 In contrast on the right column we see topics that are clearly more related to AAPL including a topic about computer technology and another about the Web and Internet  While not all topics can be easily interpreted it is clear the use of the time series as context has impacted the topics discovered and enabled discovery of specific topics that are intuitively related to the time series.
While topics easily use series context enabled intuitively series These topics look topics series These results important causal topics model stock	 While not all topics can be easily interpreted it is clear the use of the time series as context has impacted the topics discovered and enabled discovery of specific topics that are intuitively related to the time series  These topics can serve as entry points for analysts to further look into the details for any potential causal relations between topics and time series  These results also clearly suggest the important role that humans must play in any real application of causal topic discovery but these topics can be immediately used as features in a predictive model for predicting stock prices.
reasonable assume topics better simple features topics discovered collection considering series context.	 It is reasonable to assume that some of these topics would make better features than simple features such as ngrams or ordinary topics discovered from the collection without considering the time series context.
results presidential time data Iowa Electronic market New Times data May–October matched i.	18 we see some additional results from analyzing presidential election time series  The time series data used here is from the Iowa Electronic market and the text data is the New York Times data from May–October 2000 that matched at least one candidate’s name i.
market text New Times 2000 matched candidate’s e Bush Gore.	 The time series data used here is from the Iowa Electronic market and the text data is the New York Times data from May–October 2000 that matched at least one candidate’s name i e  either Bush or Gore.
e	e  either Bush or Gore.
The causal help issues	 either Bush or Gore  The goal here was to see if we can use causal topic mining to help understand what issues mattered in the 2000 presidential campaign and election.
use help understand issues	 The goal here was to see if we can use causal topic mining to help understand what issues mattered in the 2000 presidential campaign and election.
campaign.	 Intuitively they are indeed quite related to the campaign.
relevance presidential candidate names additional	 The high relevance of topics discovered is at least partly due to the use of the presidential candidate names as an additional context i.
non text	 as filters which helped eliminate a lot of non relevant text data.
list contain presidential election et discussion	 However what is interesting is that the list does contain a few topics that are known to be important in that presidential election including tax cut oil energy abortion and gun control see Kim et al  2013 for a more detailed discussion of the results  19.
results.	 2013 for a more detailed discussion of the results.
general analyzing text manner based big	6 Summary In this chapter we discussed the general problem of analyzing both structured data and unstructured text data in a joint manner which is needed for predictive modeling based on big data.
help new Joint Analysis Structured Data discussed based prediction useful making especially nontext applications Nontext context mining data patterns nontext tion.	 Since textbased prediction can help us infer new knowledge about the world 440 Chapter 19 Joint Analysis of Text and Structured Data some of which can even go beyond what’s discussed in the content of text text based prediction is often very useful for optimizing our decision making especially when combined with other nontext data that are often also available and it has widespread applications  Nontext data can provide a context for mining text data while text data can also help interpret patterns discovered from nontext data such as pattern annota tion.
Nontext provide data help discovered annota The nontext frontier.	 Nontext data can provide a context for mining text data while text data can also help interpret patterns discovered from nontext data such as pattern annota tion  The joint analysis of text and nontext data is a relatively new active research frontier.
The joint analysis text relatively new research chapter covered analysis data contextual probabilistic latent semantic context variables location directly topic topic uses structure regularize topic discovery time discovering text Due provided brief ideas included sample results enable.	 The joint analysis of text and nontext data is a relatively new active research frontier  In this chapter we covered a number of general techniques that com bine topic analysis with nontext data including contextual probabilistic latent semantic analysis CPLSA that embeds context variables such as time and location directly in a topic model networksupervised topic modeling that uses companion networkgraph structure of text data to regularize topic discovery and time series as context for discovering potentially causal topics from text data  Due to space limitations we only provided a brief introduction to the highlevel ideas of these ap proaches without elaboration but we have included sample results of all of them to help understand the potential applications that they can enable.
number topic analysis nontext probabilistic time location networksupervised topic uses companion data time series potentially text	 In this chapter we covered a number of general techniques that com bine topic analysis with nontext data including contextual probabilistic latent semantic analysis CPLSA that embeds context variables such as time and location directly in a topic model networksupervised topic modeling that uses companion networkgraph structure of text data to regularize topic discovery and time series as context for discovering potentially causal topics from text data.
space limitations highlevel ideas elaboration sample help understand applications	 Due to space limitations we only provided a brief introduction to the highlevel ideas of these ap proaches without elaboration but we have included sample results of all of them to help understand the potential applications that they can enable.
general potentially different domains Reading The dissertation Mei excellent discussion contextual explorations context topic modeling Specifically semantic 2006 topic network context Mei et al.	 These approaches are all general and thus they can be potentially applied to many different domains  Bibliographic Notes and Further Reading The dissertation Mei 2009 has an excellent discussion of contextual text mining with many specific explorations of using context for text analysis notably with topic modeling  Specifically both the contextual probabilistic latent semantic analysis model Mei and Zhai 2006 and topic modeling with network as context Mei et al.
2008 discussed iterative et al 2013.	 2008 are discussed in detail in the dissertation  The iterative topic modeling ap proach with time series for supervision is described in Kim et al  2013.
The iterative topic proach Kim et applications 2010.	 The iterative topic modeling ap proach with time series for supervision is described in Kim et al  2013  A general discussion of textdriven forecasting and its many applications can be found in Smith 2010.
general discussion textdriven forecasting applications	 2013  A general discussion of textdriven forecasting and its many applications can be found in Smith 2010.
discussion forecasting applications 2010.	 A general discussion of textdriven forecasting and its many applications can be found in Smith 2010.
2009 Unified System Management previous introduced specific managing analyzing data These algorithms separate toolkits develop unified support	 2009  20Toward A Unified System for Text Management and Analysis In the previous chapters we introduced many specific algorithms and techniques for managing and analyzing text data  These algorithms are currently implemented in separate toolkits but from an application perspective it would be beneficial to develop a unified system that can support potentially all these algorithms in a general way so that it can be used in many different applications.
separate potentially algorithms general	 These algorithms are currently implemented in separate toolkits but from an application perspective it would be beneficial to develop a unified system that can support potentially all these algorithms in a general way so that it can be used in many different applications.
In chapter model different algorithms unified analysis framework potentially	 In this chapter we discuss how we can model these different algorithms as operators in a unified analysis framework and potentially develop a general system to implement such a framework.
goal text data access enable users cation support	 The goal of text data access is to enable users to identify and obtain the most useful text data relevant to an appli cation problem which is often achieved through the support of a search engine.
The current search primarily support way relevant documents Browsing users relevant	 The current search engines primarily support querying which is however only one way to help users find relevant documents  Browsing is another useful form of information access where the users can take the initiative to navigate through a structure into relevant information.
Browsing information structure relevant browsing generally achieved text data	 Browsing is another useful form of information access where the users can take the initiative to navigate through a structure into relevant information  The support of browsing is generally achieved by adding structures to text data by using clustering categorization or topic analy sis.
text humans meant current comput accurate understanding generally necessary 446 Toward Text text data In sense analysis like leverage intelligence optimize making Thus	 Since text data are created by humans and often meant to be consumed by humans—and the current NLP techniques are still not mature enough for comput ers to have accurate understanding of text data—it is generally necessary to involve 446 Chapter 20 Toward A Unified System for Text Management and Analysis humans in any text data application  In this sense a text data management and analysis system should serve as an intelligent assistant for users requiring informa tion or the analysts that would like to leverage text data for intelligence to optimize decision making  Thus it is quite important to optimize the collaboration of humans and ma chines.
sense data management analysis intelligent users requiring informa tion like optimize decision	 In this sense a text data management and analysis system should serve as an intelligent assistant for users requiring informa tion or the analysts that would like to leverage text data for intelligence to optimize decision making.
means computer’s ability handle large text data detailed language engage users com accomplishing goal user’s problem	 This means that we should take advantage of a computer’s ability to handle large amounts of text data while exploiting humans’ expertise in detailed language understanding and assessing knowledge for decision making  Supporting an inter active process to engage users in a “dialogue” with the intelligent text information system is generally beneficial as it enables the users and system to have more com munications between each other and assist each other to work together toward accomplishing the common goal of solving a user’s problem by analyzing text data.
text information beneficial enables com work solving user’s	 Supporting an inter active process to engage users in a “dialogue” with the intelligent text information system is generally beneficial as it enables the users and system to have more com munications between each other and assist each other to work together toward accomplishing the common goal of solving a user’s problem by analyzing text data.
problem dividing work user machine extreme access support	 Looking at the problem in this way we can easily see the possibility of dividing the work between a human user and a machine in different ways  At one extreme we would mostly rely on users to complete access and analysis tasks and have the computer to do only the support that the computer can robustly provide.
support robustly provide scenario engine manual text	 At one extreme we would mostly rely on users to complete access and analysis tasks and have the computer to do only the support that the computer can robustly provide  This is the current scenario when people use a search engine to perform mostly manual text mining as shown in Figure 20.
For use web search information buried large Web information essentially manual mining In search plays important	1  For example we can all use a web search engine to help us get access to the most relevant information buried in the large amounts of text data on the Web and then read all the relevant information which is essentially manual text mining and analysis  In such a scenario the search engine plays two important roles.
For web engine relevant buried Web relevant manual mining scenario search engine enables identify relevant text data	 For example we can all use a web search engine to help us get access to the most relevant information buried in the large amounts of text data on the Web and then read all the relevant information which is essentially manual text mining and analysis  In such a scenario the search engine plays two important roles  For one it enables a user to quickly identify the most relevant text data which is often what we really need for solving a problem thus avoiding dealing with a huge amount of nonrelevant text data.
scenario search roles For user quickly identify relevant text solving	 In such a scenario the search engine plays two important roles  For one it enables a user to quickly identify the most relevant text data which is often what we really need for solving a problem thus avoiding dealing with a huge amount of nonrelevant text data.
identify need problem huge	 For one it enables a user to quickly identify the most relevant text data which is often what we really need for solving a problem thus avoiding dealing with a huge amount of nonrelevant text data.
Such tackling scalability caused sense easily navigate examine reliability depth.	 Such a strategy of data selection reduction is logically the very first step we should take when tackling the scalability problem caused by the size of data  Secondly it provides knowledge provenance in the sense that it allows a user to easily navigate into any source to examine its reliability in depth.
knowledge allows user navigate source extreme minimum support However	 Secondly it provides knowledge provenance in the sense that it allows a user to easily navigate into any source to examine its reliability in depth  At this extreme the computer has done the minimum if anything at all to support text analysis  However the system i.
At extreme minimum support text	 At this extreme the computer has done the minimum if anything at all to support text analysis.
e amounts quickly.	e  search engine is very robust and can handle large amounts of data quickly.
handle large data provide directly minimize	 search engine is very robust and can handle large amounts of data quickly  At the other extreme the system can attempt to provide task support directly to the user so as to minimize the user’s effort as illustrated in Figure 20.
Providing support large possible language lack requirements.	2  Providing such a support for arbitrary tasks or even a large class of tasks is im possible due to the inability of computers to understand natural language and the lack of knowledge about specific task requirements.
support arbitrary large tasks im possible computers understand natural lack specific specialized particular gen provide maximum inevitably task.	 Providing such a support for arbitrary tasks or even a large class of tasks is im possible due to the inability of computers to understand natural language and the lack of knowledge about specific task requirements  As a result we can only build very specialized predictive models for a particular problem where the features gen erally have to be designed by humans  This kind of system may provide maximum support for a decision task but the function of the system is inevitably restricted to a specific task.
result build predictive models particular features gen humans This decision function task.	 As a result we can only build very specialized predictive models for a particular problem where the features gen erally have to be designed by humans  This kind of system may provide maximum support for a decision task but the function of the system is inevitably restricted to a specific task.
provide inevitably task.	 This kind of system may provide maximum support for a decision task but the function of the system is inevitably restricted to a specific task.
Since requires use nontext requires additional analysis developing help users extract particular tion	 Since predictive modeling generally requires the use of machine learning techniques and combining features extracted from both text and nontext data it clearly requires additional support beyond text management and analysis  Yet we can envision the possibility of developing a relatively general text analysis system to help users identify and extract effective features for a particular predic tion task.
text analysis inevitably different requirements standardized	 However each specific text analysis application would inevitably have different requirements thus the opera tors must also be standardized and compatible with each other so that they can be flexibly combined to support potentially many different workflows.
data access querying operators text similar comprehensive package toolkits brings car	 Note that text data access functions such as querying browsing and recommendation can all be regarded as instances of specific operators to be “blended” with other operators such as categorization clustering or topic analysis  The benefit of such an operatorbased text analysis system for supporting text mining and analysis is similar to the benefit that a comprehensive package of carrepairing toolkits brings to car repairers.
The text similar comprehensive In problem fix repairers need combine ad hoc probe component Although actual varies specific car common set tools sufficient tasks.	 The benefit of such an operatorbased text analysis system for supporting text mining and analysis is similar to the benefit that a comprehensive package of carrepairing toolkits brings to car repairers  In order to diagnose the problem of a car and fix it repairers need to use many different tools and combine them in an ad hoc way to open up components in a car or probe a component with electrical testers  Although the actual workflow often varies depending on the specific problem and models of a car a common set of tools is often sufficient to accommodate all kinds of tasks.
need use combine actual workflow specific problem tools accommodate order digest relevant data analysts combine open text patterns useful tasks.	 In order to diagnose the problem of a car and fix it repairers need to use many different tools and combine them in an ad hoc way to open up components in a car or probe a component with electrical testers  Although the actual workflow often varies depending on the specific problem and models of a car a common set of tools is often sufficient to accommodate all kinds of tasks  In much the same way in order to integrate scattered information digest all the latent relevant knowledge about a problem in text data analysts can also combine various text analysis tools to open up complex connections between entities or topics buried in text data identify interesting patterns and compute useful features for prediction tasks.
actual workflow problem models kinds tasks.	 Although the actual workflow often varies depending on the specific problem and models of a car a common set of tools is often sufficient to accommodate all kinds of tasks.
digest knowledge problem text data text analysis entities text interesting patterns compute features prediction tasks case car workflow vary dramatically depending application common extensible Most introduced book potentially analysis tools unified way.	 In much the same way in order to integrate scattered information digest all the latent relevant knowledge about a problem in text data analysts can also combine various text analysis tools to open up complex connections between entities or topics buried in text data identify interesting patterns and compute useful features for prediction tasks  As in the case of car repairing although the actual workflow may vary dramatically depending on the specific application a common set of extensible and trainable analysis tools might also be sufficient to accommodate all kinds of text analysis applications  Most algorithms we introduced in this book can potentially serve as such analysis tools thus providing a basis for developing a unified system to enable analysts to perform text analysis in such a way.
As repairing actual workflow vary dramatically depending specific set extensible trainable tools sufficient text analysis algorithms introduced serve tools unified perform designing building	 As in the case of car repairing although the actual workflow may vary dramatically depending on the specific application a common set of extensible and trainable analysis tools might also be sufficient to accommodate all kinds of text analysis applications  Most algorithms we introduced in this book can potentially serve as such analysis tools thus providing a basis for developing a unified system to enable analysts to perform text analysis in such a way  Although there are still many challenges in designing and building such a system it is an important goal to work on.
20.	 20.
Similarly	 Similarly we may identify a common set of operators for supporting text analysis.
Naturally	 Naturally the most important data type is a TEXTOBJECT which can be defined as a sequence of words in a vocabulary set V .
word paragraph type TEXTOBJECT.	 Clearly a word a phrase a sentence a paragraph and an article can all be regarded as specific instances of the type TEXTOBJECT.
naturally derived data types based TEXTOBJECT including g naturally articles case sentence	 We can then also naturally define derived data types based on TEXTOBJECT including e g  TEXTOBJECTSET which naturally captures instances such as a collection of text articles a set of sentences which further covers a set of terms as a special case when a sentence is just a term.
	g.
captures text covers set special term TEXTOBJECTSEQUENCE care order data structures list sentences.	 TEXTOBJECTSET which naturally captures instances such as a collection of text articles a set of sentences which further covers a set of terms as a special case when a sentence is just a term  Another possibility is TEXTOBJECTSEQUENCE where we care about order of the text objects TEXTOBJECTSEQUENCE can capture interesting data structures such as a ranked list of articles or sentences.
Again ranked	 Again a special case is a ranked list of terms.
As spe case words text WEIGHTEDTEXTOBJECTSET WEIGHTEDTEXTOBJECTSEQUENCE results scores.	 As a spe cial case we can have words as text objects and thus have a word distribution represented as a WEIGHTEDTEXTOBJECTSET  WEIGHTEDTEXTOBJECTSEQUENCE can cover a ranked list of search results with scores.
cover need data	 WEIGHTEDTEXTOBJECTSEQUENCE can cover a ranked list of search results with scores  Second we need to define operators on top of various data types.
operators types.	 Second we need to define operators on top of various data types.
Here large operators depending specific analysis commonly operators potentially combine support different required	 Here we can potentially have a very large number of operators depending on specific text analysis algorithms  Here we briefly discuss a few most commonly used algorithms that we covered in the previous chapters of the book and show that even with a few operators we can already potentially combine them in many different ways to flexibly support very different workflows required in a particular application context.
Here potentially different flexibly support application For	 Here we briefly discuss a few most commonly used algorithms that we covered in the previous chapters of the book and show that even with a few operators we can already potentially combine them in many different ways to flexibly support very different workflows required in a particular application context  For example in Figure 20.
Since goal operators	 Since our goal is mainly to present some speculative ideas we present these operators in a mostly informal and nonrigorous way.
	 Select.
TEXTOBJECTSET.	 TEXTOBJECTSET → TEXTOBJECTSET.
The operator maps subset	 The Select operator maps a set of text objects into a subset of text objects.
additionally Ranking selection.	 We can additionally further apply a Ranking operator to each selection.
	 Split.
→ .	 TEXTOBJECTSET → TEXTOBJECTSET .
.	 .
TEXTOBJECTSET.	    TEXTOBJECTSET.
The maps objects.	  TEXTOBJECTSET  The Split operator maps a set of text objects into multiple subsets of text objects.
The Split operator maps text	 The Split operator maps a set of text objects into multiple subsets of text objects.
clustering sets.	 Text catego rization and text clustering are all instances of Split  Union and Intersection  these are standard set operators that can be applied to any sets.
standard applied	 these are standard set operators that can be applied to any sets.
	 Ranking.
As case query TopicExtraction.	 As a special case the WEIGHTED TEXTOBJECTSET can be a word distribution representing a query language model  TopicExtraction.
	 TopicExtraction.
→ The TopicExtraction text set	 TEXTOBJECTSET → TOPICSET  The TopicExtraction operator maps a set of text objects into a set of topics.
TopicExtraction operator maps set topics → TEXTOBJECTSET.	 The TopicExtraction operator maps a set of text objects into a set of topics  Interpret  TOPIC TEXTOBJECTSET → TEXTOBJECTSET.
The set text	 Interpret  TOPIC TEXTOBJECTSET → TEXTOBJECTSET  The Interpret operator maps a topic and a set of text objects into another set of text objects.
→ TEXTOBJECTSET.	 TOPIC TEXTOBJECTSET → TEXTOBJECTSET.
maps objects set text TEXTOBJECTSET	 The Interpret operator maps a topic and a set of text objects into another set of text objects  Compare  TEXTOBJECTSET .
	 Compare  TEXTOBJECTSET .
.	   .
TOPICSET The comparable common covered comparable sets text objects sets contextspecific topics.	  TOPICSET  The Compare operator maps a set of comparable sets of text objects into a set of common topics covered in all the comparable sets of text objects and sets of contextspecific topics.
Compare objects topics covered sets objects The formalization illustrated 20.	 The Compare operator maps a set of comparable sets of text objects into a set of common topics covered in all the comparable sets of text objects and sets of contextspecific topics  The formalization of some of the operators is illustrated in Figure 20.
word interesting For example	4 where θ denotes a weighted word vector  Even with these few operators we already have some interesting combinations  For example in Figure 20.
Even For example 20.	 Even with these few operators we already have some interesting combinations  For example in Figure 20.
For example 5 topic Figure 20.	 For example in Figure 20 5 we see an example of combination of multiple topic selections followed by a comparison operator which would then be followed by an Interpret operator  In Figure 20.
5 example combination multiple selections comparison followed 20 example Split operator Interpret	5 we see an example of combination of multiple topic selections followed by a comparison operator which would then be followed by an Interpret operator  In Figure 20 6 we show another example of combination of a Split operator followed by a comparison operator which would then be followed by an Interpret operator.
Figure 6 example Split operator followed	 In Figure 20 6 we show another example of combination of a Split operator followed by a comparison operator which would then be followed by an Interpret operator.
Split followed operator Interpret	6 we show another example of combination of a Split operator followed by a comparison operator which would then be followed by an Interpret operator.
It combine	 It is easy to imagine that there are many other ways to combine these operators  20.
20 general highlevel unified sup text Figure 20.	 20 2 System Architecture In general we can envision a highlevel architecture of a unified system for sup porting text management and analysis as shown in Figure 20.
highlevel architecture unified porting text management analysis Figure 7 levels provided users preprocessing step language lowlevel access browsing analysis includes general operators highlevel support includes applicationspecific user tasks It user lev unified interface working personalized specific application	2 System Architecture In general we can envision a highlevel architecture of a unified system for sup porting text management and analysis as shown in Figure 20 7 where multiple levels of services are provided to users including a preprocessing step of natural language processing a lowlevel service for multimode text data access which in cludes querying browsing and recommendation a mediumlevel service for text data analysis which includes general analysis operators that can be combined with each other and highlevel application support which includes support of applicationspecific user tasks  It is important that a user has access to all these lev els via a unified interaction interface where the user also has access to a working space that is personalized according to a specific user and a specific application task.
services users preprocessing language processing lowlevel multimode recommendation text analysis operators combined highlevel	7 where multiple levels of services are provided to users including a preprocessing step of natural language processing a lowlevel service for multimode text data access which in cludes querying browsing and recommendation a mediumlevel service for text data analysis which includes general analysis operators that can be combined with each other and highlevel application support which includes support of applicationspecific user tasks.
It important user access els unified interaction user space personalized according specific specific application availability generally serve mod ules analysis case nontext text process	 It is important that a user has access to all these lev els via a unified interaction interface where the user also has access to a working space that is personalized according to a specific user and a specific application task  In this figure we also show the availability of the nontextual data which would generally need a database system to manage it and serve some other mod ules such as text analysis in this case nontext data can be used as context for analyzing text data  Finally we see that we can often further apply general data mining algorithms such as EM and predictive modeling to process the results that our analysts have obtained.
Finally apply EM modeling results consider application performs large number documents	 Finally we see that we can often further apply general data mining algorithms such as EM and predictive modeling to process the results that our analysts have obtained  As an example consider a news mining application  If a user performs a keyword search a large number of documents containing matching text may be returned.
performs large containing	 If a user performs a keyword search a large number of documents containing matching text may be returned.
promising merged different	 If the user finds two promising clusters they can be merged together and searched again with different keywords.
3 META Unified section extending general unified data management	 20 3 META as a Unified System In this section we discuss some interesting possibilities of extending META to a general and unified text data management and analysis system.
In discuss META unified text data Figure 20.	3 META as a Unified System In this section we discuss some interesting possibilities of extending META to a general and unified text data management and analysis system  In one direction we can envision implementing the architecture shown in Figure 20.
direction envision implementing architecture shown Figure 8 search topic improved text summarization text common storage mechanism applications	 In one direction we can envision implementing the architecture shown in Figure 20 8 where we extend a search engine to support various topic models which can further enable improved text categorization text summarization and text clustering  Indexes forward and inverted are a common storage mechanism for most text mining applications in META.
forward inverted storage applied If stored	 Indexes forward and inverted are a common storage mechanism for most text mining applications in META  Analyzers and tokenizers are the first operators applied on text where various features are created and term IDs are assigned  If the terms are not stored in an index they are usually stored in a metasequence which maintains term order for further analysis.
Analyzers tokenizers text features IDs terms stored usually metasequence order Additionally text	 Analyzers and tokenizers are the first operators applied on text where various features are created and term IDs are assigned  If the terms are not stored in an index they are usually stored in a metasequence which maintains term order for further analysis  Additionally most text mining applications take an index as input.
stored index metasequence maintains	 If the terms are not stored in an index they are usually stored in a metasequence which maintains term order for further analysis.
Additionally structure tasks act way described	 Additionally most text mining applications take an index as input  This common structure for all tasks enables higherlevel “wrapper” functions to be implemented that act in the same way as previously described in this chapter.
common structure “wrapper” implemented act way	 This common structure for all tasks enables higherlevel “wrapper” functions to be implemented that act in the same way as previously described in this chapter.
represented META metacorpusdocument	 TEXTOBJECT is represented in META as a metacorpusdocument .
	 TEXTOBJECTSET is metaindexforwardindex or metaindex invertedindex .
One	 One example of a TEXTOBJECTSEQUENCE is metasequencesequence.
	 .
	 One example of a WEIGHTEDTEXTOBJECTSEQUENCE is the output from the score function in metaindexranker.
metacorpus metadata structured stored document.	 Using metacorpus metadata allows basic structured data to be stored for each document.
ad vanced structured like commands supported functions inmetaindexrankerandmetaclassifyclas 3 System 455 index structure facilitates mentioned earlier text mining run dataset.	 More ad vanced structured operations like those found in databases are not currently imple mented but simple filtering and grouping commands are easily supported such as the filtering functions inmetaindexrankerandmetaclassifyclas sifierknn  20 3 META as a Unified System 455 The index structure also facilitates the idea mentioned earlier advanced text mining techniques are run on a smaller relevant subset of the entire dataset.
455 earlier advanced relevant subset entire	 20 3 META as a Unified System 455 The index structure also facilitates the idea mentioned earlier advanced text mining techniques are run on a smaller relevant subset of the entire dataset.
algorithms Select sequence.	g  topic modeling algorithms on the relevant documents  Thus the analysis operators such as Select and Split take META’s index objects as input and return objects such as a sequence.
Thus index objects input sequence algorithms run engine Split	 topic modeling algorithms on the relevant documents  Thus the analysis operators such as Select and Split take META’s index objects as input and return objects such as a sequence  The algorithms that run for Select could be a filter or search engine and Split could be metatopicsldamodel or some other clustering algorithm.
	 A.
Beta Distribution 2 5 distribution A prior represent “prior model.	1 Binomial Estimation and the Beta Distribution From section 2 1 5 we already know the likelihood of our binomial distribution is but what about the prior pθ A prior should represent some “prior belief” about the parameters of the model.
know represent belief” model.	1 5 we already know the likelihood of our binomial distribution is but what about the prior pθ A prior should represent some “prior belief” about the parameters of the model.
pθ A prior “prior parameters	5 we already know the likelihood of our binomial distribution is but what about the prior pθ A prior should represent some “prior belief” about the parameters of the model.
For coin e binomial sense prior proportional powers 1 θ.	 For our coin flipping i e  binomial distribution it would make sense to have the prior also be proportional to the powers of θ and 1 − θ.
distribution sense prior powers pθ − θT θbT .	e  binomial distribution it would make sense to have the prior also be proportional to the powers of θ and 1 − θ  Thus the posterior will also be proportional to those powers pθ  D ∝ pθpD  θ θa1 − θbθH1 − θT θaH1 − θbT .
posterior D θ θa1 − θT θbT .	 Thus the posterior will also be proportional to those powers pθ  D ∝ pθpD  θ θa1 − θbθH1 − θT θaH1 − θbT .
called Beta distribution px α α xα−11 xβ−1.	 Luckily there is something called the Beta distribution  We say x ∼ Betaα  β if for x ∈ 0 1 px  α  β  α  β αβ xα−11 − xβ−1.
x 0 1 px α α αβ	 We say x ∼ Betaα  β if for x ∈ 0 1 px  α  β  α  β αβ xα−11 − xβ−1  A.
	 A.
But	 But what is α  β αβ A.
3 The function thought continuous factorial function.	3 The   is the Gamma function  It can be thought of as the continuous version of the factorial function.
function thought continuous	 is the Gamma function  It can be thought of as the continuous version of the factorial function.
thought continuous x	 It can be thought of as the continuous version of the factorial function  That is x  x − 1x − 1.
Or x ∈ Z x That explain −	 Or rather for an x ∈ Z x  x − 1  That still doesn’t explain the purpose of that constant in front of xα−11 − xβ−1.
In ensures given β parameters support As necessity probability 1 0 − αβ .	 458 Appendix A Bayesian Statistics In fact this constant just ensures that given the α and β parameters the Beta distribution still integrates to one over its support  As you probably recall this is a necessity for a probability distribution  Mathematically we can write this as∫ 1 0 xα−11 − xβ−1dx  αβ α  β .
As	 As you probably recall this is a necessity for a probability distribution.
write 0 xβ−1dx α β 4 Note sum reciprocal constant.	 Mathematically we can write this as∫ 1 0 xα−11 − xβ−1dx  αβ α  β   A 4 Note that the sum over the support of x is the reciprocal of that constant.
4 Note reciprocal constant If multiply reciprocal desired∫ β −	4 Note that the sum over the support of x is the reciprocal of that constant  If we divide by it multiply by reciprocal we will get one as desired∫ 1 0 α  β αβ xα−11 − xβ−1dx  1  A.
If reciprocal α − 1.	 If we divide by it multiply by reciprocal we will get one as desired∫ 1 0 α  β αβ xα−11 − xβ−1dx  1.
If you’re calculus Wolfram Alpha similar confirm fact distribution value α β	 A 5 If you’re proficient in calculus or know how to use Wolfram Alpha or similar you can confirm this fact for yourself  One more note on the Beta distribution its expected value is α α  β .
If proficient Wolfram similar note α	5 If you’re proficient in calculus or know how to use Wolfram Alpha or similar you can confirm this fact for yourself  One more note on the Beta distribution its expected value is α α  β   A.
One note Beta expected value A 6	 One more note on the Beta distribution its expected value is α α  β   A 6 We’ll see how this can be useful in a minute.
minute pθ The observed H .	6 We’ll see how this can be useful in a minute  Let’s finally rewrite our estimate of pθ  D  The data we have observed is H  T .
finally rewrite D H	 Let’s finally rewrite our estimate of pθ  D  The data we have observed is H  T .
α distribution They’re eters parameters prior distribution.	 Additionally we are using the two hyperparameters α and β for our Beta distribution prior  They’re called hyperparam eters because they are parameters for our prior distribution.
eters distribution.	 They’re called hyperparam eters because they are parameters for our prior distribution.
pθ H T α ∝ pH θpθ β θH1 − θT − θT β−1 Beta Namely T BetaH α	 pθ  H  T  α  β ∝ pH  T  θpθ  α  β ∝ θH1 − θT θα−11 − θβ−1 θHα−11 − θT β−1  But this is itself a Beta distribution Namely pθ  H  T  α  β  BetaH  α  T  β  A.
Beta distribution pθ β α Bayesian parameter estimation.	 But this is itself a Beta distribution Namely pθ  H  T  α  β  BetaH  α  T  β  A 7 Finally we can get our Bayesian parameter estimation.
A parameter	 A 7 Finally we can get our Bayesian parameter estimation.
Finally parameter Unlike maximum likeli MLE integrate D.	7 Finally we can get our Bayesian parameter estimation  Unlike maximum likeli hood estimation MLE where we have the parameter that maximizes our data we integrate over all possible θ  and find its expected value given the data Eθ  D.
A 8 We won’t isn’t What final	 A 8 We won’t go into detail with solving the integral since that isn’t our focus  What we do see though is our final result.
What final result Bayesian estimate Beta	 What we do see though is our final result  This result is general for any Bayesian estimate of a binomial parameter with a Beta prior.
This result general Bayesian binomial prior	 This result is general for any Bayesian estimate of a binomial parameter with a Beta prior  A.
A Pseudo Counts How interpret Bayesian estimate binomial parameters Eθ T A.	 A 2 Pseudo Counts Smoothing and Setting Hyperparameters How can we interpret our result for a Bayesian estimate of binomial parameters Eθ  D  H  α H  T  α  β A.
Smoothing Setting Eθ D H α T α A 9 Beta distributions	2 Pseudo Counts Smoothing and Setting Hyperparameters How can we interpret our result for a Bayesian estimate of binomial parameters Eθ  D  H  α H  T  α  β A 9 We know that the Beta and binomial distributions are similar.
We Beta distributions similar In stated distribution conjugate prior distributions family conjugate priors.	9 We know that the Beta and binomial distributions are similar  In fact their re lationship can be stated as the Beta distribution is the conjugate prior of the binomial distribution  All distributions in the exponential family have conjugate priors.
In prior binomial distribution distributions family conjugate	 In fact their re lationship can be stated as the Beta distribution is the conjugate prior of the binomial distribution  All distributions in the exponential family have conjugate priors.
distributions exponential conjugate	 All distributions in the exponential family have conjugate priors.
For coin binomial ended distribution—this conjugate prior having prior reasonable guess data want assume coin.	 For our coin flipping case the likelihood was a binomial distribution  We picked our prior to be the Beta distribution and our posterior distribution ended up also being a Beta distribution—this is because we picked the conjugate prior In any event the whole reasoning behind having a prior is so we can include some reasonable guess for the parameters before we even see any data  For coin flipping we might want to assume a “fair” coin.
For coin want “fair” If incorporate knowledge	 For coin flipping we might want to assume a “fair” coin  If for some reason we believe that the coin may be biased we can incorporate that knowledge as well.
If imagine setting prediction estimate θ .	 If we look at the estimate for θ  we can imagine how setting our hyperparameters can influence our prediction  Recall that θ is the probability of heads if we want to make our estimate biased toward more heads we can set an α  β since this increases θ .
Recall θ heads estimate heads α increases θ	 Recall that θ is the probability of heads if we want to make our estimate biased toward more heads we can set an α  β since this increases θ .
α mean prior 80	 This agrees with the mean of the prior as well α αβ   Setting the mean equal to 0 8 means that our prior belief is a coin that lands heads 80 of the time.
mean means coin lands 80 α β 16 0.	 Setting the mean equal to 0 8 means that our prior belief is a coin that lands heads 80 of the time  460 Appendix A Bayesian Statistics This can be accomplished with α  4 β  1 or α  16 β  4 or even α  0.
means 460 Appendix A Statistics This accomplished 4 β 4 0.	8 means that our prior belief is a coin that lands heads 80 of the time  460 Appendix A Bayesian Statistics This can be accomplished with α  4 β  1 or α  16 β  4 or even α  0.
460 This accomplished β	 460 Appendix A Bayesian Statistics This can be accomplished with α  4 β  1 or α  16 β  4 or even α  0.
β 0 But difference Figure	4 β 0 1  But what is the difference Figure A.
difference	1  But what is the difference Figure A.
But Figure distribu tion parameters.	 But what is the difference Figure A 1 shows a comparison of the Beta distribu tion with varying parameters.
Beta distribu varying	1 shows a comparison of the Beta distribu tion with varying parameters.
important remember ∼ β distribution Even it’s range 1 prior Perhaps unimodal 0.	 It’s also important to remember that a draw from a Beta prior θ ∼ Betaα  β gives us a distribution  Even though it’s a single value on the range 0 1 we are still using the prior to produce a probability distribution  Perhaps we’d like to choose a unimodal Beta prior with a mean 0.
0 Figure	 Perhaps we’d like to choose a unimodal Beta prior with a mean 0 8  As we can see from Figure A.
As	8  As we can see from Figure A.
As A set 0.	 As we can see from Figure A 1 the higher we set α and β the sharper the peak at 0.
H T β A.	8 will be  Looking at our parameter estimation H  α H  T  α  β A.
parameter H α H α 10 hyperparameters counts—counts outcome experiments pseudo	 Looking at our parameter estimation H  α H  T  α  β A 10 we can imagine the hyperparameters as pseudo counts—counts from the outcome of experiments already performed  The higher the hyperparameters are the more pseudo counts we have which means our prior is “stronger.
imagine outcome	10 we can imagine the hyperparameters as pseudo counts—counts from the outcome of experiments already performed.
” As total increases	” As the total number of experiments increases the sum H  T also increases which means we have less dependence on our priors.
small number accurate θ—we’d like estimate number approaches “large enough” sense prior estimation estimate greatly smooth decent prior.	 As we all know a small number of flips will not give an accurate estimate of the true θ—we’d like to see what our estimate becomes as our number of flips approaches infinity or some “large enough” value  In this sense our prior also smooths our estimation  Rather than the estimate fluctuating greatly initially it could stay relatively smooth if we have a decent prior.
Rather estimate fluctuating greatly smooth decent	 Rather than the estimate fluctuating greatly initially it could stay relatively smooth if we have a decent prior.
	 A.
talk Dirichlet let’s figure probability observing vocabulary In language single document.	 Before we talk about the Dirichlet distribution let’s figure out how to represent the probability of observing a word from a vocabulary  For this we can use a categorical distribution  In a text information system a categorical distribution could represent a unigram language model for a single document.
For In text information represent unigram language single	 For this we can use a categorical distribution  In a text information system a categorical distribution could represent a unigram language model for a single document.
information model V	 In a text information system a categorical distribution could represent a unigram language model for a single document  Here the total number of outcomes is k  V  the size of our vocabulary.
Here total V vocabulary The index probability pi occurring words’ probabilities A.	 Here the total number of outcomes is k  V  the size of our vocabulary  The word at index i would have a probability pi of occurring and the sum of all words’ probabilities would sum to one  A.
4 461 categorical multinomial distribution Bernoulli The ki xi n trials.	 A 4 The Dirichlet Distribution 461 The categorical distribution is to the multinomial distribution as the Bernoulli is to the binomial  The multinomial is the probability of observing each word ki occur xi times in a total of n trials.
occur times n trials.	 The multinomial is the probability of observing each word ki occur xi times in a total of n trials.
given document use observing documents position.	 If we’re given a document vector of counts we can use the multinomial to find the probability of observing documents with those counts of words regardless of position.
probability density function given relate distribution counterpart.	 The probability density function is given as follows It should be straightforward to relate the more general multinomial distribution to its binomial counterpart.
4 likelihood k comes conjugate prior multinomial	 A 4 The Dirichlet Distribution We now have the likelihood function determined for a distribution with k out comes  The conjugate prior to the multinomial is the Dirichlet.
The conjugate multinomial use prior posterior Dirichlet.	 The conjugate prior to the multinomial is the Dirichlet  That is if we use a Dirichlet prior the posterior will also be a Dirichlet.
vectors Like Dirichlet reals.	 The “simplex” is the name of the space where these vectors live  Like the Beta distribution the parameters of the Dirichlet are reals.
Like distribution Dirichlet notation pθ α θ Dirichlet	 Like the Beta distribution the parameters of the Dirichlet are reals  Here’s the pdf In this notation we have pθ  α  θ is what we draw from the Dirichlet in the Beta it was the parameter to be used in the binomial.
Here vector parameters multinomial In Dirichlet	 Here it is the vector of parameters to be used in the multinomial  In this sense the Dirichlet is a distribution that produces distributions so is the Beta.
Dirichlet produces emphasis.	 In this sense the Dirichlet is a distribution that produces distributions so is the Beta  The hyperparameters of the Dirichlet are also a vector denoted with an arrow for emphasis.
hyperparameters emphasis.	 The hyperparameters of the Dirichlet are also a vector denoted with an arrow for emphasis.
Beta Dirichlet k—one A	 Instead of just two hyperparameters as in the Beta the Dirichlet needs k—one for each multinomial probability  462 Appendix A Bayesian Statistics Figure A.
Appendix A Statistics A.	 462 Appendix A Bayesian Statistics Figure A.
2 parameter affects Dirichlet distribution parameters.	2 How the α parameter affects the shape and sparsity of the Dirichlet distribution of three parameters.
right α 1 10.	 From left to right α  0 1 1 10.
set value.	 Because of this we set them all to the same value.
instead pθ α	 So instead of writing pθ  α where α is e g.
g.	g.
	 0 1 0.
.	1 .
	    0.
1 simply representing identical values	  0 1 we simply say pθ  α where alpha is a scalar representing a vector of identical values  θ ∼ Dirα and θ ∼ Dir0.
θ Dir0 ∼ Betaα β θ	 θ ∼ Dirα and θ ∼ Dir0 1 are also commonplace as is θ ∼ Betaα  β or θ ∼ Beta0.
commonplace Betaα β Beta0 4	1 are also commonplace as is θ ∼ Betaα  β or θ ∼ Beta0 4 0.
0	4 0 1  Figure A.
Figure 2 choice α characterizes	 Figure A 2 shows how the choice of α characterizes the Dirichlet.
2 shows α Dirichlet The point vector moment Dirichlet	2 shows how the choice of α characterizes the Dirichlet  The higher the area the more likely a point representing a vector will be drawn  Let’s take a moment to understand what a point drawn from the Dirichlet means.
area point vector	 The higher the area the more likely a point representing a vector will be drawn.
moment drawn Dirichlet Look right	 Let’s take a moment to understand what a point drawn from the Dirichlet means  Look at the far right graph in Figure A.
Look right graph A 2.	 Look at the far right graph in Figure A 2.
	2.
If point draw peak we’ll multinomial vector example θ	 If the point we draw is from the peak of the graph we’ll get a multinomial parameter vector with a roughly equal proportion of each of the three components  For example θ  0.
	33 0.
	33 0.
With it’s we’ll θ middle kind θ	 With α  10 it’s very likely that we’ll get a θ like this  In the middle picture we’re unsure what kind of θ we’ll draw.
Finally prior like β 1 mixture components means likely	 Finally the plot on the left is a sparse prior like a Beta where α  β  1  Note a uniform prior does not mean that we get an even mixture of components it means it’s equally likely to get any mixture.
draw uniform A actually relevant probability rest low sound law.	 This could be confusing since the distribution we draw may actually be a uniform distribution  A sparse prior is actually quite relevant in a textual application if we have a few dimensions with very high probability and the rest with relatively low occurrences this should sound just like Zipf’s law.
Dirichlet enforce word distribution θ 9 0 02	 We can use a Dirichlet prior to enforce a sparse word distribution per topic θ  0 9 0 02 0.
	02 0.
08 topic use Dirichlet force sparse distribution document.	08  In topic modeling we can use a Dirichlet distribution to force a sparse topic distribution per document.
In modeling use sparse topic discusses handful Multinomial unrepresented like common words preternatural	 In topic modeling we can use a Dirichlet distribution to force a sparse topic distribution per document  It’s most likely that a document mainly discusses a handful of topics while the rest are A 5 Bayesian Estimate of Multinomial Parameters 463 largely unrepresented just like the words the to of  and from are common while many words such as preternatural are rare.
5 Bayesian Multinomial 463 largely words rare.	5 Bayesian Estimate of Multinomial Parameters 463 largely unrepresented just like the words the to of  and from are common while many words such as preternatural are rare.
Bayesian Multinomial Parameters parameter estimation relate θi n	5 Bayesian Estimate of Multinomial Parameters Let’s do parameter estimation with our multinomial distribution and relate it to the Betabinomial model from before  For MLE we would have θi  xi n .
For xi .	 For MLE we would have θi  xi n .
14 product We ratio observe Dirichlet conjugacy Bayesian estimate we’d fully substitute multi distributions posterior	14 Using Bayes’ rule we represent the posterior as the product of the likelihood multinomial and prior Dirichlet We say these are proportional because we left out the constant of proportionality in the multinomial and Dirichlet distributions the ratio with Gammas  We can now observe that the posterior is also a Dirichlet as expected due to the conjugacy  To actually obtain the Bayesian estimate we’d need to fully substitute the multi nomial and Dirichlet distributions into the posterior and integrate over all θs to get our estimate.
posterior expected	 We can now observe that the posterior is also a Dirichlet as expected due to the conjugacy.
actually fully Dirichlet θs estimate isn’t note answer D n .	 To actually obtain the Bayesian estimate we’d need to fully substitute the multi nomial and Dirichlet distributions into the posterior and integrate over all θs to get our estimate  Since this isn’t a note on calculus we simply display the final answer as Eθi  D  xi  αi n  ∑k j1 αj .
calculus final xi ∑k similar Dirichlet parameters counts	 Since this isn’t a note on calculus we simply display the final answer as Eθi  D  xi  αi n  ∑k j1 αj   A 15 This looks very similar to the binomial estimation We see the Dirichlet hyper parameters act as pseudo counts smoothing our estimate.
	 A.
We Dirichlet hyper parameters counts smoothing estimate prior smoothing information formula pw μpw C	15 This looks very similar to the binomial estimation We see the Dirichlet hyper parameters act as pseudo counts smoothing our estimate  In Dirichlet prior smoothing for information retrieval we have the formula pw  d  cw d  μpw  C d  μ   A.
prior smoothing information retrieval formula d d μpw A x d n d length	 In Dirichlet prior smoothing for information retrieval we have the formula pw  d  cw d  μpw  C d  μ   A 16 So we have x  cw d and n  d the count of the current word in a document and the length of the document respectively.
	 A.
16 n d count document length respectively.	16 So we have x  cw d and n  d the count of the current word in a document and the length of the document respectively.
C μ αj counts	 Then we have αi  μpw  C and μ  ∑k j1 αj  the number of pseudo counts for word w and the total number of pseudo counts.
hyperparameters 464 Bayesian It’s μ C C	 Can you tell what the vector of hyperparameters for query likelihood 464 Appendix A Bayesian Statistics smoothing would be now It’s μ μpw1  C μpw2  C .
μpwk A.	      μpwk  C A.
μpwk	  μpwk  C A.
Looking smoothing prior smoothing drew uniform we’d cw d A.	 Looking back at Add1 smoothing we can imagine this as a special case of Dirichlet prior smoothing  If we drew the uniform distribution from our Dirichlet we’d get pw  d  cw d  1 d  V  A.
Dirichlet we’d A.	 If we drew the uniform distribution from our Dirichlet we’d get pw  d  cw d  1 d  V  A.
This implies word equally likely model case k ∑k i1 1 1 .	18 This implies that each word is equally likely in our collection language model which is most likely not the case  Note V   k or ∑k i1 1 since μ  1 1 .
Note ∑k 1 1	 Note V   k or ∑k i1 1 since μ  1 1     .
1.	      1.
	 .
	  1.
	 A.
Starting distribution coin flip expanded trials distribution.	6 Conclusion Starting with the Bernoulli distribution for a single coin flip we expanded it into a set of trials with the binomial distribution.
	 We investigated parameter estimation via MLE and then moved onto a Bayesian approach.
We pseudo hyperparameters affected distribution.	 We compared the Bayesian re sult to smoothing with pseudo counts and saw how hyperparameters affected the distribution.
model Dirichletmultinomial model inspected Dirichlet query	 We saw how the Betabinomial model is related to the Dirichletmultinomial model and inspected it in the context of Dirichlet prior smoothing for query likelihood in IR.
The algorithm maxi “incomplete” involves	 BAPPENDIX Expectation Maximization The ExpectationMaximization EM algorithm is a general algorithm for maxi mumlikelihood estimation where the data are “incomplete” or the likelihood function involves latent variables.
related data incomplete observe latent variables similarly associate variable	 Note that the notion of “incomplete data” and “latent variables” are related when we have a latent variable we may regard our data as being incomplete since we do not observe values of the latent variables similarly when our data are incomplete we often can also associate some latent variable with the missing data.
language estimate parameters exact data point generated hidden EM alternates called i.	 For language modeling the EM algorithm is often used to estimate parameters of a mixture model in which the exact component model from which a data point is generated is hidden from us  Informally the EM algorithm starts with randomly assigning values to all the pa rameters to be estimated  It then iteratively alternates between two steps called the expectation step i.
algorithm starts assigning values estimated iteratively alternates steps expectation i.	 Informally the EM algorithm starts with randomly assigning values to all the pa rameters to be estimated  It then iteratively alternates between two steps called the expectation step i.
step i.	e  the “Estep” and the maximization step i.
i.	 the “Estep” and the maximization step i.
“Mstep” socalled computed distribution latent variables i.	e  the “Mstep” respectively  In the Estep it computes the expected likelihood for the complete data the socalled Qfunction where the expectation is taken with respect to the computed conditional distribution of the latent variables i.
	 the “Mstep” respectively.
Estep computes expected likelihood complete data respect computed conditional variables	 In the Estep it computes the expected likelihood for the complete data the socalled Qfunction where the expectation is taken with respect to the computed conditional distribution of the latent variables i.
	e.
vari data In	 the “hidden vari ables” given the current settings of parameters and our observed incomplete data  In the Mstep it reestimates all the parameters by maximizing the Qfunction.
In parameters new	 In the Mstep it reestimates all the parameters by maximizing the Qfunction  Once we have a new generation of parameter values we can repeat the Estep and another Mstep.
Once generation parameter values Mstep This continues converges reaching	 Once we have a new generation of parameter values we can repeat the Estep and another Mstep  This process continues until the likelihood converges reaching a local maxima.
continues reaching	 This process continues until the likelihood converges reaching a local maxima.
approach guaranteed reach reach depends able global When local maximas identify hill.	 The EM algorithm is a hillclimbing approach thus it can only be guaranteed to reach a local maxima  When there are multiple maximas whether we will actually reach the global maxima depends on where we start if we start at the “right hill” we will be able to find a global maxima  When there are multiple local maximas it is often hard to identify the “right hill.
When maximas actually reach global depends start “right	 When there are multiple maximas whether we will actually reach the global maxima depends on where we start if we start at the “right hill” we will be able to find a global maxima.
hard identify “right hill There 466 B Expectation problem different choose solution converged likelihood	 When there are multiple local maximas it is often hard to identify the “right hill ” There are two commonly used strategies 466 Appendix B Expectation Maximization to solving this problem  The first is that we try many different initial values and choose the solution that has the highest converged likelihood value.
try values solution value uses model ideally maxima complex	 The first is that we try many different initial values and choose the solution that has the highest converged likelihood value  The second uses a much simpler model ideally one with a unique global maxima to determine an initial value for more complex models.
second model ideally unique determine value	 The second uses a much simpler model ideally one with a unique global maxima to determine an initial value for more complex models.
The idea simpler hopefully help region search accurate optima complex model introduce specific problem—estimating model indepth EM McLachlan	 The idea is that a simpler model can hopefully help locate a rough region where the global optima exists and we start from a value in that region to search for a more accurate optima using a more complex model  Here we introduce the EM algorithm through a specific problem—estimating a simple mixture model  For a more indepth introduction to EM please refer to McLachlan and Krishnan 2008.
Here introduce algorithm problem—estimating	 Here we introduce the EM algorithm through a specific problem—estimating a simple mixture model.
For Krishnan B.	 For a more indepth introduction to EM please refer to McLachlan and Krishnan 2008  B.
	 B.
1 A Simple Mixture Model mixture feedback Zhai 2001	1 A Simple Mixture Unigram Language Model In the mixture model feedback approach Zhai and Lafferty 2001 we assume that the feedback documents F  d1   .
.	 .
“generated” multinomial	  dk are “generated” from a mixture model with two multinomial component models.
	 w is a word.
The idea model non loglikelihood document data mixture parameter indicates noise” documents empirically.	 The idea is to model the common non discriminative words in F with pw  C so that the topic model θF would attract more discriminative contentcarrying words  The loglikelihood of the feedback document data for this mixture model is where dij is the j th word in document di di is the length of di and λ is a parameter that indicates the amount of “background noise” in the feedback documents which will be set empirically.
The loglikelihood document model dij th word di length di indicates set empirically assume λ estimate pw	 The loglikelihood of the feedback document data for this mixture model is where dij is the j th word in document di di is the length of di and λ is a parameter that indicates the amount of “background noise” in the feedback documents which will be set empirically  We thus assume λ to be known and want to estimate pw  θF .
known estimate pw	 We thus assume λ to be known and want to estimate pw  θF   B.
maximum likelihood estimator maximizes likelihood	 B 2 Maximum Likelihood Estimation A common method for estimating θF is the maximum likelihood ML estimator in which we choose a θF that maximizes the likelihood of F   That is the estimated topic model denoted by θ̂F  is given by B.
model denoted vs.	 That is the estimated topic model denoted by θ̂F  is given by B 3 Incomplete vs.
Incomplete vs Data 467 The right equation θF variables.	3 Incomplete vs  Complete Data 467 The right side of this equation is easily seen to be a function with pw  θF  as variables.
To θ̂F principle use optimization function logarithm obtain simple Lagrange general algorithms.	 To find θ̂F  we can in principle use any optimization methods  Since the function involves a logarithm of a sum of two terms it is difficult to obtain a simple analytical solution via the Lagrange Multiplier approach so in general we must rely on numerical algorithms.
Since obtain simple Multiplier rely There happens natural local maximum maximum.	 Since the function involves a logarithm of a sum of two terms it is difficult to obtain a simple analytical solution via the Lagrange Multiplier approach so in general we must rely on numerical algorithms  There are many possibilities EM happens to be just one of them which is quite natural and guaranteed to converge to a local maxima which in our case is also a global maximum since the likelihood function can be shown to have one unique maximum.
There converge local maxima case global function shown	 There are many possibilities EM happens to be just one of them which is quite natural and guaranteed to converge to a local maxima which in our case is also a global maximum since the likelihood function can be shown to have one unique maximum.
B.	 B.
EM data latent variables “complete” likelihood purpose The original treated ” maximize data likelihood goal expected maximize values hidden variables likelihood incomplete likelihood hidden variables.	 Complete Data The main idea of the EM algorithm is to “augment” our data with some latent variables so that the “complete” data has a much simpler likelihood function— simpler for the purpose of finding a maximum  The original data are thus treated as “incomplete ” As we will see we will maximize the incomplete data likelihood our original goal through maximizing the expected complete data likelihood since it is much easier to maximize where expectation is taken over all possible values of the hidden variables since the complete data likelihood unlike our original incomplete data likelihood would contain hidden variables.
Let th document variable defined 1 word assume data words corresponding z.	 Let dij be the j th word in document di  We have a corresponding variable zij defined as follows zij 1 if word dij is from background 0 otherwise  We thus assume that our complete data would have contained not only all the words in F  but also their corresponding values of z.
defined zij dij background complete contained words F z.	 We have a corresponding variable zij defined as follows zij 1 if word dij is from background 0 otherwise  We thus assume that our complete data would have contained not only all the words in F  but also their corresponding values of z.
We data contained z complete data difference LcθF LθF outside log assume know generated dij What relationship LθF original data X hidden variable 468 Maximization H	 We thus assume that our complete data would have contained not only all the words in F  but also their corresponding values of z  The loglikelihood of the complete data is thus Note the difference between LcθF  and LθF  the sum is outside of the log arithm in LcθF  and this is possible because we assume that we know which component model has been used to generated each word dij   What is the relationship between LcθF  and LθF  In general if our parameter is θ  our original data is X and we augment it with a hidden variable H  then 468 Appendix B Expectation Maximization pX H  θ  pH  X θpX  θ.
The complete Note LθF outside possible assume component .	 The loglikelihood of the complete data is thus Note the difference between LcθF  and LθF  the sum is outside of the log arithm in LcθF  and this is possible because we assume that we know which component model has been used to generated each word dij .
log θ log pX pH Lθ θ B.	 Thus Lcθ  log pX H  θ  log pX  θ  log pH  X θ  Lθ  log pH  X θ  B.
A Lower Bound pa values	 B 4 A Lower Bound of Likelihood Algorithmically the basic idea of EM is to start with some initial guess of the pa rameter values θ0 and then iteratively search for better values for the parameters.
Lower basic idea initial pa values Assuming likelihood Lθ.	4 A Lower Bound of Likelihood Algorithmically the basic idea of EM is to start with some initial guess of the pa rameter values θ0 and then iteratively search for better values for the parameters  Assuming that the current estimate of the parameters is θn our goal is to find another θn1 that can improve the likelihood Lθ.
likelihood potentially better value θ likelihood θn relate complete likelihood Lθ Lθn equivalent Lθ.	 Let us consider the difference between the likelihood at a potentially better parameter value θ and the likelihood at the current estimate θn and relate it with the corresponding difference in the complete likelihood Our goal is to maximize Lθ − Lθn which is equivalent to maximizing Lθ.
Now equation w.	 Now take the expectation of this equation w.
r.	r.
hidden given current estimate parameters e.	t  the conditional distribution of the hidden variable given the data X and the current estimate of parameters θn i e.
i.	 the conditional distribution of the hidden variable given the data X and the current estimate of parameters θn i.
e pH θn left	e  pH  X θn  Note that the left side of the equation remains the same as the variable H does not occur there.
The term pH X θn We We	 The last term can be recognized as the KLdivergence of pH  X θn and pH  X θ which is always nonnegative  We thus have We thus obtain a lower bound for the original likelihood function.
lower likelihood main idea maximize maximize	 We thus have We thus obtain a lower bound for the original likelihood function  The main idea of EM is to maximize this lower bound so as to maximize the original incomplete likelihood.
lower maximize	 The main idea of EM is to maximize this lower bound so as to maximize the original incomplete likelihood.
terms lower treated contain B term complete socalled “Qfunction” Qθ	 Note that the last two terms in this lower bound can be treated as constants as they do not contain the variable θ  so the lower bound is essentially B 5 The General Procedure of EM 469 the first term which is the expectation of the complete likelihood or the socalled “Qfunction” denoted by Qθ  θn  The Qfunction for our mixture model is the following.
Procedure term likelihood θn.	5 The General Procedure of EM 469 the first term which is the expectation of the complete likelihood or the socalled “Qfunction” denoted by Qθ  θn.
	 The Qfunction for our mixture model is the following  B.
General Clearly Qθn1 Lθn.	5 The General Procedure of EM Clearly if we find a θn1 such that Qθn1 θn  Qθn θn then we will also have Lθn1  Lθn.
Thus procedure EM algorithm	 Thus the general procedure of the EM algorithm is the following.
θ0 heuristically according parameter	 Initialize θ0 randomly or heuristically according to any prior knowledge about where the optimal parameter value might be.
improve θ following	 2  Iteratively improve the estimate of θ by alternating between the following two steps 1.
improve steps	 Iteratively improve the estimate of θ by alternating between the following two steps 1.
Compute θn maximization Q θn.	 the Estep expectation Compute Qθ  θn and 2  the Mstep maximization Reestimate θ by maximizing the Q function θn1  argmaxθQθ  θn.
Mstep maximization θ maximizing θn1 argmaxθQθ likelihood	 the Mstep maximization Reestimate θ by maximizing the Q function θn1  argmaxθQθ  θn  3  Stop when the likelihood Lθ converges.
Lθ converges mentioned earlier Lcθ easier maximize values hidden variable	 3  Stop when the likelihood Lθ converges  As mentioned earlier the complete likelihood Lcθ is much easier to maximize as the values of the hidden variable are assumed to be known.
Stop likelihood earlier complete likelihood Lcθ maximize hidden variable known Lcθ easier likelihood	 Stop when the likelihood Lθ converges  As mentioned earlier the complete likelihood Lcθ is much easier to maximize as the values of the hidden variable are assumed to be known  This is why the Qfunction which is an expectation of Lcθ is often much easier to maximize than the original likelihood function.
As mentioned complete maximize known Qfunction expectation easier maximize original function.	 As mentioned earlier the complete likelihood Lcθ is much easier to maximize as the values of the hidden variable are assumed to be known  This is why the Qfunction which is an expectation of Lcθ is often much easier to maximize than the original likelihood function.
exist natural variable hidden function maximize.	 In cases when there does not exist a natural latent variable we often introduce a hidden variable so that the complete likelihood function is easy to maximize.
The carried Estep X complicated simple 470 depend F	 The major computation to be carried out in the Estep is to compute pH X θn which is sometimes very complicated  In our case this is simple 470 Appendix B Expectation Maximization Note that in general zij may depend on all the words in F .
simple 470 Appendix Expectation Maximization .	 In our case this is simple 470 Appendix B Expectation Maximization Note that in general zij may depend on all the words in F   In our model however it only depends on the corresponding word dij .
The Mstep involves maximizing Qfunction well.	 The Mstep involves maximizing the Qfunction  This may sometimes be quite complex as well.
This complex well.	 This may sometimes be quite complex as well.
case analytical solution order use method vocabulary Note sum word distinct vocabulary Note explicitly compute Qfunction compute distribution hidden variable z obtain values Qfunction.	 But again in our case we can find an analytical solution  In order to achieve this we use the Lagrange multiplier method since we have the following constraint on the parameter variables pw  θF w∈V  where V is our vocabulary Note that we changed the notation so that the sum over each word position in document di is now a sum over all the distinct words in the vocabulary  Note that we never need to explicitly compute the Qfunction instead we compute the distribution of the hidden variable z and then directly obtain the new parameter values that will maximize the Qfunction.
In order achieve use Lagrange method following parameter variables w∈V Note notation sum word document di distinct	 In order to achieve this we use the Lagrange multiplier method since we have the following constraint on the parameter variables pw  θF w∈V  where V is our vocabulary Note that we changed the notation so that the sum over each word position in document di is now a sum over all the distinct words in the vocabulary.
z obtain new maximize CAPPENDIX This appendix detailed KLdivergence function prior query likelihood framework We briefly	 Note that we never need to explicitly compute the Qfunction instead we compute the distribution of the hidden variable z and then directly obtain the new parameter values that will maximize the Qfunction  CAPPENDIX KLdivergence and Dirichlet Prior Smoothing This appendix is a more detailed discussion of the KLdivergence function and its relation to Dirichlet prior smoothing in the generalized query likelihood smoothing framework  We briefly touched upon KLdivergence in Chapter 7 and Chapter 13.
Prior discussion KLdivergence function smoothing framework KLdivergence 7 seen given functions p defined Dp‖q p	 CAPPENDIX KLdivergence and Dirichlet Prior Smoothing This appendix is a more detailed discussion of the KLdivergence function and its relation to Dirichlet prior smoothing in the generalized query likelihood smoothing framework  We briefly touched upon KLdivergence in Chapter 7 and Chapter 13  As we have seen given two probability mass functions px and qx Dp‖q the KullbackLeibler divergence or relative entropy between p and q is defined as It is easy to show that Dp‖q is always nonnegative and is zero if and only if p  q.
KLdivergence Chapter 7 13 mass functions Dp‖q KullbackLeibler relative entropy p It nonnegative zero p symmetric useful “distance” 1991.	 We briefly touched upon KLdivergence in Chapter 7 and Chapter 13  As we have seen given two probability mass functions px and qx Dp‖q the KullbackLeibler divergence or relative entropy between p and q is defined as It is easy to show that Dp‖q is always nonnegative and is zero if and only if p  q  Even though it is not a true distance between distributions because it is not symmetric and does not satisfy the triangle inequality it is still often useful to think of the KLdivergence as a “distance” between distributions Cover and Thomas 1991.
As probability mass px p q It Dp‖q p triangle inequality useful think KLdivergence distributions	 As we have seen given two probability mass functions px and qx Dp‖q the KullbackLeibler divergence or relative entropy between p and q is defined as It is easy to show that Dp‖q is always nonnegative and is zero if and only if p  q  Even though it is not a true distance between distributions because it is not symmetric and does not satisfy the triangle inequality it is still often useful to think of the KLdivergence as a “distance” between distributions Cover and Thomas 1991.
Even distance triangle inequality useful Cover	 Even though it is not a true distance between distributions because it is not symmetric and does not satisfy the triangle inequality it is still often useful to think of the KLdivergence as a “distance” between distributions Cover and Thomas 1991.
Suppose query generative model pq denot ing query unigram	 C 1 Using KLdivergence for Retrieval Suppose that a query q is generated by a generative model pq  θQ with θQ denot ing the parameters of the query unigram language model.
1 KLdivergence Retrieval Suppose pq θQ ing model assume document generated pd rameters unigram language If query document language models relevance d respect measured negative function entropy query model	1 Using KLdivergence for Retrieval Suppose that a query q is generated by a generative model pq  θQ with θQ denot ing the parameters of the query unigram language model  Similarly assume that a document d is generated by a generative model pd  θD with θD denoting the pa rameters of the document unigram language model  If θ̂Q and θ̂D are the estimated query and document language models respectively then the relevance value of d with respect to q can be measured by the following negative KLdivergence function Zhai and Lafferty 2001 Note that the second term on the righthand side of the formula is a query dependent constant or more specifically the entropy of the query model θ̂Q.
Similarly assume generated generative rameters document model.	 Similarly assume that a document d is generated by a generative model pd  θD with θD denoting the pa rameters of the document unigram language model.
θ̂Q query relevance d q negative Zhai second formula specifically entropy query model θ̂Q ranking documents In involves words according θ̂Q.	 If θ̂Q and θ̂D are the estimated query and document language models respectively then the relevance value of d with respect to q can be measured by the following negative KLdivergence function Zhai and Lafferty 2001 Note that the second term on the righthand side of the formula is a query dependent constant or more specifically the entropy of the query model θ̂Q  It can be ignored for the purpose of ranking documents  In general the computation of the above formula involves a sum over all the words that have a nonzero probability according to pw  θ̂Q.
	 It can be ignored for the purpose of ranking documents.
formula words probability However certain general smoothing method involve according occur document d.	 In general the computation of the above formula involves a sum over all the words that have a nonzero probability according to pw  θ̂Q  However when θ̂D is based on certain general smoothing method the computation would only involve a sum over those that both have a nonzero probability according to pw  θ̂Q and occur in document d.
Such computed The following d	 Such a sum can be computed much more efficiently with an inverted index  We now explain this in detail  The general smoothing scheme we assume is the following pw  θ̂D psw  d if word w is seen αdpw  C otherwise.
detail.	 We now explain this in detail.
The general following θ̂D d psw word collection language model unseen	 The general smoothing scheme we assume is the following pw  θ̂D psw  d if word w is seen αdpw  C otherwise  where psw  d is the smoothed probability of a word seen in the document pw  C is the collection language model and αd is a coefficient controlling the probability mass assigned to unseen words so that all probabilities sum to one.
probability word controlling	 where psw  d is the smoothed probability of a word seen in the document pw  C is the collection language model and αd is a coefficient controlling the probability mass assigned to unseen words so that all probabilities sum to one.
	g.
number distinct advantage probability retrieval difference versions It shown KLdivergence essentially based sum terms according pw document	 the total number of distinct words in the collection  One advantage of the smoothed version is that it would never give a zero probability to any term but in terms of retrieval performance there will not be any significant difference in these two versions since It can be shown that with such a smoothing scheme the KLdivergence scoring formula is essentially the two sides are equivalent for ranking documents Note that the scoring is now based on a sum over all the terms that both have a nonzero probability according to pw  θ̂Q and occur in the document i e.
	e.
	 all “matched” terms.
	 C.
2 prior smoothing method smoothing equation	2 Using Dirichlet Prior Smoothing Dirichlet prior smoothing is one particular smoothing method that follows the general smoothing scheme mentioned in the previous section  In particular Plugging these into equation C.
In particular equation C smoothing scoring compute	 In particular Plugging these into equation C 1 we see that with Dirichlet prior smoothing our KLdivergence scoring formula is  You may be wondering how we can compute pw  θ̂Q.
Dirichlet KLdivergence scoring	1 we see that with Dirichlet prior smoothing our KLdivergence scoring formula is.
exactly different simplest likelihood text easily KLdivergence formula Zhai Lafferty A θ̂Q	 This is exactly where the KLdivergence retrieval method is better than the simple query likelihood method— we can have different ways of computing it The simplest way is to estimate this probability by the maximum likelihood estimator using the query text as evidence  Using this estimated value you should see easily that the KLdivergence scoring for mula is essentially the same as the query likelihood retrieval formula as presented in Zhai and Lafferty 2004  A more interesting way of computing pw  θ̂Q is to exploit feedback documents.
Specifically simple pmlw model θF based − C needs set	 Specifically we can interpolate the simple pmlw  θ̂Q with a feedback model pw θF  estimated based on feedback documents  That is pw  θ̂Q  1 − αpmlw  θ̂Q  αpw  θF  C 2 where α is a parameter that needs to be set empirically.
pw 1 2 α parameter set αd	 That is pw  θ̂Q  1 − αpmlw  θ̂Q  αpw  θF  C 2 where α is a parameter that needs to be set empirically  Please note that this α is different from αd in the smoothing formula.
Please α smoothing course pw θF assume following component model feedback KLdivergence component pw C collection language model F	 Please note that this α is different from αd in the smoothing formula  Of course the next question is how to estimate pw  θF  One approach is to assume the following two component mixture model for the feedback documents 476 Appendix C KLdivergence and Dirichlet Prior Smoothing where one component model is pw  θF  and the other is pw  C the collection language model  where F  d1 .
course question estimate assume component feedback documents Appendix Dirichlet Prior component model pw pw collection F d1 .	 Of course the next question is how to estimate pw  θF  One approach is to assume the following two component mixture model for the feedback documents 476 Appendix C KLdivergence and Dirichlet Prior Smoothing where one component model is pw  θF  and the other is pw  C the collection language model  where F  d1 .
param indicates “background set empirically.	    dk is the set of feedback documents and λ is yet another param eter that indicates the amount of “background noise” in the feedback documents and that needs to be set empirically.
set documents λ eter indicates noise” feedback Now given λ feedback collection model likelihood estimate θF Appendix	  dk is the set of feedback documents and λ is yet another param eter that indicates the amount of “background noise” in the feedback documents and that needs to be set empirically  Now given λ the feedback documents F  and the collection language model pw  C we can use the EM algorithm to compute the maximum likelihood estimate of θF  as detailed in Appendix B.
given use EM algorithm compute maximum detailed .	 Now given λ the feedback documents F  and the collection language model pw  C we can use the EM algorithm to compute the maximum likelihood estimate of θF  as detailed in Appendix B .
.	.
