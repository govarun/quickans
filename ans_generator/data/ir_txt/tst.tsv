include finding large collection data text data actually application effectively big raw text smaller relevant text easily humans.	 The first is information retrieval systems which include search engines and recommender systems they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem thus effectively turning big raw text data into much smaller relevant text data that can be more easily processed by humans.
English text pages social tweets scientific emails government documents kinds play	 English text or Chinese text all the web pages social media data such as tweets news scientific literature emails government documents and many other kinds of enterprise data  Text data play an essential role in our lives.
urgent intelligent systems text quickly major growth engine	 Thus there is an urgent need for developing intelligent information retrieval systems to help people manage the text data and get access to the needed relevant information quickly and accurately at any time  This need is a major reason behind the recent growth of the web search engine industry.
In structured welldefined schemas easy computers text structure tools requires content encoded text.	 In contrast to structured data which conform to welldefined schemas and are thus relatively easy for computers to handle text has less explicit structure so the development of intelligent software tools discussed above requires computer processing to understand the content encoded in text.
current natural reached point enable precisely text reason involved wide text	 The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text a main reason why humans often should be involved in the loop but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades.
They usually applied manage data language This provide systematic introduction approaches knowledge skills required build text systems.	 They are usually very robust and can be applied to analyze and manage text data in any natural language and about any topic  This book intends to provide a systematic introduction to many of these approaches with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems.
CS410 Information Illinois Urbana–Champaign Open Online “Text Retrieval Analytics” taught Coursera materials MOOCs reference	 CS410 Text Information Systems at the University of Illinois at Urbana–Champaign as well as the two Massive Open Online Courses MOOCs on “Text Retrieval and Search Engines” and “Text Mining and Analytics” taught by the first author on Coursera in 2015  Most of the materials in the book directly match those of these two MOOCs with also similar structures of topics  As such the book can be used as a main reference book for any of these two MOOCs.
2008 Compared books information retrieval cover text mining attempts paint building information support information	 2009 and Introduction to Information Retrieval by Manning et al  2008  Compared with these existing books on information retrieval our book has a broader coverage of topics as it attempts to cover topics in both information retrieval and text mining and attempts to paint a general roadmap for building a text information system that can support both text information access and text analysis.
book coverage topics attempts retrieval text text information includes word mining probabilistic nontext data available existing retrieval	 Compared with these existing books on information retrieval our book has a broader coverage of topics as it attempts to cover topics in both information retrieval and text mining and attempts to paint a general roadmap for building a text information system that can support both text information access and text analysis  For example it includes a detailed introduction to word association mining probabilistic topic modeling and joint analysis of text and nontext data which are not available in any existing information retrieval books.
association mining probabilistic topic analysis data available contrast far	 For example it includes a detailed introduction to word association mining probabilistic topic modeling and joint analysis of text and nontext data which are not available in any existing information retrieval books  In contrast with IR Text Mining TM is far from mature and is actually still in its infancy.
As textbook	 As such it appears that there is not yet a textbook on TM.
By introducing want integration IR text IR important TM The enable fast reduction data size filtering data application problem.	 By introducing TM and IR in a unified framework we want to emphasize the importance of integration of IR and TM in any practical text information system since IR plays two important roles in any TM application  The first is to enable fast reduction of the data size by filtering out a large amount of nonrelevant text data to obtain a small set of most relevant data to a particular application problem.
The second analyst verify interpret text analyst search browsing functions pattern feature availability toolkit	 The second is to support an analyst to verify and interpret any patterns discovered from text data where an analyst would need to use search and browsing functions to reach and examine the most relevant support data to the pattern  Another feature that sets this book apart is the availability of a companion toolkit for information retrieval and text mining i.
feature sets retrieval	 Another feature that sets this book apart is the availability of a companion toolkit for information retrieval and text mining i e  the META toolkit available at httpsmetatoolkit.
toolkit available	e  the META toolkit available at httpsmetatoolkit.
implementations book Many toolkit help readers acquire experimenting techniques applying solve application	org which contains implementations of many techniques discussed in the book  Many exercises in the book are also designed based on this toolkit to help readers acquire practical skills of experimenting with the learned techniques from the book and applying them to solve realworld application problems.
required content book book readers knowledge science particularly structures comfortable statistics probability estimation background basic ideas discussed background carefully 2 reading Bibliographical chapter solid concepts	 The required background knowledge to understand the content in this book is minimal since the book is intended to be mostly selfcontained  However readers are expected to have basic knowledge about computer science particularly data structures and programming languages and be comfortable with some basic concepts in probability and statistics such as conditional probability and parameter estimation  Readers who do not have this background may still be able to follow the basic ideas of most of the algorithms discussed in the book they can also acquire the needed background by carefully studying Chapter 2 of the book and if necessary reading some of the references mentioned in the Bibliographical Notes section of that chapter to have a solid understanding of all the major concepts mentioned therein.
readers data structures concepts conditional background able basic ideas algorithms book studying 2 mentioned Bibliographical section solid META experiment require knowledge	 However readers are expected to have basic knowledge about computer science particularly data structures and programming languages and be comfortable with some basic concepts in probability and statistics such as conditional probability and parameter estimation  Readers who do not have this background may still be able to follow the basic ideas of most of the algorithms discussed in the book they can also acquire the needed background by carefully studying Chapter 2 of the book and if necessary reading some of the references mentioned in the Bibliographical Notes section of that chapter to have a solid understanding of all the major concepts mentioned therein  META can be used by anyone to easily experiment with algorithms and build applications but modifying it or extending it would require at least some basic knowledge of C programming.
Readers background acquire needed background carefully studying Chapter 2 book reading references Notes section solid understanding major	 Readers who do not have this background may still be able to follow the basic ideas of most of the algorithms discussed in the book they can also acquire the needed background by carefully studying Chapter 2 of the book and if necessary reading some of the references mentioned in the Bibliographical Notes section of that chapter to have a solid understanding of all the major concepts mentioned therein.
” al.	” Lyman et al.
documents terabytes.	 office documents represent 195 terabytes.
Of course blog forum posts scientific literature Roe 2012 updates count 2003 trillion emails sent 2010.	” Of course there are also blog articles forum posts tweets scientific literature government documents etc  Roe 2012 updates the email count from 610 billion emails in 2003 to 107 trillion emails sent in 2010.
natural language natural way As result	 Text natural language is the most natural way of encoding human knowledge  As a result most human knowledge is encoded in the form of text data.
information strong software tools provide following people exploit big text	 The explosive growth of online text information has created a strong demand for intelligent software tools to provide the following two related services to help people manage and exploit big text data.
fact data produced humans communication purposes generally contain knowledge preferences offer opportunity applications ions preferences text data opinionated reviews forum media text topics product service.	 Due to the fact that text data are produced by humans for communication purposes they are generally rich in semantic content and often contain valuable knowledge information opinions and preferences of people  As such they offer great opportunity for discovering various kinds of knowledge useful for many applications especially knowledge about human opin ions and preferences which is often directly expressed in text data  For example it is now the norm for people to tap into opinionated text data such as product reviews forum discussions and social media text to obtain opinions about topics interesting to them and optimize various decisionmaking tasks such as purchasing a product or choosing a service.
services discussed text mining correspond natural shown Figure	 The two services discussed above i e  text retrieval and text mining conceptually correspond to the two natural steps in the process of analyzing any “big text data” as shown in Figure 1.
	e.
In step goal users relevant text 6 Introduction obtain small relevant text text data digest content text This goal discover knowledge patterns data support	 In this first step the main goal is to connect users or applications with the most relevant text data  6 Chapter 1 Introduction Once we obtain a small set of most relevant text data we would need to further analyze the text data to help users digest the content and knowledge in the text data  This is the text mining step where the goal is to further discover knowledge and patterns from text data so as to support a user’s task.
For access information querying recommender relevant items main user relevant TIS offering capability minimum text data information user’s original	 For example a search engine enables a user to access text information through querying whereas a recommender system can push relevant information to a user as new information items become available  Since the main purpose of Information Access is to connect a user with relevant information a TIS offering this capability generally only does minimum analysis of text data sufficient for matching relevant information with a user’s information need and the original inforation items e.
From perspective generally information exploit information Knowledge Acquisition Text	 From the perspective of text analysis a user would generally need to read the information items to further digest and exploit the delivered information  Knowledge Acquisition Text Analysis.
Knowledge Text Analysis.	 Knowledge Acquisition Text Analysis.
	 Text Organization.
access classified push.	 Information access can be further classified into two modes pull and push.
information access hoc 1 Functions Information Systems 9	 This mode of information access is often very useful when a user has an ad hoc 1 1 Functions of Text Information Systems 9 information need i e.
1 Information information information	1 Functions of Text Information Systems 9 information need i e  a temporary information need e.
need	g  an immediate need for opinions about a product.
language developed methods text communities slightly different problem text mining e.	 Both the data mining community and the natural language processing NLP community have developed methods for text mining although the two communities tend to adopt slightly different perspective on the problem  From a data mining perspective we may view text mining as mining a special kind of data i e.
naturally discover extract interesting text data latent trends outliers From text text convert form represen tation inferences based	 Following the general goals of data mining the goal of text mining would naturally be regarded as to discover and extract interesting patterns in text data which can include latent topics topical trends or outliers  From an NLP perspective text mining can be regarded as to partially understand natural language text convert text into some form of knowledge represen tation and make limited inferences based on the extracted knowledge.
information aims extract	 Thus a key task is to perform information extraction which often aims to identify and extract mentions of various entities e.
g	g  people organization and location and their relations e.
location relations e g.	 people organization and location and their relations e g.
classified direct discovered directly consumed users discovered isn’t necessarily user information access based discovered.	 Applications of text mining can be classified as either direct applications where the discovered knowledge would be directly consumed by users or indirect applications where the discovered knowledge isn’t necessarily directly useful to a user but can indirectly help a user through better support of information access  Knowledge acquisition can also be further classified based on what knowledge is to be discovered.
based range variations impossible use number Nevertheless identify common categories cover book.	 Knowledge acquisition can also be further classified based on what knowledge is to be discovered  However due to the wide range of variations of the “knowledge” it is impossible to use a small number of categories to cover all the variations  Nevertheless we can still identify a few common categories which we cover in this book.
Framework Information information consist illustrated Figure	2 Conceptual Framework for Text Information Systems Conceptually a text information system may consist of several modules as illustrated in Figure 1.
query return documents The TIS called engine.	 Search  Take a user’s query and return relevant documents  The search component in a TIS is generally called a search engine.
The TIS called engines useful engines effectively text	 The search component in a TIS is generally called a search engine  Web search engines are among the most useful search engines that enable users to effectively and efficiently deal with a huge amount of text data.
Depending relevant nonrelevant component TIS items items user relevant	 Depending on whether the system focuses on recognizing relevant items or nonrelevant items this component in a TIS may be called a recommender system whose goal is to recommend relevant items to users or a filtering system whose goal is to filter out nonrelevant items to allow a user to keep only the relevant items.
kinds meaningful enriching enables deeper	 The categorization component in a TIS can annotate text objects with all kinds of meaningful categories thus enriching the representation text data which further enables more effective and deeper text analysis.
Subject categorizers sentiment sentiment polarity specific examples text system.	 Subject categorizers that classify a text article into one or multiple subject categories and sentiment taggers that classify a sentence into positive negative or neutral in sentiment polarity are both specific examples of a text categorization system.
	 Topic Analysis.
companion nontextual time location data topic analysis generate interesting trends distributions profiles Extraction.	 When combined with the companion nontextual data such as time location authors and other meta data topic analysis can generate many interesting patterns such as temporal trends of topics spatiotemporal distributions of topics and topic profiles of authors  Information Extraction.
	     .
The plays important role helping information	   The clustering component of a TIS plays an important role in helping users explore an information space.
It useful browsing large	 It uses empirical data to create meaningful structures that can be useful for browsing text objects and obtaining a quick understanding of a large text data set.
display	 Visually display patterns in text data.
3 visualization results text mining generally	3 Organization of the Book 13 visualization of the results generated from various text mining algorithms is generally desirable.
Information focus general data language information	 Information extraction is not covered in this book since we want to focus on general approaches that can be readily applied to text data in any natural language but information extraction often requires languagespecific techniques.
techniques references chapter 1.	 Readers interested in these techniques can find some useful references in the Bibliographic Notes at the end of this chapter  1.
3 The book parts	 1 3 Organization of the Book The book is organized into four parts as shown in Figure 1.
Book book Figure	3 Organization of the Book The book is organized into four parts as shown in Figure 1 4  Part I.
	4  Part I.
	 Overview and Background.
language text data informative text needed text applications.	 This part also gives a brief overview of natural language processing techniques needed for understanding text data and obtaining informative representation of text needed in all text data analysis applications.
Part IV Text Management	 Part IV  Unified Text Management and Analysis System.
discuss requiring detailed analysis data natural language processing techniques Such “deep techniques like indepth text human	 Most techniques we discuss can be implemented without any human effort or only requiring minimal human effort this is in contrast to some more detailed analysis of text data particularly using natural language processing techniques  Such “deep analysis” techniques are obviously very important and are indeed necessary for some applications where we would like to go indepth to understand text in detail  However at this point these techniques are often not scalable and they tend to require a large amount of human effort.
However techniques scalable effort.	 However at this point these techniques are often not scalable and they tend to require a large amount of human effort.
META large contribution.	 The provided code in META should give a large head start and allow you to focus more on your contribution.
logical choose As prerequisite assume basic knowledge probability modifying config	 If used in class there are several logical flows that an instructor may choose to take  As prerequisite knowledge we assume some basic knowledge in probability and statistics as well as programming in a language such as C or Java  META is written in modern C although some exercises may be accomplished only by modifying config files.
As knowledge assume C modern modifying	 As prerequisite knowledge we assume some basic knowledge in probability and statistics as well as programming in a language such as C or Java  META is written in modern C although some exercises may be accomplished only by modifying config files.
2 undergraduate introduction Information Retrieval search assume basic programming experience choose 1.	 For example Part 1 and Part 2 of the book may be used as an undergraduate introduction to Information Retrieval with a focus on how search engines work  Exercises assume basic programming experience and a little mathematical background in probability and statistics  A different undergraduate course may choose to survey 1.
nature toolkit experiments existing	 Due to the modular nature of the toolkit additional programming experiments may be created by extending the existing system or implementing other wellknown algorithms that do not come with META by default.
students use META learned complete programming different yield challenges e.	 Finally students may use components of META they learned through the exercises to complete a larger final programming project  Using different corpora with the toolkit can yield different project challenges e.
review summary analysis	 review summary vs  sentiment analysis  Practitioners.
Then deeper Since programming required.	 Then you may choose a chapter relevant to your current interests and delve deeper or refresh your knowledge  Since many applications in META can be used simply via config files we anticipate it as a quick way to get a handle on your dataset and provide some baseline results without any programming required.
The exercises particular	 The exercises at the end of each chapter can be thought of as default implementations for a particular task at hand.
Croft et al.	 1999 and Belew 2008  The most recent ones are Manning et al  2008 Croft et al.
book treats books conceptual analyzing data gap applications providing companion Readers know history research early milestones look collection readings Sparck topic mining multiple	 More importantly this book treats all these topics in a more systematic way than existing books by framing them in a unified coherent conceptual framework for managing and analyzing big text data the book also attempts to minimize the gap between abstract explanation of algorithms and practical applications by providing a companion toolkit for many exercises  Readers who want to know more about the history of IR research and the major early milestones should take a look at the collection of readings in Sparck Jones and Willett 1997  The topic of text mining has also been covered in multiple books e.
	e.
2 help text knowledge	 data reduction and 2 help users verify the source text articles from which knowledge is discovered by a text mining algorithm i.
provides coverage text book readily text data human	 knowledge provenance  As a result this book provides a more complete coverage of techniques required for developing big text data applications  The focus of this book is on covering algorithms that are general and robust which can be readily applied to any text data in any natural language often with no or minimum human effort.
interested IE Sarawagi 2008 review Jiang	 Readers who are interested in knowing more about IE can start with the survey book Sarawagi 2008 and review articles Jiang 2012.
2011 learning	 BaezaYates and RibeiroNeto 2011 and books on machine learning e.
Basics Statistics models important text mining algorithms section sufficient vocabulary understand	1 Basics of Probability and Statistics As we will see later in this chapter and in many other chapters probabilistic or statistical models play a very important role in text mining algorithms  This section gives every reader a sufficient background and vocabulary to understand these probabilistic and statistical approaches covered in the later chapters of the book.
We represent probability distribution collection index 16 second index corresponds 16	 We can represent our probability distribution as a collection of probabilities where the first index corresponds to pred  16  the second index corresponds to porange  16  and so on.
text mining usually The estimate accurate probabilistic is.	 In our text mining tasks we usually try to estimate θ given some knowledge about  The different methods to estimate θ will determine how accurate or useful the probabilistic model is.
drawn variable x drawn θ random variable takes certain ∼ red	 We read this as x is drawn from theta or the random variable x is drawn from the probability distribution θ   The random variable x takes on each value from with a certain probability defined by θ   For example if we had x ∼ θ ′ then there is a 2 3 chance that x is either red or orange.
θ probability	 Thus if we have some word w we can write pw  θ read as the probability of w given θ .
If 0.	 If w is the word data we might have pw  data  θ  0.
003 003 considered	003 or equivalently pθw  data 0 003  In our examples we have only considered discrete probability distributions.
examples considered That models assign reality continuous probability distributions infinite countable.	 In our examples we have only considered discrete probability distributions  That is our models only assign probabilities for a finite discrete set of outcomes  In reality there are also continuous probability distributions where there are an infinite number of “events” that are not countable.
sanity models.	 We discuss them now since they are a good sanity check when designing your own models.
	1 .
The disjoint events sums one∑ 1.	 The probability of all disjoint events sums to one∑ ω∈ pθω  1.
Note strictly defined space happens random e.	 2 3 Note that strictly speaking an event is defined as a subset of the probability space  and we say that an event happens if and only if the outcome from a random experiment i e.
e.	e.
1 Joint Probabilities let’s modify original die We indicating θC 1 6 1 assume shape.	1 Joint and Conditional Probabilities For this section let’s modify our original die rolling example  We will keep the original distribution as θC indicating the color probabilities θC 1 6 1 6 1 6 1 6 1 6 1 6   Let’s also assume that each color is represented by a particular shape.
Let’s shape This like orange blue green purple respectively.	 Let’s also assume that each color is represented by a particular shape  This makes our die look like 24 Chapter 2 Background  where the colors of the shapes are red orange yellow blue green and purple respectively.
For example red xS circle green circle This notation In case 6	 For example what is the probability that xC  red and xS  circle Since there are no red circles this has probability zero  How about pxC  green xS  circle This notation signifies the joint probability of the two random variables  In this case the joint probability is 1 6 because there is only one green circle.
This notation joint probability joint 1 6 blue	 How about pxC  green xS  circle This notation signifies the joint probability of the two random variables  In this case the joint probability is 1 6 because there is only one green circle  Consider a modified die  where we changed the color of the blue circle the fourth element in the set to green.
modified changed	 Consider a modified die  where we changed the color of the blue circle the fourth element in the set to green.
What pxC green xS criteria 1 3 example die 5 7 combinations color green 12 probability measures likelihood event given event occurred.	 What would pxC  green xS  circle be Since two out of the six elements satisfy both these criteria the answer is 2 6  1 3   As another example if we had a 12sided fair die with 5 green circles and 7 other combinations of shape and color then pxC  green xS  circle  5 12   A conditional probability measures the likelihood that one event occurs given that another event has already occurred.
measures likelihood unique colors.	 A conditional probability measures the likelihood that one event occurs given that another event has already occurred  Let’s use the original die with six unique colors.
We conditional random following pX Y pY y	 We can write the conditional probabilities for two random variables X and Y based on their joint probability with the following equation pX  x  Y  y  pX  x  Y  y pY  y .
	4 2.
y observed pY true	 both x and y have been observed which is normalized by pY  y the probability that the condition is true i e.
	e.
situation ∼ That color distribution Does knowledge probability No draw	 Consider another situation where we have c1 c2 ∼ θC  That is we draw two colors from the color distribution  Does the knowledge of c1 inform the probability of c2 No since each draw is done “independently” of the other.
For example given observing word information What probability retrieval know	 For example given a document what is the probability of observing the word information and retrieval in the same sentence What is the probability of observing retrieval if we know information has occurred 2.
2 Rule rule Y X setting joint pY We pX XpX pY	2 Bayes’ Rule Bayes’ rule may be derived using the definition of conditional probability pX  Y   pX Y pY and pY  X  pY  X pX   Therefore setting the two joint probabilities equal pX  Y pY   pX Y   pY  XpX  We can simplify them as pX  Y   pY  XpX pY .
word text easily mapped flipping coin The heads 1	 For example we may be interested in modeling the presence or absence of a particular word in a text document which can be easily mapped to a coin flipping problem  There are two possible outcomes in coin flipping heads or tails  The probability of heads is denoted as θ  which means the probability of tails is 1 − θ .
possible flipping tails probability heads tails .	 There are two possible outcomes in coin flipping heads or tails  The probability of heads is denoted as θ  which means the probability of tails is 1 − θ .
probability distribution The Bernoulli distribution gives probability event—flipping	 To model the probability of success in our case “heads” we can use the Bernoulli distribution  The Bernoulli distribution gives the probability of success for a single event—flipping the coin once.
The Bernoulli distribution probability event—flipping want k binomial distribution.	 The Bernoulli distribution gives the probability of success for a single event—flipping the coin once  If we want to model n throws and find the probability of k successes we instead use the binomial distribution.
The binomial distribution	 The binomial distribution is a discrete distribution since k is an integer.
But Well break means − tails observing heads probability remaining n tails − θ .	7 But why is it this formula Well let’s break it apart  If we have n total binary trials and want to see k heads that means we must have flipped k heads and n − k tails  The probability of observing each of the k heads is θ  while the probability of observing each of the remaining n − k tails is 1 − θ .
assume flips independent simply multiply outcomes don’t additionally multiply number ways k set items What care order For particular sequence outcomes t h h respectively easy	 Since we assume all these flips are independent we simply multiply all the outcomes together  Since we don’t care about the order of the outcomes we additionally multiply by the number of possible ways to choose k items from a set of n items  What if we do care about the order of the outcomes For example what is the probability of observing the particular sequence of outcomes h t  h h t where h and t denote heads and tails respectively Well it is easy to see that the probability 2.
Since additionally possible choose k set items order probability observing outcomes h heads Well	 Since we don’t care about the order of the outcomes we additionally multiply by the number of possible ways to choose k items from a set of n items  What if we do care about the order of the outcomes For example what is the probability of observing the particular sequence of outcomes h t  h h t where h and t denote heads and tails respectively Well it is easy to see that the probability 2.
Probability 27 sequence simply i.	1 Basics of Probability and Statistics 27 of observing this sequence is simply the product of observing each event i.
4 Likelihood Now model given observed data For observe data n 5 D h h t.	 2 1 4 Maximum Likelihood Parameter Estimation Now that we have a model for our coin flipping how can we estimate its parameters given some observed data For example maybe we observe the data D that we discussed above where n  5 D  h t  h h t.
need log f − 2 0 θ 35 generally let H	 Thus we just need to solve the following equation d log f θ dθ 3 θ − 2 1 − θ 0 and we easily find that the solution is θ  35  More generally let H be the number of heads and T be the number of tails.
θ Thus value max perform transformation	 pD  θ reach its maximum  Thus the value of an arg max expression stays the same if we perform any monotonic transformation of the function inside arg max.
max perform transformation arg use logarithm	 Thus the value of an arg max expression stays the same if we perform any monotonic transformation of the function inside arg max  This is why we could use the logarithm transformation in the example above which made it easier to compute the derivative.
outcomes probability according This especially sample small motivates parameter discuss	 Note that a consequence of this is that all unobserved outcomes would have a zero probability according to MLE  This is often not reasonable especially when the data sample is small a problem that motivates Bayesian parameter estimation which we discuss below  2.
probability	0 while the probability of tails is 0.
estimation distribution treat parameter Specifically use pθ possible prior value data provide evidence prior	 In Bayesian parameter estimation we consider a distribution over all the possible values for the parameter that is we treat the parameter itself as a random variable  Specifically we may use pθ to represent a distribution over all possible values for θ  which encodes our prior belief about what value is the true value of θ  while the data D provide evidence for or against that belief  The prior belief pθ can then be updated based on the observed evidence.
Since likelihood data remains observing pθ D usually pθ That posterior times posterior characterizes value including estimate	 Since the likelihood of the data remains constant observing the constraint that pθ  D must sum to one over all possible values of θ  we usually just say pθ  D ∝ pθpD  θ  That is the posterior is proportional to the prior times the likelihood  The posterior distribution of the parameter θ fully characterizes the uncertainty of the parameter value and can be used to infer any quantity that depends on the parameter value including computing a point estimate of the parameter i.
xf xdx 12 mode Posteriori arg max	11 while in a continuous distribution EX ∫ x xf xdx 2 12 Sometimes we are interested in using the mode of the posterior distribution as our estimate of the parameter which is called Maximum a Posteriori MAP estimate given by θMAP  arg max θ pθ  D  arg max θ pD  θpθ  2.
13 30 2 deviate consideration maximizing probability according prior encoded prior address overfitting problem prior strongly prefer tails zero	13 30 Chapter 2 Background Here it is easy to see that the MAP estimate would deviate from the MLE with consideration of maximizing the probability of the parameter according to our prior belief encoded as pθ  It is through the use of appropriate prior that we can address the overfitting problem of MLE since our prior can strongly prefer an estimate where neither heads nor tails should have a zero probability.
For continuation indepth Appendix	 For a continuation and more indepth discussion of this material consult Appendix A  2.
Such model model model text	 Such a model is often called statistical language model or a generative model for text data i.
model words.	e  a probabilistic model that can be used for sampling sequences of words.
g.	g.
Learning means parameters distribution element workflow like following.	” Learning a model means estimating its parameters  In the case of a distribution over words we have one parameter for each element in V   The workflow looks like the following.
In wish individual words In step actually set word.	 In our example we wish to capture the probabilities of individual words occurring in our corpus  In the second step we need to figure out actually how to set the probabilities for each word.
second actually probabilities obvious calculate individual total words 2.	 In the second step we need to figure out actually how to set the probabilities for each word  One obvious way would be to calculate the probability of each individual word in the corpus itself  That is the count of a unique word wi divided by the total number of words 2.
One obvious calculate probability individual	 One obvious way would be to calculate the probability of each individual word in the corpus itself.
formally quantified value variable coin depicting heads representing outcomes	 The problem can be formally defined as the quantified uncertainty in predicting the value of a random variable  In the common example of a coin the two values would be 1 or 0 depicting heads or tails and the random variable representing these outcomes is X.
words variable difficult	 In other words X 1 if heads 0 if tails  The more random this random variable is the more difficult the prediction of heads or tails will be.
varies 0 1 example space heads entropy 0 log2 pX pX 1 log2 pX fair 0 .	 Thus the amount of randomness varies from 0 to 1  For our coin example where the sample space is two events heads or tails the entropy function looks like HX  −pX  0 log2 pX  0 − pX  1 log2 pX  1  For a fair coin we would have pX  1  pX  0  1 2 .
If plot heads receive shown 1 ends probability 1	 If we plot HX for our coin example against the probability of heads pX  1 we receive a plot like the one shown in Figure 2 1  At the two ends of the xaxis the probability of X  1 is either very small or very large.
probability 1 small cases function value outcome	1  At the two ends of the xaxis the probability of X  1 is either very small or very large  In both these cases the entropy function has a low value because the outcome is not very random.
Since symmetric U shape plot 1 varies.	 Since the two probabilities are symmetric we get a symmetric inverted U shape as the plot of HX as pX  1 varies.
It’s word predict	 It’s not surprising to see this word in a document thus it is easy to predict that Wthe  1.
When attempt Y X given observe taken distribution Y 0 Y Y entropy	 When we attempt to quantify uncertainties of conditional probabilities we can also define conditional entropy HX  Y  which indicates the expected uncertainty of X given that we observe Y  where the expectation is taken under the distribution of all possible values of Y   Intuitively if X is completely determined by Y  then HX  Y   0 since once we know Y  there would be no uncertainty in X whereas if X and Y are independent then HX  Y  would be the same as the original entropy of X i e.
I X Y 15 It shown X Y HY	 I X Y   HX − HX  Y   2 15 It can be shown that mutual information can be equivalently written as I X Y   HY − HY  X.
	 2.
16 It easy I tends Y correlated Y extreme Y reduction HX I X completely 0 I X HX Intuitively mutual measure variables.	16 It is easy to see that I X Y  tends to be large if X and Y are correlated whereas I X Y  would be small if X and Y are not so related indeed in the extreme case when X and Y are completely independent there would be no reduction of entropy and thus HX  HX  Y  and I X Y   0  However if X is completely determined by Y  then HX  Y   0 thus I X Y   HX  Intuitively mutual information can measure the correlation of two random variables.
Y HX I X Y Intuitively information measure correlation	 However if X is completely determined by Y  then HX  Y   0 thus I X Y   HX  Intuitively mutual information can measure the correlation of two random variables.
Clearly correlation measure X mutual information symmetric.	 Clearly as a correlation measure on X and Y  mutual information is symmetric.
concepts conditional entropy discussed later book	 Applications of these basic concepts including entropy conditional entropy and mutual information will be further discussed later in this book  2.
Machine Learning learning problems In text indepth topic book introduce learning needed	3 Machine Learning Machine learning is a very important technique for solving many problems and has very broad applications  In text data management and analysis it has also many uses  Any indepth treatment of this topic would clearly be beyond the scope of this book but here we introduce some basic concepts in machine learning that are needed to better understand the content later in the book.
techniques supervised learning unsupervised In supervised learning learn function ŷ f set value data expected value y x form	 Machine learning techniques can often be classified into two types supervised learning and unsupervised learning  In supervised learning a computer would learn how to compute a function ŷ  f x based on a set of examples of the input value x called training data and the corresponding expected output value y  It is called “supervised” because typically the y values must be provided by humans for each x and thus the computer receives a form of supervision from the humans.
learning ŷ based set input called training data output called “supervised” typically x Once able unseen compute function	 In supervised learning a computer would learn how to compute a function ŷ  f x based on a set of examples of the input value x called training data and the corresponding expected output value y  It is called “supervised” because typically the y values must be provided by humans for each x and thus the computer receives a form of supervision from the humans  Once the function is learned the computer would be able to take unseen values of x and compute the function f x.
serve x multiple correct labels called classification classification binary	 can serve as a classifier in that it can be used to map an instance x to the “right” label or multiple correct labels when appropriate  Thus the problem can be called a classification problem  The simplest case of the classification problem is when we have just two labels known as binary classification.
takes regression	3 Machine Learning 35 takes a real value the problem is often called a regression problem.
In classification far problem text categorization classification later book Chapter 15.	 In textbased applications both forms may occur although the classification problem is far more common in which case the problem is also called text categorization or text classification  We dedicate a chapter to this topic later in the book Chapter 15.
chapter topic The regression occur use text predict nontext stock prices discussed later regression data feature potential likely value	 We dedicate a chapter to this topic later in the book Chapter 15  The regression problem may occur when we use text data to predict another nontext variable such as sentiment rating or stock prices both cases are also discussed later  In classification as well as regression the input data instance x is often represented as a feature vector where each feature provides a potential clue about which y value is most likely the value of f x.
training data influence final value “Optimal” simply error e.	 What the computer learns from the training data is an optimal way to combine these features with weights on them to indicate their importance and their influence on the final function value y  “Optimal” here simply means that the prediction error on the training data is minimum i e.
correct predicted 36 Chapter 2 generate cases data apply machine	 determine how many correct labels were predicted  In applications the training data are generally 36 Chapter 2 Background all the labelled examples that we can generate and the test cases are the data points to which we would like to apply our machine learning program.
In contrast unsupervised knowing compute y x.	 In contrast to supervised learning in unsupervised learning we only have the data instances X without knowing Y  In such a case obviously we cannot really know how to compute y based on an x.
based Since effort	 In such a case obviously we cannot really know how to compute y based on an x  However we may still learn latent properties or structures of X  Since there is no human effort involved such an approach is called unsupervised.
model estimate values data parameter values knowledge approach analyzing latent topics text Chapter Notes Reading probability textbooks Lehmann 1970.	 By fitting the model to our data we can estimate the parameter values that can best explain the data and treat the obtained parameter values as the knowledge discovered from the data  Applications of such an approach in analyzing latent topics in text are discussed in detail in Chapter 17  Bibliographic Notes and Further Reading Detailed discussion of the basic concepts in probability and statistics can be found in many textbooks such as Hodges and Lehmann 1970.
topics	 Applications of such an approach in analyzing latent topics in text are discussed in detail in Chapter 17.
Data In chapter introduce language processing	 3Text Data Understanding In this chapter we introduce basic concepts in text data understanding through natural language processing NLP.
While human instantly understand native language challenging In general following Lexical analysis.	 While a human can instantly understand a sentence in their native language it is quite challenging for a computer to make sense of one  In general this may involve the following tasks  Lexical analysis.
purpose analysis sentence analysis.	 The purpose of syntactic analysis is to determine how words are related with each other in a sentence thus revealing the syntactic structure of a sentence  Semantic analysis.
purpose meaning sentence.	 The purpose of semantic analysis is to determine the meaning of a sentence.
The purpose analysis meaning	 Pragmatic analysis  The purpose of pragmatic analysis is to determine meaning in context e.
	g.
3 simple English playground The case determining syntactic categories noun verb.	 In Figure 3 1 we show what is involved in understanding a very simple English sentence “A dog is chasing a boy on the playground ” The lexical analysis in this case involves determining the syntactic categories parts of speech of all the words for example dog is a noun and chasing is a verb.
playground playground prepositional Semantic phrases entities relations meaning	 So do the and playground and on the playground is a prepositional phrase  Semantic analysis is to map noun phrases to entities and verb phrases to relations so as to obtain a formal representation of the meaning of the sentence.
	 d1.
verb predicate figure Note infer based knowledge.	 The verb phrase can be mapped to a relation predicate chasingd1b1p1 as shown in the figure  Note that with this level of understanding one may also infer additional information based on any relevant common sense knowledge.
For example chased b1	 For example if we assume that if someone is being chased he or she may be scared we could infer that the boy being chased b1 may be scared.
general natural designed contrast designed facilitate understanding Specifically	1 it is in general very challenging to do this kind of analysis for unrestricted natural language text  The main reason for this difficulty is because natural language is designed to make human communication efficient this is in contrast with a programming language which is designed to facilitate computer understanding  Specifically there are two reasons why NLP is very difficult.
e problems intelligence.	e  as difficult as any other difficult problems in artificial intelligence.
example language processing language” “natural Another example man saw boy syntactic result phrase PP	 For example natural language processing can have two different interpretations “processing of natural language” vs  “natural processing of language” ambiguous modification  Another example A man saw a boy with a telescope has two distinct syntactic structures leading to a different result regarding who had the telescope ambiguous prepositional phrase PP attachment.
He quit inferences way	 He has quit smoking implies that he smoked before making such inferences in a general way is difficult.
clear stated	 Soon however it was clear as stated in BarHillel’s report in 1960 that fullyautomatic highquality translation could not be accomplished without knowledge.
One Eliza play natural dialogue human world feasibility natural 1970s–1980s paid process naturallanguage data	 One is the Eliza project where shallow rules are used to enable a computer to play the role of a therapist to engage a natural language dialogue with a human  The other is the block world project which demonstrated feasibility of deep semantic understanding of natural language when the language is limited to a toy domain with only blocks as objects  In the 1970s–1980s attention was paid to process realworld naturallanguage text data particularly story understanding.
knowledge heuristic rules simple stories understand need representation	 Many formalisms for knowledge representation and heuristic inference rules were developed  However the general conclusion was that even simple stories are quite challenging to understand by a computer confirming the need for largescale knowledge representation and inferences under uncertainty.
In symbolic statistical tend instead NLP Text rely solely data learning	 In contrast to symbolic approaches statistical approaches tend to be more robust because they have less reliance on humangenerated rules instead they often take advantage of regularities and patterns in 3 2 NLP and Text Information Systems 43 empirical uses of language and rely solely on labeled training data by humans and application of machine learning techniques.
shows TIS different	 Figure 3 2 shows a number of TIS tasks that require somewhat different levels of NLP.
end question understanding example parse language structure similarly comes application task difficulty understanding natural solve problem.	 At the other end however tasks such as machine translation and question answering would require much more precise understanding for example a wrong parse of a sentence generally would lead to a wrong translation unless the target language has a similar ambiguity structure and similarly a wrong understanding of the question would lead to wrong answers  When it comes to a specific application task it is often possible to bypass the difficulty in accurately understanding natural language and go directly to solve the application problem.
	nott.
ac	ac ukpszgxkcoursesg5aiai002historyeliza.
	ukpszgxkcoursesg5aiai002historyeliza.
httpwww.	htm httpwww.
ELIZA Can come	 ELIZA Can you think of a specific example Person Well my boyfriend made me come here.
sorry It’s true	 ELIZA I am sorry to hear that you are depressed  Person It’s true  I am unhappy.
I unhappy.	 I am unhappy.
e.	e.
g elements rule like machine translation statistical	g  specify the elements in a rule  Even difficult tasks like machine translation can be done by such statistical approaches.
Let’s look sentence dog chasing playground Figure	 Let’s take a look at the example sentence A dog is chasing a boy on the playground in Figure 3 3.
3.	3.
represent sentence	 We can represent this sentence in many different ways  First we can always represent such a sentence as a string of characters  This is true for every language.
represent sentence	 First we can always represent such a sentence as a string of characters.
representing data possibilities representation string it’s actually identify word text characters words.	 Therefore representing text data as a sequence of words opens up a lot of interesting analysis possibilities  However this level of representation is slightly less general than a string of characters  In some languages such as Chinese it’s actually not that easy to identify all the word boundaries since in such a language text is a sequence of characters with no spaces in between words.
If language POS tags words frequent kind verbs This interesting opportunities analysis.	 If we go further in natural language processing we can add partofspeech POS tags to the words  This allows us to count for example the most frequent nouns or we could determine what kind of nouns are associated with what kind of verbs  This opens up more interesting opportunities for further analysis.
Note 3 additional tags word sequence additional data.	 Note in Figure 3 3 that we use a plus sign on the additional features because by representing text as a sequence of part of speech tags we don’t necessarily replace the original word sequence  Instead we add this as an additional way of representing text data.
sign additional features sequence don’t replace Instead add additional representing text	3 that we use a plus sign on the additional features because by representing text as a sequence of part of speech tags we don’t necessarily replace the original word sequence  Instead we add this as an additional way of representing text data.
Again interesting styles analysis We recognize person	 Again this further opens up more interesting analysis of for example the writing styles or grammatical error correction  If we go further still into semantic analysis then we might be able to recognize dog as an animal  We also can recognize boy as a person and playground as a location and analyze their relations.
Such representation words POS tags.	 Such a highlevel representation is even less robust than the sequence of words or POS tags.
As text string They robust general In don’t know word help.	 As a sequence of characters text can only be processed by string processing algorithms  They are very robust and general  In a compression application we don’t need to know word boundaries although knowing word boundaries might actually help.
In compression boundaries help.	 In a compression application we don’t need to know word boundaries although knowing word boundaries might actually help.
words opposed important level indicating supports analysis analysis	 Sequences of words as opposed to characters offer a very important level of representation it’s quite general and relatively robust indicating that it supports many analysis techniques such as word relation analysis topic analysis and sentiment analysis.
Moving we’ll add	 Moving down we’ll see we can gradually add additional representations.
stylistic 3 Data Understanding analysis generally requires	 For example stylistic 50 Chapter 3 Text Data Understanding analysis generally requires syntactical structure representation.
For ontology information good example application level representation biologists.	 For example we can add an ontology on top of extracted information from text to make inferences  A good example of an application enabled by this level of representation is a knowledge assistant for biologists.
able manage relevant research problem understanding functions	 This system is able to manage all the relevant knowledge from literature about a research problem such as understanding gene functions  The computer can make inferences about some of the hypotheses that a biologist might be interested in.
support sophisticated statistical model language model distribution word	 Still other levels can be combined in order to support more linguistically sophisticated applications as needed  3 4 Statistical Language Models A statistical language model or just language model for short is a probability distribution over word sequences.
solution probability Today Wednesday.	 In the language model shown above the sequence The equation has a solution has a smaller probability than Today is Wednesday.
Given sports news baseball information retrieval If enumerate possible probability sequence model complex number parameters infinite potentially	 Given that a user is interested in sports news how likely would it be for the user to use baseball in a query This is directly related to information retrieval  If we enumerate all the possible sequences of words and give a probability to each sequence the model would be too complex to estimate because the number of parameters is potentially infinite since we have a potentially infinite number of word sequences.
data	 That is we would never have enough data to estimate these parameters.
Thus probability sequence probability word.	 Thus the probability of a sequence of words would be equal to the product of the probability of each word.
i1	  wn n∏ i1 pwi.
e.	e.
e best data θ̂ pD 3.	e  best explain the data θ̂  arg maxθ pD  θ  3.
3 4 D word D number D.	 That is pw  θ̂   cw D D  3 3 3 4 Statistical Language Models 53 where cw D is the count of word w in D and D is the length of D or total number of words in D.
Statistical cw D number	4 Statistical Language Models 53 where cw D is the count of word w in D and D is the length of D or total number of words in D.
Such maximize estimate research article use likelihood estimator model based clearly assign zero probability words abstract probability uses abstract.	 Such an estimate is optimal in the sense that it would maximize the probability of the observed data but whether it is really optimal for an application is still questionable  For example if our goal is to estimate the language model in the mind of an author of a research article and we use the maximum likelihood estimator to estimate the model based only on the abstract of a paper then it is clearly nonoptimal since the estimated model would assign zero probability to any unseen words in the abstract which would make the whole article have a zero probability unless it only uses words in the abstract.
estimate token event event assigned word probabilities sum likelihood data We discuss techniques maximum estimator later techniques called Although extremely unigram language	 Note that in general the maximum likelihood estimate would assign zero probability to any unseen token or event in the observed data this is so because assigning a nonzero probability to such a token or event would take away probability mass that could have been assigned to an observed word since all probabilities must sum to 1 thus reducing the likelihood of the observed data  We will discuss various techniques for improving the maximum likelihood estimator later by using techniques called smoothing  Although extremely simple a unigram language model is already very useful for text analysis.
text database database mining research paper In words functional frequently text going contentcarrying topical	 a general English text database a computer science research article database and a text mining research paper  In general the words with the highest probabilities in all the three models are those functional words in English because such words are frequently used in any text  After going further down on the list of words one would see more contentcarrying and topical words.
words semantically associated like	 For example we can use them to find what words are semantically associated with a word like computer.
” However frequent words likely English simply	” However the most frequent words according to this model would likely be functional words in English or words that are simply common in the data but have no strong association with computer.
	e.
background language model purpose So background normalize model pw word values semantically associated occur context general.	 a background language model would serve the purpose well  So we can use the background language model to normalize the model pw  computer and obtain a probability ratio for each word  Words with high ratio values can then be assumed to be semantically associated with computer since they tend to occur frequently in its context but not frequently in general.
illustrated Figure 3	 This is illustrated in Figure 3 7.
indepth models Statistical 2000 review	 An indepth coverage of statistical language models can be found in the book Statistical Methods for Speech Recognition Jelinek 1997  Rosenfeld 2000 provides a concise yet comprehensive review of statistical language models.
concise comprehensive review language	 Rosenfeld 2000 provides a concise yet comprehensive review of statistical language models.
designed exploratory analysis requires effort By default executables solve example experiment user run following command	 They are designed such that exploratory analysis usually requires no programming effort from the user  By default META is packaged with various executables that can be used to solve a particular task  For example for a classification experiment the user would run the following command in their terminal1 .
toml This standard file parameter configuration file explained later essentially user dataset 1.	classify config toml This is standard procedure for using the default executables they take only one configuration file parameter  The configuration file format is explained in detail later in this chapter but essentially it allows the user to select a dataset a way 1.
The 20newsgroups dataset specified default downloaded metatoolkit.	 The 20newsgroups dataset is specified in the default META config file and can be downloaded here https metatoolkit.
	orgdata20newsgroups tar.
	tar.
gz Place directory Chapter Unified Text tokenize dataset classification algorithm run example.	gz  Place it in the metadata directory  58 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis to tokenize the dataset and a particular classification algorithm to run for this example.
Chapter dataset	 58 Chapter 4 META A Unified Toolkit for Text Data Management and Analysis to tokenize the dataset and a particular classification algorithm to run for this example.
There rarely wide algorithms good Fan al 2008 While implementations linear solely kernelless methods.	 There is rarely a single location for a wide variety of algorithms a good example of this is the LIBLINEAR Fan et al  2008 software package for SVMs  While this is the most cited of the open source implementations of linear SVMs it focuses solely on kernelless methods.
	 META addresses these issues.
Finally readers community jointly contribute functionality benefiting involved 4 2 META future sections book META installed.	 Finally since META will always be free and opensource readers as a community can jointly contribute to its functionality benefiting all those involved  4 2 Setting up META All future sections in this book will assume the reader has META downloaded and installed.
Here we’ll repository actually software command	 Here we’ll show how to set up META  META has both a website with tutorials and an online repository on GitHub  To actually download the toolkit users will check it out with the version control software git in their command line terminal after installing any necessary prerequisites.
	orgsetupguide.
To examples software reader downloaded version 2	 To ensure that the commands and examples sync up with the software the reader has downloaded we will ensure that META is checked out with version 2 2 0.
2.	2.
default placed metabuild updated Once reader able complete run demo.	0  By default the model files should be placed in the metabuild directory but you can place them anywhere as long as the paths in the config file are updated  Once these steps are complete the reader should be able to complete any exercise or run any demo.
reader run demo.	 Once these steps are complete the reader should be able to complete any exercise or run any demo.
input dataset files	 There are four corpus input formats  linecorpus  each dataset consists of one to three files corpusname.
consists corpusname.	 each dataset consists of one to three files corpusname.
class document line order corpusname dat labels	 optional file that includes the class or label of the document on each line again corresponding to the order in corpusname dat  These are the labels that are used for the classification tasks.
	dat.
labels file	 These are the labels that are used for the classification tasks  filecorpus  each document is its own file and the name of the file becomes the name of the document.
file document There	 each document is its own file and the name of the file becomes the name of the document  There is also a corpusnamefullcorpus.
contains required class document followed path file If class	txt which contains on each line a required class label for each document followed by the path to the file on disk  If there are no class labels a placeholder label should be required e g.
gzcorpus.	 gzcorpus.
compressed version corpusname.	gz  compressed version of corpusname.
4 classification LIBSVMformatted input forwardindex.	4 Tokenization with META 61 libsvmcorpus  If only being used for classification META can also take LIBSVMformatted input to create a forwardindex.
chine available 2 For information corpus consult	 There are many ma chine learning datasets available in this format on the LIBSVM site 2 For more information on corpus storage and configuration settings we suggest the reader consult httpsmetatoolkit.
follows icutokenizer documents Unicode standards sentence word	 Some examples are as follows  icutokenizer  converts documents into streams of tokens by following the Unicode standards for sentence and word segmentation.
	twcjlinlibsvmtoolsdatasets 3  httpsite icuproject.
	twcjlinlibsvmtoolsdatasets httpsite.
use stop list stop porter2stemmer transforms according	 For example one could use a stop word list and reject stop words  porter2stemmer  this filter transforms each token according to the Porter2 English Stemmer rules.
Porter2 English	 this filter transforms each token according to the Porter2 English Stemmer rules.
This filter chain work tokenization boundary detection Unicode bigrams file look following analyzers	 This filter chain should work well for most languages as all of its operations including but not lim ited to tokenization and sentence boundary detection are defined in terms of the Unicode standard wherever possible  To consider both unigrams and bigrams the configuration file should look like the following analyzers method  ngramword ngram  1 filter  defaultchain analyzers 4.
html	html 4.
In particular analyzersfilters capabilities.	 In particular the analyzerstokenizers namespace and the analyzersfilters namespace should give a good idea of the capabilities.
IndriLemur.	org IndriLemur.
cs.	cs.
NLP	stanford edusoftwarecorenlp shtml Illinois NLP Curator.
Curator.	shtml Illinois NLP Curator.
illinois.	illinois.
However capabilities text necessary text analysis design philosophy tight capabilities capabilities analysis functions provide text application META designed modularity extensibility objectoriented design.	org However there is a lack of seamless integration of search engine capabilities with various text analysis functions which is necessary for building a unified system for supporting text management and analysis  A main design philosophy of META which also differentiates META from the existing toolkits is its emphasis on the tight integration of search capabilities indeed text access capabilities in general with text analysis functions enabling it to provide full support for building a power ful text analysis application  To facilitate education and research META is designed with an emphasis on modularity and extensibility achieved through objectoriented design.
text machine toolkit text single	 META can also be used to generate a text representation that would be fed into a different data mining or machine learning toolkit  Exercises In its simplest form text data could be a single document in .
We’ll Two book called twocities	 We’ll use the novel A Tale of Two Cities by Charles Dickens as example text  The book is called twocities txt and is located at httpsifaka.
	cs uiuc.
	uiuc.
plaintext files	 You can also use any of your own plaintext files that have multiple English sentences.
.	 Running .
OPTION stem word stop remove words posreplace words POS tags create grammatical parse frequnigram sort freqbigram sort bigram freqtrigram count run httpsifaka	toml file txt OPTION where OPTION is one or more of stem perform stemming on each word stop remove stop words pos annotate words with POS tags posreplace replace words with their POS tags parse create grammatical parse trees from file content frequnigram sort and count unigram words freqbigram sort and count bigram words freqtrigram sort and count trigram words all run all options httpsifaka cs.
profile prints correctly We’ll mean	profile prints out this information then everything has been set up correctly  We’ll look into what each of these options mean in the following exercises.
options mean following	 We’ll look into what each of these options mean in the following exercises  4.
1.	1.
Stemming base	 Stemming  Stemming is the process of reducing a word to a base form.
This useful search engines user books running containing word runs	 This is especially useful for search engines  If a user wants to find books about running documents containing the word run or runs would not match.
	orgalgorithmsenglishstemmer.
.	 .
	txt stem httpsnowball tartarus.
	tartarus orgalgorithmsenglishstemmer.
Exercises 67 Like original stemmed	html Exercises 67 Like stop word removal stemming tries to keep the basic meaning behind the original text  Can you still make sense of it after it’s stemmed 4 3.
3.	3.
subset Not speech tag.	 This is just a subset of about 80 commonly used tags  Not every word has a unique part of speech tag.
fliesNNS	  FruitNN fliesNNS likeV BP aDT bananaNN .
analysis alternate tation	 POS tags can be used in text analysis as an alternate or additional represen tation to words.
profile	profile config.
txt Note POS minute look accurate words file.	txt posreplace Note that POS tagging the book may take up to one minute to complete  Does it look like META’s POS tagger is accurate Can you find any mistakes When replacing the words with their tags is it possible to determine what the original sentence was Experiment with the book or any other text file.
	4.
phrase tree Consider Figure 4 1.	 They represent sentence phrase hierarchy as a tree structure  Consider the example in Figure 4 1.
sentence preterminals direct parents leaves partofspeech tags tree tree categories node used.	 The leaves of the tree are the words in the sentence and the preterminals the direct parents of the leaves are partofspeech tags  Some common features from a parse tree are production rules such as S → NP V P  tree depth and structural tree features  Syntactic categories node labels alone can also be used.
similar highfrequency	 Intuitively similar documents should have some of the same highfrequency words .
Instead look strings called vacation 1grams I 1 2 2 beach 2grams I 1 vacation 1 69 took vacation vacation 1 .	 Instead of single words we can also look at strings of n words called ngrams  Consider this sentence I took a vacation to go to a beach  1grams unigrams I  1 took  1 a  2 vacation  1 to  2 go  1 beach  1 2grams bigrams I took  1 took a  1 a vacation  1 vacation to  1 to go  1 go to  1 to a  1 a beach  1 Exercises 69 3grams trigrams I took a  1 took a vacation  1 a vacation to  1 .
txt frequnigram .	txt frequnigram .
profile	txt freqbigram  profile config.
toml twocities This file freq.	toml twocities txt freqtrigram This will give the output file twocities freq.
1 txt option unigram	1 txt for the option freq unigram and so on.
What makes clear Think	 What makes the output reasonably clear Think back to stop words and stemming.
humans frequency algorithms 4.	 Not only does this make it easier for humans to interpret the frequency analysis but it can improve text mining algorithms too 4.
Think	 Think back to the stop words.
At words given document.	 At the same time many words may only appear once in a given document.
Zipf’s describes shape plots.	 Zipf’s law describes the shape of these plots.
ond enables results appropriate provides data provenance	 Sec ond it enables interpretation of any analysis results or discovered knowledge in appropriate context and provides data provenance origin.
This users initiative information takes	 This connection can be done in two ways pull where the users take the initiative to fetch relevant information out from the system and push where the system takes the initiative to offer relevant information to users.
In chapters specific access push	 In the following chapters we will cover specific techniques for supporting text access in both push and pull modes.
essential user ad hoc i.	 This mode of text access is essential when a user has an ad hoc information need i.
trigger querying common way accessing data pull mode way text user query	 a person which can also trigger a search activity  While querying is the most common way of accessing text data in the pull mode browsing is another complementary way of accessing text data in the pull mode and can be very useful when a user does not know how to formulate an effective query or finds it inconvenient to enter a keyword query e.
way mode way data useful know effective enter query e.	 While querying is the most common way of accessing text data in the pull mode browsing is another complementary way of accessing text data in the pull mode and can be very useful when a user does not know how to formulate an effective query or finds it inconvenient to enter a keyword query e.
g smartphone topic fixed goal Indeed searching Web mix	g  through a smartphone or simply wants to explore a topic with no fixed goal  Indeed when searching the Web users tend to mix querying and browsing e.
knows taxi directly attraction similar user looking formulate query	 When a tourist knows the exact address of an attraction the tourist can simply take a taxi directly to the attraction this is similar to when a user knows exactly what he or she is looking for and can formulate a query with the 5.
example regarded time comparison information i.	 For example a researcher’s research interests can be regarded as relatively stable over time  In comparison the information stream i.
recommendation delivered recommended	 The recommendation can be delivered through email notifications or recommended through a search engine result page.
A shortterm tion initiative information user Ad hoc retrieval extremely important ad hoc information far frequently recommendation	 A shortterm information need is temporary and usually satisfied through search or navigation in the informa tion space whereas a longterm information need can be better satisfied through filtering or recommendation where the system would take the initiative to push the relevant information to a user  Ad hoc retrieval is extremely important because ad hoc information needs show up far more frequently than longterm informa tion needs  The techniques effective for ad hoc retrieval can usually be reused for filtering and recommendation as well.
techniques ad retrieval usually	 The techniques effective for ad hoc retrieval can usually be reused for filtering and recommendation as well.
Due training data solved learning techniques ex cover ad retrieval	 Due to the availability of training data the problem of filtering or recommendation can usually be solved by using supervised machine learning techniques which are covered well in many ex isting books  Thus we will cover ad hoc retrieval in much more detail than filtering and recommendation.
5 Multimode Interactive Access users access text push integrated information environment querying seamlessly maximum query In	 5 2 Multimode Interactive Access Ideally the system should provide support for users to have multimode interactive access to relevant text data so that the push and pull modes are integrated in the same information access environment and querying and browsing are also seamlessly integrated to provide maximum flexibility to users and allow them to query and browse at will  In Figure 5.
2 Multimode Interactive Ideally support users multimode interactive relevant data pull modes information environment querying seamlessly integrated provide maximum flexibility query	2 Multimode Interactive Access Ideally the system should provide support for users to have multimode interactive access to relevant text data so that the push and pull modes are integrated in the same information access environment and querying and browsing are also seamlessly integrated to provide maximum flexibility to users and allow them to query and browse at will  In Figure 5 2 we show a snapshot of a prototype system httptiman.
2 prototype httptiman.	2 we show a snapshot of a prototype system httptiman.
uiuc eduprojsosurf topic map based queries collected search dining Sample interface browsing querying	uiuc  eduprojsosurf where a topic map automatically constructed based on a set of queries collected in a commercial search engine has been added to a regular search ClickThrough Search Result for dining table Figure 5 2 Sample interface of browsing with a topic map where browsing and querying are naturally integrated.
eduprojsosurf 5 enable user browse With interface	eduprojsosurf 5 2 Multimode Interactive Access 77 engine interface to enable a user to browse the information space flexibly  With this interface a user can do any of the following at any moment.
Multimode 77 interface enable	2 Multimode Interactive Access 77 engine interface to enable a user to browse the information space flexibly.
map shortrange walk.	 Navigating on the map shortrange walk.
map space browse relevant documents reformulate Viewing topic region The user doubleclick topic map covered	 Such a map enables the user to “walk” in the information space to browse into relevant documents without needing to reformulate queries  Viewing a topic region  The user may doubleclick on a topic node on the map to view the documents covered in the topic region.
text combine multiple information access current people daily	 Thus we can see how one text access system can combine multiple modes of information access to suit a user’s current needs  5 3 Text Retrieval The most important tool for supporting text data access is a search engine which is why web search engines are used by many people on a daily basis.
Retrieval text data access search web people	 5 3 Text Retrieval The most important tool for supporting text data access is a search engine which is why web search engines are used by many people on a daily basis.
Moreover effective useful implementation recommender	 Moreover the techniques used to implement an effective search engine are often also useful for implementation of a recommender system as well as many text analysis functions.
We large	 We thus devote a large portion of this book to discussing search engine techniques.
rank query From perspective use relevant	 This provides a basis for us to discuss in the next chapter how to rank documents for a query  From a user’s perspective the problem of TR is to use a query to find relevant documents in a collection of text documents.
Although TR term “in formation types informa tion	 Although TR is sometimes used interchangeably with the more general term “in formation retrieval” IR the latter also includes retrieval of other types of informa tion such as images or videos.
retrieval retrieval types text retrieval match data nontextual	 It is worth noting though that retrieval techniques for other nontextual data are less mature and as a result retrieval of other types of information tends to rely on using text retrieval techniques to match a keyword query with companion text data with a nontextual data element.
g U S.	g  U S.
need especially isn’t familiar	 the information need may be difficult to describe precisely especially when the user isn’t familiar with the topic and .
difficult har relevant information task engines work navigational queries simple queries researching	 Indeed even though the current web search engines may appear to be sufficient sometimes it may still be difficult for a user to quickly locate and har vest all the relevant information for a task  In general the current search engines work very well for navigational queries and simple popular informational queries but in the case where a user has a complex information need such as analyzing opinions about products to buy or researching medical information about some symptoms they often work poorly.
Retrieval Retrieval It similar problem retrieval.	4 Text Retrieval vs  Database Retrieval It is useful to make a comparison of the problem of TR and the similar problem of database retrieval.
Both retrieval tasks users relevant difference data tasks important First data managed engine	 Both retrieval tasks are to help users find relevant information but due to the difference in the data managed by these two tasks there are many important differences  First the data managed by a search engine and a database system are different.
data engine field meaning according table	 First the data managed by a search engine and a database system are different  In databases the data are structured where each field has a clearly defined meaning according to a schema  Thus the data can be viewed as a table with wellspecified columns.
Thus viewed wellspecified columns For database type contrast data text	 Thus the data can be viewed as a table with wellspecified columns  For example in a bank database system one field may be customer names another may be the address and yet another may be the balance of each type of account  In contrast the data managed by a search engine are unstructured text which can be difficult for computers to understand.
field address balance account.	 For example in a bank database system one field may be customer names another may be the address and yet another may be the balance of each type of account.
In search engine queries queries	 In a search engine however the queries are generally keyword queries which are only a vague specification of what documents should be returned.
example grammar welldefined To 5S works	” For example from a linguistics perspective grammar provides welldefined structure  To study this matter further see the 5S societies scenarios spaces structures and streams works by Fox et al  2012 5.
4 Retrieval vs Database Retrieval problem user unlikely completely Finally expected different.	4 Text Retrieval vs  Database Retrieval 81 literature to a research problem the user is unlikely able to clearly and completely specify which documents should be returned  Finally the expected results in the two applications are also different.
differences building database somewhat different.	 Due to these differences the challenges in building a useful database and a useful search engine are also somewhat different.
modeling tasks important clearly best answer user user query document set.	 In TR modeling a user’s information need and search tasks is important again due to the difficulty for a user to clearly specify information needs and the difficulty in NLP  Since what counts as the best answer to a query depends on the user in TR the user is actually part of our input together with the query and document set.
best answer query depends TR user actually query document set Thus mathematical prove prove	 Since what counts as the best answer to a query depends on the user in TR the user is actually part of our input together with the query and document set  Thus there is no mathematical way to prove that one answer is better than another or prove one method is better than another.
Instead test database main issue prove better study face text retrieval—the reflect	 Instead we always have to rely on empirical evaluation using some test collections and users  In contrast in database research since the main issue is efficiency one can prove one algorithm is better than another by analyzing the computational complexity or do some simulation study  Note that however when doing simulation study to determine which algorithm is faster we also face the same problem as in text retrieval—the simulation may not accurately reflect the real applications.
like	 Soon we will find search technologies to have widespread use just like databases.
	 5.
V w1 .	 Let V  w1 .
wN set particular language user’s q2 .	  wN be a vocabulary set of all the words in a particular natural language where wi is a word  A user’s query q  q1 q2   .
	   .
5 Document Selection vs.	5 Document Selection vs.
For Twitter document text query long.	 For example in a Twitter search each document is a tweet which is very short and a user may also cut and paste a text segment from an existing document as a query which can be very long.
Our C d1	 Our text collection C  d1     .
	   .
Rq ⊂ C relevant query documents useful query Naturally query	e  Rq ⊂ C which are relevant to the user’s query q that is they are relevant documents or documents useful to the user who typed in the query  Naturally this relevant set depends on the query q.
⊂ user’s documents useful user query.	 Rq ⊂ C which are relevant to the user’s query q that is they are relevant documents or documents useful to the user who typed in the query.
g case ambiguous This return set unlike database search feasible.	g  in an extreme case a query word may be ambiguous  This means that it is unrealistic to expect a computer to return exactly the set Rq unlike the case in database search where this is feasible.
case query	 in an extreme case a query word may be ambiguous.
q q 0 nonrelevant.	 If f q  d  1 d would be assumed to be relevant whereas if f q  d  0 it would be nonrelevant.
d C absolute	 Thus R′q  df q  d  1 d ∈ C  Using such a strategy the system must estimate the absolute relevance i e.
user decide implement d descending values ranking	 An alternative strategy is to rank documents and let the user decide a cutoff  That is we will implement a ranking function f q  d ∈ R and rank all the documents in descending values of this ranking function.
That ranking f q rank values user stop R′q defined partly score based position stopped.	 That is we will implement a ranking function f q  d ∈ R and rank all the documents in descending values of this ranking function  A user would then browse the ranked list and stop whenever they consider it appropriate  In this case the set R′q is actually defined partly by the system and partly by the user since the user would implicitly choose a score threshold θ based on the rank position where he or she stopped.
Since estimation relevance intuitively expect easier ranking	 Since estimation of relative relevance is intuitively easier than that of absolute relevance we can expect it to be easier to implement the ranking strategy.
selection multiple reasons user prescribe exact documents binary classifier unlikely	 Indeed ranking is generally preferred to document selection for multiple reasons  First due to the difficulty for a user to prescribe the exact criteria for selecting relevant documents the binary classifier is unlikely accurate.
accurate user prioritization relevant documents user relevant degree reasons ranking appropriately main ranking shown assumptions based ranking principle returning documents descending relevance optimal	 Even if the classifier can be accurate a user would still benefit from prioritization of the matched relevant documents for examination since a user can only examine one document at a time and some relevant documents may be more useful than others relevance is a matter of degree  For all these reasons ranking documents appropriately becomes a main technical challenge in designing an effective text retrieval system  The strategy of ranking is further shown to be optimal theoretically under two assumptions based on the probability ranking principle Robertson 1997 which states that returning a ranked list of documents in descending order of predicted relevance is the optimal strategy under the following two assumptions.
documents text retrieval	 For all these reasons ranking documents appropriately becomes a main technical challenge in designing an effective text retrieval system.
So problem following sequence document that’s sequence words	 So the problem is the following  We have a query that has a sequence of words and a document that’s also a sequence of words and we hope to define the func tion f   .
compute based query	 that can compute a score based on the query and document.
Now clearly measure d query	 Now clearly this means our function must be able to measure the likelihood that a document d is relevant to a query q.
introduce 6.	 We introduce retrieval models in the Chapter 6.
Bibliographic Notes Further discussion avail Gonçalves 2004 filtering “push” discussed Belkin	 Bibliographic Notes and Further Reading A broader discussion of supporting information access via a digital library is avail able in Gonçalves et al  2004  The relation between retrieval “pull” and filtering “push” has been discussed in the article Belkin and Croft 1992.
Ex ploratory search search requires information querying spe et 2006	 Ex ploratory search is a particular type of search tasks that often requires multimodal information access including both querying and browsing  It was covered in a spe cial issue of Communications of ACM White et al  2006 and White and Roth 2009.
2006 ranking Robertson framing retrieval problem	 2006 and White and Roth 2009  The probability ranking principle Robertson 1997 is generally regarded as the the oretical foundation for framing the retrieval problem as a ranking problem.
chapter introduce retrieval models query likelihood retrieval models We overview basic i.	 6Retrieval Models In this chapter we introduce the two main information retrieval models vector space and query likelihood which are among the most effective and practically useful retrieval models  We begin with a brief overview of retrieval models in general and then discuss the two basic models i.
vector query afterward 1 Overview decades researchers kinds retrieval categories Zhai 2008 detailed review.	 the vector space model and the query likelihood model afterward  6 1 Overview Over many decades researchers have designed various different kinds of retrieval models which fall into different categories see Zhai 2008 for a detailed review.
1975 chapter.	 1975 which we will cover more in detail later in this chapter.
assume documents variables 1 0 indicate relevant query We define probability random equal given document query.	 We assume that queries and documents are all observations from random variables and we assume there is a binary random variable called R with a value of either 1 or 0 to indicate whether a document is relevant to a query  We then define the score of a document with respect to a query as the probability that this random variable R is equal to 1 given a particular document and query.
This family models practice models essentially model	 This family of models is theoretically appealing but in practice they are often reduced to models essentially similar to vectorspace model or a regular probabilistic retrieval model  Finally there is also a family of models that use axiomatic thinking Fang et al.
In case desired constraints.	 In this case the problem is to find a good ranking function that can satisfy all the desired constraints.
Although models perimentation prove effective	 Although many models have been proposed very few have survived extensive ex perimentation to prove effective and robustness.
In chosen cover e.	 In this book we have chosen to cover four specific models i e.
BM25 pivoted length likeli JM smoothing query Dirichlet	 BM25 pivoted length normalization query likeli hood with JM smoothing and query likelihood with Dirichlet prior smoothing that are among the very few most effective and robust models.
1 First assumption representation	1  First these models are all based on the assumption of using a bagofwords representation of text.
query presidential news respect d based computed query	 With this assumption the score of a query like presidential campaign news with respect to a document d would be based on scores computed on each individual query word.
effective readers know Rijs different corresponding document matches query number	 PL2 is another very effective model that the readers should also know of Amati and Van Rijs bergen 2002  We can see there are three different components each corresponding to how well the document matches each of the query words  Inside of these functions we see a number of heuristics.
factor called collection.	 Finally there is a factor called document frequency  This looks at how often presidential occurs at least once in any document in the entire collection.
attempts term In rare score matching term document length main pretty retrieval	 DF attempts to characterize the popularity of the term in the collection  In general matching a rare term in the collection is contributing more to the overall score than matching a common term  TF DF and document length capture some of the main ideas used in pretty much all stateoftheart retrieval models.
TF length main stateoftheart use characterize	 TF DF and document length capture some of the main ideas used in pretty much all stateoftheart retrieval models  In some other models we might also use a probability to characterize this information.
natural Which turns list major models generally regarded stateoftheart	 A natural question is Which model works the best It turns out that many models work equally well so here we list the four major models that are generally regarded as stateoftheart .
Zaragoza Chapter 6 Models .	 Okapi BM25 Robertson and Zaragoza 2009 90 Chapter 6 Retrieval Models .
When perform similarly al	 When optimized these models tend to perform similarly as discussed in detail in Fang et al  2011.
discuss combined rank later book Space Retrieval space VS retrieval effective retrieval.	 We’ll discuss how exactly they are combined to rank documents later in this book  6 3 Vector Space Retrieval Models The vector space VS retrieval model is a simple yet effective method of designing ranking functions for information retrieval.
Intuitively representation d1 suggest topic presi Now document d2 case covers programming library doesn’t presidential.	 Intuitively in this representation d1 seems to suggest a topic in either presi dential or library  Now this is different from another document which might be represented as a different vector d2  In this case the document covers programming and library but it doesn’t talk about presidential.
d3 direction programming.	 Next d3 is pointing in a direction that might be about presidential and programming.
main vector model framework In	 This is the main idea of the vector space model  To be more precise the VS model is a framework  In this framework we make some assumptions.
Each document corresponding case measured based	 Each document vector is also similar it has a number of elements and each value of each element is indicating the weight of the corresponding term  The relevance in this case is measured based on the similarity between the two vectors.
operational implement	 These questions must be addressed before we can have an operational function that we can actually implement using a programming language.
3 1 Vector In discuss specific vector space model doesn’t specify	3 1 Instantiation of the Vector Space Model In this section we will discuss how to instantiate a vector space model so that we can get a very specific ranking function  As mentioned previously the vector space model is really a framework it doesn’t specify many things.
look vectors Here simplest use bit vector yi value When it’s means corresponding present	 Now let’s look at how we place vectors in this space  Here the simplest strategy is to use a bit vector to represent both a query and a document and that means each element xi and yi would take a value of either zero or one  When it’s one it means the corresponding word is present in the document or query.
In Figure 6 3 x1 plus product	 In Figure 6 3 we see that it’s the product of x1 and y1 plus the product of x2 and y2 and so on.
end we’ve	 In the end we’ve got a specific retrieval function shown in Figure 6 3.
campaign example cover different	 The query is news about presidential campaign  For this example we will examine five documents from the corpus that cover different terms in the query.
They ranked	 They match news presidential and campaign so they should be ranked on top.
The	 The other three d1 d2 and d5 are nonrelevant.
rest Document ones news zeros.	 The query has four words so for these four words there would be a one and for the rest there will be zeros  Document d1 has two ones news and about while the rest of the dimensions are zeros.
In words news matched	 In this case you can see the result is three because d3 matched the three distinct query words news presidential and campaign whereas d1 only matched two.
bit scoring function terms matched document terms document assumed relevant	 The bit vector scoring function counts the number of unique query terms matched in each docu ment  If a document matches more unique query terms then the document will be assumed to be more relevant that seems to make sense.
d4 score Upon right d3 d3 Another score d2 matched.	 The only problem is that there are three documents d2 d3 and d4 that are tied with a score of three  Upon closer inspection it seems that d4 should be right above d3 since d3 only mentioned presidential once while d4 mentioned it many more times  Another problem is that d2 and d3 also have the same score since for d2 news about and campaign were matched.
Upon mentioned presidential problem d3	 Upon closer inspection it seems that d4 should be right above d3 since d3 only mentioned presidential once while d4 mentioned it many more times  Another problem is that d2 and d3 also have the same score since for d2 news about and campaign were matched.
discussed space Indeed it’s simplest vector derive.	 Based on this idea we discussed a very simple way to instantiate the vector space model  Indeed it’s probably the simplest vector space model that we can derive.
section 6.	 This is the topic for the next section  6.
6 3.	 6 3.
The problem capture	 The problem here is that this function couldn’t capture the following characteristics.
	 .
First d4 presidential d3.	 First we would like to give more credit to d4 because it matches presidential more times than d3.
Second matching presidential matching word content.	   Second matching presidential should be more important than matching about because about is a very common word that occurs everywhere it doesn’t carry that much content.
instantiating model realize coming vector space fix assumptions.	 If we look back at the assumptions we made while instantiating the VS model we will realize that the problem is really coming from some of those assumptions  In particular it has to do with how we place the vectors in the vector space  Naturally in order to fix these problems we have to revisit those assumptions.
natural thought occurrences consider TF instead absence In document term times query term occurred consider term simplest express word w document	 A natural thought is to consider multiple occurrences of a term in a document as opposed to binary representation we should consider the TF instead of just the absence or presence  In order to consider the difference between a document where a query term occurred multiple times and one where the query term occurred just once we have to consider the term frequency—the count of a term in a document  The simplest way to express the TF of a word w in a document d is T Fw d  countw d.
The word T d 6 bit vector absence term	 The simplest way to express the TF of a word w in a document d is T Fw d  countw d  6 1 With the bit vector we only captured the presence or absence of a term ignoring the actual number of times that a term occurred.
Let’s represent dimension’s query document Figure	 Let’s add the count information back we will represent a document by a vector with as each dimension’s weight  That is the elements of both the query vector and the document vector will not be zeroes and ones but instead they will be the counts of a word in the query or the document as illustrated in Figure 6.
zeroes word query document 6 let’s look representation.	 That is the elements of both the query vector and the document vector will not be zeroes and ones but instead they will be the counts of a word in the query or the document as illustrated in Figure 6 7  Now let’s see what the formula would look like if we change this representation.
We model.	 We can see whether this would fix the problems of the bit vector VS model.
documents Figure vector exactly	 Look at the three documents again in Figure 6 8  The query vector is the same because all these words occurred exactly once in the query.
8.	8.
How prob lem Is way determine doesn’t content word	 How can we solve this prob lem in a general way Is there any way to determine which word should be treated more importantly and which word can be essentially ignored About doesn’t carry that much content so we should be able to ignore it  We call such a word a stop word.
If overall score d3	 If we can do that then we can expect that d2 will get an overall score of less than three while d3 will get a score of about three.
The document count documents term Here occur documents.	 The document frequency is the count of documents that contain a particular term  Here we say inverse document frequency because we actually want to reward a word that doesn’t occur in many documents.
The way incorporate multiplying IDF corresponding word Figure 6 9 We low	 The way to incorporate this into our vector is to modify the frequency count by multiplying it by the IDF of the corresponding word as shown in Figure 6 9  We can now penalize common words which generally have a low IDF and reward informative words that have a higher IDF.
defined	 IDF can be defined as IDFw M  1 dfw 6.
total number number containing w.	2 where M is the total number of documents in the collection and df  counts the document frequency the total number of documents containing w.
use linear diagonal shown IDF	 If we use a linear function like the diagonal line as shown in the figure it may not be as reasonable as the IDF function we just defined.
In Figure 12 seen scores reasonable.	 In Figure 6 12 we show all the five documents that we have seen before and their new scores using TFIDF weighting  We see the scores for the first four documents seem to be quite reasonable.
look formula query	 If you look at the formula carefully you will see it involves a sum over all the matched query terms.
The occurrence says lot goes Once word it’s likely	 The first occurrence of a term says a lot about matching of this term because it goes from a zero count to a count of one and that increase is very informative  Once we see a word in the document it’s very likely that the document is talking about this word.
Once document it’s word.	 Once we see a word in the document it’s very likely that the document is talking about this word.
transformation function word word document.	14  This transformation function is going to turn the raw count of word into a TF weight for the word in the document.
going turn weight document On xaxis TF previous implicitly	 This transformation function is going to turn the raw count of word into a TF weight for the word in the document  On the xaxis is the raw count and on the yaxis is the TF weight  In the previous ranking functions we actually have implicitly used some kind of transformation.
bit function count zero one.	 For example in the zeroone bit vector representation we actually used the binary transformation function as shown here  If the count is zero then it has zero weight  Otherwise it would have a weight of one.
With logarithm looks lines influence high weight it’s influence	 With a logarithm we can have a sublinear transformation that looks like the red lines in the figure  This will control the influence of a very high weight because it’s going to lower its influence yet it will retain the influence of a small count.
0 zero transformation.	 When k  0 we have a zero one bit transformation.
influence particular	 This upper bound is useful to control the influence of a particular term.
diminishing high single The BM25 upper bound effective.	 This ensures that we represent the intuition of diminishing return from high term counts  It also avoids a dominance by one single term over all others  The BM25 TF formula we discussed has an upper bound while being robust and effective.
In discuss issue far model term We global statistic IDF.	5 Document Length Normalization In this section we will discuss the issue of document length normalization  So far in our exploration of the vector space model we considered the TF or the count of a term in a document  We have also considered the global statistic IDF.
We considered global	 We have also considered the global statistic IDF.
considered length.	 However we have not considered the document length.
short words Conversely matchings query scattered manner.	 Document d4 is very short with only one hundred words  Conversely d6 has five thousand words  If you look at the matching of these query words we see that d6 has many more matchings of the query words one might reason that d6 may have matched these query words in a scattered manner.
For example think research article It abstract.	 For example think about a research paper article  It would use more words than the corresponding abstract.
If shorter average there’s reward.	 If it’s shorter than the average document length there’s even some reward.
document document length sense document gives benefit unit property parameter	 When we first divide the length of the document by the average document length this not only gives us some sense about how this document is compared with the average document length but also gives us the benefit of not worrying about the unit of length  This normalizer has an interesting property first we see that if we set the parameter b to zero then the normalizer value would be one indicating no length normalization at all.
If set value higher longer average document length smaller shorter sense penalization degree penalization	 If we set b to a nonzero value then the value would be higher for documents that are longer than the average document length whereas the value of the normalizer will be smaller for shorter documents  In this sense we see there’s a penalization for long documents and a reward for short documents  The degree of penalization is controlled by b.
In there’s long documents reward	 In this sense we see there’s a penalization for long documents and a reward for short documents.
degree penalization controlled By adjusting varies zero control	 The degree of penalization is controlled by b  By adjusting b which varies from zero to one we can control the degree of length normalization.
length normalization ranking shown 18 them.	 If we plug this length normalization factor into the vector space model ranking functions that we have already examined we will end up with stateoftheart retrieval models some of which are shown in Figure 6 18  Let’s take a look at each of them.
The pivoted tion TFIDF discussed The IDF appears	 The first one is called pivoted length normaliza tion  We see that it’s basically the TFIDF weighting model that we have discussed  The IDF component appears in the last term.
double sublinear We document length normalizer TF formula penalty smaller TF weight The document b.	 For this we have the double logarithm as we discussed before this is to achieve a sublinear transformation  We also put a document length normalizer in the denominator of the TF formula which causes a penalty for long documents since the larger the denominator is the smaller the TF weight is  The document length normalization is controlled by the parameter b.
This played determining However examine	 This has played an important role in determining the performance of the ranking function  However there are also other considerations that we did not really examine in detail.
We’ve assumed document bag words Obviously	 We’ve assumed that we can represent a document as a bag of words  Obviously we can see there are many other choices.
stop word don’t carry	 We also need to perform stop word removal this removes some very common words that don’t carry any content such as the a or of .
We latent documents belong We units like dimensions representation representation.	 We could use phrases or even latent semantic analysis which characterizes documents by which cluster words belong to  We can also use smaller units like character ngrams which are sequences of n characters as dimensions  In practice researchers have found that the bagofwords representation with phrases or “bagofphrases” is the most effective representation.
We use smaller character ngrams sequences characters In researchers bagofwords efficient far major search	 We can also use smaller units like character ngrams which are sequences of n characters as dimensions  In practice researchers have found that the bagofwords representation with phrases or “bagofphrases” is the most effective representation  It’s also efficient so this is still by far the most popular document representation method and it’s used in all the major search engines.
In practice bagofwords “bagofphrases” representation.	 In practice researchers have found that the bagofwords representation with phrases or “bagofphrases” is the most effective representation.
far search engines employ	 It’s also efficient so this is still by far the most popular document representation method and it’s used in all the major search engines  Sometimes we need to employ languagespecific and domainspecific represen tation  This is actually very important as we might have variations of the terms that prevent us from matching them with each other even though they mean the same thing.
languagespecific represen tation.	 Sometimes we need to employ languagespecific and domainspecific represen tation.
Take example segment text obtain originally sequence A word characters.	 Take Chinese for example  We first need to segment text to obtain word boundaries because it’s originally just a sequence of characters  A word might cor respond to one character or two characters or even three characters.
compute angle vectors distance dot reasons it’s it’s general consider weighting different ways measure regarded	 We could compute the cosine of the angle between two vectors or we can use a Euclidean distance measure  The dot product still seems the best and one of the reasons is because it’s very general in fact it’s sufficiently general  If you consider the possibilities of doing weighting in different ways cosine measure can be regarded as the dot product of two normal ized vectors.
consider possibilities different ways regarded dot normal vectors That means product.	 If you consider the possibilities of doing weighting in different ways cosine measure can be regarded as the dot product of two normal ized vectors  That means we first normalize each vector and then we take the dot product.
That equivalent measure We effective development changed BM25 fundamentally.	 That would be equivalent to the cosine measure  We mentioned that BM25 seems to be one of the most effective formulas—but there has also been further development in improving BM25 although none of these works have changed the BM25 fundamentally.
In people BM25F Here stands documents structure.	 In one line of work people have derived BM25F  Here F stands for field and this is BM25 for documents with structure.
stands documents	 Here F stands for field and this is BM25 for documents with structure.
These appropriate fields improve scoring document field combines scores global	 These can all be combined with an appropriate weight on different fields to help improve scoring for each document  Essentially this formulation applies BM25 on each field and then combines the scores but keeps global i e.
sublinear TF important 110 Chapter 6 Models contributes weight.	 Recall that in the sublinear transformation of TF the first occurrence is very important 110 Chapter 6 Retrieval Models and contributes a large weight.
term large combine counts extra occurrences occurrences method worked scoring structured	 If we do that for all the fields then the same term might have gained a large advantage in every field  When we just combine counts on each separate field the extra occurrences will not be counted as fresh first occurrences  This method has worked very well for scoring structured documents.
When counts extra counted fresh occurrences.	 When we just combine counts on each separate field the extra occurrences will not be counted as fresh first occurrences.
Summary In vector retrieval models relevance ing document query correlated implies query way high dimensional vector	3 7 Summary In vector space retrieval models we use similarity as a notion of relevance assum ing that the relevance of a document with respect to a query is correlated with the similarity between the query and the document  Naturally that implies that the query and document must be represented in the same way and in this case we represent them as vectors in a high dimensional vector space.
BM25 VS measures idea remains In space	 Finally BM25 and pivoted length nor malization seem to be the most effective VS formulas  While there has been some work done in improving these two powerful measures their main idea remains the same  In the next section we will discuss an alternative approach to the vector space representation.
different ranking vector model probabilistic define function based probability document	4 Probabilistic Retrieval Models In this section we will look at a very different way to design ranking functions than the vector space model that we discussed before  In probabilistic models we define the ranking function based on the probability that a given document d is relevant 6.
Note documents length	 Note that in the vector space model we assume that documents are all equal length vectors.
prob vector space section form similar types special case retrieval functions language	 The classic prob abilistic model has led to the BM25 retrieval function which we discussed in the vector space model section because its form is quite similar to these types of mod els  We will discuss another special case of probabilistic retrieval functions called language modeling approaches to retrieval.
likelihood effective models models line called divergencefrom randomness It’s models.	 In particular we’re going to discuss the query likelihood retrieval model which is one of the most effective models in probabilistic models  There is also another line of functions called divergencefrom randomness models such as the PL2 function Amati and Van Rijsbergen 2002  It’s also one of the most effective stateoftheart retrieval models.
1 q q 1 look specific trying compute q1.	 Clearly pR  1  d  q  pR  0  d  q  1  Let’s take a look at some specific examples  Suppose we are trying to compute this probability for d1 d2 and d3 for q1.
We d2 d3 q1 based basic retrieval model rank given d2 1.	 We now have a score for d1 d2 and d3 for q1  We can simply rank them based on these probabilities—that’s the basic idea of probabilistic retrieval model  In our example it’s going to rank d2 above all the other documents because in all the cases given q1 and d2 R  1.
documents cases given d2 1 volumes clickthrough learn results small	 In our example it’s going to rank d2 above all the other documents because in all the cases given q1 and d2 R  1  With volumes of clickthrough data a search engine can learn to improve its results  This is a simple example that shows that with even a small number of entries we can already estimate some probabilities.
shows estimate	 This is a simple example that shows that with even a small number of entries we can already estimate some probabilities.
19 assumption table.	19  Otherwise we would have similar problems as before  By making this assumption we have some way to bypass the big table.
Otherwise similar bypass big	 Otherwise we would have similar problems as before  By making this assumption we have some way to bypass the big table.
example.	 Let’s look at how this new model works for our example.
	 6.
Figure 6 assumes user ideal document document’s In “presidential campaign news.	 Figure 6 20 shows how the query likelihood model assumes a user imagines some ideal document and generates a query based on that ideal document’s con tent  In this example the ideal document is about “presidential campaign news.
Whether allowed conditional probability	 Whether a user actually follows this pro cess is a different question  Importantly though this assumption has allowed us to formally characterize the conditional probability of a query given a document without relying on the big table that was presented earlier.
allowed formally given table	 Importantly though this assumption has allowed us to formally characterize the conditional probability of a query given a document without relying on the big table that was presented earlier.
Since likelihood probability query words word probability product query relative document.	 Since we are computing a query likelihood then the total probability is the probability of this particular query which is a sequence of words  Since we make the assumption that each word is generated independently the probability of the query is just a product of the probability of each query word where the probability of each word is just the relative frequency of the word in the document.
word generated independently probability product probability query frequency	 Since we make the assumption that each word is generated independently the probability of the query is just a product of the probability of each query word where the probability of each word is just the relative frequency of the word in the document.
word update word assumption user word update	 Consider the word update none of the documents contain this word  According to our assumption that a user would pick a word from a document to generate a query the probability of obtaining a word like update 116 Chapter 6 Retrieval Models would be zero.
cause documents query.	 Clearly this causes a problem because it would cause all these documents to have zero probability of generating this query.
word order fix necessarily improved model Instead let’s imagine actually language Figure	 We have made an assumption that every query word must be drawn from the document in the user’s mind—in order to fix this we have to assume that the user could have drawn a word not necessarily from the document  So let’s consider an improved model  Instead of drawing a word from the document let’s imagine that the user would actually draw a word from a document language model as depicted in Figure 6.
Instead drawing word actually word document model 6.	 Instead of drawing a word from the document let’s imagine that the user would actually draw a word from a document language model as depicted in Figure 6.
23 possible language models based d1 mining independence pq probability documents probabilities	23 we show two possible language models based on documents d1 and d2 and a query data mining algorithms  By making an independence assump tion we could have pq  d as a product of the probability of each query word in each document’s language model  We score these two documents and then rank them based on the probabilities we calculate.
formally state query q w2	 Let’s formally state our scoring process for query likelihood  A query q contains the words q  w1 w2   .
scoring ranking probability given document	    wn such that q  n  The scoring or ranking function is then the probability that we observe q given that a user is thinking of a particular document d.
function probability q document	 The scoring or ranking function is then the probability that we observe q given that a user is thinking of a particular document d.
5 We avoid probabilities underflow precision loss.	 6 5 We do this to avoid having numerous small probabilities multiplied together which could cause underflow and precision loss.
transforming rithm maintain order underflow problem.	 By transforming using a loga rithm we maintain the order of these documents while simultaneously avoiding the underflow problem.
term equation sum iterate word Essentially considering query contribution	 Note the last term in the equation above in this sum we have a sum over all the possible words in the vocabulary V and iterate through each word in the query  Essentially we are only considering the words in the query because if a word is not in the query its contribution to the sum would be zero.
Smoothing Document Model likelihood score probabilities given i.	2 Smoothing the Document Language Model When calculating the query likelihood retrieval score recall that we take a sum of log probabilities over all of the query words using the probability of a word in the query given the document i.
In detail.	 In this section we look into this task in more detail.
This smoothing improving	 This is called smoothing and smoothing has to do with improving the estimate by including the probabilities of unseen words.
case choice collection That going assume proportional probability collection.	 In the case of retrieval a natural choice would be to take the collection LM as the reference LM  That is to say if you don’t observe a word in the document we’re going to assume that the probability of this word would be proportional to the probability of the word in the whole collection.
Now smoothing formula plug ranking Figure	 Now that we have this smoothing formula we can plug it into our query likeli hood ranking function illustrated in Figure 6.
allows likelihood efficiently consider query.	 The second benefit is that it also allows us to compute the query likelihood more efficiently since we only need to consider terms matched in the query.
This words smooth document collection words w ∈ .	 This is much better than if we take the sum over all the words  After we smooth the document using the collection language model we would have nonzero probabilities for all the words w ∈ V .
query vector model dot product frequency vector query corresponding effect TF IDF weighting.	 First we can already see it has a frequency of the word in the query just like in the vector space model  When we take the dot product the word frequency in the query appears in the sum as a vector element from the query vector  The corresponding term from the document vector encodes a weight that has an effect similar to TF IDF weighting.
This means popular term smaller Only	 This means a popular term carries a smaller weight—this is precisely what IDF weighting is doing Only now we have a different form of TF and IDF.
It mass long assume observed words written If document unseen words need smoothing	 It encodes how much probability mass we want to give to unseen words or how much smoothing we are allowed to do  Intuitively if a document is long then we need to do less smoothing because we can assume that it is large enough that we have probably observed all of the words that the author could have written  If the document is short the number of unseen words is expected to be large and we need to do more smoothing in this case.
smoothing assume probably observed words written short number large smoothing case.	 Intuitively if a document is long then we need to do less smoothing because we can assume that it is large enough that we have probably observed all of the words that the author could have written  If the document is short the number of unseen words is expected to be large and we need to do more smoothing in this case.
Thus penalizes documents smaller long The αd occurs places Thus necessarily long documents αd penalize long specific way.	 Thus αd penalizes long documents since it tends to be smaller for long documents  The variable αd actually occurs in two places  Thus its overall effect may not necessarily be penalizing long documents but as we will see later when we consider smoothing methods αd would always penalize long documents in a specific way.
variable αd actually occurs	 The variable αd actually occurs in two places.
necessarily documents smoothing methods αd specific way.	 Thus its overall effect may not necessarily be penalizing long documents but as we will see later when we consider smoothing methods αd would always penalize long documents in a specific way.
Imagine logarithm TF IDF modeling given achieves “weights In summary models rules	 Imagine if we drop this logarithm we would still have TF and IDF weighting  But what’s nice with probabilistic modeling is that we are automatically given a logarithm function which achieves sublinear scaling of our term “weights ” In summary a nice property of probabilistic models is that by following some assumptions and probabilistic rules we’ll get a formula by derivation.
general use word higher That proportional probability entire we’ve derive query retrieval models vector space TFIDF normalization.	 The general idea of smoothing in retrieval is to use the collection language model to give us some clue about which unseen word would have a higher proba bility  That is the probability of the unseen word is assumed to be proportional to its probability in the entire collection  With this assumption we’ve shown that we can derive a general ranking formula for query likelihood retrieval models that au tomatically contains the vector space heuristics of TFIDF weighting and document length normalization.
We having following∑ q log pseenw d C q αd	 We end up having a retrieval function that looks like the following∑ w∈d q cw q log pseenw  d αd   pw  C q log αd .
pw query sum count terms query weight term	 pw  C q log αd   6 7 We can see it’s a sum of all the matched query terms and inside the sum it’s a count of terms in the query with some weight for the term in the document.
7 it’s sum matched terms inside sum terms query term IDF captured term αd	7 We can see it’s a sum of all the matched query terms and inside the sum it’s a count of terms in the query with some weight for the term in the document  We saw in the previous section how TF and IDF are captured in this sum  We also mentioned how the second term αd can be used for document length normalization.
previous section captured sum We mentioned αd length normalization.	 We saw in the previous section how TF and IDF are captured in this sum  We also mentioned how the second term αd can be used for document length normalization.
mentioned document length normalization.	 We also mentioned how the second term αd can be used for document length normalization.
This smoothing	 This is also called JelinekMercer smoothing  The idea is actually quite simple.
Figure estimate document	 The idea is actually quite simple  Figure 6 26 shows how we estimate the document language model by using MLE.
distri probability words document currently	 By mixing the two distri butions together we achieve the goal of assigning nonzero probability to unseen words in the document that we’re currently scoring.
	 pw  C 1 − λ .
	 Based on Figure 6.
pw μ words 100 p“network”d 0.	 pw  C d  μ   6 8 Document d Total words  100 p“network”d   0.
08	08 … computer 0.
0 … 6.	001 network 0 001 mining 0 0009 … Figure 6.
Chapter Here change MLE count	 126 Chapter 6 Retrieval Models Here we can easily see what change we have made to the MLE  In this form we see that we add a count of μ .
In form .	 In this form we see that we add a count of μ .
word corpus C pseudocounts.	 pw  C to every word which is proportional to the probability of w in the entire corpus  We pretend every word w has μ   pw  C additional pseudocounts.
For prior αd dμ coefficient collection language slightly detailed consult A.	 For Dirichlet prior we have αd μ dμ which is the interpolation coefficient applied to the collection language model  For a slightly more detailed derivation of these variables the reader may consult Appendix A.
consult A Now defined likelihood	 For a slightly more detailed derivation of these variables the reader may consult Appendix A  Now that we have defined pseen and αd for both smoothing methods let’s plug these variables in the original smoothed query likelihood retrieval function.
defined αd let’s plug original query likelihood function start JelinekMercer smoothing pseenw d pw λ .	 Now that we have defined pseen and αd for both smoothing methods let’s plug these variables in the original smoothed query likelihood retrieval function  Let’s start with JelinekMercer smoothing pseenw  d αd   pw  C 1 − λ .
Let’s JelinekMercer αd pw .	 Let’s start with JelinekMercer smoothing pseenw  d αd   pw  C 1 − λ .
Finally d form length overall αd clearly long	 Finally we can see the d in the denominator is a form of document length normalization since as d grows the overall term weight would decrease suggesting that the impact of αd in this case is clearly to penalize a long document.
We know pseen d d cw d μ .	 We know what pseen is and we know that αd  μ dμ pseenw  d  cw d  μ   pw  C d  μ d d  μ   cw d d  μ d  μ .
pw 11 αd	 pw  C 6 11 therefore pseenw  d αd .
pwC dμ cw d C 6.	pwC dμ 1  cw d μ   pw  C   6.
pw C q log d	 pw  C q log μ μ  d   6.
form function looks scoring	 6 13 The form of the function looks very similar to the JelinekMercer scoring func tion.
Dirichlet prior normalization ferently JelinekMercer smoothing Here document large extra score d small mass effectively rewarding short	 The difference here is that Dirichlet prior smoothing can capture document length normalization dif ferently than JelinekMercer smoothing  Here we have retained the q log αd term since αd depends on the document namely d  If d is large then less extra mass is added onto the final score if d is small more extra mass is added to the score effectively rewarding a short document.
Here log αd term αd depends If d extra mass final score effectively rewarding short	 Here we have retained the q log αd term since αd depends on the document namely d  If d is large then less extra mass is added onto the final score if d is small more extra mass is added to the score effectively rewarding a short document.
In cases able function articulated heuristic space els.	 In most cases we can see by using these smoothing methods we will be able to reach a retrieval function where the assumptions are clearly articulated making them less heuristic than some of the vector space mod els.
didn’t explicitly naturally arrived TFIDF weighting document normaliza justifying inclusion smoothing λ μ need set smoothing parameters estimate	 Even though we didn’t explicitly set out to define the popular VS heuristics in the end we naturally arrived at TFIDF weighting and document length normaliza tion perhaps justifying their inclusion in the VS models  Each of these functions also has a smoothing parameter λ or μ with an intuitive meaning  Still we need to set these smoothing parameters or estimate them in some way.
Each functions parameter	 Each of these functions also has a smoothing parameter λ or μ with an intuitive meaning.
generated query If document proportional collection smoothing background	 Query words are generated independently allowing us to decompose the probability of the whole query into a product of probabilities of observed words in the query  3  If a word is not seen in the document its probability is proportional to its probability in the collection smoothing with the background collection.
smoothing JelinekMercer smoothing Dirichlet smoothing.	 Finally we made one of two assumptions about the smoothing using either JelinekMercer smoothing or Dirichlet prior smoothing.
likelihood retrieval model proposed useful BM25 retrieval Robertson 2009.	 The query likelihood retrieval model was initially proposed in Ponte and Croft 1998  A useful reference for the BM25 retrieval function is Robertson and Zaragoza 2009.
First retrieve contain query	 First we used computer to retrieve all the documents that contain that word  That is imagine the query is computer  Then the results will be those documents that contain computer.
That query	 That is imagine the query is computer  Then the results will be those documents that contain computer.
k estimate probabilities counting set topic language model.	 We take the top k results that match computer well and we estimate term probabilities by counting them in this set for our topic language model.
background language choose terms set collection contrast ideas we’ll related These related expand query documents words like original query.	 Lastly we use the background language model to choose the terms that are frequent in this retrieved set but not frequent in the whole collection  If we contrast these two ideas what we can find is that we’ll learn some related terms to computer  These related words can then be added to the original query to expand the query which helps find documents that don’t necessarily match computer but match other words like program and software that may not have been in the original query.
user clicked skipped gives document useful	 Instead we are going to observe how the users interact with the search results by observing their clickthroughs  If a user clicked on one document and skipped another this gives a clue about whether a document is useful or not.
If user clicked document gives clue document useful not.	 If a user clicked on one document and skipped another this gives a clue about whether a document is useful or not.
We place query vector term closer relevant documents weights terms terms result query	 We want to place the query vector in a better position in the high dimensional term space plotting it closer to relevant documents  We might adjust weights of old terms or assign weights to new terms in the query vector  As a result the query will usually have more terms which is why this is often called query expansion.
The query vector positive	 The query vector is in the center and the  positive or − negative represent documents.
diagram vector circle encompasses This basic idea	 By looking at this diagram we see that we should move the query vector so that the dotted circle encompasses more  documents than − documents  This is the basic idea behind Rocchio feedback.
Algebraically formula vector notation clarity α	 Algebraically it means we have the following formula using the arrow vector notation for clarity qm  α   q  β Dr .
q ∑ dj∈Dr dj γ dj∈Dn	 q  β Dr   ∑ dj∈Dr dj − γ Dn   ∑ dj∈Dn dj  7.
Dr set relevant feedback documents feedback β γ vector terms movement original query factor terms boosted factor shrunk γ .	 Dr is the set of relevant feedback documents and Dn is the set of nonrelevant feedback documents  Additionally we have the parameters α  β  and γ which are weights that control the amount of movement of the original vector  In terms of movement we see that the terms in the original query are boosted by a factor of α and terms from positive documents are boosted by a factor of β while terms from negative documents are shrunk by a factor of γ .
The negative feedback − pres food d1 1.	 The negative feedback documents are prefixed with −  news about pres  campaign food text − d1  1.
campaign d1	 campaign food text − d1  1 5 0.
1 0 0.	1 0 0 0 0 0.
0.	0 0 0 0.
0.	0 0.
0 0 0 d2	0 0 0 − d2  1 5 0.
3 0 2	0 3 0 2 0 0.
0.	5 0.
0	0 2 0 0 0.
02 02.	02 02.
5γ α 067γ 2β − 2.	5γ  α − 0 067γ  α  3 5β  α  2β − 2.
−1 33γ 0 Chapter We query weight happened one.	67γ  −1 33γ  0  138 Chapter 7 Feedback We have the parameter α controlling the original query term weight which all happened to be one.
α controlling original	 138 Chapter 7 Feedback We have the parameter α controlling the original query term weight which all happened to be one.
apply method problem centroids weights vector contain weights considering small words.	 If we apply this method in practice we will see one potential problem we would be performing a somewhat large computation to calculate the centroids and modify all the weights in the new query  Therefore we often truncate this vector and only retain the terms which contain the highest weights considering only a small number of words.
efficiency examples non useful compared positive examples.	 This is for efficiency  Additionally negative examples or non relevant examples tend not to be very useful especially compared with positive examples.
examples positive examples reason distract average doesn’t exactly positive documents tend direction respect query.	 Additionally negative examples or non relevant examples tend not to be very useful especially compared with positive examples  One reason is because negative documents distract the query in all directions so taking the average doesn’t really tell us where exactly it should be moving to  On the other hand positive documents tend to be clustered together and they are often in a consistent direction with respect to the query.
7 2 Models This section feedback making term independence.	 7 2 Feedback in Language Models This section is about feedback for language modeling in the query likelihood model of information retrieval  Recall that we derive the query likelihood ranking function by making various assumptions such as term independence.
information assumed words ideal document easy information.	 Many times the feedback information is additional information about the query but since we assumed that the query is generated by assembling words from an ideal document language model we don’t have an easy way to add this additional information.
retrieval model generalizes term able general	 On top we have the query likelihood retrieval function  The KLdivergence retrieval model generalizes the query term frequency into a probabilistic distribution  This distribution is the only difference which is able to characterize the user’s query in a more general way.
difference language KLdivergence	 difference between two distributions one is the query model pw  θ̂Q and the other is the document language model from before  We won’t go into detail on KLdivergence but there is a more detailed explanation in appendix C.
won’t appendix C q ∑ w2q cw log f q pwC log cw Q — Q pwθ̂Q αd likelihood Query KLdivergence cross entropy	 We won’t go into detail on KLdivergence but there is a more detailed explanation in appendix C  f q d pseenwd — αd pwC ∑ w2d w2q cw q log   n log αd f q d pseenwd — αd pwC ∑ w2dpwθQ0 pwθ̂Q log cw Q — Q pwθ̂Q log αd Query likelihood Query LM KLdivergence cross entropy Figure 7.
d — αd cw log αd f αd log Q Q likelihood LM KLdivergence Figure 7 KLdivergence changes represent	 f q d pseenwd — αd pwC ∑ w2d w2q cw q log   n log αd f q d pseenwd — αd pwC ∑ w2dpwθQ0 pwθ̂Q log cw Q — Q pwθ̂Q log αd Query likelihood Query LM KLdivergence cross entropy Figure 7 3 The KLdivergence retrieval model changes the way we represent the query.
3 KLdivergence incorporated	3 The KLdivergence retrieval model changes the way we represent the query  This enables feedback information to be incorporated into the query more easily.
recover likelihood formula query query length term constant Figure document estimate model compute D.	 We can recover the original query likelihood formula by simply setting the query language model to be the relative frequency of a word in the query which eliminates the query length term n  q which is a constant  Figure 7 4 shows that we first estimate a document language model then we estimate a query language model and we compute the KLdivergence often denoted by D.
Figure shows estimate language model estimate query language model KLdivergence	 Figure 7 4 shows that we first estimate a document language model then we estimate a query language model and we compute the KLdivergence often denoted by D.
The	 Of course these extremes are generally not desirable  The main question is how to compute this θF .
isn’t good interpolate original model Clearly rid stop words.	 This isn’t very good for feedback be cause we will be adding many such words to our query when we interpolate with our original query language model  Clearly we need to get rid of these stop words.
we’re talk approach	 Instead we’re going to talk about another approach which is more principled.
In duce probability explain	 In order to re duce its probability in this model we have to have another model to explain such a common word.
Conversely λ going use words Once way exactly adjust set best asking model explain we’re going explain	 Conversely if λ is very small we’re going to use only our topic words  Once we’re thinking this way we can do exactly the same as what we did before by using MLE to adjust this model and set the parameters to best explain the data  The difference however is that we are not asking the unknown topic model alone to explain all the words rather we’re going to ask the whole mixture model to explain the data.
thinking way explain	 Once we’re thinking this way we can do exactly the same as what we did before by using MLE to adjust this model and set the parameters to best explain the data.
The asking unknown topic going explain data As result doesn’t high probabilities exactly	 The difference however is that we are not asking the unknown topic model alone to explain all the words rather we’re going to ask the whole mixture model to explain the data  As a result it doesn’t have to assign high probabilities to words like the which is exactly what we want.
log feedback ments F parameter λ feedback documents.	 Mathematically we have to compute the log likelihood of the feedback docu ments F with another parameter λ which denotes noise in the feedback documents.
it’s fixed word probabilities θ	 Assuming it’s fixed then we only have word probabilities θ as parameters just like in the simplest unigram language model.
1 −	 log 1 − λ   pw  θ  λ .
Here problem opti	 Here though the mathematical problem is to solve this opti mization problem.
We possible values expression probability query course values θ use EM algo rithm parameters	 We could try all possible θ values and select the one that gives the whole expression the maximum probability  Once we have done that we ob tain this θF that can be interpolated with the original query model to do feedback  Of course in practice it isn’t feasible to try all values of θ  so we use the EM algo rithm to estimate its parameters Zhai and Lafferty 2001.
multiple component model discuss chapter 17 Figure feedback web	 Such a model involving multiple component models combined together is called a mixture model and we will further discuss such models in more detail in the topic analysis chapter Chapter 17  Figure 7 6 shows some examples of the feedback model learned from a web docu ment collection for performing pseudo feedback.
feedback docu collection pseudo We 10 documents use λ 0 λ	6 shows some examples of the feedback model learned from a web docu ment collection for performing pseudo feedback  We just use the top 10 documents and we use the mixture model with parameters λ  0 9 and λ  0.
air port documents returned engine feed model.	 The query is air port security  We select the top ten documents returned by the search engine for this query and feed them to the mixture model.
	 But we also see beverage alcohol bomb and terrorist.
λ “choose” probability background text rely model topic model	 Remember that λ can “choose” the probability of using the background model to generate to the text  If we don’t rely much on the background model we still have to use the topic model to account for the common words.
Buckley Xu A available 2009 2010 appears effective model pseudo feedback.	 Buckley 1994 Xu and Croft 1996  A comparison of feedback approaches in language models is available in Lv and Zhai 2009  The positional relevance model proposed in Lv and Zhai 2010 appears to be one of the most effective methods for estimating a query language model for pseudo feedback.
Due large engine data implicit based users’ important engines users user Joachims et al.	 Due to the availability of a large amount of search engine log data implicit feedback based on users’ interaction behavior has become a very important and very effective technique to enable web search engines to improve their accuracy over time as more and more users are using the systems though the interpretation of user clickthroughs must take position bias into con sideration which is discussed in detail in Joachims et al.
bibliography 2003.	 2007  A bibliography on implicit feedback can be found in Kelly and Teevan 2003.
	e.
learning rank discussed book query tutorial 146 7	 learning to rank techniques they are discussed briefly in Chapter 10 of the book  For a more thorough discussion of mining query logs see the tutorial Silvestri 2010  146 Chapter 7 Feedback 7.
	 7.
How required tradeoff space time benefits majority users Implementation This chapter focuses implement IR search engine In IR consists	 How could you optimize the amount of space required What kind of solutions provide a tradeoff between space and query time How about an online system that benefits the majority of users or the majority of queries 8Search Engine Implementation This chapter focuses on how to implement an information retrieval IR system or a search engine  In general an IR system consists of four components.
	 Tokenizer.
documents index consume We losslessly compressing data usually integers	 The documents we index could consume hundreds of gigabytes or terabytes  We can simultaneously save disk space and increase disk read efficiency by losslessly compressing the data in our index which is usually just integers 1 Caching.
We save disk space increase disk read compressing data usually	 We can simultaneously save disk space and increase disk read efficiency by losslessly compressing the data in our index which is usually just integers 1 Caching.
Tokenizer tokenization step mining saw chapter rep resent vectors single	1 Tokenizer Document tokenization is the first step in any text mining task  This determines how we represent a document  We saw in the previous chapter that we often rep resent documents as document vectors where each index corresponds to a single word.
The stored index raw occurrences word particular When information retrieval scoring vectors ally prefer representation term smoothed In systems leave	 The value stored in the index is then a raw count of the number of occurrences of that word in a particular document  When running information retrieval scoring functions on these vectors we usu ally prefer some alternate representation of term count such as smoothed term count or TFIDF weighting  In real search engine systems we often leave the term scoring up to the index scorer module.
tokenization scorer term	 Thus in tokenization we will simply use the raw count of features words since the raw count can be used by the scorer to calculate some weighted term representation.
like TFIDF simple single calculate IDF Furthermore use different	 Additionally calculating something like TFIDF is more involved than a simple scanning of a single document since we need to calculate IDF  Furthermore we’d like our scorer to be able to use different 1.
As instead strings efficiency	 As we will discuss the string terms themselves are almost always represented as term IDs and most of the processing on “words” is done on integer IDs instead of strings for efficiency  8.
8 functions require TFIDF weighting.	 8 1 Tokenizer 149 scoring functions as necessary storing only TFIDF weight would then require us to always use TFIDF weighting.
A appear it.	 A document is then represented by how many and what kind of tokens appear in it.
The tokens formulate basic whitespace This delimits	 The raw counts of these tokens are used by the scorer to formulate the retrieval scoring functions that we discussed in the previous chapter  The most basic tokenizer we will consider is a whitespace tokenizer  This tok enizer simply delimits words by their whitespace.
1 A slightly lowercase sentence split	 1  A slightly more advanced unigram words tokenizer could first lowercase the sentence and split the words based on punctuation.
slightly advanced words tokenizer lowercase split words There period	 A slightly more advanced unigram words tokenizer could first lowercase the sentence and split the words based on punctuation  There is a special case here where the period after Mr.
	 1.
tokenizer It refer strings	 Another common task of the tokenizer is to assign document IDs  It is much more efficient to refer to documents as unique numbers as opposed to strings such as homejeremydocsfile473 txt.
In C use structure stdunordered know gives time uint64t particular	 In C we could of course use some structure internally such as stdunordered mapstdstring uint64t  As you know using a hash table like this gives amortized O1 lookup time to find a uint64t corresponding to a particular stdstring.
150 8 Search Engine stdvectoruint64t takes ID index look	 150 Chapter 8 Search Engine Implementation However using term IDs we can instead write stdvectoruint64t  This data structure takes up less space and allows true O1 access to each uint64t using a term ID integer as the index into the stdvector  Thus for term ID 57 we would look up index 57 in the array.
Once define index tokenization critical	 Once we define how to conceptualize documents we can index them cluster them and classify them among many other text mining tasks  As mentioned in the Introduction tokenization is perhaps the most critical component of our indexer since all downstream operations depend on its output  8.
doesn’t world Google	 This doesn’t even take into account real world production systems such as Google that index the entire Web.
match relatively corpora inverted index lookup	 Scanning over every document in the corpus to match terms in the query will not be sufficient even for relatively small corpora  An inverted index is the main data structure used in a search engine  It allows for quick lookup of documents that contain any given term.
It lookup documents term relevant 1 table termspecific information postings perdocument term frequency	 It allows for quick lookup of documents that contain any given term  The relevant data structures include 1 the lexicon a lookup table of termspecific information such as document frequency and where in the postings file to access the perdocument term counts and 2 the postings file mapping from any term integer ID to a list of document IDs and frequency information of the term in those documents.
The structures include lookup access term term integer ID IDs documents.	 The relevant data structures include 1 the lexicon a lookup table of termspecific information such as document frequency and where in the postings file to access the perdocument term counts and 2 the postings file mapping from any term integer ID to a list of document IDs and frequency information of the term in those documents.
check query matched window text	 Such position information can be used to check whether all the query terms are matched within a certain window of text e.
image actually offsets represent bit postings example want lexicon.	 The arrows in the image are actually integer offsets that represent bit or byte indices into the postings file  For example if we want to score the term computer which is term ID 56 we look up 56 in the lexicon.
tokenizer term IDs indexed	 Since the tokenizer assigned term IDs sequentially we could represent the lexicon as a large array indexed by term ID.
Each element large store total	 Each element in the large array would store tuples of document frequency total count offset information.
If term list corpus This true	 If the term appears in all documents we’d have a list of the length of the number of documents in the corpus  This is true for all unique terms.
created	 A forward index may be created in a very similar way to the inverted index.
example document’s content we’d scan postings terms specific index vector ID.	 For example clustering or classification would need to access an entire document’s content at once  Using an inverted index to do this is not efficient at all since we’d have to scan the entire postings file to find all the terms that occur in a specific document  Thus we have the forward index structure that records a term vector for each document ID.
Chapter	 154 Chapter 8 Search Engine Implementation Algorithm 8.
Termatatime accumulator q d count ∈ Idx.	1 Termatatime Ranking scores    score accumulator maps doc IDs to scores for w ∈ q do for d  count ∈ Idx.
3.	3.
	1 with query q.
term fetch frequency	 For each term fetch the corresponding entries frequency counts in the inverted index.
Documentatatime size score documents matching	2 Documentatatime Ranking One disadvantage to termatatime ranking is that the size of the score accumula tors scores will be the size of the number of documents matching at least one term.
As priority queue.	 As we score a complete document it is added on the priority queue.
	3.
3 Filtering documents	3 Filtering Documents Another common task is only returning documents that meet a certain criteria.
d counts 0 count counts score termcount end	fetch docsw do contextd appendcount end for end for priority queue    low score is treated as high priority for d  term counts ∈ context do score  0 for count ∈ term counts do score  score  score termcount end for priority queue.
8	 8 3.
Index keeping search engine achieved merging number equal All stored final chunk pieces.	4 Index Sharding Index sharding is the concept of keeping more than one inverted index for a partic ular search engine  This is easily achieved by stopping the postings chunk merging process when the number of chunks is equal to the number of desired shards  All the same data is stored as one final chunk but it’s just broken down into several pieces.
This design—distributing merging results—is Reduce.	 This type of algorithm design—distributing the work and then merging the results—is a very common paradigm called Map Reduce.
documents index modify query tor estimate language likelihood.	 Use the top k documents and the forward index to either modify the query vec tor Rocchio or estimate a language model and interpolate with the feedback model query likelihood.
g saved techniques later methods consider caching expanded	g  only storing the very frequently expanded queries or using query similarity to search for a similar query that has been saved  The caching techniques discussed in a later section are also applicable to feedback methods so consider how to adopt them from caching terms to caching expanded queries.
documents like chosen method feedback	 Once we know which documents we’d like to include in the chosen feedback method all implementations are the same since they deal with a list of feedback documents.
frequent longer The compression rate related taking values consider—skewed distributions lower easier compress It methods support random like particular position file having pre vious data.	 Intuitively we will assign a short code to values that are frequent at the price of using longer codes for rare values  The optimal compression rate is related to the entropy of the random variable taking the values that we consider—skewed distributions would have lower entropy and are thus easier to compress  It is important that all of our compression methods need to support random access decoding that is we could like to seek to a particular position in the postings file and start decompressing without having to decompress all the pre vious data.
document uniformly bution skewed inverted gaps.	 The document IDs would otherwise be distributed relatively uniformly but the distri bution of their gaps would be skewed since when a term is frequent its inverted list would have many document IDs leading to many small gaps.
exact store creates smaller numbers	    Instead of storing these exact numbers we can store the offsets between them this creates more smaller numbers which are easier to compress since they take 8.
Instead storing store easier compress 5 frequent 1 2 6 2	 Instead of storing these exact numbers we can store the offsets between them this creates more smaller numbers which are easier to compress since they take 8 5 Compression 159 up less space and are more frequent 23 2 9 1 4 4 6 2 6 2 .
The on.	 The third is 25  9  34 and so on.
reference block	 frame of reference block  8 5.
	 Unary.
know 1 01 3 001 0000000000000000001 Note can’t encode methods example sequence 000100100010000000101000100001 3 4 2 Chapter As support random access	 The one acts as a delimiter and lets us know when to stop reading 1 → 1 2 → 01 3 → 001 4 → 0001 5 → 00001 19 → 0000000000000000001 Note that we can’t encode the number zero—this is true of most other methods as well  An example of a unaryencoded sequence is 000100100010000000101000100001  4 3 4 8 2 4 5  160 Chapter 8 Search Engine Implementation As long as the lexicon has a pointer to the beginning of a compressed integer we can easily support random access decoding.
Chapter Search Engine Implementation As beginning random access We small numbers larger space The compression concept	 160 Chapter 8 Search Engine Implementation As long as the lexicon has a pointer to the beginning of a compressed integer we can easily support random access decoding  We also have the property that small numbers take less space while larger numbers take up more space  The next two compression methods are built on the concept of unary encoding.
prepend k → 010 3 00100 5 000010011 → To decode count k additional k bits.	 Then prepend k − 1 zeros to the binary number 1 → 1 2 → 010 3 → 011 4 → 00100 5 → 00101 19 → 000010011 47 → 00000101111 To decode read and count k zeros until you hit a one  Read the one and additional k bits in binary  Note that all γ codes will have an odd number of bits.
Delta short δencoding encoding γ encoding unary 1 2 → 0100 → → 00100 → → 01101 001010011 → 0011001111 To decode decode γ code position k binary	 Delta  In short δencoding is γ encoding a number and then γ encoding the unary prefix including the one 1 → 1 → 1 2 → 010 → 0100 3 → 011 → 0101 4 → 00100 → 01100 5 → 00101 → 01101 19 → 000010011 → 001010011 47 → 00000101111 → 0011001111 To decode decode the γ code at your start position to get an integer k  Write a one and then read the next k  1 bits in binary including the one you wrote.
binary including wrote.	 Write a one and then read the next k  1 bits in binary including the one you wrote.
bit operations required δencode Block compression number CPU expense	 count how many bit operations are required to δencode the integer 47  Block compression seeks to reduce the number of CPU instructions required in decoding at the expense of using more storage.
It lower seven bit flag number.	 vByte stands for variable byte encoding  It uses the lower seven bits of a byte to store a binary number and the most significant bit as a flag  The flag signals whether the decoder should keep reading the binary number.
It binary number significant	 It uses the lower seven bits of a byte to store a binary number and the most significant bit as a flag.
	 The parentheses below are added for emphasis.
left shift k number bytes far Therefore sum integer 20000 0100000 0 000000100000000000000b 20000d In method “blocks” bytes k encodes k method	 For every “link” we follow we left shift the byte to add by 7 × k where k is the number of bytes read so far  Therefore the sum we have to decode the integer 20000 is 0100000  0  0011100  7  0000001  14 which is the same as 0100000b 00111000000000b 000000100000000000000b 000000100111000100000b 20000d In this method the “blocks” that we are encoding are bytes  Frame of reference encoding takes a block size k and encodes k integers at a time so a block in this method is actually a sequence of numbers.
transform subtracting value element 6 25 27 min	 We transform this block by subtracting the minimum value in the list from each element 0 2 6 14 18 19 25 27 min  45.
However instead compression use smallest bits number chunk 27 store integers 5 bits 10011 min	 However instead of encoding each value with a bitwise compression such as γ  or δencoding we will simply use binary with the smallest number of bits possible  Since the maximum number in this chunk is 27 we will store each of the integers in 5 bits 00000 00010 00110 01110 10010 10011 11001 11011 min  45 bytes  5.
g binary	g  fixed binary length.
reason postings data objects we’ll different	 For this reason it is very common for a realworld search engine to also employ some sort of caching structure stored in memory for postings data objects  In this section we’ll overview two different caching strategies.
overview different strategies idea frequently fast acquire.	 In this section we’ll overview two different caching strategies  The basic idea of a cache is to make frequently accessed objects fast to acquire.
objects access	 In our case the objects we access are postings lists.
Due expect number keyed ID accessed time But set cutoff memory address	 Due to Zipf’s law Zipf 1949 we expect that a relatively small number of postings lists keyed by term ID will be accessed the majority of the time  But how do we know which terms to store Even if we knew which terms are most queried how do we set a cutoff for memory consumption The two cache designs we describe address both these issues.
LRU follows retrieve list First search If cache list x.	 The LRU algorithm is as follows assuming we want to retrieve the postings list for term ID x  First search the cache for term ID x  If it exists in the cache return the postings list corresponding to x.
enables O1 delete LRU determine	 This enables O1 amortized insert find and delete operations  The interesting part of the LRU cache is how to determine the access order of the objects.
shows DBLRU size six.	4 shows a DBLRU cache with size six.
The DBLRU hash follows assuming list term x primary term ID exists return	 The DBLRU cache is just two hash tables named primary and secondary  The algorithm is as follows assuming we want to retrieve the postings list for term ID x  First search primary for term ID x if it exists return it.
primary search If it’s secondary Insert return it.	 If it’s not in primary search secondary  If it’s in secondary delete it  Insert it in primary and return it.
org Notes A classic search engines book Man al.	org Bibliographic Notes and Further Reading A classic reference book for the implementation of search engines is the book Man aging Gigabytes Witten et al.
2010 excellent implementations search engines This chapter evaluation text retrieval systems In number different functions best answer question we’ll evaluate methods.	 2010 also have an excellent coverage of implementations of search engines  9Search Engine Evaluation This chapter focuses on the evaluation of text retrieval TR systems  In the previous chapter we talked about a number of different TR methods and ranking functions but how do we know which one works the best In order to answer this question we have to compare them and that means we’ll have to evaluate these retrieval methods.
9 1 Introduction think evaluation use evaluation figure retrieval works	 9 1 Introduction Let’s think about why we have to do evaluation  There are two main reasons the first is that we have to use evaluation to figure out which retrieval method works the best.
Introduction think	1 Introduction Let’s think about why we have to do evaluation.
book Chapter discussed problem text database retrieval.	 Previously in this book Chapter 6 we discussed the problem of text retrieval and compared it with database retrieval.
Imagine building works users In measures actual users opposed measures user studies—where users inter	 Imagine you’re building your own applications you would be interested in knowing how well your search engine works for your users  In this case measures must reflect the utility to the actual users in the real application as opposed to measures on each individual retrieval result  Typically this has been done via user studies—where human users inter act with the corpus via the system.
measures need usu test main talking chapter.	 The measures only need to be good enough to determine which method works better  This is usu ally done by using a test collection which is a main idea that we’ll be talking about in this chapter.
1 1 What Measure There	 9 1 1 What to Measure There are many aspects of a search engine we can measure—here are the three major ones.
	1.
Effectiveness accuracy How accurate search we’re ranking	 Effectiveness or accuracy  How accurate are the search results In this case we’re measuring a system’s capability of ranking relevant documents on top of nonrelevant ones.
Additional Sanderson Kelly cover user studies testing later 9.	 Additional readings are Sanderson 2010 and Kelly 2009 which cover user studies and AB testing concepts that are discussed later in this chapter  9.
image defined	 For example in image processing or other fields where the problem is empirically defined we typically would need to use a method such as this.
idea build measures test test different Using test allow performance al	 The basic idea of this approach is to build reusable test collections and define measures using these collections  Once such a test collection is built it can be used again and again to test different algorithms or ideas  Using these test collections we will define measures that allow us to quantify the performance of a system or al gorithm.
Using test collections define measures allow quantify	 Using these test collections we will define measures that allow us to quantify the performance of a system or al gorithm.
sample information need.	 We can also have a sample set of queries or topics that simulate the user’s information need.
need relevance judg ments.	 Then we need to have relevance judg ments.
judgments queries Ideally queries people know documents search	 These are judgments of which documents should be returned for which queries  Ideally they have to be made by users who formulated the queries because those are the people that know exactly what the documents search results would be used for.
measures ranked constructed methodology set reused times comparison	 Finally we have to have measures to quantify how well a system’s result matches the ideal ranked list that would be constructed based on users’ relevance judgements  This methodology is very useful for evaluating retrieval algorithms because the test set can be reused many times  It also provides a fair comparison for all the methods since the evaluation is exactly the same for each one.
In methodology works.	 In Figure 9 1 we illustrate how the Cranfield evaluation methodology works.
A’s approximation relevant documents B’s As differences returned	 Thus RA is system A’s approximation of relevant documents and RB is system B’s approximation  Let’s take a look at these results—which is better As a user which one would you like There are some differences and there are some documents that are returned by both systems.
So This user’s task individual users users A interested getting doesn’t documents example literature survey.	 So which one is better and how do we quantify this This question highly depends on a user’s task and it depends on the individual users as well For some users maybe system A is better if the user is not interested in getting all the relevant documents so he or she doesn’t have to read too much  On the other hand imagine a user might need to have as many relevant documents as possible for example in writing a literature survey.
we’ll information user We users results 9.	 In either case we’ll have to also define measures that would quantify the information need of a user  We will need to define multiple measures because users have different perspectives when looking at the results  9.
define users different perspectives looking 9.	 We will need to define multiple measures because users have different perspectives when looking at the results  9.
	 9.
Precision return	1 Precision and Recall Let’s return to Figure 9.
Retrieval seen relevant judged query thought better In relevant.	2 Evaluation of Set Retrieval 171 We have only seen three relevant documents there but we can imagine there are other documents judged for this query  Intuitively we thought that system A is better because it did not have much noise  In particular we have seen out of three results two are relevant.
results relevant.	 In particular we have seen out of three results two are relevant.
like A	 Based on this it looks like system A is more accurate.
	60.
shows A better according wish relevant possible case compare documents retrieved.	 This shows that system A is better according to precision  But we also mentioned that system B might be preferred by some other users who wish to retrieve as many relevant documents as possible  So in that case we have to compare the number of total relevant documents to the number that is actually retrieved.
recall These evaluating	 Therefore system B is better according to recall  These two measures are the very basic foundation for evaluating search engines.
Now measures measures evaluate retrieved	 Now let’s define these two measures more precisely and how these measures are used to evaluate a set of retrieved documents.
We cases depending shown	 We can distinguish the results in four cases depending on the situation of a document as shown in Figure 9 2.
	2  A document is either retrieved or not retrieved since we’re talking about the set of results.
relevant user document.	 The document can be also relevant or not relevant depending on whether the user thinks this is a useful document.
number documents retrieved c documents retrieved d retrieved table ratio relevant retrieved documents .	 We can have a represent the number of documents that are retrieved and relevant b for documents that are not retrieved but relevant c for documents that are retrieved but not relevant and d for documents that are both not retrieved and not relevant  With this table we have defined precision as the ratio of the relevant retrieved documents a to the total number of retrieved documents a and c a ac .
With ratio documents	 With this table we have defined precision as the ratio of the relevant retrieved documents a to the total number of retrieved documents a and c a ac .
results returned relevant didn’t relevant there’s returned reality recall associated documents include decreases precision web	0 that means all the results that we returned are relevant and we didn’t miss any relevant documents there’s no single nonrelevant document returned  In reality however high recall tends to be associated with low precision as you go down the list to try to get as many relevant documents as possible you tend to include many non relevant documents which decreases the precision  We often are interested in the precision up to ten documents for web search.
This means results documents actually relevant.	 This means we look at the top ten results and see how many documents among them are actually relevant.
results typically In we’ll combine measures	 This is a very meaningful measure because it tells us how many relevant documents a user can expect to see on the first page of search results where there are typically ten results  In the next section we’ll see how to combine precision and recall into one score that captures both measures  9.
section we’ll recall	 In the next section we’ll see how to combine precision and recall into one score that captures both measures  9.
	 9 2.
One metric called Fβ 3 case β harmonic precision recall.	 One metric that is often used is called the Fβ measure displayed in Figure 9 3  In the case where β  1 it’s a harmonic mean of precision and recall.
3 In case 1 it’s precision	3  In the case where β  1 it’s a harmonic mean of precision and recall.
Considering parameter β measure figure Often set indicates equal preference precision	 Considering the parameter β and after some simplification we can see the F measure may be written in the form on the righthand side of the figure  Often β set to one which indicates an equal preference towards precision and recall.
β set indicates equal precision	 Often β set to one which indicates an equal preference towards precision and recall.
This popular measure combined precision emphasis precision	 This is a popular measure that is often used as a combined precision and recall score  If β is not equal to one it controls the emphasis between precision and recall.
That means high R care value effect perfect recall returning precision.	 That means if you have a very high P or a very high R then you really don’t care about whether the other value is low since the whole sum would be high  This is not the desirable effect because one can easily have a perfect recall by returning all the documents Then we have a perfect recall and a low precision.
extremely result	 So it would penalize a case with an extremely high result for only one of them.
precision documents relevant In list don’t	 As you can see if we keep doing this we can also get to D8 and have a precision of four out of eight because there are eight documents and four of them are relevant  There the recall is four out of ten  When can we get a recall of five out of ten In this list we don’t have it.
retrieval	 But this is okay for the relative comparison of two text retrieval methods.
This important point different particular method.	 This is the most important point to keep in mind when you compare different algorithms the key is to avoid any bias toward a particular method.
On numbers display recall certain	 On the yaxis are the precision values  We plot precision recall numbers so that we display at what recall we can obtain a certain precision.
precision display	 We plot precision recall numbers so that we display at what recall we can obtain a certain precision.
come red A.	 Then you have come up with a new idea and test it with results shown in red as system A.
what’s difference Well that—in high recall A better That means cares high recall low high what’s happening	 So what’s the difference here Well the difference is just that—in the low level of recall system B is better but in the high recall region system A is better  That means it depends on whether the user cares about high recall or low recall with high precision  Imagine someone is just going to check out what’s happening today and wants to find out something relevant in the news.
Imagine happening wants relevant Which better In better user results i.	 Imagine someone is just going to check out what’s happening today and wants to find out something relevant in the news  Which system is better for that task In this case system B is better because the user is unlikely to examine many results i.
common documents	 Dividing by four is a common mistake this favors a system that would retrieve very few documents making the denominator very small.
Di pi divide documents we’ve seen document p2 .	 If Di is relevant to obtain pi we divide the number of relevant documents we’ve seen so far by the current position in the list which is i  If the first relevant document is at the second rank then p2  1 2 .
increase	 If we move the third or fourth relevant document up it would increase the averages.
There measure It distinguish small differences lists that’s	 There fore this is a good measure because it’s sensitive to the ranking of each individual relevant document  It can distinguish small differences between two ranked lists and that’s exactly what we want.
In precision documents That perspective.	 In contrast if we look at the precision at ten documents it’s easy to see that it’s four out of ten  That precision is very meaningful because it tells us what a user would see from their perspective.
If precision remains average subtle rank 9.	 If they are moved around the top ten spots the precision at ten remains the same  In contrast average precision is a much better measure since subtle differences in rank affect the overall score  9.
.	   .
We define m∏ avpLi 1 m 9.	 We define it below mathematically as gMAPL m∏ i1 avpLi 1 m 9.
Imagine testing tested multiple queries precision	 9 4 Imagine you are again testing a new algorithm  You’ve tested multiple topics queries and have the average precision for each topic.
want improve queries MAP So depends preferences measure going represent needs special mean average think precisely document entire	 If you just want to improve over all kinds of queries then perhaps MAP would be preferred  So again the answer depends on your users’ tasks and preferences  Which measure is most likely going to represent your users’ needs As a special case of the mean average precision we can also think about the case where there is precisely one relevant document in the entire collection.
	 So again the answer depends on your users’ tasks and preferences.
question In answers goal rank particular	 Or in another application such as question answering there is only one answer  In this scenario if you rank the answers then your goal is to rank that one particular answer on top.
scenario goal answer case average boil reciprocal	 In this scenario if you rank the answers then your goal is to rank that one particular answer on top  In this case the average precision will boil down to the reciprocal rank.
That 1 position document.	 In this case the average precision will boil down to the reciprocal rank  That is 1 r where r is the position rank of the single relevant document.
popular search item meaningful relevant	 It’s a very popular measure for known item search or any problem where you have just one relevant item  We can see this r is quite meaningful it indicates how much effort a user would have to make in order to find that one relevant document.
For r difference.	 Again it would make a difference  For one single topic using r or using 1 r wouldn’t make any difference.
From highly ranked reciprocal rank emphasize difference Think rank rank difference	 From a user’s perspective we care more about the highly ranked documents so by taking this transformation by using reciprocal rank we emphasize more on the difference on the top  Think about the difference between rank one and rank two and the difference between rank 100 and 1000 using each method.
In summary precisionrecall overall list We utility ranked users comparing ranking combines precision sensitive rank relevant	 Is one more preferable than the other In summary we showed that the precisionrecall curve can characterize the overall accuracy of a ranked list  We emphasized that the actual utility of a ranked list depends on how many top ranked results a user would examine some users will examine more than others  Average precision is a standard measure for comparing two ranking methods it combines precision and recall while being sensitive to the rank of every relevant document.
Multilevel text systems multiple So discussed binary judgements—that documents judged relevant	 9 4 Evaluation with Multilevel Judgements In this section we will explain how to evaluate text retrieval systems when there are multiple levels of relevance judgments  So far we discussed about binary judgements—that means a documents is judged as being relevant or nonrelevant.
7 example level relevant marginally nonrelevant How operates relevance level information MAP MRR average can’t either.	7 we show an example of three relevance levels level three for highly relevant two for marginally relevant and one for nonrelevant  How do we evaluate a new system using these judgements We can’t use average precision since it only operates on binary relevance values if we treat level two and three as only one level then we lose the information gained from comparing these two categories  MAP gMAP and MRR depend on average precision so we can’t use them either.
gain let range n set example We n∑ i1 ri	 Let ri be the gain of result i and let i range from one to n where we set n to ten in our example  We then have the cumulative gain CG as CGL n∑ i1 ri .
9 5 If documents gain cost spending	 9 5 If the user looks at more documents the cumulative gain is more  This is at the cost of spending more time to examine the list.
spending time list.	 This is at the cost of spending more time to examine the list.
The position doesn’t discounted sees document second bit small We divide gain weight capture positionbased	 The document at position one doesn’t need to be discounted because you can assume that the user always sees this document but the second one will be discounted a little bit because there’s a small possibility that the user wouldn’t notice it  We divide this gain by a weight based on the position in order to capture this positionbased penalty.
	 9.
idea gain NDCG .	 The idea here is normalized discounted cumulative gain NDCG NDCGL  DCGL IDCG .
IDCG ideal relevant documents order relevance For imagine documents Then	 The IDCG is the DCG of an ideal ranked list with the most relevant documents at the top sorted in decreasing order of relevance  For example imagine that we have nine documents in the whole collection rated three  Then our ideal ranked list would have put all these nine documents on the very top.
All followed that’s best threes Then ideal ranked	 All this would be followed by a two because that’s the best we could do after we have run out of threes  Then we can compute the DCG for this ideal ranked list.
order normalize 0 best result possibly query affect relative comparison	 This becomes the denominator for NDCG in order to normalize our own DCG in the range 0 1  Essentially we compare the actual DCG with the best result you can possibly get for this query  This doesn’t affect the relative comparison of systems for just one topic because this ideal DCG is the same for all the systems.
NDCG measuring relevance general basically applied ranked task large scale judgments hand.	 Thus NDCG is used for measuring relevance based on much more than one relevance level  In a more general way this is basically a measure that can be applied through any ranked task with a large range of judgments  Furthermore the scale of the judgments can be dependant on the application at hand.
Practical Evaluation order create create turns requirement challenges.	5 Practical Issues in Evaluation In order to create a test collection we have to create a set of queries a set of documents and a set of relevance judgments  It turns out that each requirement has its own challenges.
documents queries They users with.	 First the documents and queries must be representative  They must represent real queries and real documents that users interact with.
retrieval	 In order to evaluate a highrecall retrieval task we must ensure there exist many relevant doc uments for each query.
probability uation scores systems significance The difference scores	 With a certain probability we can mathematically quantify whether the eval uation scores of two systems are indeed different  The way we do this is with a statistical significance test  The significance test gives us an idea as to how likely a difference in evaluation scores is due to random chance.
In confident Based easily better it’s	 In the other case you might not feel as confident  Based on only the MAP score we can easily say that system B is better  After all it’s 0.
MAP score easily B better After 0 4 0.	 Based on only the MAP score we can easily say that system B is better  After all it’s 0 4 which is twice as much as 0.
it’s 4	 After all it’s 0 4 which is twice as much as 0 2.
Clearly performance look detailed results 1 fact precisions consistently better	2  Clearly that’s better performance  But if you look at these two experiments and look at the detailed results you will see that we’ll be more confident to say that in experiment 1 that system B is in fact better since the average precisions are consistently better than system A’s.
look results we’ll experiment 1 better average consistently In B fluctuate wildly significance	 But if you look at these two experiments and look at the detailed results you will see that we’ll be more confident to say that in experiment 1 that system B is in fact better since the average precisions are consistently better than system A’s  In experiment 2 we’re not sure that system B is better since the scores fluctuate so wildly  How can we quantitatively answer this question This is why we need to do a statistical significance test.
2 we’re B fluctuate	 In experiment 2 we’re not sure that system B is better since the scores fluctuate so wildly.
The fact This value fluctuation In case probability surely random fluctuation.	 The fact that the average is larger doesn’t tell us anything This intuition can be quantified by the concept of a p value  A p value is the probability that this result is in fact from random fluctuation  In this case the probability is one it means it surely is a random fluctuation.
test signedrank look signs scores.	 A particularly interesting test is the Wilcoxon signedrank test  It’s a nonparametric test and we not only look at the signs but also consider the magnitude of the difference in scores.
	10.
But random depending difference difference left right shows values deviating MAP vice Based random observation.	 But we assume that because of random fluctuations depending on the queries we might observe a difference thus the actual difference might be on the left side or right side  This curve shows the probability that we would observe values that are deviating from zero here when we subtract system A’s MAP from system B’s MAP or even vice versa  Based on the picture we see that if a difference is observed at the dot shown in the figure then the chance is very high that this is in fact a random observation.
Inside interval values random 95 observe value tails likely value drawn x	 Inside this interval the observed values are from random fluctuation with 95 chance  If you observe a value in the tails on the side then the difference is unlikely from random fluctuation only 5 likely  This 95 value determines where the lines are drawn on the x axis.
queries avoid incorrect deter place boundary lines chance difference let’s problem making relevance	 The takeaway message here is that we need to use many queries to avoid jumping to an incorrect conclusion that one system is better than another  There are many different ways of doing this statistical significance test which is essentially deter mining where to place the boundary lines between random chance and an actual difference in systems  Now let’s discuss the problem of making relevance judgements.
it’s documents small set The can’t judging documents subset judge pooling.	 As mentioned earlier it’s very hard to judge all the documents completely unless it is a very small data set  The question is if we can’t afford judging all the documents in the collection which subset should we judge The solution here is pooling.
We hope methods nominate relevant documents goal pick relevant documents users That way ranking	 We hope these methods can help us nominate likely relevant documents  The goal is to pick out the relevant documents so the users can make judgements on them  That way we would have each system return the top k documents according to its ranking function.
k pool documents human judge Of documents unique returned idea ranking pool broad.	 We then simply combine all these top k sets to form a pool of documents for human assessors to judge  Of course there will be many duplicated documents since many systems might have retrieved the same documents  There are also unique documents that are only returned by one system so the idea of having a diverse set of result ranking methods is to ensure the pool is broad.
unique having diverse set result methods pool include possible possible assessors	 There are also unique documents that are only returned by one system so the idea of having a diverse set of result ranking methods is to ensure the pool is broad  We can include as many possible random documents as possible  Then the human assessors would make complete judgements on this data set or pool.
Then complete set pool.	 Then the human assessors would make complete judgements on this data set or pool.
However evaluating new returns judged assumed nonrelevant haven’t covered AB testing methods showing users Of users don’t result users judge results click engine application.	 However this is problematic for evaluating a new system that may not have contributed to the pool since the documents it returns may not have been judged and are assumed to be nonrelevant  What we haven’t covered are some other evaluation strategies such as AB testing this is where an evaluating system would mix the results of two methods randomly showing the mix of results to users  Of course the users don’t see which result is from which method so the users would judge those results or click on those documents in a search engine application.
Text evaluation extremely empirically If users there’s inappropriate research drawing wrong	 Text retrieval evaluation is extremely important since the task is empirically defined  If we don’t rely on users there’s no way to tell whether one method works better  If we have an inappropriate experiment design we might misguide our research or applications drawing the wrong conclusions.
documents easier perspective documents likely real application Notes Reading research problem empirical general.	 Finally retrieving up to ten documents or some small number is easier to interpret from a user’s perspective since this is the number of documents they would likely see in a real application  Bibliographic Notes and Further Reading Evaluation has always been an important research problem in information re trieval and in empirical AI problems in general.
information retrieval algorithms developed web created opportunity algorithms classical algorithms fully challenges encountered web search scalability	 Although many information retrieval algorithms had been developed before the web was born it created the best opportunity to apply those algorithms to a major application problem that everyone cares about  Naturally there had to be some further extensions of the classical search algorithms to fully address new challenges encountered in web search  First this is a scalability challenge.
handle web coverage How quickly queries search relatively small usually libraries questions serious.	 How can we handle the size of the web and ensure completeness of coverage of all its information be it textual or not How can we serve many users quickly by answering all their queries Before the web was born the scale of search was relatively small usually focused on libraries so these questions were not serious.
The talked space application.	 The algorithms that we talked about such as the vector space model are general— they can be applied to any search application.
On hand special characteristics pages documents specific applica	 On the other hand they also don’t take advantage of special characteristics of pages or documents in specific applica tions such as web search.
tech nique indexing addresses issue scalability particular influential techniques developed addressing spam problem.	 One such tech nique is parallel indexing and searching  This addresses the issue of scalability in particular Google’s MapReduce framework is very influential  There are also techniques that have been developed for addressing the spam problem.
web query user’s	 This is a program that downloads web page content that we wish to search  The second component is the indexer which will take these downloaded pages and create an inverted index  The third component is retrieval which answers a user’s query by talking to the user’s browser.
add page’s links search satisfied.	 We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied.
tricky with.	 Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with.
similar exclusion protocol.	 In a similar vein a crawler should respect the robot exclusion protocol.
txt paths allowed crawl.	txt at the root of the site tells crawlers which paths they are not allowed to crawl.
strategies breadthfirst balances	 So what are the major crawling strategies In general breadthfirst search is the most common because it naturally balances server load.
One	 One interesting variation is called focused crawling.
g.	g.
extreme version example forum In	 An even more extreme version of focused crawling is for example downloading and indexing all forum posts on a particular forum  In this case we might have a URL such as httpwww.
refers forum.	 In this case we might have a URL such as httpwww textdatabookforum comboardsid3 which refers to the third post on the forum.
textdatabookforum.	textdatabookforum.
crawling new pages created crawler ran.	 Another challenge in crawling is to find new pages that have been created since the crawler last ran.
links	 If they are then you can probably find them by following links from existing pages in your index.
In information index new The	 In general we can use the standard information retrieval techniques for creating the index but there are new challenges that we have to solve for web scale indexing  The two main challenges are scalability and efficiency.
The single	 The index will be so large that it cannot actually fit into any single machine or single disk so we have to store the data on multiple machines.
second MapReduce software supporting computation known source implementation MapReduce 10.	 The second is MapReduce which is a general software framework for supporting parallel computation  Hadoop is the most well known open source implementation of MapReduce now used in many applications  Figure 10.
Figure architecture File	 Figure 10 1 shows the architecture of the Google File System GFS.
doesn’t worry it’s filesystem From perspective file.	 All of these details are something that the programmer doesn’t have to worry about and it’s all taken care of by this filesystem  From the application perspective the programmer would see a normal file.
result grammer effort create application parallel details communications balancing execution.	 As a result the pro grammer can make minimum effort to create an application that can be run on a large cluster in parallel  Some of the lowlevel details hidden in the framework are communications load balancing and task execution.
tolerance built server finished Here MapReduce mechanism know task completed automat task servers Again doesn’t	 Fault tolerance is also built in if one server goes down some tasks may not be finished  Here the MapReduce mechanism would know that the task has not been completed and would automat ically dispatch the task on other servers that can do the job  Again the programmer doesn’t have to worry about this.
doesn’t In MapReduce input number key value pairs.	 Again the programmer doesn’t have to worry about this  In MapReduce the input data are separated into a number of key value pairs.
map process pairs key map	 The map function will then process these key value pairs and generate a number of other key value pairs  Of course the new key is usually different from the old key that’s given to map as input.
The key together.	 The result is that all the values that are associated with the same key will be grouped together.
input partitioned parts reduce	 With such a framework the input data can be partitioned into multiple parts which are processed in parallel first by map and then processed again in parallel once we reach the reduce stage.
Figure 10 shows input containing tokenized words output want generate number rences word.	 Figure 10 3 shows an example of word counting  The input is files containing tokenized words and the output that we want to generate is the number of occur rences of each word.
One thought task different parallel counts idea MapReduce parallelize input More key pair string	 So how can we solve this problem One natural thought is that this task can be done in parallel by simply counting different parts of the file in parallel and combining all the counts  That’s precisely the idea of what we can do with MapReduce we can parallelize lines in this input file  More specifically we can assume the input to each map function is a key  value pair that represents the line number and the string on that line.
1 World Bye pair	 The first line is the pair 1 Hello World Bye World  This pair will be sent to amap function that counts the words in this line.
counts line In words word count The shown figure simple.	 This pair will be sent to amap function that counts the words in this line  In this case there are only four words and each word gets a count of one  The map pseudocode shown at the bottom of the figure is quite simple.
case count	 In this case there are only four words and each word gets a count of one.
grouping	 As mentioned the collector will do the internal grouping or sorting.
e number These lines.	e  a number of counts  These counts represent the occurrences of this word in different lines.
finishes word.	4 shows how the reduce function finishes the job of counting the total occurrences of this word.
counter array shown figure.	 We have a counter and then iterate over all the words that we see in this array shown in pseudocode at the bottom of the figure.
word word ID inverted document information function it.	 In the new pairs each key is a word and the value is the count of this word in this document followed by the document ID  Later in the inverted index we would like to keep this document ID information so the map function keeps track of it.
going work 1 index construction.	 The reduce function is going to do very minimal work  Algorithm 10 1 can be used for this inverted index construction.
10 index	 Algorithm 10 1 can be used for this inverted index construction.
10 adapted Lin Dyer andreduce functions.	 Algorithm 10 1 adapted from Lin and Dyer 2010 describes themap andreduce functions.
1 2010 A functions run MapReduce	1 adapted from Lin and Dyer 2010 describes themap andreduce functions  A programmer would specify these two functions to run on top of a MapReduce cluster.
programmer specify functions MapReduce cluster.	 A programmer would specify these two functions to run on top of a MapReduce cluster.
described associative array dictionary ID simply entry document	 As described before map counts the occurrences of a word using an associative array dictionary and outputs all the counts together with the document ID  The reduce function simply concatenates all the input that it has been given and as single entry for this document ID key.
different programmer details.	 Data can be processed by different machines and the programmer doesn’t have to take care of the details.
10 3 Link Analysis In we’re web search utilize links improve search section large MapReduce GFS.	 10 3 Link Analysis In this section we’re going to continue our discussion of web search particularly focusing on how to utilize links between pages to improve search  In the previous section we talked about how to create a large index on using MapReduce on GFS.
10 3 201 text”summary indicate utility Description link tell	 10 3 Link Analysis 201 Authority “Extra text”summary for a doc Links indicate the utility of a doc Hub Description “anchor text” What does a link tell us Figure 10.
clickthroughs.	 There are also algorithms to exploit large scale implicit feedback information in the form of clickthroughs.
general web search algorithms combine kinds features.	 In general web search ranking algorithms are based on machine learning algorithms to combine all kinds of features.
com online pointing description type query query online big	com front page the person might make a link called the big online bookstore pointing to Amazon  The description is very similar to what the user would type in the query box when they are looking for such a page  Suppose someone types in a query like online bookstore or big online bookstore.
The description user type looking page.	 The description is very similar to what the user would type in the query box when they are looking for such a page.
Suppose big online	 Suppose someone types in a query like online bookstore or big online bookstore.
provides that’s Amazon entry Thus describes provides pointed 202 10 Search 10.	 This actually provides evidence for matching the page that’s been pointed to—the Amazon entry page  Thus if you match the anchor text that describes the link to a page it provides good evidence for the relevance of the page being pointed to  202 Chapter 10 Web Search On the bottom of Figure 10.
links like Think page pointing similar	 PageRank captures page popularity which is another word for authority  The intuition is that links are just like citations in literature  Think about one page pointing to another page this is very similar to one paper citing another paper.
Think pointing paper.	 Think about one page pointing to another page this is very similar to one paper citing another paper.
Thus page cited PageRank takes principled counting	 Thus if a page is cited often we can assume this page is more useful  PageRank takes advantage of this intuition and implements it in a principled approach  In its simplest sense PageRank is essentially doing citation counting or inlink counting.
In short important On pointed	 In short if important pages are pointing to you you must also be important  On the other hand if those pages that are pointing to you are not pointed to by many other pages then you don’t get that much credit.
cited papers influential good you’re cited papers attracted case like links idea smooth accommodate potential	 If you are cited by ten papers that are not very influential that’s not as good as if you’re cited by ten papers that themselves have attracted a lot of other citations  Clearly this is a case where we would like to consider indirect links which is exactly what PageRank does  The other idea is that it’s good to smooth the citations to accommodate potential citations that have not yet been observed.
The it’s good smooth potential	 The other idea is that it’s good to smooth the citations to accommodate potential citations that have not yet been observed.
Another PageRank concept surfer look Figure	 Another way to understand PageRank is the concept of a random surfer visiting every web page  Let’s take a look at this example in detail illustrated in Figure 10.
7 left d2 d4 10.	7  On the left there is a small graph where each document d1 d2 d3 and d4 is a web 10.
left d1 d2 d4	 On the left there is a small graph where each document d1 d2 d3 and d4 is a web 10.
random page current randomly choose document So random surfer d1 d4 The surfing ignore actual links page	 When the random surfer decides to move to a different page they can either randomly follow a link from the current page or randomly choose a document to jump to from the entire collection  So if the random surfer is at d1 with some probability that random surfer will follow the links to either d3 or d4  The random surfing model also assumes that the surfer might get bored sometimes and decide to ignore the actual links randomly jumping to any page on the web.
random surfing model links randomly jumping If surfer takes reach link	 The random surfing model also assumes that the surfer might get bored sometimes and decide to ignore the actual links randomly jumping to any page on the web  If the surfer takes that option they would be able to reach any of the other pages even though there is no link directly to that page.
We columns 0 1 12 12 Transition matrix di d1 12 0 0 0 0 N j1 Mij 1 7 Example web corresponding matrix.	 We have zeros for the first two columns for 0 1 0 12 0 0 1 12 Transition matrix Mij  probability of going from di to dj d1 d2 d3 d4 12 0 0 0 M 12 0 0 0 N ∑ j1 Mij  1 Figure 10 7 Example of a web graph and the corresponding transition matrix.
Thus probability di dj pages.	 Thus Mij is the probability of going from di to dj   Each row’s values should sum to one because the surfer will have to go to precisely one of these pages.
Each values sum pages Now probability visiting particular page On probability dj 1 count righthand probability time step.	 Each row’s values should sum to one because the surfer will have to go to precisely one of these pages  Now how can we compute the probability of a surfer visiting a particular page On the lefthand side is the probability of visiting page dj at time t  1 the next time count  On the righthand side we can see the equation involves the probability at page di at time t  the current time step.
equation involves probability t current time equation captures possibilities page dj t	 On the righthand side we can see the equation involves the probability at page di at time t  the current time step  The equation captures the two possibilities of reaching a page dj at time t  1 through random surfing or following a link.
The equation captures probability page following	 The first part of the equation captures the probability that the random surfer would reach this page by following a link.
sums possible t .	 This term sums over all the possible N pages that the surfer could have been at time t .
di follow dj	 So in order to reach this dj page the surfer must first be at di at time t and would have to follow the link to go from di to dj .
1 transition matrix elements 1 N .	 You can think this 1 N comes from another transition matrix that has all the elements as 1 N .
let’s drop equal N	 So let’s drop the time index and assume that they would be equal this would give us N equations since each page has its own equation.
problem boils write The vector p matrix p The trans matrix N written matrix	 The problem boils down to solving this system of equations which we can write in the following form The vector p equals the transpose of a matrix multiplied by p again  The trans posed matrix is in fact the sum from 1 to N written in matrix form.
linear algebra precisely eigenvector.	 Recall from linear algebra that this is precisely the equation for an eigenvector.
update started matrices set	 This is how we update the vector we started with some initial values and iteratively multiply the matrices together which generates a new set of scores.
From know zero iteration guaranteed At scores pages Interestingly propagating	 From linear algebra we know that since there are no zero values in the matrix such iteration is guaranteed to converge  At that point we will have the PageRank scores for all the pages  Interestingly this update formula can be interpreted as propagating scores across the graph.
interpreted graph We imagine values initialized look equation lead reaching page.	 Interestingly this update formula can be interpreted as propagating scores across the graph  We can imagine we have values initialized on each of these pages and if you look at the equation we combine the scores of the pages that would lead to reaching a page.
combine scores reaching page scores propagated score score document.	 We can imagine we have values initialized on each of these pages and if you look at the equation we combine the scores of the pages that would lead to reaching a page  That is we’ll look at all the pages that are pointing to a page and combine their scores with the propagated score in order to get the next score for the current document.
We efficient sparse—that means link page don’t equation somewhat form relative ranking	 We repeat this for all documents which transfers probability mass across the network  In practice the calculation of the PageRank score is actually quite efficient because the matrices are sparse—that means that if there isn’t a link into the current page we don’t have to worry about it in the calculation  It’s also possible to normalize the equation and that will give a somewhat different form although the relative ranking of pages will not change.
It’s possible different relative ranking The address potential problem	 It’s also possible to normalize the equation and that will give a somewhat different form although the relative ranking of pages will not change  The normalization is to address the potential problem of zero outlinks.
In case probabilities current probability there’s links case follow.	 In that case the probabilities of reaching the next page from the current page will not sum to 1 because we have lost some probability mass when we assume that there’s some probability that the surfer will try to follow links although in this case there are no links to follow.
For PageRank simply assume surfer gets pages	 For example in this topicspecific PageRank we can simply assume when the surfer gets bored they won’t randomly jump into any page on the web  Instead they jump to only those pages that are relevant to the query.
generic	 Clearly this would be better than using a generic PageRank score for the entire web.
We PageRank link indicates friendship relation	 We can imagine if you compute a person’s PageRank score on a social network where a link indicates a friendship relation you’ll get some meaningful scores for people  10.
2 We’ve PageRank way capture authority hub	2 HITS We’ve talked about PageRank as a way to capture authority pages  In the beginning of this section we also mentioned that hub pages are useful.
construct A 1 Aij links	 First we construct the adjacency matrix A it contains a 1 at position Aij if di links to dj and a zero otherwise.
This	 This forms an iterative reinforcement mechanism.
These different expressing equations forms equation That authority hub	 These are just different ways of expressing these equations  What’s interesting is that if you look at the matrix forms you can plug the authority equation into the first one  That is you can actually eliminate the authority vector completely and you get the equation of only hub scores.
The PageRank actually multiplication adjacency matrix transpose Mathematically In initialize matrix equations	 The difference between this and PageRank is that now the matrix is actually a multiplication of the adjacency matrix and its transpose  Mathematically then we would be computing a very similar problem  In HITS we would initialize the values to one and apply the matrix equations ATA and AAT.
HITS values	 In HITS we would initialize the values to one and apply the matrix equations ATA and AAT.
assume given pair q define number These don’t based features.	 We assume that given a querydocument pair q  d we can define a number of features  These features don’t necessarily have to be content based features.
query according function BM25 query length normalization There score like PageRank HITS retrieval anchor text descriptions These document relevant query.	 They could be a score of the document with respect to the query according to a retrieval function such as BM25 query likelihood pivoted length normalization PL2 etc  There also can be a linkbased score like PageRank or HITS or an application of retrieval models to the anchor text of the page which are the descriptions of links that point to d  These can all be clues about whether this document is relevant or not to the query.
clues relevant	 These can all be clues about whether this document is relevant or not to the query.
question course combine features	 The question is of course how can we combine these features into a single score In this approach we simply hypothesize that the probability that this docu ment is relevant to this query is a function of all these features.
hypothesize relevance features particular parameters influence different features course	 We hypothesize that the probability of relevance is related to these features through a particular func tion that has some parameters  These parameters control the influence of different features on the final relevance  This is of course just an assumption.
parameters different final assumption open question.	 These parameters control the influence of different features on the final relevance  This is of course just an assumption  Whether this assumption really makes sense is still an open question.
judged know know documents users approximated clickthrough We try search engine’s accuracy e.	 This is data that have been judged by users so we already know the relevance judgments  We know which documents should be highly ranked for which queries and this information can be based on real judgments by users or can be approximated by just using clickthrough information as we discussed in Chapter 7  We will try to optimize our search engine’s retrieval accuracy using e.
scoring linear β linear logistic instead map combination 1 This connect probability 0 linear	 We could have had a scoring function directly based on the linear com bination of β and X but then the value of this linear combination could easily go beyond 1  Thus the reason why we use the logistic regression instead of linear re gression is to map this combination onto the range 0 1  This allows us to connect the probability of relevance which is between 0 and 1 to a linear combination of arbitrary coefficients.
One BM25 docu query PageRank score document query We ank score depend query.	 One is the BM25 score of the docu ment for the query  One is the PageRank score of the document which might or might not depend on the query  We might also have a topicsensitive PageR ank score that would depend on the query.
PageRank document depend We PageR query.	 One is the PageRank score of the document which might or might not depend on the query  We might also have a topicsensitive PageR ank score that would depend on the query.
document instance case Of course overly example instances use maximum likelihood estimate	 The document d2 is another training instance with differ ent feature values but in this case it’s nonrelevant  Of course this is an overly simplified example where we just have two instances but it’s sufficient to illustrate the point  We use the maximum likelihood estimator to estimate the parameters.
going predict status feature The model hypothesize probability features way We’re values relevance	 That is we’re going to predict the relevance status of the document based on the feature values  The likelihood of observing the relevance status of these two documents using our model is We hypothesize that the probability of relevance is related to the features in this way  We’re going to see for what values of β we can predict the relevance effectively.
There algorithms regressionbased reproaches generally retrieval NDCG.	 There are many more advanced learning algorithms than the regressionbased reproaches  They generally attempt to theoretically optimize a retrieval measure such as MAP or NDCG.
optimization objective directly maximizing prediction optimize ranking One prediction bad	 Note that the optimization objective we just discussed is not directly related to a retrieval measure  By maximizing the prediction of one or zero we don’t necessarily optimize the ranking of those documents  One can imagine that while our prediction may not be too bad the ranking can be wrong.
By necessarily	 By maximizing the prediction of one or zero we don’t necessarily optimize the ranking of those documents.
We d1 So retrieval advanced approaches	 We might have a larger probability of relevance for d2 than d1  So that won’t be good from a retrieval perspective even though by likelihood the function is not bad  More advanced approaches will try to correct this problem.
predicted high truly documents receive scores greater 0.	 Even though the predicted score is very high as long as the truly relevant documents receive scores that are greater than 0.
	 10.
A information access	 A third trend might be the integration of information access  Search navigation and recommendation might be combined to form a fullfledged information man agement system.
In beginning push engine unsatisfied “note” made.	 In the beginning of this book we talked about push access versus pull access these modes can be combined  For example if a search engine detects that a user is unsatisfied with search results a “note” may be made.
current search engines support tasks.	 While there is good support for shopping current search engines do not provide good task support for many other tasks.
Currently support task writing paper We think intelligent information	 Currently there’s not much support for a task such as writing a paper  We can think about any intelligent system—especially intelligent information systems—specified by three nodes.
The These different connect specify different types systems.	10 The DataUserService triangle  These questions specify an information system there are many different ways to connect them  Depending on how they are connected we can specify all different types of systems.
On 10.	 On the top of Figure 10.
literature kinds services including search alert new documents analyzing research trends support related closer intelligent type literature	 We could connect scientists with literature information to provide all kinds of services including search browsing alert to new relevant documents mining or analyzing research trends or task and decision support  For example we might be able to provide support for automatically generating a related works section for a research paper this would be closer to task support  Then we can imagine this intelligent information system would be a type of literature assistant.
We analyze product choosing product buy.	 We can provide data mining capabilities to analyze reviews compare products and product sentiment and pro vide task or decision support on choosing which product to buy.
5 The Future Search customer	 Or we can connect 10 5 The Future of Web Search 215 customer service people with emails from the customers.
Future service Imagine provide tomers.	5 The Future of Web Search 215 customer service people with emails from the customers  Imagine a system that can provide an analysis of these emails to find that the major complaints of the cus tomers.
10.	 All of these aim to help people to improve their productivity  Figure 10.
shows infor In connects keyword queries bagofwords That current search provides model data representa tion.	10 shows the trend of technology and characterizes intelligent infor mation systems with three angles  In the center of the figure there’s a triangle that connects keyword queries to search a bagofwords representation  That means the current search engines basically provides search support to users and mostly model users based on keyword queries seeing the data through a bagofwords representa tion.
Current “understand” information documents center keyword queries user search user’s task information.	 Current search engines don’t really “understand” information in the indexed documents  Consider some trends to push each node toward a more advanced func tion away from the center  Imagine if we can go beyond keyword queries look at the user search history and then further model the user to completely understand the user’s task environment context or other information.
On implementation This we’ll people’s names potentially useful information.	 On the document side we can also go beyond a bagofwords implementation to have an entityrelation represen tation  This means we’ll recognize people’s names their relations locations and any other potentially useful information.
today’s nat Google Once level better service.	 This is already feasible with today’s nat ural language processing techniques  Google has initiated some of this work via its Knowledge Graph  Once we can get to that level without much manual human effort the search engine can provide a much better service.
knowledge representation inference rules search	 In the future we would like to have a knowledge representation where we can perhaps add some inference rules making the search engine more intelligent.
it’s easier	 That is it’s easier to make progress in one particular domain.
In future information interactive emphasize combined system.	 In this dimension we anticipate that future intelligent information systems will provide interactive task support  We should emphasize interactive here because it’s important to optimize the combined intelligence of users and the system.
We help users assuming	 We can get some help from users in a natural way without assuming the system has to do everything.
This combined intelligence general overall solving systems fully covered chapters	 This combined intelligence will be high and in general we can minimize the user’s overall effort in solving their current problem  This is the big picture of future intelligent information systems and this hope fully can provide us with some insights about how to make further innovations on top of what we have today and also motivate the additional techniques to be covered in the later chapters of the book.
excellent survey research work rank Systems In discussions search hoc	 Liu 2009 gives an excellent survey of research work on learning to rank  11Recommender Systems In our many discussions of search engine systems we have addressed the issue of shortterm ad hoc information need.
Examples include news filter moviebook ture distinction filtering exclusion recommender convenience feedback expect lot feedback important	 Examples include a news filter email filter moviebook recommender or litera ture recommender  Although there is some distinction between a recommender system emphasizing delivery of useful items to users and a filtering system em phasizing exclusion of useless items the techniques used are similar so we will use recommender and filtering interchangeably for convenience  Unlike ad hoc search where we may not get much feedback from a user in filtering we can expect to collect a lot of feedback information from the user making it important to learn from the feedback information to improve filtering performance.
essential question x approach question following apply filtering look u likes characterize	 The essential filtering question is will user u like item x Our approach to answering this question defines which of the following two strategies we apply    Contentbased filtering look at what u likes and characterize x .
Contentbased filtering usually extending retrieval add thresholding component.	 Contentbased filtering can usually be done by extending a retrieval system to add a thresholding component.
challenges associated threshold setting First beginning set information time optimize	 There are two challenges associated with threshold setting  First at the beginning we must set an initial threshold without requiring much information from a user  Second over time we need to learn from feedback to optimize the threshold.
ratings given	 all the ratings given by this user.
similarity vectors based cosine weighted ratings x given similar weight user As systems extensions retrieval systems techniques discussed	 The similarity of two vectors can be measured based on the cosine similarity or Pearson correlation of the two vectors which tends to perform very well empirically  In the second step we compute a weighted average of the ratings of x given by all these retrieved similar users where the weight is the correlation between the active user and the corresponding user to the weight  As we will see in this chapter many recommender systems are extensions of the information retrieval systems and techniques we have discussed previously.
2 information knowledge called summary keywords user interested case text recommen dation information initialization user’s categories.	2 in an information filtering system there would be a binary classifier1 that would have some knowledge of the user’s interests called the user interest profile  Originally the user profile could be a text summary or keywords of what the user is interested in for the case of text document recommen dation  This information is set in an initialization module that would take a user’s input perhaps from the user’s specified keywords or chosen categories.
text keywords user interested chosen	 Originally the user profile could be a text summary or keywords of what the user is interested in for the case of text document recommen dation  This information is set in an initialization module that would take a user’s input perhaps from the user’s specified keywords or chosen categories.
−	 R − 2   R′ 11.
game.	 In a way we can treat this as a gambling game.
utility function measures accumulate lose considering kind game want maximize deliver good possible simultaneously delivery One interesting question	 This utility function measures how much money you would accumulate or lose by considering this kind of game  It’s clear that if you want to maximize this utility function your strategy should be to deliver as many good items as possible while simultaneously minimizing the delivery of bad items  One interesting question here is how to set these coefficients.
How utility output use 10 −1 reward good one.	 How would these utility functions affect the system’s output If we use 10 and −1 you will see that while we get a big reward for delivering a good document we incur only a small penalty for delivering a bad one.
The basic filtering following started limited text description	 The three basic components in contentbased filtering are the following  Initialization module  Gets the system started based only on a very limited text description or very few examples from the user.
document delivered Learning module Learn limited relevance deliv	 Given a text document and a profile description of the user decide whether the document should be delivered or not  Learning module  Learn from limited user relevance judgments on the deliv ered documents.
Of need learn Rocchio What don’t learn set θ .	 Of course we still need to learn from the history and for this we can use the traditional feedback techniques to learn to improve scoring such as Rocchio  What we don’t know how to do yet is learn how to set θ .
set initially update delivered information Figure shows model problems.	 We need to set it initially and then we have to learn how to update it over time as more documents are delivered to the user and we have more information  Figure 11 3 shows what the system might look like if we generalized a vector space model for filtering problems.
The vector score This score fed module value	 The profile vector can be matched with the document vector to generate the score  This score will be fed into a thresholding module that would say yes or no depending on the current value of θ .
This fed thresholding depending θ The filtering document user user	 This score will be fed into a thresholding module that would say yes or no depending on the current value of θ   The evaluation would be based on the utility for the filtering results  If it says yes the document will be sent to the user and then the user could give some feedback.
says yes document user user	 If it says yes the document will be sent to the user and then the user could give some feedback.
threshold learning component little	 The threshold learning is a new component that we need to talk a little bit more about.
status relevance document score 36 5 it’s relevant.	 We have the scores and the status of relevance  The first document has a score 36 5 and it’s relevant.
The document it’s relevant.	 The first document has a score 36 5 and it’s relevant.
5 relevant The second relevant.	5 and it’s relevant  The second one is not relevant.
judged random sample it’s data general labeled data data In case labeled data decision.	 Thus the judged documents are not a random sample it’s biased or censored data which creates some difficulty for learning an optimal θ   Secondly there are in general very little labeled data and very few relevant data which make it challenging for machine learning approaches which require a large amount of training data  In the extreme case at the beginning we don’t even have any labeled data at all but the system still has to make a decision.
Secondly little relevant data machine learning approaches require large don’t labeled data decision.	 Secondly there are in general very little labeled data and very few relevant data which make it challenging for machine learning approaches which require a large amount of training data  In the extreme case at the beginning we don’t even have any labeled data at all but the system still has to make a decision.
In extreme labeled	 In the extreme case at the beginning we don’t even have any labeled data at all but the system still has to make a decision  This issue is called the explorationexploitation tradeoff.
called	 This issue is called the explorationexploitation tradeoff.
However don’t opportunity user difficult problem	 However if you don’t deviate at all then you don’t explore at all and you might miss the opportunity to learn another interest of the user  Clearly this is a dilemma and a difficult problem to solve.
So problem little bit.	 So how do we solve this problem We can lower the threshold to explore a little bit.
	 1998.
utility threshold threshold interpolation parameter set α deviate utility point This solve problem encourage mechanism θzero point point necessarily	 As one can see from the formula the threshold will be just the interpolation of the zero utility threshold and the optimal threshold via the interpolation parameter α  Now the question is how we should set α and deviate from the optimal utility point  This can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore only up to the θzero point which is still a safe point but not necessarily reach all the way to it.
	e.
As N greater encourages	 As N becomes greater it encourages less exploration.
2 Filtering want risk strategy The utility learning solving active research	2 Collaborative Filtering 229 don’t want to risk losing money so it’s a “safe” strategy in that sense  The problem is of course that this approach is purely heuristic and the zero utility lower bound is often too conservative in practice  There are more advanced machine learning projects that have been proposed for solving these problems it is actually a very active research area.
Given collaborative filtering finds Based similar current preferences	 Given a user collaborative filtering finds a set of similar users  Based on the set of similar users it predicts the current user’s preferences  This method makes some assumptions.
given user rank similarity u1	6 given a user u we will rank other users based on similarity u1   .
	    um.
object position user rating Xij Again exact doesn’t	 If we have a judgment by that user for that object the element in that position would be the user rating Xij   Again note that the exact content of each item doesn’t matter at all.
user function values.	   that maps a user and object to a rating  In the matrix X we have observed there are some output values of this function and we want to infer the value of this function for other pairs that don’t have values.
X output want function	 In the matrix X we have observed there are some output values of this function and we want to infer the value of this function for other pairs that don’t have values.
approaches In major We discuss approach.	 As usual there are many approaches to solving this problem  In fact there is a major conference specifically dedicated to this problem  We will discuss what is called a memorybased approach.
fact major conference dedicated	 In fact there is a major conference specifically dedicated to this problem.
discuss approach.	 We will discuss what is called a memorybased approach.
e similar preference	e  similar users to the current user  Then we use those users to predict the preference of the current user.
directly compared normalize interested recommending “active” particular interested recommending oj	 So their ratings can not be directly compared with each other or aggregated together which is why we first normalize  Let ua be the user that we are interested in recommending items to the “active” user  In particular we are interested in recommending oj to ua.
interested In interested oj ua.	 Let ua be the user that we are interested in recommending items to the “active” user  In particular we are interested in recommending oj to ua.
w.	 m∑ i1 wua  ui   Vij  11 3 where w.
	 Vij  11 3 where w  .
imagine similarity	 As you may imagine there are many possibilities of similarity functions.
improve similarity practical issues values average users.	 There are some ways to improve this approach most of which consider the user similarity measure  There are some practical issues to deal with here as well for example there will be many missing values  We could set them to default values or the average ratings of other users.
preliminary prediction predicted values improve function.	 In fact in memory based collaborative filtering we can predict judgements with missing values  As you can imagine we could apply an iterative approach where we first do some preliminary prediction and then use the predicted values to further improve the similarity function.
As imagine iterative prediction use similarity IDF inverse user frequency users ratings.	 As you can imagine we could apply an iterative approach where we first do some preliminary prediction and then use the predicted values to further improve the similarity function  Another idea which is quite similar to the idea of IDF that we have seen in text research is called the inverse user frequency or IUF  Here the idea is to look at where the two users share similar ratings.
Conversely rare viewed users emphasizing similarity	 Conversely if it’s a rare item that has not been viewed by many users then it says more about their similarity emphasizing more on similarity of items that are not viewed by many users.
In sense task recommendation easy task difficult easy	 In some sense the filtering task of recommendation is easy and in another sense the task is rather difficult  It’s easy because the user expectation is low.
takes initiative user doesn’t recommend items useless information Thus easy.	 The system takes initiative to push the information to the user so the user doesn’t really make an effort  Unless you recommend only noisy items or useless documents any information would be appreciated  Thus in that sense it’s easy.
problem collaborative it’s past ratings ratings solve problem.	 If you think of this as a learning problem in collaborative filtering for example it’s purely based on learning from the past ratings  If you don’t have many ratings there’s really not much you can do  As we mentioned there are strategies that have been proposed to solve the problem.
The filtering talked pull strategies getting access data.	 The other is user similarity which is collaborative filtering  We talked about push vs  pull as two strategies for getting access to the text data.
users push engines users pull mode.	 Recommender systems aid users in push mode whereas search engines assist users in pull mode.
3 Systems In collaborative filtering predicted ratings useritem pairs.	 11 3 Evaluation of Recommender Systems In evaluation setups for collaborative filtering we have a set P of pairs of predicted ratings r̂ and actual ratings r across all useritem pairs.
pushes items user thinks like item In explicit ratings vs	 In information filtering tasks a system pushes items to a user if it thinks the user would like the item  In this case there are no explicit ratings for each item but rather a relevant vs notrelevant judgement.
item vs notrelevant judgement user determines suggestion For it’s evaluation metrics retrieval discussed	 In this case there are no explicit ratings for each item but rather a relevant vs notrelevant judgement  Once the user sees the item the user then determines if the suggestion was a good one  For a single user it’s easy to see that we can use some evaluation metrics from information retrieval as discussed in Chapter 9.
In simple way aggregate scores average user e g.	 In a true information filtering system there will be many users who receive all pushed items  A simple way to aggregate scores would be to take an average of the individual user metrics e g.
number increases overall precision precision flat likely θopt given current learningovertime evaluation generalized multiple users way	 Ideally as the number of documents increases the overall precision or precision of the last k documents should increase  Once the precision has reached a flat line we are most likely at θopt given the current system setup  Of course this learningovertime evaluation can also be generalized to multiple users in the same way as previously discussed.
flat setup Of multiple way types affecting	 Once the precision has reached a flat line we are most likely at θopt given the current system setup  Of course this learningovertime evaluation can also be generalized to multiple users in the same way as previously discussed  As a final note for both recommender system types it is valuable to find those users affecting the evaluation metric the most.
A memorybased collaborative filtering 1998 provides different filtering algorithms A multiple collaborative filtering algorithms Cacheda et al.	 A description of memorybased collaborative filtering algorithm can be found in Breese et al  1998 which also provides a comparison of different collaborative filtering algorithms  A more recent comparison of multiple collaborative filtering algorithms can be found in the Cacheda et al.
12Overview Analysis In grouped covered techniques text processing significantly data set processed truly relevant data	 12Overview of Text Data Analysis In the previous chapters that we grouped under Part II we have covered techniques for text data access which is logically an initial step for processing text data for the purpose of both significantly reducing the size of the data set to be further processed either by humans or machines and filtering out any obvious noise in the text data so as to focus on the truly relevant data to a particular application problem.
Multiple combined Specifically view humans data making kinds data enabling text data	 Multiple operators may be combined  Specifically we will view humans as “subjective sensors” of our world and text data as data generated by such subjective sensors making text data more similar to other kinds of data generated by objective machine sensors and enabling us to naturally discuss how to jointly analyze text and nontext data together.
term weighitng discussed tasks categorization Part III Part III useful improving retrieval algorthm Part	 the term weighitng methods discussed in Part II are also very useful for many tasks such as clustering categorization and summarization in Part III and clustering in Part III can be useful for improving retrieval algorthm covered in Part II.
Thus increasingly develop analysis help data	 Thus it becomes increasingly essential to develop ad vanced text analysis tools to help us digest and make use of text data effectively and efficiently.
One important domain business intelligence For example feedback received products competitors.	 One important application domain of text analysis is business intelligence  For example product managers may be interested in hearing customer feedback about their products knowing how well their products are being received as compared to the products of competitors.
interested hearing feedback compared competitors This data form reviews Web master techniques tap information knowledge people products help product managers gain	 For example product managers may be interested in hearing customer feedback about their products knowing how well their products are being received as compared to the products of competitors  This can be a good opportunity for leveraging text data in the form of product reviews on the Web  If we can develop and master text mining techniques to tap into such an information source to extract the knowledge and opinions of people about these products then we can help these product managers gain business intelligence or gain feedback from their customers.
Yet broad category applications leverage mize making imagine sensor “listen” text especially social events time interesting relevant	 Yet another broad category of applications is to leverage social media to opti mize decision making  In general we can imagine building an intelligent sensor system to “listen” to all the text data produced in real time especially social me dia data such as tweets which report realworld events almost in real time and monitor interesting patterns relevant to an application.
response benefit early warning natural disaster tweets real time.	 Disaster response and management would benefit early discovery of any warning signs of a natural disaster which is possible through analyzing tweets in real time.
kind presents knowledge text knowledge easy	 As a special kind of data text data presents unique opportunities to help us “see” virtually all kinds of knowledge we encode in text especially knowledge about people’s opinions and thoughts which may not be easy to see in other kinds of data.
Nontext Humans purpose useful view humans subjective sensors We physical Any sensor monitors signal	 Nontext Data Humans as Subjective Sensors For the purpose of data mining it is useful to view text data as data generated by humans as subjective sensors  We can compare humans as subjective sensors to physical sensors such as a network sensor or a thermometer  Any sensor monitors the real world in some way it senses some signal from the real world and then reports the signal as various forms of data.
example thermometer temperature world report temperature data format like Fahrenheit	 For example a thermometer would sense the temperature of the real world and then report the temperature as data in a format like Fahrenheit or Celsius.
idea 12 Looking way types data world particular	 This idea is illustrated in Figure 12  Looking at text data in this way has an advantage of being able to integrate all types of data together which is instrumental in almost all data mining problems  In a data mining scenario we would be dealing with data about our world that are related to a particular problem.
Text contain opinions text examine framework.	 Text data is also very important because they contain knowledge about users especially preferences and opinions  By treating text data as data observed from human sensors we can examine all this data together in the same framework.
general include nontext For joint mining nontext problem topics text	 So a more general picture would be to include nontext data as well  For this reason we might be concerned with joint mining of text and nontext data  With this problem definition we can now look at the landscape of the topics in text mining and analytics.
Specifically observer perspective people looking world different they’ll pay attention things.	 Specifically a human sensor or human observer would look at the world from some perspective  Different people would be looking at the world from different angles and they’ll pay attention to different things.
users humangenerated exactly know world actually like human expresses observed English text data.	 As the users of humangenerated data we will never exactly know what the real world actually looked like at the moment when the author made the observation  The human expresses what is observed using a natural language such as English the result is text data.
As 12 4 distinguish types	 As illustrated in Figure 12 4 we can distinguish four types of text mining tasks.
world.	 Mining knowledge about the observed world.
example discover particular entity 248 Chapter 12 Overview Data Analysis This content world author’s mind observers text	 For example we can discover everything that has been said about a particular person or a particular entity  248 Chapter 12 Overview of Text Data Analysis This can be regarded as mining content to describe the observed world in the author’s mind  Mining knowledge about the observers text producers.
observers text producers sub text expressed humans human	 Mining knowledge about the observers text producers  Since humans are sub jective sensors the text data expressed by humans often contain subjective statements and opinions that may be unique to the particular human observer text producers.
mining observed world mining text producer text data mixture statements world subjective reflecting extract separately Inferring properties real world On left figure illustrate text	 Note that we distinguish mining knowledge about the observed world from mining knowledge about the text producer because text data is generally a mixture of objective statements about the world observed and subjective statements or comments reflecting the text producer’s opinions and beliefs and it is possible and useful to extract each separately  Inferring knowledge about properties of the real world  On the left side of the figure we illustrate that text mining can also allow us to infer values of interesting real world variables by leveraging the correlation of the values of such variables and the content in text data.
knowledge properties real world On figure illustrate allow variables For correlation changes market events news data	 Inferring knowledge about properties of the real world  On the left side of the figure we illustrate that text mining can also allow us to infer values of interesting real world variables by leveraging the correlation of the values of such variables and the content in text data  For example there may be some correlation between the stock price changes on the stock market and the events reported in the news data e.
news data	 For example there may be some correlation between the stock price changes on the stock market and the events reported in the news data e.
g.	g.
leveraged forecasting text prediction variables related g.	 Such correlations can be leveraged to perform textbased forecasting where we use text data as a basis for prediction of other variables that may only be remotely related to text data e g.
For example discover	 For example if we can mine text data to discover topics we would be able to use topics i e.
words	e  a set of semantically related words rather than individual words as features.
products liked said text data e reviews case mining observer clearly	 Another example is to predict what products may be liked by a user based on what the user has said in text data e g  reviews in which case the results from mining knowledge about the observer would clearly be very useful for prediction.
predictive analysis For example predict prices prices stock best articles useful prediction accuracy contributing features computed based	 Futhermore nontext data can be very important in predictive analysis  For example if you want to predict stock prices or changes of stock prices the historical stock price data are presumably the best data to use for prediction even though online discussions news articles or social media may also be useful for further improvement of prediction accuracy by contributing additional effective features computed based on text data which would be combined with nontext features.
discovered discovered text temporal	 associating the knowledge discovered from text data with the nontext data e g  associating topics discovered from text with time would generate temporal trends of topics.
content expressed text text context associated example location document.	 When we look at the text data alone we’ll be mostly looking at the content or opinions expressed in the text  However text data generally also has context associated with it  For example the time and the location of the production of the text data are both useful “metadata” values of a text document.
We text data 19 fairly landscape topics text mining analytics book selectively cover different text	 We discuss joint analysis of text and nontext data in detail in Chapter 19  This is a fairly general landscape of the topics in text mining and analytics  In this book we will selectively cover some of those topics that are representative of the different kinds of text mining tasks.
remaining Part book text IR discussed earlier text knowledge language.	 In the remaining chapters of Part III of the book we will start to enumerate different text mining tasks that build upon the NLP and IR techniques discussed earlier  First we will discuss how to mine word associations from text data Chapter 13 revealing lexical knowledge about language.
This objects allowing exploratory categorization expands introduction Chapter We different	 This groups similar objects together allowing exploratory analysis among many other applications  Chapter 15 covers text categorization which expands on the introduction to machine learning given in Chapter 2  We also explore different methods of text summarization Chapter 16.
analyze text array applica tions introduce mining observer ered Chapter	 This is only one way to analyze content of text but it’s very useful and used in a wide array of applica tions  Then we will introduce opinion mining and sentiment analysis  This can be regarded as one example of mining knowledge about the observer and will be cov ered in Chapter 18.
opinion analysis example knowledge ered Finally discuss prediction problems try realworld variable based text data cuttingedge joint analysis	 Then we will introduce opinion mining and sentiment analysis  This can be regarded as one example of mining knowledge about the observer and will be cov ered in Chapter 18  Finally we will briefly discuss textbased prediction problems where we try to predict some realworld variable based on text data and present a number of cuttingedge research results on how to perform joint analysis of text and nontext data Chapter 19.
13Word In we’re going associations words text knowledge natural text We’ll association explain	 13Word Association Mining In this chapter we’re going to talk about how to mine associations of words from text  This is an example of knowledge about the natural language that we can mine from text data  We’ll first talk about what word association is and then explain why discovering such relations is useful.
means syntactic class.	 That means the two words that have paradigmatic relation would be in the same semantic class or syntactic class.
cat sentence car sentence Therefore previous pairs syntagmatic relation paradigmatic fact capture basic arbitrary sequences.	 However we cannot replace cat with sit in a sentence or car with drive in the sentence and still have a valid sentence  Therefore the previous pairs of words have a syntagmatic relation and not a paradigmatic relation  These two relations are in fact so fundamental that they can be generalized to capture basic relations between units in arbitrary sequences.
general mining We paradigmatic relations similar sentence Syntagmatic cooccurring sequence.	 If you think about the general problem of sequence mining then we can think about any units being words  We think of paradigmatic relations as relations that are applied to units that tend to occur in a similar location in a sentence or a sequence of data elements in general  Syntagmatic relations capture cooccurring elements that tend to show up in the same sequence.
If know techniques learn If able know larger expression based learning	 If you know two words are synonyms for example that would help with many different tasks  Grammar learning can be also done by using such techniques if we can learn paradigmatic relations then we can form classes of words  If we learn syntagmatic relations then we would be able to know the rules for putting together a larger expression based on component expressions by learning the sentence structure.
learning form classes If learn relations know rules putting based component learning useful retrieval mining.	 Grammar learning can be also done by using such techniques if we can learn paradigmatic relations then we can form classes of words  If we learn syntagmatic relations then we would be able to know the rules for putting together a larger expression based on component expressions by learning the sentence structure  Word relations can be also very useful for many applications in text retrieval and mining.
In use word modify query making As 7	 In search and text retrieval we can use word associations to modify a query for feedback making search more effective  As we saw in Chapter 7 this is often called query expansion.
In look strongly feature word like positive	 In order to do that we can look at what words are most strongly associated with a feature word like battery in positive vs  negative reviews.
essentially similar 13.	 Here we essentially can take advantage of similar context  Figure 13.
example words Generally context.	 Figure 13 1 shows a simple example using the words dog and cat  Generally we see the two words occur in similar context.
	 We can have different perspectives to look at the context.
We look general	 We can even look at the general context this includes all the words in the sentence or in sentences around this word.
2 sam example Here interested knowing	2 shows the same sam ple of text as the example before  Here however we’re interested in knowing what other words are correlated with the verb eats.
perform	 This fact suggests that we can perform a joint discovery of the two relations.
look try	 Naturally our idea of discovering such a relation is to look at the context of each word and then try to compute the similarity of those contexts.
13 3 context.	 In Figure 13 3 we have taken the word cat out of its context.
We word dog general similarity context cat word formally context define	 We can do the same thing for another word like dog  In general we would like to capture such contexts and then try to assess the similarity of the context of cat and the context of a word like dog  The question is how to formally represent the context and define the similarity function between contexts.
pseudo document	 These words can be regarded as a pseudo document but there are also different ways of looking at the context.
case on.	 In this case we will see words like my his big a the and so on.
Similarly collect words cat right	 Similarly we can also collect the words that occur after the word cat which is called the right context R.
generally look words For example window target	 More generally we can look at all the words in the window of text around the target word  For example we can take a window of eight words around the target word.
window word These word left wordbased representation define	 For example we can take a window of eight words around the target word  These word contexts from the left or from the right form a bag of words repre sentation  Such a wordbased representation would actually give us a useful way to define the perspective of measuring context similarity.
words immediately capture related syntactic	 If we only used words immediately to the left and right we would likely capture words that are very much related by their syntactic categories.
general represent context cat frequency vector word similarity	 In general we can represent a pseudo document or context of cat as one frequency vector d1 and another word dog would give us a different context d2  We can then measure the similarity of these two vectors.
dot dot fact gives contexts try word context contexts similar words picked	 For similarity we simply use a dot product of two vectors  The dot product in fact gives us the probability that two randomly picked words from the two contexts are identical  That means if we try to pick a word from one context and try to pick another word from another context we can then ask the question are they identical If the two contexts are very similar then we should expect we frequently will see the two words picked from the two contexts are identical.
The dot fact gives picked contexts	 The dot product in fact gives us the probability that two randomly picked words from the two contexts are identical.
chance seeing identical small This contexts Let’s exact	 If they are very different then the chance of seeing identical words being picked from the two contexts would be small  This is quite intuitive for measuring similarity of contexts  Let’s look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical.
Each term overlap particular word xi pick d1 probability word expected words like work	 Each term in the sum gives us the probability that we will see an overlap on a particular word wi where xi gives us a probability that we will pick this particular word from d1 and yi gives us the probability of picking this word from d2  This is how expected overlap of words in context similarity works  As always we would like to assess whether this approach would work well.
sense formula higher score lap contexts However formula carefully potential problems distinct	 Initially it does make sense because this formula will give a higher score if there is more over lap between the two contexts  However if you analyze the formula more carefully then you also see there might be some potential problems  The first problem is that it might favor matching one frequent term very well over matching more distinct terms.
However analyze problems matching distinct terms.	 However if you analyze the formula more carefully then you also see there might be some potential problems  The first problem is that it might favor matching one frequent term very well over matching more distinct terms.
second problem equally If match know matching surprising	 The second problem is that it treats every word equally  If we match a word like the it will be the same as matching a word like eats although we know matching the isn’t really surprising because it occurs everywhere.
A frequency inverse weighting we’d like schemes 6.	 A sublinear transformation of term frequency and inverse document frequency IDF weighting are exactly what we’d like here we discussed these types of weighting schemes in Chapter 6.
able problems count d1 similarity For scheme document ments BM25 TF shown Figure	 It is able to solve the above two problems by sublinearly transforming the count of wi in d1 and including the IDF weighting heuristic in the similarity measure  For this similarity scheme we define the document vector as containing ele ments representing normalized BM25 TF values as shown in Figure 13.
IDF factor function IDF word xiyi	 For the IDF factor the similarity function multiplies the IDF of word wi by xiyi which is the similarity in the ith dimension.
highly weighted Of byproduct discovering relations.	 For this reason the highly weighted terms in this idea of a weighted vector can also be assumed to be candidates for syntagmatic relations  Of course this is only a byproduct of our approach for discovering paradigmatic relations.
course byproduct approach discovering paradigmatic discover syntagmatic relations	 Of course this is only a byproduct of our approach for discovering paradigmatic relations  In the next section we’ll talk more about how to discover syntagmatic relations in particular.
This discussion discover ing relations.	 This discussion clearly shows the relation between discover ing the two relations.
To main idea relations collect context form represented bag similarity corresponding pairs e.	 To summarize the main idea of computing paradigmatic relations is to collect the context of a candidate word to form a pseudo document which is typically represented as a bag of words  We then compute the similarity of the corresponding context documents of two candidate words highly similar word pairs have the highest paradigmatic relations i e.
Specifically talked retrieval models More specifically TF IDF relations.	 Specifically we talked about using text retrieval models to help us design an effective similarity function to compute the paradigmatic relations  More specifically we used BM25 TF and IDF weighting to discover paradigmatic relations.
weighting discover relations Finally relations byproduct discover paradigmatic relations.	 More specifically we used BM25 TF and IDF weighting to discover paradigmatic relations  Finally syntagmatic relations can also be discovered as a byproduct when we discover paradigmatic relations.
There strong words That context tend word.	3 Discovery of Syntagmatic Relations There are strong syntagmatic relations between words that have correlated co occurrences  That means when we see one word occur in some context we tend to see the other word.
specific Figure 13 7 ques tion eats occurs sentences occur eats cat	 Consider a more specific example shown in Figure 13 7  We can ask the ques tion whenever eats occurs what other words also tend to occur Looking at the sentences on the left we see some words that might occur together with eats like cat dog or fish.
Is w present absent 13 8 easier predict words—if look words shown unicorn easier If think easier everywhere.	 Is w present or absent in the segment from Figure 13 8 Some words are actually easier to predict than other words—if you take a look at the three words shown in the figure meat the and unicorn which one do you think is easier to predict If you think about it for a moment you might conclude that the is easier to predict because it tends to occur everywhere.
terms frequency predict segment.	 However meat is somewhere in between in terms of frequency making it harder to predict since it’s possible that it occurs in the segment.
To concept let’s look We word occurs segment.	 To explain this concept let’s first look at the scenario we had before when we know nothing about the segment  We have probabilities indicating whether a word occurs or doesn’t occur in the segment.
	9.
Suppose eats means value Xeats.	 Suppose we know eats is present which means we know the value of Xeats.
probabilities probabilities We absence meat occurred That	 That fact changes all these probabilities to conditional probabilities  We look at the pres ence or absence of meat given that we know eats occurred in the context  That is we have pXmeat  Xeats  1.
means occurs sentence meat occurs segment know That entropy	 This means we know where meat occurs in the sentence and we hope to predict whether the meat occurs in the sentence  This is zero because once we know whether the word occurs in the segment we’ll already know the answer of the prediction That also happens to be when this conditional entropy reaches the minimum  Let’s look at some other cases.
word That happens	 This is zero because once we know whether the word occurs in the segment we’ll already know the answer of the prediction That also happens to be when this conditional entropy reaches the minimum.
meat trying	 One is knowing the and trying to predict meat  Another is the case of knowing eats and trying to predict meat.
case trying predict We ques smaller HXmeat HXmeat We entropy means predict.	 Another is the case of knowing eats and trying to predict meat  We can ask the ques tion which is smaller HXmeat  Xthe or HXmeat  Xeats We know that smaller entropy means it is easier to predict.
In case related meat absence eats help meat oc reduces entropy	 In the case of eats since eats is related to meat knowing presence or absence of eats would help us predict whether meat oc curs  Thus it reduces the entropy of meat.
1 For word w1 enumerate w2 2.	 1  For each word w1 enumerate all other words w2 from the corpus  2.
2.	 2.
use number value cutoff conditional strongly particular syntagmatic	 Note that we need to use a threshold to extract the top words this can be the number of top candidates to take or a value cutoff for the conditional entropy  This would allow us to mine the most strongly correlated words with a particular word w1  But this algorithm does not help us mine the strongest k syntagmatic relations from the entire collection.
The eats occurs framed mutual information high need mutual words.	10  The question we ask is whenever eats occurs what other words also tend to occur This question can be framed as a mutual information question that is which words have high mutual information with eats So we need to compute the mutual information between eats and other words.
random variables Inside sum distri	 Continuing to inspect this formulation of mutual information we see that it is also summed over many combinations of different values of the two random variables  Inside the sum we are doing a comparison between the two joint distri butions.
Again numerator variables random	 Again the numerator has the actually observed joint distribution of the two random variables while the denominator can be interpreted as the expected joint distribution of the two random variables.
distribution probabilities comparison variables If numerator denominator variables independent	 If the two random variables are in dependent their joint distribution is equal to the product of the two probabilities so this comparison will tell us whether the two variables are indeed independent  If they are indeed independent then we would expect that the numerator and denominator are the same  If the numerator is different from the denominator that would mean the two variables are not independent and their difference can measure the strength of their association.
simply values In case values If look form mutual information measures expected distribution independence	 The sum is simply to take all of the combinations of the values of these two random variables into consideration  In our case each random variable can choose one of the two values zero or one so we have four combinations  If we look at this form of mutual information it shows that the mutual information measures the divergence of the actual joint distribution from the expected distribution under the independence assumption.
The new allow probabilities probabilities simplify	 The other three new constraints have a similar interpretation  These equations allow us to compute some probabilities based on other probabilities and this can simplify the computation.
occurred together.	 Assume that we also have available the probability that they occurred together.
segments words ones In cases column To estimate probabilities need collect counts w1 w2 columns	 In some segments you see both words occur which is indicated as ones for both columns  In some other cases only one will occur so only that column has a one and the other column has a zero  To estimate these probabilities we simply need to collect the three counts the count of w1 the total number of segments that contain w1 the segment count for w2 and the count when both words occur both columns have ones.
	5.
information random conditional entropy know Y entropy reduction knowing X.	 We’ve used three concepts from information theory entropy which measures the uncertainty of a random variable X conditional entropy which measures the entropy of X given we know Y  and mutual information of X and Y  which matches the entropy reduction of X due to knowing Y  or entropy reduction of Y due to knowing X.
And words compute words context way weighting word	 And if we do the same for all the words then we can cluster these words or compute the similarity between these words based on their context similarity  This provides yet another way to do term weighting for paradigmatic relation discovery  To summarize this chapter about word association mining we introduced two basic associations called paradigmatic and a syntagmatic relations.
These apply items units phrases introduced multiple statistical showing pure approaches viable	 These are fairly general since they apply to any items in any language that is the units don’t have to be words they can be phrases or entities  We introduced multiple statistical approaches for discovering them mainly showing that pure statistical approaches are viable for discovering both kinds of relations.
These approaches applied text human effort based word discover word relations use different ways define For words	 These approaches can be applied to any text with no human effort mostly because they are based on simple word counting yet they can actually discover interesting word relations  We can also use different ways to define context and segment and this would lead us to some interesting variations of applications  For example the context can be very narrow like a few words around a word a sentence or maybe paragraphs.
For narrow word paragraphs differing contexts flavors Of information data mining.	 For example the context can be very narrow like a few words around a word a sentence or maybe paragraphs  Using differing contexts would allow discovery of different flavors of paradigmatic relations  Of course these associations can support many other applications in both information retrieval and text data mining.
precision MAP metric Of numerical relevance scores metric	 Thus we could compute average precision for each word and use MAP as a summary metric over each word that we evaluate  Of course if such ranked lists contained numerical relevance scores we could instead use NDCG and average NDCG  A humanbased evaluation metric would be intrusion detection.
course ranked lists scores average NDCG.	 Of course if such ranked lists contained numerical relevance scores we could instead use NDCG and average NDCG.
detection Chang	 For example consider the following two examples of intrusion detection presented in Chang et al  2009.
The 5 items chosen word additional word	 The top k  5 items are chosen for some word in the vocabulary and an additional random word from the vocabulary is also added.
can’t intruder meaning word k candidates L2 good Performing type different words vocabulary strictly word associations.	 In L2 we can’t really tell which word is the intruder meaning the word association algorithm used to create the k candidates in L2 is not as good as the one used to generate L1  Performing this type of experiment over many different words in the vocabulary is a good yet expensive way to strictly evaluate the word associations.
manager 0 01 effort running maintaining query This actually general technology replace existing one.	 The application manager would have to decide whether an increase in MAP of 0 01 is worth the effort of implementing running and maintaining the query expansion program  This is actually quite a realistic and general issue whenever new technology is proposed to replace or extend an existing one.
effort running query	01 is worth the effort of implementing running and maintaining the query expansion program  This is actually quite a realistic and general issue whenever new technology is proposed to replace or extend an existing one.
actually realistic issue proposed extend As scientist	 This is actually quite a realistic and general issue whenever new technology is proposed to replace or extend an existing one  As a data scientist it is often part of the job to feature vectors for each term ID.
Then term return 14Text Clustering exploratory text	 Then given a query term return the most similar terms  14Text Clustering Clustering is a natural problem in exploratory text analysis.
sense clustering	 In its most basic sense clustering i e.
technique exploring sets When facing collection text reveal data form	 Clustering is a general data mining technique very useful for exploring large data sets  When facing a large collection of text data clustering can reveal natural se mantic structures in the data in the form of multiple clusters i e.
facing text natural data clusters	 When facing a large collection of text data clustering can reveal natural se mantic structures in the data in the form of multiple clusters i e.
example retrieval result way documents themes Term powerful concepts create thesaurus.	 For example clustering of retrieval results can be used as a result summary or as a way to remove redundant documents  Clustering the documents in our entire corpus lets us find common underlying themes and can give us a better understanding of the type of data it contains  Term clustering is a powerful way to find concepts or create a thesaurus.
objects potentially different ways depending definition similarity clustering middle clustering similarity defined based shape object results set similarity based	 The figure on the left shows a set of objects that can be potentially clustered in different ways depending on the definition of similarity or clustering bias  The figure in the middle shows the clustering results when similarity is defined based on the shape of an object  The figure on the right shows the clustering results of the same set of objects when similarity is defined based on size.
	1.
e.	e.
inject bias.	 inject a particular “clustering bias.
similarity object clustering figure However based size different shown right define important	” If we define similarity based on the shape of an object we will obtain a clustering result as shown in the picture in the middle of the figure  However if we define the similarity based on the size of an object then we would have very different results as shown in the figure on the right side  Thus when we define a clustering task it is important to state the desired perspective of measuring similarity which we refer to as a “clustering bias.
	g.
techniques different Next discuss clustering methods	 We first start with an overview of clustering techniques where we categorize the different approaches  Next we discuss similaritybased clustering via two common methods hierarchical and divisive methods.
document clustering groups	 As mentioned previously document clustering groups documents together into clusters.
tion Any objects potential similar depending	 Similaritybased clustering   These clustering algorithms need a similarity func tion to work  Any two objects have the potential to be similar depending on how they are viewed.
modelbased method data	 This is a “hard assignment” unlike the clusters we receive from a modelbased method  Modelbased techniques design a probabilistic model to capture the latent struc ture of data i e.
query It query possible number documents index.	 There will be much more discussion on this in the topic analysis chapter  Term clustering has applications in query expansion  It allows similar terms to be added to the query increasing the possible number of documents matched from the index.
query expansion.	 Term clustering has applications in query expansion.
17 output modelbased topic analysis ally gives similar words fact delivers document clusters	 As we will see in Chapter 17 output from a modelbased topic analysis addition ally gives us groups of similar words in fact these are the “topics”  Thus topic analysis delivers both term and document clusters to the user.
Such supervision clustering incorporate clustering flexible	 Such supervision is useful when we have some knowledge about the clustering problem that we would like to incorporate into a clustering algorithm and allows users to “steer” the clustering in a flexible way.
labeling task form summarization brief implementation	 This labeling task can be regarded as a form of text summarization which we will further discuss in Chapter 16  Finally a brief note on the implementation of clustering algorithms.
especially software text data access general different objects	 Leveraging the data structures already in place for supporting search is especially desirable in a unified software system for supporting both text data access and text analysis  The clustering techniques we discuss are general so they can be potentially used for clustering many different types of objects including e g.
Document In examine methods divisive clustering.	 14 2 Document Clustering In this section we examine similaritybased document clustering through two methods agglomerative clustering and divisive clustering.
Recall cosine vector sine similarity score 1 mentioned term weights raw counts TFIDF	 Recall that cosine similarity is defined as Since all term weights in our document vector representation are positive the co sine similarity score ranges from 0 1  As mentioned the term weights may be raw counts TFIDF or anything else the user could imagine.
Another similarity similarity This metric absence magnitude.	 Another common similarity metric is Jaccard similarity  This metric is a set similarity that is it only captures the presence and absence of terms with no regard to magnitude.
set presence Y Y 2 document vector	 This metric is a set similarity that is it only captures the presence and absence of terms with no regard to magnitude  It is defined as follows simJaccardX Y   X ∩ Y X ∪ Y   14 2 where X and Y represent the set of elements in the document vector x and y respectively.
X y plain shared	2 where X and Y represent the set of elements in the document vector x and y respectively  In plain English it captures the ratio of shared objects and total objects in both sets.
total objects sets For study sug gest Huang 2008.	 In plain English it captures the ratio of shared objects and total objects in both sets  For a more indepth study of similarity measures and their effectiveness we sug gest that the reader consult Huang 2008.
	 14 2.
2 1 Hierarchical ready general	2 1 Agglomerative Hierarchical Clustering We are now ready to discuss our first general clustering strategy.
Until defined similarity measures input.	 Until now we have only defined similarity measures that take two documents as input.
To treat ual clusters	 To simplify this problem we will treat individ ual documents as clusters thus we only need to compare clusters for similarity.
This clusters need perform merge.	 This results in “looser” clusters since we only need to find two individually close elements in each cluster in order to perform the merge.
centroids new	 Recompute centroids of the new clusters found from previous step Max.
Here represent cluster centroid represents cluster average values.	 Here we represent a cluster with a centroid a centroid is a special document that represents all other documents in its cluster usually as an average of all its members’ values.
Figure Steps Kmeans clustering small set data points clustered	 Figure 14 4 Steps in the Kmeans clustering algorithm for a small set of data points to be clustered shown in a.
Then data based shown lines c.	 Then all the data points are each assigned to one of the three clusters based on their distances to each centroid the decision boundaries are shown as lines in c.
algorithm new centroids data e seen close optimal centroids clusters analysis PLSA algorithm.	 The algorithm continues to iterate with the new centroids as the starting centroids to reassign all the data points  The new boundaries are shown in e which are easily seen to be already very close to the optimal centroids for generating three clusters from this data set  analysis through the PLSA algorithm.
realize particular manifestation assignment EM Figure	 For this chapter it is sufficient to realize that Kmeans is a particular manifestation of hard cluster assignment via EM  Figure 14.
multiple needed obtain good algorithm repeatedly gradually clusters hierarchy similar agglomerative algorithm.	 Thus multiple trials are generally needed in order to obtain a good local minimum  The Kmeans algorithm can be repeatedly applied to divide the data set gradually into smaller and smaller clusters thus creating a hierarchy of clusters similar to what we can achieve with the agglomerative hierarchical clustering algorithm.
Similarly allow set K Kmeans algorithm creating hierarchy 14 Term Clustering goal clustering related	 Similarly if we only allow a binary tree then we also do not have to set K in the Kmeans algorithm for creating a hierarchy  14 3 Term Clustering The goal of term clustering is quite similar to document clustering we wish to find related terms.
3 Term Clustering usually mean	3 Term Clustering The goal of term clustering is quite similar to document clustering we wish to find related terms  By “related” we usually mean words that have a similar semantic meaning.
techniques sequence parse tree fragments It important mind designed words	 The techniques we describe in this section will generally work for any sequence of features whether they are words or parse tree fragments  It is important to keep in mind however that the algorithms we discuss were designed for use on words in most cases.
14 3 After models assign scores pw C	 14 3 After we estimated the topic and background language models we used the following formula to assign scores to words in our vocabulary scorew  pw  computer pw  C .
14 related	 14 4 The score indicates how related a word is to our topic language model term computer.
usually denominator numerator resulting	 Words that usually do not occur in the context of computer will have a denominator less than the numerator resulting in a score less than one.
assume corpus document mentioned Using artichoke	 For example assume the word artichoke appears only once in the corpus and it happens to be in a document where computer is mentioned  Using the above formula will have artichoke and computer very highly related even though we know this is not true.
Clustering 287 With zero probability probability replace previous function version In example brings artichoke lower “pretend”	3 Term Clustering 287 With such a formula an unseen word would not have a zero probability and the estimated probability is in general more accurate  We can replace pw  C in the previous scoring function with our smoothed version  In the example this brings the score for artichoke much lower since we “pretend” to have seen a count of it in the background.
words cooccur wi n	 For example to find words that cooccur with wi using a window of size n we look at the words wi−n   .
rep resented joint probability	  win  This allows us to calculate the probability of wi and wj cooccurring which is rep resented as the joint probability pwi  wj.
This wi cooccurring wj.	 This allows us to calculate the probability of wi and wj cooccurring which is rep resented as the joint probability pwi  wj.
pwj formula pmixi log pwi wj	 Along with the individual probabilities pwi and pwj we can write the formula for PMI pmixi  xj  log pwi  wj pwipwj   14 7 Note that if wi and wj are independent then pwipwj  pwi  wj.
Below term The methods	 Below we will briefly introduce two advanced methods for term clustering  The windowing technique employed here is critical in both of the following advanced methods  14.
3.	3.
3 Ngram Models Brown modelbased algorithm term word ngram class model.	3 1 Ngram Class Language Models Brown clustering Brown et al  1992 is a modelbased term clustering algorithm that constructs term clusters called word classes to maximize the likelihood of an ngram class language model.
	 .
pwn wn−1 .	    w1 as pwn  wn−1 .
pwn cnpcn cn−1 .	    w1  pwn  cnpcn  cn−1 .
words words e class language model parameters regular language	 It essentially assumes that the probability of observing wn only depends on the classes of the previous words but does not depend on the specific words thus unless C is the same as vocabulary size i e  every word is in its own class the ngram class language model always has fewer parameters than the regular ngram language model.
e class language fewer language	e  every word is in its own class the ngram class language model always has fewer parameters than the regular ngram language model.
	 .
c1 finally word cn.	      c1 and finally sample a word at the nth position by using pw  cn.
c1 sample nth pw	    c1 and finally sample a word at the nth position by using pw  cn.
Tuesday Wednesday prediction original word minimize mutual information.	g  Tuesday and Wednesday since by putting such words in the same class the prediction power of the class would be about the same as that of the original word allowing to minimize the loss of mutual information.
Figure 6 shows sample Brown al.	 Figure 14 6 shows some sample word clusters taken from Brown et al.
adjacent model “sticky phrases” Figure 7 results phrases composition meanings words methods	 adjacent cooccurrences the model can discover “sticky phrases” see Figure 14 7 for sample results which are noncompositional phrases whose meaning is not a direct composition of the meanings of individual words  Such noncompositional phrases can also be discovered using some other statistical methods see e.
Such methods	 Such noncompositional phrases can also be discovered using some other statistical methods see e g.
Neural language model word In Chapter 13 represent words context term similarity vector representations view representation clustering use viewing term “document” vector.	2 Neural language model word embedding In Chapter 13 we discussed in length how to represent a term as a term vector based on the words in the context where the term occurs and compute term similarity based on the similarity of their vector representations  Such a contextual view of term representation can not only be used for discovering paradigmatic relations but also support term clustering in general since we can use any document cluster ing algorithm by viewing a term as a “document” represented by a vector.
This words representations elements means vector size semantically word improve statistical	 This technique is not limited to unigram words and we can think of other representations for the vector such as partofspeech tags or even elements like sentiment  Adding these additional features means expanding the word vector from V  to whatever size we require  Additionally aside from finding semantically related terms using this richer word representation has the ability to improve downstream tasks such as grammatical parsing or statistical machine translation.
As use model Mikolov et learn vector representation word objective	 As an alternative we can use a neural language model Mikolov et al  2010 to sys tematically learn a vector representation for each word by optimizing a meaningful objective function.
models neural For lan model pwn	 These language models are called neural language models because they can be represented as a neural network  For example to model an ngram lan guage model pwn  wn−1   .
For model ngram guage wn−1	 For example to model an ngram lan guage model pwn  wn−1 .
input output neural interpreted represen word elements edges	  w1 as input and wn as the output  In some neural language models the hidden layer in the neural network connected to a word can be interpreted as a vector represen tation of the word with the elements being the weights on the edges connected to the word.
In layer connected vector word example language et	 In some neural language models the hidden layer in the neural network connected to a word can be interpreted as a vector represen tation of the word with the elements being the weights on the edges connected to the word  For example in the skipgram neural language model Mikolov et al.
example language et al function use words context window word pw1 w2 v1 vi	 For example in the skipgram neural language model Mikolov et al  2013 the objective function is to use each word to predict all other words in its context as defined by a window around the word and the probability of predicting word w1 given word w2 is given by pw1  w2  exp v1  v2∑ wi∈V exp vi .
vectors related similar generate implementation skipgram called word2vec Mikolov al.	 In effect we would want the vectors representing two semantically related words which tend to cooccur together in a window to be more similar so as to generate a higher value when taking their dot product  Google’s implementation of skipgram called word2vec Mikolov et al.
	 Figure 14.
shows tool sim results obtained paradigmatic discovery	8 shows example output from using this tool  Although sim ilar results can also be obtained by using heuristic paradigmatic relation discovery e g.
far computation similarities methods In summary shown term similarity term clustering started information.	 As a result the utility of these word vectors has so far been mostly limited to computation of word similarities which can also obtained by using many other methods  In summary we have shown several methods to measure term similarity which can then be used for term clustering  We started with a unigram language modeling approach followed by pointwise mutual information.
approach We introduced modelbased based language models language models embedding methods improve documents objects allowing inexact terms	 We started with a unigram language modeling approach followed by pointwise mutual information  We then briefly introduced two modelbased approaches one based on ngram language models and one based on neural language models for word embedding  These term clustering methods can be leveraged to improve the computation of similarity between documents or other text objects by allowing inexact matching of terms e.
2 Although slightly challenging utility captured output measured quantitatively example clustering different clustering algorithm MAP NCDG 9.	2 Although slightly more challenging the concept of utility can also be captured if the final system output can be measured quantitatively  For example if clustering is used as a component in search we can see if using a different clustering algorithm improves F1 MAP or NCDG see Chapter 9.
wish groups similar way mainly words book different feature engineering nent implementing fact algorithm general.	 After all we wish to find groups of objects that are similar to one another in some way  We mainly discussed unigram words representations though in this book we have elaborated on many different feature types  Indeed feature engineering is an important compo nent of implementing a clustering algorithm and in fact any text mining algorithm in general.
chapter features important role classification chapter algorithms simi larity	 In the next chapter on text categorization Chapter 15 we also discuss how choosing the right features plays an important role in the over all classification accuracy  As we saw in this chapter similaritybased algorithms explicitly encode a simi larity function in their implementation.
The algorithm dominates class origi nal careful parameter set match	 The clustering algorithm would be effective if for each Ci one predefined category dominates and scarcely appears in other clusters  Effectively the clustering algorithm recreated the class assignments in the origi nal dataset without any supervision  Of course however we have to be careful if this is a parameter to set the final number of clusters to match the number of classes.
bias applications In modelbased approaches potential clustering” model Bibliographic Further Clustering technique mining usually Han	 Which method works the best highly depends on whether the bias definition of similarity reflects our perspective for clustering accurately and whether the as sumptions made by an approach hold for the problem and applications  In general modelbased approaches have more potential for doing “complex clustering” by encoding more constraints into the probabilistic model  Bibliographic Notes and Further Reading Clustering is a general technique in data mining and is usually covered in detail in any book on data mining Han 2005 Aggarwal 2015.
Further general data usually There chapter text 2012 text methods reviewed An comparison al.	 Bibliographic Notes and Further Reading Clustering is a general technique in data mining and is usually covered in detail in any book on data mining Han 2005 Aggarwal 2015  There is also a chapter on text clustering in Aggarwal and Zhai 2012 where many text clustering methods are reviewed  An empirical comparison of some document clustering techniques can be found in Steinbach et al.
An empirical comparison Steinbach et	 An empirical comparison of some document clustering techniques can be found in Steinbach et al.
text categorization follows.	 In general the text categorization problem is as follows.
set predefined categories possibly training set text text unseen	 Given a set of predefined categories possibly forming a hierarchy and often also a training set of labeled text objects i e  text objects with known labels of categories the task of text categorization is to label unseen text objects with one or more categories.
Indeed application text categorization textbased prediction different goals distinguished based For enriching text resentation tend characterize	 Indeed in such an application text categorization should really be called textbased prediction  These two somewhat different goals can also be distinguished based on the difference in the categories in each case  For the purpose of enriching text rep resentation the categories tend to be “internal” categories that characterize a text object e.
author meaningful text potentially links Computationally text categories.	 author at tribution or any other meaningful categories associated with text data potentially through indirect links  Computationally however these variations are all similar in that the input is a text object and the output is one or multiple categories.
similar output	 Computationally however these variations are all similar in that the input is a text object and the output is one or multiple categories.
Text Methods When data available e text categories explic solve categorization.	2 Overview of Text Categorization Methods When there is no training data available i e  text data with known categories explic itly labelled we often have to manually create heuristic rules to solve the problem of categorization.
labelled manually create rules problem categorization.	e  text data with known categories explic itly labelled we often have to manually create heuristic rules to solve the problem of categorization.
fall instancebased classifiers model explicitly instances lack training performed	 These automatic categorization methods generally fall into three categories  Lazy learners or instancebased classifiers do not model the class labels explicitly but compare the new instances with instances seen before usually with a similarity measure  These models are called “lazy” due to their lack of explicit generalization or training step most calculation is performed at testing time.
Generative distribution e g.	 Generative classifiers model the data distribution in each category e g.
unigram language category.	g  unigram language model for each category.
object likelihood Discriminative compute text object provide clue control Parameters categorization errors	 They classify an object based on the likelihood that the object would be observed according to each distribution  Discriminative classifiers compute features of a text object that can provide a clue about which category the object should be in and combine them with parameters to control their weights  Parameters are optimized by minimizing categorization errors on training data.
This nation definition Next text classification.	 This chapter starts out with an expla nation of the categorization problem definition  Next we examine what types of features text representation are often used for classification.
intuitive categorizing documents example Chapter mathematical form term vector features i.	3 Text Categorization Problem Let’s take our intuitive understanding of categorizing documents and rewrite the example from Chapter 2 into a more mathematical form  Let our collection of doc uments be X perhaps they are stored in a forward index see Chapter 8  Therefore one xi ∈ X is a term vector of features that represent document i.
xi term features document trieval setup assigned	 Therefore one xi ∈ X is a term vector of features that represent document i  As with our re trieval setup each xi has xi  V  one dimension for each feature as assigned by the tokenizer.
Recall document consisted long book mr.	 1  Recall that if a document xj consisted of the text long long book it would be mr.
0 we’d xi 1 xj 0 2 0 ith We labels	 0  In our forward index we’d store xi  1 1 1 1 1 2 1 1 xj  0 0 0 1 0 0 2 0 so xik is the kth term in the ith document  We also have Y which is a vector of labels for each document.
ŷ true sports classifier prediction Notice algorithm true In use true	 In this case ŷ  sports and the true y is also sports the classifier was correct in its prediction  Notice how we can only evaluate a classification algorithm if we know the true labels of the data  In fact we will have to use the true labels in order to learn a good function f .
evaluate know true	 Notice how we can only evaluate a classification algorithm if we know the true labels of the data.
seeing correct labels predicted f actually Consider simple example positive	 seeing how many correct labels were predicted  But what does the function f   actually do Consider this very simple example that determines whether a news article has positive or negative sentiment i.
type classifier support distinguishing classes Multiclass classification labels.	 Depending on the type of classifier it may only support distinguishing between two different classes  Multiclass classification can support an arbitrary number of labels.
4 Text Categorization Chapter 6 importance document representation retrieval	4 Features for Text Categorization In Chapter 6 we emphasized the importance of the document representation in retrieval performance.
That distinguish useful Determining subset point.	 That is each feature can be used to distinguish between positive or negative sentiment  Usually most features are not useful and the bulk of the decision is based on a smaller subset of features  Determining this smaller subset is the definition of feature selection but we do not discuss this in depth at this point.
I thought movie	 I thought the movie was bad  1  This we assume.
	 1.
exercise tokenizer length feature Can sentiment classification Again experiment tokenizer.	 As an exercise create a document tokenizer for META that uses sentence length as a feature  Can you get a decent sentiment classification accuracy 2  Again try this experiment in META using the same sentencelength tokenizer.
captures view produc tion structure right square structural feature syntactic category authors lexical features	 This captures a more abstract view of the produc tion rules focused more on structure  Lastly the right square is a fully abstracted structural feature set with no syntactic category labels at all  The authors found that these structural features combined with lowlevel lexical features i.
combined lowlevel classification	 The authors found that these structural features combined with lowlevel lexical features i e  unigram words improved the classification accuracy over using only one feature type.
example feature representation effectiveness “edit effective classifier model	 This is just one example of a feature representation that goes beyond bagofwords  The effectiveness of these “edit features” determines how effective the classifier can be in learning a model to separate different classes.
features writers Again it’s important type course affected.	 In this example the features can be used to distinguish between different native languages of essay writers  Again it’s important to em phasize that almost all machine learning algorithms are not affected by the type of features employed in terms of operation of course the accuracy may be affected.
processing time.	 With these more advanced techniques comes a requirement for more processing time.
The syntactic usually practitioner’s tolerance memory	 The level of sophistication in the syntactic or semantic features usually depends on the practitioner’s tolerance for processing time and memory usage.
We different able META continue use sentiment negative label assume cor pus training partition testing	 We examine three different algorithms all of which are avail able in META  We will continue to use the sentiment analysis example classifying new text into either the positive or negative label  Let’s also assume we split our cor pus into a training partition and testing partition.
	 15.
pens outliers close classified incorrectly	 If it so hap pens that there are a few outliers from a different class close to our query it will be classified incorrectly  There are several variations on the basic kNN framework.
basic framework For neighbors distance knearest neighbors	 There are several variations on the basic kNN framework  For one we can weight the votes of the neighbors based on distance to the query in weighted knearest neighbors  That is a closer neighbor to the query would have more influence or a higherweighted vote.
n classes simply class thought ideal document	 Here if we have n classes we simply see which of the n is closest to the query  The centroid of each class label may be thought of as a prototype or ideal representation of a document from that class.
2 Naive Bayes classifier tion label distribution labels	2 Naive Bayes Naive Bayes is an example of a generative classifier  It creates a probability distribu tion of features over each class label in addition to a distribution of the class labels themselves.
Essentially distribution pxi y feature Given unseen document calculate py	 Essentially we will have a feature distribution pxi  y for each class label y where xi is a feature  Given an unseen document we will calculate the most likely class distribution that it is generated from  That is we wish to calculate py  x for each label y ∈ Y.
It Bayes called This estimate distributions py class exact way language estimation.	 It is for this reason that Naive Bayes is called naive  This means we need to estimate the following distributions py for all classes and pxi  y for each feature in each class  This estimation is done in the exact same way as our unigram language model estimation.
inverted	 An inverted index is not necessary for this usage.
5.	 15 5.
3 Linear classifiers dot product vector V .	3 Linear Classifiers Linear classifiers are inherently binary classifiers  It takes the dot product between the unseen document vector and the weights vector w where w  x  V .
learning decision combination feature weights x Figure	 We call this group of learning algorithms linear classifiers because their decision is based on a linear combination of feature weights w and x  Figure 15.
boundaries shown line barely margin classes SVM previous attempts maximize decision boundary classes new examples decision boundary	 Two possible decision boundaries are shown the almost vertical line barely separates the two classes while the other line has a wide margin between the two classes  The SVM algorithm mentioned in the previous paragraph attempts to maximize the margin between the decision boundary and the two classes thus leaving more “room” for new examples to be classified correctly as they will fall on the side of the decision boundary close to the examples of the same class.
Of it’s data points separable boundary	 Of course it’s possible that not all data points are linearly separable so the de cision boundary will be created such that it splits the two classes as accurately as possible.
There rate discuss book familiarize reader general spirit algorithm reader Bishop details theoretical	 There are many discussions about the choice of learning rate convergence criteria and more but we do not discuss these in this book  Instead we hope to familiarize the reader with the general spirit of the algorithm and again refer the reader to Bishop 2006 for many more details on algorithm implementation and theoretical properties.
In −1 active feature w opposite ŷ weight	 In the case where yi  ŷ the object should have been clas sified as −1 so weight is subtracted from each active feature index in w  In the opposite case yi  ŷ weight is added to each active feature in w.
opposite added	 In the opposite case yi  ŷ weight is added to each active feature in w.
“active fea ture” mean example 0 contribute update w.	 By “active fea ture” we mean features that are present in the current example x only features xij  0 will contribute to the update in w.
With ambiguities multiple 1	 With this scheme there may be ambiguities if multiple classifiers predict 1 at test time.
A score 0 1 “more 1.	 A confidence score such as 0 588 or 1 045 represents the 1 label but the latter is “more confident” than the former so the class that the algorithm predicting 1.
As information retrieval use precision recall F1 true false positives true	6 Evaluation of Text Categorization As with information retrieval evaluation we can use precision recall and F1 score by considering true positives false positives true negatives and false negatives.
	g.
determined documents.	 The accuracy is determined using the development documents.
In n rounds partition n − 1 n The folds hint algorithm.	 In n rounds one partition is selected as the testing set and the remaining n − 1 are used for training  The final accuracy F1 score or any other evaluation metric is then averaged over the n folds  The variance in scores between the folds can be a hint at the overfitting potential of your algorithm.
overfit.	e  it overfit.
baseline “beat”	 Another important concept is baseline accuracy  This represents the minimum score to “beat” when using your classifier.
random correct 1 3 time Your classifier accuracy useful In classes uneven distribution class documents classes In baseline 66 majority result 2 time.	 In this case random guessing would give you about 33 accuracy since you’d be correct approximately 1 3 of the time  Your classifier would have to do better than 33 accuracy in order to make it useful In another example consider the 3000 documents and three classes but with an uneven class distribution one class has 2000 documents and the other two classes have 500 each  In this case the baseline accuracy is 66 since picking the majority class label will result in correct predictions 2 3 of the time.
Based matrix English 80.	 Based on the matrix we see that predicting Chinese was 80 2 accurate with native English and Japanese as 80.
native 80 7	2 accurate with native English and Japanese as 80 7 and 99.
“Japanese” The table dataset.	 Based on the matrix the classifier seems to default to the label “Japanese”  The table doesn’t tell us why this is but we can make some hypotheses based on our dataset.
important access points having text good However data reduce	 It is obviously very important for text data access where it can help users see the main content or points in the text data without having to read all the text  Summarization of search engine results is a good example of such an application  However summarization can also be useful for text data analysis as it can help reduce the amount of text to be processed thus improving the efficiency of any analysis algorithm.
Summarization engine results good summarization useful text data analysis reduce text processed analysis However summarization nontrivial	 Summarization of search engine results is a good example of such an application  However summarization can also be useful for text data analysis as it can help reduce the amount of text to be processed thus improving the efficiency of any analysis algorithm  However summarization is a nontrivial task.
nontrivial convey points Although summary In summarization like information	 However summarization is a nontrivial task  Given a large document how can we convey the important points in only a few sentences And what do we mean by “document” and “important” Although it is easy for a human to recognize a good summary it is not as straightforward to define the process  In short for any text summarization application we’d like a semantic compression of text that is we would like to convey essentially the same amount of information in less space.
readable general need purpose summarization hard one.	 The output should be fluent readable language  In general we need a purpose for summarization although it is often hard to define one.
shoppers online positive negative reviews approach	 The review summaries also let the shoppers make comparisons between different products when searching online  Reviews can be further broken down into summaries of positive reviews and summaries of negative reviews  An even more granular approach described in Wang et al.
output works human imagine able reviews user chapter overview main paradigms different	 Although the output in these two works is not a human readable summary we could imagine a system that is able to summarize all the hotel reviews in English or any other language for the user  In this chapter we overview two main paradigms of summarization techniques and investigate their different applications.
The methods evaluation focused possible use	 The two methods each have evaluation metrics that are particularly focused towards their respective implementation but it is possible to use e g.
Finally summarization	 Finally we look into some applications of text summarization and see how they are implemented in realworld systems.
1.	 1.
For passage “compress” sentences retains sentences summary document Step	 For each passage “compress” its sentences into a smaller number of relevant yet not redundant sentences  This strategy retains coherency since the sentences in the summary are mostly in the same order as they were in the original document  Step one is portrayed in Figure 16.
Figure 16 The sentences normalized similarity measure Chapter 14	 Step one is portrayed in Figure 16 1  The sentences in the document are traversed in order and a normalized symmetric similarity measure see Chapter 14 is applied on adjacent pairs of sentences.
change similarity sentences inspect changes low e.	 The plot on the righthand side of the figure shows the change in similarity between the sentences  We can inspect these changes to segment the document into passages when the similarity is low i e.
alternative simply operated information strategy discourse analysis	 An alternative approach to this segmentation is to simply use paragraphs if the document being operated on contains that information although most of the time this is not the case  This rudimentary partitioning strategy is a task in discourse analysis a subfield of NLP.
passages remove diversity resulting step MMR reranking applied problem.	 Now that we have our passages how can we remove redundancy and increase diversity in the resulting summarization during step two The technique maximal marginal relevance MMR reranking can be applied to our problem.
output select passage MMR	 Figure 16 2 shows the output of the algorithm when we only select one sentence from each passage  The MMR algorithm is as follows.
The MMR algorithm original p	 The MMR algorithm is as follows  Assume we are given an original list R and a profile p to construct the set of selected sentences S where S  R.
p construct set R.	 Assume we are given an original list R and a profile p to construct the set of selected sentences S where S  R.
text entire formulated user According relevance sentence si S defined	 Since our task deals with sentence retrieval p can be a user profile text about the user the entire document itself or it could even be a query formulated by the user  According to marginal relevance the next sentence si to be added into the selected list S is defined as 16.
	e.
metrics measures cosine fact measure 1998.	 Again the two similarity metrics may be any normalized symmetric measures  The simplest instantiation for the sim ilarity metric would be cosine similarity and this is in fact the measure used in Carbonell and Goldstein 1998.
simplest cosine fact measure	 The simplest instantiation for the sim ilarity metric would be cosine similarity and this is in fact the measure used in Carbonell and Goldstein 1998.
param calculation greater similarity.	   is the number of sentences between the two param eters  Note the “one minus” in front of the distance calculation since a smaller distance implies a greater similarity.
Note distance calculation smaller distance implies greater	 Note the “one minus” in front of the distance calculation since a smaller distance implies a greater similarity.
Text Summarization An abstractive creates sentences exist original documents vector language original	 16 3 Abstractive Text Summarization An abstractive summary creates sentences that did not exist in the original docu ment or documents  Instead of a document vector we will use a language model to represent the original text.
vector language way	 Unlike the document vector our language model gives us a principled way in which to generate text.
In model create text probability distribution.	 In our language model we would have a parameter repre senting the probability of each word occurring  To create our own text we will draw words from this probability distribution.
θ summary wi occur original approximate	 from θ that will comprise our summary  We want the word wi to occur in our summary with about the same probability it occurred in the original document—this is how our generated text will approximate the longer document  Figure 16.
First create sum probabilities allow use random number 0 word random point number zero iterate words summing probabilities random	 First we create a list of all our parameters and incrementally sum their probabilities this will allow us to use a random number on 0 1 to choose a word wi  Simply we get a uniform random floating point number between zero and one  Then we iterate through the words in our vocabulary summing their probabilities until we get to the random number.
floating point number zero	 Simply we get a uniform random floating point number between zero and one.
Then iterate words summing number We output	 Then we iterate through the words in our vocabulary summing their probabilities until we get to the random number  We output the term and repeat the process.
	   pcat  pdog  pa  .
pcat pa	  pcat  pdog  pa    .
At thinking text sense—that true use word context.	 At this point you may be thinking that the text we generate will not make any sense—that is certainly true if we use a unigram language model since each word is generated independently without regard to its context.
sentence generation bigram language follows e g	 The sentence generation from a bigram language model proceeds as follows start with e g  The.
reproducing original document like ngram large reproduce major	 Because of this we would just be reproducing the original document which is not a very good summary  In practice we would like to choose an ngram value that is large enough to produce coherent text yet small enough to not simply reproduce the corpus  There is one major disadvantage to this abstractive summarization method.
Advanced abstractive natural ing document Named people businesses	1 Advanced Abstractive Methods Some advanced abstractive methods rely more heavily on natural language process ing to build a model of the document to summarize  Named entity recognition can be used to extract people places or businesses from the text.
Dependency relation entities 324 Chapter 16 Text	 Dependency parsers and other syntactic techniques can be used to find the relation between the entities 324 Chapter 16 Text Summarization and the actions they perform.
actual text represen collection sentences created based called realization gener abstractive model described templated document structure 2→conclusion spot.	 To generate the actual text some represen tations are chosen from the parsed collection and English sentences are created based on them this is called realization  Such realization systems have much more finegrained control over the gener ated text than the basic abstractive language model generator described above  A templated document structure may exist such as intro→paragraph 1→paragraph 2→conclusion and the structures are chosen to fill each spot.
In environment possible similar tences conjunctions depending To natural entity names	 In this environment it would be possible to merge similar sen tences with conjunctions such as and or but depending on the context  To make the summary sound even more natural pronouns can be used instead of entity names if the entity name has already been mentioned.
→ Gold today.	 → Gold and silver prices fell today.
Company today biggest mover.	 Company A lost 9 43 today  Company A was the biggest mover.
→ A	 → Company A lost 9.
today mover mover 9.	43 today  It was the biggest mover  Even better would be Company A was today’s biggest mover losing 9.
For consult 9.	 For a more detailed explanation of IR evaluation measures please consult Chapter 9.
Although summary sentences passage entire list composed multiple passages Therefore use recall score.	 Although our summary is generated by ranked sentences per passage the entire output is not a ranked list since the original document is composed of multiple passages  Therefore we can use precision recall and F1 score.
5 Applications Summarization 325 possible retrieval function position dependent precision NDCG need entire likely useful researchers methods.	5 Applications of Text Summarization 325 It is possible to rank the passage scoring retrieval function using position dependent metrics such as average precision or NDCG but with the final output this is not feasible  Thus we need to decide whether to evaluate the passage scoring or the entire output or both  Entire output scoring is likely more useful for ac tual users while passage scoring could be useful for researchers to finetune their methods.
“perfect” compared generated g	 This “perfect” summary would be compared with the generated one and some measure e g  ROUGE would be used to quantify the difference.
For measure text	 For the comparison measure we have many possibilities—any measure that can compare two groups of text would be potentially applicable.
For example similarity gold	 For example we can use the cosine similarity between the gold standard and generated summary.
Of course completed words An alternative learn ngram model loglikelihood	 Of course this has the downside that fluency is completed ignored using unigram words  An alternative means would be to learn an ngram language model over the gold standard summary and then calculate the loglikelihood of the generated summary.
The effectiveness questions original Was able important information needs original textbook summary sufficient information answer This metric extractive abstractive measures Using model score extractive vs.	 The overall effectiveness of a summary can be tested if users read a summary and then answer questions about the original text  Was the summary able to capture the important information that the evaluator needs If the original text was an entire textbook chapter could the user read a threeparagraph summary and obtain sufficient information to answer the provided exercises This is the only metric that can be used for both extractive and abstractive measures  Using a language model to score an extractive summary vs.
Text Summarization beginning we’ve touched applications mentioned news results Summarization saves users simultaneously enhancing preexisting ” aspect speaking topic.	5 Applications of Text Summarization At the beginning of the chapter we’ve already touched on a few summarization applications we mentioned news articles retrieval results and opinion summa rization  Summarization saves users time from manually reading the entire corpus while simultaneously enhancing preexisting data with summary “annotations ” The aspect opinion analysis mentioned earlier segments portions of user reviews into speaking about a particular topic.
saves time manually reading corpus summary ” opinion analysis portions topic.	 Summarization saves users time from manually reading the entire corpus while simultaneously enhancing preexisting data with summary “annotations ” The aspect opinion analysis mentioned earlier segments portions of user reviews into speaking about a particular topic.
The wellknown Summarizing lets investigators quickly dig try approach.	 The Enron email dataset1 is a wellknown corpus in this field  Summarizing email correspondence between two people or a department lets investigators quickly decide whether they’d like to dig deeper in a particular area or try another approach.
linking receivers structured content email Perhaps reading book ability summarize field.	 Finally linking email correspondence together from sender to receivers is a structured complement to the unstructured text content of the email itself  Perhaps of more interest to those reading this book is the ability to summarize research from a given field.
Wang	 2010 Wang et al.
al summarizer Ganesan al.	 A typical extractive summarizer is presented in Radev et al  2004 a typical abstractive summarizer is presented in Ganesan et al.
abstractive summarizer et al.	 2004 a typical abstractive summarizer is presented in Ganesan et al.
For NLG language good Reiter Dale This chapter analysis covering unsupervised text mining dis	 For advanced NLG natural language generation techniques a good starting point is Reiter and Dale 2000  17Topic Analysis This chapter is about topic mining and analysis covering a family of unsupervised text mining techniques called probabilistic topic models that can be used to dis cover latent topics in text data.
For talk sentence topic topic research topics different	 For example we can talk about the topic of a sentence the topic of an article the topic of a paragraph or the topic of all the research articles in a library  Different granularities of topics have different applications.
For talking international interested current research topics mining different To questions topics data literature topics past comparison.	 For example we might be interested in knowing about what Twitter users are talking about today  Are they talking about NBA sports international events or another topic We may also be interested in knowing about research topics one might be interested in knowing the current research topics in data mining and how they are different from those five years ago  To answer such questions we need to discover topics in the data mining literature including specifically topics in today’s literature and those in the past so that we can make a comparison.
g context analyzing topics.	g  about a product or a person  Besides text data we often also have some nontext data which can be used as additional context for analyzing the topics.
Besides text analyzing topics.	 about a product or a person  Besides text data we often also have some nontext data which can be used as additional context for analyzing the topics.
data know time associated data data produced text.	 Besides text data we often also have some nontext data which can be used as additional context for analyzing the topics  We might know the time associated with the text data or locations where the text data were produced or the authors of the text or the sources of the text.
2 hand covered Topic 2 cover Topic	 Doc 2 on the other hand covered Topic 2 very well but it did not cover Topic 1 at all.
	 .
17 1 Topics Terms simplest natural define term term word phrase.	 17 1 Topics as Terms The simplest natural way to define a topic is just as a term  A term can be a word or a phrase.
Terms simplest way define term A	1 Topics as Terms The simplest natural way to define a topic is just as a term  A term can be a word or a phrase.
coverage topics look what’s	4  If we define a topic in this way we can then analyze the coverage of such topics in each document based on the occurrences of these topical terms  A possible scenario may look like what’s shown in Figure 17.
define way document	 If we define a topic in this way we can then analyze the coverage of such topics in each document based on the occurrences of these topical terms.
	 Recall that we have two tasks  One is to discover the topics and the other is to analyze coverage.
way text data terms words case term.	 One natural way is to first parse the text data in the collection to obtain candidate terms  Here candidate terms can be words or phrases  The simplest case is to just take each word as a term.
Here terms words case word words topics.	 Here candidate terms can be words or phrases  The simplest case is to just take each word as a term  These words then become candidate topics.
For example favor title topic we’re	 For example in news we might favor title words because the authors tend to use the title to describe the topic of an article  If we’re dealing with tweets we could also favor hashtags which are invented to denote topics.
semantically similar synonyms collection meaning	 That is they are semantically similar or closely related or even synonyms  This is not desirable since we also want to have a good coverage over all the content in the collection meaning that we would like to remove redundancy.
term considering candidate terms picked balance terms The Chapter	 So while we are considering the ranking of a term in the list we are also considering the redundancy of the candidate term with respect to the terms that we already picked  With appropriate thresholding we can then get a balance of redundancy removal and picking terms with high scores  The MMR technique is described in more detail in Chapter 16.
instructive specific So examine sample 6.	 However it is often also instructive to analyze some specific examples  So now let’s examine the simple approach we have been discussing with a sample document in Figure 17 6.
sample Figure	 So now let’s examine the simple approach we have been discussing with a sample document in Figure 17.
	g.
For example topic sports words.	 For example a very specialized topic in sports would be harder to describe by using just a word or one phrase  We need to use more words.
A analyzing simple approach defining single lacks expressive power simple words	 We need to use more words  A key takeaway point from analyzing this simple example is that there are three general problems with our simple approach of defining a topic as a single term first it lacks expressive power  It can only represent the simple general topics but cannot represent the complicated topics that might require more words to describe.
topic topics involve related words introduce weights words.	 When we have more words that we can use to describe the topic we would be able to describe complicated topics  To address the second problem of how to involve related words we need to introduce weights on words.
address involve related need weights words This subtle differences topics semantically words quantitative	 To address the second problem of how to involve related words we need to introduce weights on words  This is what allows us to distinguish subtle differences in topics and to introduce semantically related words in a quantitative manner.
subtle differences introduce semantically related	 This is what allows us to distinguish subtle differences in topics and to introduce semantically related words in a quantitative manner.
turns probability unigram model denote shown Figure	 It turns out that all these can be elegantly achieved by using a probability distri bution over words i e  a unigram language model to denote a topic as shown in Figure 17.
e.	e.
It concentrated word	 It is also interesting to note that as a very special case if the probability of the mass is concentrated entirely on just one word e.
	g.
representation natural generalization representing words involve differences	 In this sense the word distribution representation is a natural generalization and extension of the singleterm representation  However representing a topic by a distribution over words can involve many words to describe a topic and model subtle differences of topics.
In “science” scientist related	 In “science” we see scientist spaceship and genomics which are all intuitively related to the corresponding topic.
probability 05 makes Similarly occurred ambiguous	 It has the highest probability for the “travel” topic 0 05 but with much smaller probabilities for “sports” and “science” which makes sense  Similarly you can see star also occurred in “sports” and “science” with reasonably high probabilities be cause the word is actually related to both topics due to its ambiguous nature.
representation use models word	 Since the representation is a probability distribution it is natural to use probabilistic models for discovering such word distributions which is referred to as probabilistic topic modeling.
3 That problem input documents topics k topics text	3 by making each topic a word distribution  That is each θi is now a word distribution and we have As a computation problem our input is text data a collection of documents C and we assume that we know the number of topics k or hypothesize that there are k topics in the text data.
The set word The second πi1	 The first is a set of topics represented by a set of θi’s each of which is a word distribution  The second is a topic coverage distribution for each document di πi1 .
second coverage di	 The second is a topic coverage distribution for each document di πi1 .
	   .
	 .
πik generate There way solving model.	  πik  The question now is how to generate such output from our input  There are potentially many different ways to do this but here we introduce a general way of solving this problem called a generative model.
The generate output potentially introduce way generative idea statistical modeling text problems.	 The question now is how to generate such output from our input  There are potentially many different ways to do this but here we introduce a general way of solving this problem called a generative model  This is in fact a very general idea and a principled way of using statistical modeling to solve text mining problems.
This fact idea principled modeling solve text mining	 This is in fact a very general idea and a principled way of using statistical modeling to solve text mining problems.
generative i.	 The basic idea of this approach is to first design a generative model for our data i.
First parameters V k document k values total	 First we have V  parameters for the probabilities of words in each word distribution so we have in total V k word probability parameters  Second for each document we have k values of π  so we have in total Nk topic coverage probability parameters.
fit estimate parameters infer In like values maximum	 Once we set up the model we can fit its parameters to our data  That means we can estimate the parameters or infer the parameters based on the data  In other words we would like to adjust these parameter values until we give our data set maximum probability.
values Like values data points What interested values highest probability.	 In other words we would like to adjust these parameter values until we give our data set maximum probability  Like we just mentioned depending on the parameter values some data points will have higher probabilities than others  What we’re interested in is what parameter values will give our data the highest probability.
parameters precisely text data data mining analysis general idea text mining We design parameter data	 These parameters are precisely what we hoped to discover from the text data so we view them as the output of our data mining or topic analysis algorithm  This is the general idea of using a generative model for text mining  We design a model with some parameter values to describe the data as well as we can.
We After treat parameters text	 We design a model with some parameter values to describe the data as well as we can  After we have fit the data we learn parameter values  We treat the learned parameters as the discovered knowledge from text data.
learn values learned data	 After we have fit the data we learn parameter values  We treat the learned parameters as the discovered knowledge from text data  17.
specifically illustrated Figure single	 More specifically as illustrated in Figure 17 9 we are interested in analyzing each document and discovering a single topic covered in the document.
independently document In output assumed document output compute single vocabulary given	 Since each document can be mined independently without loss of generality we further assume that the collection has only one document  In the output we also no longer have coverage because we assumed that the document has a 100 coverage of the topic we would like to discover  Thus the output to compute is the word distribution represent ing this single topic or probabilities of all words in the vocabulary given by this distribution as illustrated in Figure 17.
341	 17 3 Mining One Topic from Text 341 17 3.
The Simplest kind model data	1 The Simplest Topic Model Unigram Language Model When we use a generative model to solve a problem we start with thinking about what kind of data we need to model and from what perspective  Our data would “look” differently if we use a different perspective.
The choice particular partly data knowledge like	 The choice of a particular model partly depends on our domain knowledge about the data and partly depends on what kind of knowledge we would like to discover.
After model likelihood function probability data assumed model.	 After we specify the model we can formally write down the likelihood function i e  the probability of the data given the assumed model.
	e.
obtaining parameters Estimate discussed previously.	 Such a way of obtaining parameters is called the Maximum Likelihood Estimate MLE as we’ve discussed previously.
	 .
logarithm data point.	 Inside the sum there’s a count of each unique data point i e  the count of each word in the observed data which is multiplied by the logarithm of the probability of the particular unique data point.
combines ob function term encodes constraint Lagrange multiplier denoted	 We will first construct a Lagrange function which combines our original ob jective function with another term that encodes our constraint with the Lagrange multiplier denoted by λ introducing an additional parameter.
necessary condition reach sufficient However sufficient.	 1  Zero derivatives are a necessary condition for the function to reach an optimum but not sufficient  However in this case we have only one local optimum thus the condition is also sufficient.
local optimum sufficient.	 However in this case we have only one local optimum thus the condition is also sufficient.
unigram model word On probability words	 In such a case the estimated unigram language model word distribution may look like the distribution shown in Figure 17 12  On the top you will see the high probability words tend to be those very common words often function words in English.
Giving probabilities consequence assumed model distribution words downweight es word distribution second background distribution model background topic word need words.	 Giving com mon words high probabilities is a direct consequence of the assumed generative model which uses one distribution to generate all the words in the text  How can we improve our generative model to downweight such common words in the es timated word distribution for our topic The answer is that we can introduce a second background word distribution into the generative model so that the com mon words can be generated from this background model and thus the topic word distribution would only need to generate the contentcarrying topical words.
3 Background In solve problem highest common estimated document useful end	 17 3 2 Adding a Background Language Model In order to solve the problem of assigning highest probabilities to common words in the estimated unigram language model based on one document it would be 346 Chapter 17 Topic Analysis useful to think about why we end up having this problem.
Background In order highest language 346 Chapter 17 Topic Analysis useful end having It hard problem	2 Adding a Background Language Model In order to solve the problem of assigning highest probabilities to common words in the estimated unigram language model based on one document it would be 346 Chapter 17 Topic Analysis useful to think about why we end up having this problem  It is not hard to see that the problem is due to two reasons.
specifically word use probability component models use specifically known background model.	 More specifically when we generate a word we first decide which of the two distributions to use  This is controlled by a new probability distribution over the choices of the component models to use two choices in our case including specifically the probability of θd using the unknown topic model and the probability of θB using the known background model.
In pθB set	 In the figure we see that both pθd and pθB are set to 0 5.
The process generating coin probability − pθd decide use If coin shows heads use θB.	 The process of generating a word from such a mixture model is as follows  First we flip a biased coin which would show up as heads with probability pθd and thus as tails with probability pθB  1 − pθd to decide which word distribution to use  If the coin shows up as heads we would use θd  otherwise θB.
13.	13.
We observing particular mixture ability observing sequence words Let’s mixture shown 17.	 We can thus examine the probability of observing any particular word from such a mixture model and compute the prob ability of observing a sequence of words  Let’s assume that we have a mixture model as shown in Figure 17.
Let’s specific words text What’s ways	 Let’s assume that we have a mixture model as shown in Figure 17 13 and consider two specific words the and text  What’s the probability of observing a word like the from the mixture model Note that there are two ways to generate the so the probability is intuitively a sum of the probability of observing the in each case.
probability text It generalize calculation w θdpθd.	14 where we also show how to compute the probability of text  It is not hard to generalize the calculation to compute the probability of observ ing any word w from such a mixture model which would be pw  pθBpw  θB  pw  θdpθd.
The different ways word distributions term captures word example θB gives probability word w	4 The sum is over the two different ways to generate the word corresponding to us ing each of the two distributions  Each term in the sum captures the probability of observing the word from one of the two distributions  For example pθBpw  θB gives the probability of observing word w from the background language model.
form likelihood likelihood	 Such a form of likelihood actually reflects some general characteristics of the likelihood function of any mixture model.
probability mixture sum corresponding different model	 First the probability of observing a data point from a mixture model is a sum over different ways of generating the word each corresponding to using a different component model in the mixture model.
As seen later use word component	 As will be seen later more sophisticated topic models tend to use more than two components and their probability of generating a word would be of the same form as we see here except that there are more than two products in the sum more precisely as many products as the number of component models.
That probability words above.	 That is it also gives us a probability distribution over words as defined above.
box model it’s However mixture model different unigram	 When viewing the whole box as one model we can easily see that it’s just like any other generative model that would give us the probability of each word  However how this probability is determined in such a mixture model is quite different from when we have just one unigram language model.
model language model.	 However how this probability is determined in such a mixture model is quite different from when we have just one unigram language model.
useful examine exercise help interpret model intuitively relations simpler models complicated case set probability component model	 It’s often useful to examine some special cases of a model as such an exercise can help interpret the model intuitively and reveal relations between simpler models and a more complicated model  In this case we can examine what would happen if we set the probability of choosing the background component model to zero.
Once likelihood method g.	 Once we write down the likelihood function the next question is how to esti mate the parameters  As in the case of the single unigram language model we can use any method e g.
knowledge parameters twocomponent ure 17.	 the maximum likelihood estimator to estimate the pa rameters which can then be regarded as the knowledge that we discover from the text  What parameters do we have in such a twocomponent mixture model In Fig ure 17.
include mixing language models.	 Second the parameters include two unigram language models and a distribution mixing weight over the two language models.
model uncertainty generate Because likelihood contains distribution.	 We have this sum due to the mixture model where we have an uncertainty in using which model to generate a data point  Because of this uncertainty our likelihood function also contains a parameter to denote the probability of choosing each particular component distribution.
3 3	3 3 Estimation of a mixture model In this section we will discuss how to estimate the parameters of a mixture model.
18 illustrates In scenario word distribution pw	18 illustrates such a scenario  In this scenario the only parameters unknown would be the topic word distribution pw  θd.
exactly estimate	 Thus we have exactly the same number of parameters to estimate as in the case of a single unigram language model.
general embed interested parts based knowledge	 Note that this is an example of customizing a general probabilistic model so that we can embed an unknown variable that we are interested in computing while simplifying other parts of the model based on certain assumptions that we can make about them  That is we assume that we have knowledge about other variables.
assume background model based English feasible desirable goal designing generative common topic distribution known background word knowledge words counted	 That is we assume that we have knowledge about other variables  Setting the background model to a fixed word distribution based on the maximum likelihood estimate of a unigram language model of a large sample of English text is not only feasible but also desirable since our goal of designing such a generative model is to factor out the common words from the topic word distribution to be estimated  Feeding the model with a known background word distribution is a powerful technique to inject our knowledge about what words are counted as noise stop words in this case.
pθB desired words larger set common words	 Similarly the parameter pθB can also be set based on our desired percentage of common words to factor out the larger pθB is set the more common words would be removed from the topic word distribution.
It’s easy able simple case distribution Note assumed unknown likelihood estimation longer obtain θB assigns high probabilities	 It’s easy to see that if pθB  0 then we would not be able to remove any common words as the model degenerates to the simple case of using just one distribution to explain all the words  Note that we could have assumed that both θB and θd are unknown and we can also estimate both by using the maximum likelihood estimation but in such a case we would no longer be able to guarantee that we will obtain a distribution θB that assigns high probabilities to common words.
Let’s assume probability exactly flip fair coin decide	 Let’s assume that the probability of choosing each of the two models is exactly the same  That is we will flip a fair coin to decide which model to use.
Obviously oversimplification actual text useful amine behavior special	 Obviously this is a naive oversimplification of the actual text but it’s useful to ex amine the behavior in such a special case.
assume	 We further assume that the background model gives probability of 0.
Fig 19 The probability twoword document probability word distributions.	 We can write down the likelihood function in such a case as shown in Fig ure 17 19  The probability of the twoword document is simply the product of the probability of each word which is itself a sum of the probability of generating the word with each of the two distributions.
19.	19.
sum respect	 Note that the two probabilities must sum to one so we have to respect this constraint.
If set probabilities 1 0 expression.	 If there were no constraint we would have been able to set both probabilities to their maximum value which would be 1 0 to maximize the likelihood expression.
However words sum	 However we can’t do this because we can’t give both words a probability of one or otherwise they would sum to 2.
start text e	 Imagine we start with an even allocation between the and text i e  each would have a probability of 0.
	 each would have a probability of 0 5.
.	5 .
θd 5	 pthe  θd  0 5   0.
product	0 so their product reaches maximum when 0 5   ptext  θd  0.
0 1 0	 0 1  0 5 .
9 0.	9 and pthe  θd  0.
text larger probability effectively common word.	 Therefore the probability of text is indeed much larger than the probability of the effectively factoring out this common word.
estimate higher model distribution given w1 probability word given w2 In distributions different words avoid high probability observed data “bet” high	 Thus the ML estimate tends to give a word a higher probability if the background model gives it a smaller probability or more generally if one distribution has given word w1 a higher probability than w2 then the other distribution would give word w2 a higher probability than word w1 so that the combined probability of w1 given by the two distributions working together would be the same as that of w2  In other words the two distributions tend to give high probabilities to different words as if they try to avoid giving the high probability to the same word  In such a twocomponent mixture model we see that the two distributions will be collaborating to maximize the probability of the observed data but they are also competing on the words in the sense that they would tend to “bet” high probabilities on different words to gain advantages in this competition.
In twocomponent mixture collaborating maximize data words tend “bet” different probability maximize function probability assigned higher word smaller given model	 In such a twocomponent mixture model we see that the two distributions will be collaborating to maximize the probability of the observed data but they are also competing on the words in the sense that they would tend to “bet” high probabilities on different words to gain advantages in this competition  In order to make their combined probability equal so as to maximize the product in the likelihood function the probability assigned by θd must be higher for a word that has a smaller probability given by the background model θB.
behavior 17 21 estimated frequencies.	 Let’s look at another behavior of the mixture model in Figure 17 21 by examining the response of the estimated probabilities to the data frequencies.
probability larger text formula moment new function function text probability decrease reduction text sense away text term add terms occurred increasing likelihood function.	 Should we make the probability of the larger or that of text larger If you look at the formula for a moment you might notice that the new likelihood function which is our objective function for optimization is influenced more by the than text so any reduction of probability of the would cause more decrease of the likelihood than the reduction of probability of text  Indeed it would make sense to take away some probability from text which only affects one term and add the extra probability to the which would benefit more terms in the likelihood function since the occurred many times thus generating an overall effect of increasing the value of the likelihood function.
probability text term add extra probability benefit terms likelihood increasing likelihood function In repeated times function increase probability little bit impact decrease impact occurred	 Indeed it would make sense to take away some probability from text which only affects one term and add the extra probability to the which would benefit more terms in the likelihood function since the occurred many times thus generating an overall effect of increasing the value of the likelihood function  In other words because the is repeated many times in the likelihood function if we increase its probability a little bit it will have substantial positive impact on the likelihood function whereas a slight decrease of probability of text will have a relatively small negative impact because it occurred just once.
repeated times function probability substantial function probability text reveals behavior model frequency observed text abilities surprising because—after	 In other words because the is repeated many times in the likelihood function if we increase its probability a little bit it will have substantial positive impact on the likelihood function whereas a slight decrease of probability of text will have a relatively small negative impact because it occurred just once  The analysis above reveals another behavior of the ML estimate of a mixture model high frequency words in the observed text would tend to have high prob abilities in all the distributions  Such a behavior should not be surprising at all because—after all—we are maximizing the likelihood of the data so the more a word occurs the higher its overall probability should be.
analysis behavior ML frequency text tend high Such because—after all—we likelihood data word be.	 The analysis above reveals another behavior of the ML estimate of a mixture model high frequency words in the observed text would tend to have high prob abilities in all the distributions  Such a behavior should not be surprising at all because—after all—we are maximizing the likelihood of the data so the more a word occurs the higher its overall probability should be.
This fact maximum estimators case text data unknown θd higher word use probability choosing model.	 This is in fact a very gen eral phenomenon of all the maximum likelihood estimators  In our special case if a word occurs more frequently in the observed text data it would also encourage the unknown distribution θd to assign a somewhat higher probability to this word  We can also use this example to examine the impact of pθB the probability of choosing the background model.
We’ve equally	 We’ve been so far assuming that each model is equally likely i.
function 17.	 pθB  0 5  But you can again look at this likelihood function shown in Figure 17.
smaller pθd increase word general likely mixture important higher probability values mixture estimation problem mixture likelihood estimator.	 The smaller pθd is the less important for θd to respond to the increase of counts of a word in the data  In general the more likely a component is being chosen in a mixture model the more important it is for the component model to assign higher probability values to these frequent words  To summarize we discussed the mixture model the estimation problem of the mixture model and some general behaviors of the maximum likelihood estimator.
efficiently likelihood Third regulates collaboration component models component models change	 This would allow them to collaborate more efficiently to maximize the likelihood  Third the probabil ity of choosing each component regulates the collaboration and the competition between component models  It would allow some component models to respond more to the change of frequency of a word in the data.
choosing competition component component models frequency word data.	 Third the probabil ity of choosing each component regulates the collaboration and the competition between component models  It would allow some component models to respond more to the change of frequency of a word in the data.
In Bayesian compromise data prior essentially hold provided background useful mixture earlier	 In general Bayesian estimation would seek for a compromise between our prior and the data likelihood but in this case we can assume that our prior is infinitely strong and thus there is essentially no compromise hold ing one component model as constant the same as the provided background model  It is useful to point out that this mixture model is precisely the mix ture model for feedback in information retrieval that we introduced earlier in the book.
discussion use mixture topic ment words factored use background model section discuss estimate.	3 5 ExpectationMaximization The discussion of the behaviors of the ML estimate of the mixture model provides an intuition about why we can use a mixture model to mine one topic from a docu ment with common words factored out through the use of a background model  In this section we further discuss how we can compute such an ML estimate.
assumed pw pθB given θd constraint Figure 17.	 Recall that we have assumed both pw  θB and pθB are already given so the only “free” parameters in our model are pw  θd for all the words subject to the constraint that they sum to one  This is illustrated in Figure 17.
Intuitively ML values word distribution maximize probability documents imagine words partitioned groups.	 Intuitively when we compute the ML estimate we would be exploring the space of all possible values for the word distribution θd until we find a set of values that would maximize the probability of the observed documents  According to our mixture model we can imagine that the words in the text data can be partitioned into two groups.
22 ′ document d generated θd estimate θd seen document d θd compute count count words In case mixture unigram models estimated separately	22 where d ′ is used to denote the pseudo document that is composed of all the words in document d that are known to be generated by θd  and the ML estimate of θd is seen to be simply the normalized word frequency in this pseudo document d ′  That is we can simply pool together all the words generated from θd  compute the count of each word and then normalize the count by the total counts of all the words in such a pseudo document  In such a case our mixture model is really just two independent unigram language models which can thus be estimated separately based on the data points generated by each of them.
pool generated count normalize document case mixture independent unigram models based data generated Unfortunately don’t word distribution.	 That is we can simply pool together all the words generated from θd  compute the count of each word and then normalize the count by the total counts of all the words in such a pseudo document  In such a case our mixture model is really just two independent unigram language models which can thus be estimated separately based on the data points generated by each of them  Unfortunately the real situation is such that we don’t really know which word is from which distribution.
The infer word partitioning estimate improved inference partitioning leading hitting local maximum.	 The main idea of the EM algorithm is to guess infer which word is from which distribution based on a tentative estimate of parameters and then use the inferred partitioning of words to improve the estimate of param eters which in turn enables improved inference of the partitioning leading to an iterative hillclimbing algorithm to improve the estimate of the parameters until hitting a local maximum.
Mining 361 let’s tentative parameters Consider	3 Mining One Topic from Text 361 For now let’s assume we have a tentative estimate of all the parameters  How can we infer which of the two distributions a word has been generated from Consider a specific word such as text.
distributions generated Consider specific text θd probability	 How can we infer which of the two distributions a word has been generated from Consider a specific word such as text  Is it more likely from θd or θB To answer this question we compute the conditional probability pθd  text.
answer	 Is it more likely from θd or θB To answer this question we compute the conditional probability pθd  text.
opposed θB general probability given pθd	 How often is θd as opposed to θB used to generate a word in general This probability is given by pθd  If pθd is high then we’d expect pθd  text to be high.
chosen word observe This pw θd If ptext high we’d expect	 If θd is indeed chosen to generate a word how likely would we observe text This probability is given by pw  θd  If ptext  θd is high then we’d also expect pθd  text to be high.
Our rigorously captured Bayes’ pθd θd ptext θB text generated θd θB illustrated	 Our intuition can be rigorously captured by using Bayes’ rule to infer pθd  text where we essentially compare the product pθdptext  θd with the product pθB ptext  θB to see whether text is more likely generated from θd or from θB  This is illustrated in Figure 17 23.
belief distribution text pθd encourage word data ptext θd ptext gives text	 These are prior because they encode our belief about which distribution before we even observe the word text a prior that has very high pθd would encourage us to lean toward guessing θd for any word  Such a prior is then updated by incorporating the data likelihood ptext  θd and ptext  θB so that we would favor a distribution that gives text a higher probability.
Estep figure θd generated completely θd splits That 0 percent text allocated contribute estimate	 Note that the Estep essentially helped us figure out which words have been generated from θd and equivalently which words have been generated from θB except that it does not completely allocate a word to θd or θB but splits a word in between the two distributions  That is pz  0  text tells us what percent of the count of text should be allocated to θd  and thus contribute to the estimate of θd .
Estep essentially helps generated	e  Estep essentially helps us probabilistically separate words generated by the two distributions.
EM local reaching global maximum multiple local multiple times initializations gives estimated parameter values EM 17.	 The EM algorithm can guarantee reaching such a local maximum but it cannot guarantee reaching a global maximum when there are multiple local maxima  Due to this we usually repeat the algorithm multiple times with different initializations in practice using the run that gives the highest likelihood value to obtain the estimated parameter values  The EM algorithm is illustrated in Figure 17.
Note simply binary word z This variable hidden idea EM leverage simplify computa values hidden ML estimate trivial pool ues counts.	 Note that we simply assumed imagined the existence of such a binary latent variable associated with each word token but we don’t really observe these z values  This is why we referred to such a variable as a hidden variable  A main idea of EM is to leverage such hidden variables to simplify the computa tion of the ML estimate since knowing the values of these hidden variables makes the ML estimate trivial to compute we can pool together all the words whose z val ues are 0 and normalize their counts.
Knowing z values help simplify computing ML estimate fact Estep Mstep estimate	 Knowing z values can potentially help simplify the task of computing the ML estimate and EM exploits this fact by alternating the Estep and Mstep in each iteration so as to improve the parameter estimate in a hillclimbing manner.
value inferred split word counts distributions counts θd improve leading generation parameter perform new iteration improve parameter cw pz 0 e.	 Specifically the Estep is to infer the value of z for all the words while the Mstep is to use the inferred z values to split word counts between the two distributions and use the allocated counts for θd to improve its estimation leading to a new generation of improved parameter values which can then be used to perform a new iteration of Estep and Mstep to further improve the parameter estimation  In the Mstep we adjust the count cw d based on pz  0  w i e.
adjust cw pz 0 e.	 In the Mstep we adjust the count cw d based on pz  0  w i e.
discounted words words regarded pw θd Mstep w words simply single unigram model based observed makes sense Estep chance	 Thus the Mstep is simply to normalize these discounted counts for all the words to obtain a probability distribution over all the words which can then be regarded as our improved estimate of pw  θd  Note that in the Mstep if pz  0  w  1 for all words we would simply compute the simple single unigram language model based on all the observed words which makes sense since the Estep would have told us that there is no chance that any word has been generated from the background.
formulas Mstep	25 we further illustrate in detail what happens in each iteration of the EM algorithm  First note that we used superscripts in the formulas of the E step and Mstep to indicate the generation of parameters.
note formulas step parameters Mstep use generation n	 First note that we used superscripts in the formulas of the E step and Mstep to indicate the generation of parameters  Thus the Mstep is seen to use the nth generation of parameters together with the newly inferred z values to obtain a new n  1th generation of parameters i.
Mstep seen nth generation parameters obtain new 1th parameters	 Thus the Mstep is seen to use the nth generation of parameters together with the newly inferred z values to obtain a new n  1th generation of parameters i.
iteration apply word e pute 0 pz w.	 In the first iteration of the EM algorithm we will apply the Estep to infer which of the two distributions has been used to generate each word i e  to com pute pz  0  w and pz  1  w.
com pz 1 0 needed Mstep 0	 to com pute pz  0  w and pz  1  w  We only showed pz  0  w which is needed in our Mstep pz  1  w  1 − pz  0  w.
higher new parameters adjust inferred latent hidden variable leading probabili ties values generation potentially estimate In table loglikelihood iteration.	 Those words that are believed to have come from the topic word distribution θd according to the Estep would have a higher probability  This new generation of parameters would allow us to further adjust the inferred latent variable or hidden variable values leading to a new generation of probabili ties for the z values which can be fed into another Mstep to generate yet another generation of potentially improved estimate of θd   In the last row of the table we show the loglikelihood after each iteration.
iteration lead different generation parameter loglikelihood	 Since each iteration would lead to a different generation of parameter estimates it would also give a different value for the loglikelihood function.
We explanation	 We will pro vide an intuitive explanation of why it converges to a local maximum later.
intuitive converge local maximum 26 θd Xaxis Yaxis denotes function	 Next we provide some intuitive explanation why the EM algorithm will converge to a local maximum in Figure 17 26  Here we show the parameter θd on the Xaxis and the Yaxis denotes the likelihood function value.
parameter θd value θd onedimensional view makes easier	 Here we show the parameter θd on the Xaxis and the Yaxis denotes the likelihood function value  This is an oversimplification since θd is an Mdimensional vector but the onedimensional view makes it much easier to understand the EM algorithm.
general like lihood θd local	 We see that in general the original like lihood function as a function of θd may have multiple local maxima.
The goal ML estimate global e θd value makes likelihood function reach global	 The goal of computing the ML estimate is to find the global maximum i e  the θd value that makes the likelihood function reach it global maximum.
In simple model function function simple Mstep computation function directly computed function word Mstep solution optimizing directly obtained Estep algorithm applications.	 In our case of the simple mixture model we did not explicitly compute this auxiliary function in the Estep because the auxiliary function is very simple and 368 Chapter 17 Topic Analysis as a result our Mstep has an analytical solution thus we were able to bypass the explicit computation of this auxiliary function and go directly to find a reestimate of the parameters  Thus in the Estep we only computed a key component in the auxiliary function which is the probability that a word has been generated from each of the two distributions and our Mstep directly gives us an analytical solution to the problem of optimizing the auxiliary function and the solution directly uses the values obtained from the Estep  The EM algorithm has many applications.
know values hidden data identify points generated distribution	 Thus once we know the values of these hidden variables we would be able to partition data and identify the data points that are likely generated from any particular distribution thus facilitating estima tion of component model parameters.
algorithm augment data unobserved algorithm follows initialize	 In general when we apply the EM algorithm we would augment our data with supplementary unobserved hidden variables to simplify the estimation problem  The EM algorithm would then work as follows  First it would randomly initialize all the parameters to be estimated.
conditions parameters Wu Latent Semantic Analysis section PLSA basic applications.	 Only if some conditions are satisfied would the parameters be guaranteed to converge see Wu 1983  17 4 Probabilistic Latent Semantic Analysis In this section we introduce probabilistic latent semantic analysis PLSA the most basic topic model with many applications.
4 Latent probabilistic latent topic mixture discussed earlier topic text	 17 4 Probabilistic Latent Semantic Analysis In this section we introduce probabilistic latent semantic analysis PLSA the most basic topic model with many applications  In short PLSA is simply a generalization of the twocomponent mixture model that we discussed earlier in this chapter to discover more than one topic from text data.
PLSA precisely designed	 PLSA is precisely designed to perform this task.
We assumptions 17 27 blog Hurricane Katrina represented word distribution including	 We illustrate these two assumptions in Figure 17 27 where we see a blog article about Hurricane Katrina and some imagined topics each represented by a word distribution including e.
seen	 The article is seen to contain words from all these distributions.
topic analysis try decode topics figure words distribution characterizations	 The main goal of topic analysis is to try to decode these topics behind the text by segmenting them and figure out which words are from which distribution so that we can obtain both characterizations of all the topics in the text data and the coverage of topics in each document.
Once applications segmentation formal mining multiple topics illustrated 17	 Once we can do these they can be directly used in many applications such as summarization segmentation and clustering  The formal definition of mining multiple topics from text is illustrated in Fig ure 17 28.
PLSA background topics removed preprocessing As mixture process word steps choose component model controlled background set denoting use background model.	 The original PLSA Hofmann 1999 did not include a background language model thus it gives common words high probabilities in the learned topics if such common words are not removed in the preprocessing stage  As in the case of the simple mixture model the process of generating a word still consists of two steps  The first is to choose a component model to use this decision is controlled by both a parameter λB denoting the probability of choosing the background model and a set of πd  i denoting the probability of choosing topic θi if we decided not to use the background model.
prob observing topic λBπd jpw	 Specifically the prob ability of observing a word from the background distribution is λBpw  θB while the probability of observing a word from a topic θj is 1 − λBπd jpw  θj.
30 equation.	30 and that the likelihood function for the entire collection C is given by the third equation.
What parameters First percent words exist out.	 What are the parameters in PLSA First we see λB which represents the percent age of background words that we believe exist in the text data and that we would like to factor out.
j document discover	 Third we see πd j which indicates the coverage of topic θj in document d  This parameter encodes the knowledge we hope to discover from text.
figure unknown PLSA model This PLSA data After obtained likelihood function perform parameter use Likelihood estimator shown 17.	 Can you figure out how many unknown parameters are there in such a PLSA model This would be a useful exercise to do which helps us understand what exactly are the outputs that we would generate by using PLSA to analyze text data  After we have obtained the likelihood function the next question is how to perform parameter estimation  As usual we can use the Maximum Likelihood estimator as shown in Figure 17.
question parameter usual use 17.	 After we have obtained the likelihood function the next question is how to perform parameter estimation  As usual we can use the Maximum Likelihood estimator as shown in Figure 17.
articles parameters equations having parameters.	 we now have a collection of text articles instead of just one document   we have more parameters to estimate and   we have more constraint equations which is a consequence of having more parameters.
	33.
cases parameters.	 Note that the normalizers are very different in these two cases which are directly related to the constraints we have on these parameters.
variables computed expected counts normalized appropriately	 That is the distribution of the hidden variables computed in the Estep can be used to compute the expected counts of an event which can then be aggregated and normalized appropriately to obtain a reestimate of the parameters.
computation algorithm PLSA	34 we show the computation of the EM algorithm for PLSA in more detail.
algorithm Estep followed	 In each iteration the EM algorithm would first invoke the Estep followed by the Mstep.
θj counts counts word uments These obtain time	 Similarly to reestimate pw  θj the relevant counts are the sum of all the split counts of word w in all the doc uments  These aggregated counts would then be normalized by the sum of such aggregated counts over all the words in the vocabulary so that after normalization we again would obtain a distribution this time over all the words rather than all the topics.
Mstep lot memory track results Estep collect counts Mstep values	 If we complete all the computation of the Estep before starting the Mstep we would have to allocate a lot of memory to keep track of all the results from the Estep  However it is possible to interleave the Estep and Mstep so that we can collect and aggregate relevant counts needed for the Mstep while we compute the Estep  This would eliminate the need for storing many intermediate values unnecessarily.
For analyst expect topic research retrieval tell allocate retrieval topic review data laptop specific aspects battery screen size allocate topics battery life respectively Second knowledge document.	 For example an analyst may expect to see “retrieval models” as a topic in a data set with research articles about information retrieval thus we would like to tell the model to allocate one topic to capture the retrieval models topic  Similarly a user may be interested in analyzing review data about a laptop with a focus on specific aspects such as battery life and screen size thus we again want the model to allocate two topics for battery life and screen size respectively  Second users may have knowledge about what topics are or are not covered in a document.
user interested review data specific model topics battery screen	 Similarly a user may be interested in analyzing review data about a laptop with a focus on specific aspects such as battery life and screen size thus we again want the model to allocate two topics for battery life and screen size respectively.
For example tags documents users document knowledge document assigned gives generate words cooccuring pure insufficient	 For example if we have topical tags assigned to documents by users we may regard the tags assigned to a document as knowledge about what topics are covered in the document  Thus we can define a prior on the topic coverage to ensure that a document can only be generated using topics corresponding to the tags assigned to it  This essentially gives us a constraint on what topics can be used to generate words in a document which can be useful for learning cooccuring words in the context of a topic when the data are sparse and pure cooccurrence statistics are insufficient to induce a meaningful topic.
All incorporated Maximum instead Maximum Likelihood estimation denote parameters values encode	 All such prior knowledge can be incorporated into PLSA by using Maximum A Posteriori Estimation MAP instead of Maximum Likelihood estimation  Specifi cally we denote all the parameters by  and introduce a prior distribution p over all the possible values of  to encode our preferences.
sole maximize case ML Adding p encourage model pData mode maximizes p potentially different ways define	6 where pData   is the likelihood function which would be the sole term to maximize in the case of ML estimation  Adding the prior p would encourage the model to seek a compromise of the ML estimate which maximizes pData and the mode of the prior which maximizes p  There are potentially many different ways to define p.
merge derive form.	 Due to the same form of the two functions we can generally merge the two to derive a single function again of the same form.
μ 0 EM algorithm PLSA	 If μ  0 we recover the original EM algorithm for PLSA i.
e prior interpret heuristic background distribution prior	e  the word distribution is fixed to the prior  This is why we can interpret our heuristic inclusion of a background word distribution as a topic in PLSA as simply imposing such an infinitely strong prior on one of the topics.
prior This inclusion distribution prior topics.	 the word distribution is fixed to the prior  This is why we can interpret our heuristic inclusion of a background word distribution as a topic in PLSA as simply imposing such an infinitely strong prior on one of the topics.
data overriding prior collect data A updated parameter value topics effect probability topic infinitely prior topic zero probability.	 In general however as we increase the amount of data we will be able to let the data dominate the estimate eventually overriding the prior completely as we collect infinitely more data  A prior on the coverage distribution π can be added in a similar way to the updating formula for πd j to force the updated parameter value to give some topics higher probabilities by reducing the probabilities of others  In the extreme it is also possible to achieve the effect of setting the probability of a topic to zero by using an infinitely strong prior that gives such a topic a zero probability.
In PLSA distribution word LDA model assumed corresponding	 In PLSA both the topic coverage distribution and the word distributions are assumed to be unknown parameters in the model  In LDA they are no longer parameters of the model since they are assumed to be drawn from the corresponding Dirichlet prior distributions.
word distributions .	    αk and the Dirichlet distribution governing the topic word distributions has M parameters β1 .
	   .
interpreted pseudo count corresponding βi count word according effect document The function 17.	 Each αi can be interpreted as the pseudo count of the corresponding topic θi according to our prior while each βi can be interpreted as the pseudo count of the corresponding word wi according to our prior  With no additional knowledge they can all be set to uniform counts which in effect assumes that we do not have any preference for any word in each word distribution and we do not have any preference for any topic either in each document  The likelihood function of LDA is given in Figure 17.
With knowledge set counts effect assumes word preference topic	 With no additional knowledge they can all be set to uniform counts which in effect assumes that we do not have any preference for any word in each word distribution and we do not have any preference for any topic either in each document.
	 .
θk topics topic	  θk representing k topics with a topic coverage distribution πd j .
However like function document entire different topic distribution uncertainty word complicated use Naturally required optimization problem complicated	 However the like lihood function for a document and the entire collection C is clearly different with LDA adding the uncertainty of the topic coverage distribution and the uncertainty of all the word distributions in the form of an integral  Although the likelihood function of LDA is more complicated than PLSA we can still use the MLE to estimate its parameters Naturally the computation required to solve such an optimization problem is more complicated than LDA.
easy LDA fewer	 It is now easy to see that LDA has only k  M parameters far fewer than PLSA.
cost analysis	 However the cost is that the interesting output that we would like to generate in topic analysis i.
Analysis Topic evaluation information evalua tion cases usually evaluation metrics defines topic defining complicates eventual	6 Evaluating Topic Analysis Topic analysis evaluation has similar difficulties to information retrieval evalua tion  In both cases there is usually not one true answer and evaluation metrics heavily depend on the human issuing judgements  What defines a topic We ad dressed this issue the best we could when defining the models but the challenging nature of such a seemingly straightforward question complicates the eventual eval uation task.
measures heldout presented applied new information likelihood.	 Both are predictive measures meaning that heldout data is presented to the model and the model is applied to this new information calculating its likelihood.
measure coherency tions A didn’t word documenttopic distribution evaluation This test coherency discovered	 Human judges responded to intrusion detection scenarios to measure the coherency of the topicword distribu tions  A second test that we didn’t cover in the word association evaluation is the documenttopic distribution evaluation  This test can measure the coherency of top ics discovered from documents through the previously used intrusion test.
A documenttopic distribution	 A second test that we didn’t cover in the word association evaluation is the documenttopic distribution evaluation.
	.
If variant statistically signif icantly improve precision new model.	 If a different topic analysis variant is shown to statistically signif icantly improve some task precision then an argument may be made to prefer the new model.
started idea term topic discussed	 We started with the simple idea of using one term to represent a topic and discussed the deficiency of such an approach.
The parameter enabled k representing topic proportion topic document The topic topics applications.	 The estimated parameter values enabled us to discover two things one is k word distributions with each one representing a topic and the other is the proportion of each topic in each document  The topic word distributions and the detailed characterization of coverage of topics in each document can enable further analysis and applications.
coverage topics enable Bibliographic Further Reading ple particular time assess coverage time period This gener	 The topic word distributions and the detailed characterization of coverage of topics in each document can enable further analysis and applications  For exam Bibliographic Notes and Further Reading 385 ple we can aggregate the documents in a particular time period to assess the coverage of a particular topic in the time period  This would allow us to gener ate a temporal trend of topics.
retrieval text text Finally latent allocation PLSA documenttopic	g  for information retrieval text clustering and text categorization  Finally a variant of PLSA called latent Dirichlet allocation LDA extends PLSA by adding priors to the documenttopic distributions and topicword distributions.
Finally LDA generative simulate values parameters model new et	 Finally LDA is a generative model which can be used to simulate generate values of parameters in the model as well as apply the model to a new unseen document Blei et al.
particular we’re going sentiment analysis.	 In particular we’re going to talk about opinion mining and sentiment analysis.
For	 For example you might say a computer has a screen and a battery.
In contrast ” statements subjective it’s prove The word person holder.	 In contrast with this think about a sentence such as “This laptop has the best battery life” or “This laptop has a nice screen ” These statements are more subjective and it’s very hard to prove whether they are wrong or right  The word person indicates an opinion holder.
If identify basic understanding	 If you can identify these we get a basic understanding of opinions.
If enriched opinion	 If we want to understand further we need an enriched opinion representation.
We e positive feeling.	 We would also like to understand the opinion sentiment i e  whether it is a positive or negative feeling.
	 This makes the task harder.
All elements extracted NLP Analyzing it’s opinions	 All these elements must be extracted by using NLP techniques  Analyzing the sentiment in news is still quite difficult it’s more difficult than the analysis of opinions in product reviews.
opinion holder group country	 First let’s think about the opinion holder  The holder could be an individual or it could be group of people  Sometimes the opinion is from a committee or from a whole country of people.
greatly entity particular product opinion particular	 Opinion targets will vary greatly as well they can be about one entity a particular person a particular product a particular policy and so on  An opinion could also only be about one attribute of a particular entity.
attribute smartphone opinion comment	 An opinion could also only be about one attribute of a particular entity  For example it could just be about the battery of a smartphone  It could even be someone else’s opinion and one person might comment on another person’s opinion.
like different complex background topic discussed.	 We can have a simple context like a different time or different locations  There could be also complex contexts such as some background of a topic being discussed.
opinion particular discourse context interpreted it’s context From perspective	 When an opinion is expressed in a particular discourse context it has to be interpreted in different ways than when it’s expressed in another context  From a computational perspective we’re mostly interested in what opinions can be extracted from text data.
factual statement true	” Now this is in a way a factual statement because it’s either true or false.
These interesting need opinions The task mining defined set representations 18.	 These are interesting variations that we need to pay attention to when we extract opinions  The task of opinion mining can be defined as taking contextualized input to gen erate a set of opinion representations as shown in Figure 18.
saw case review holder explicitly identified mining reasons.	 We just saw an example in the case of a product review where the opinion holder and the opinion target are often explicitly identified  Opinion mining is important and useful for three major reasons.
useful major reasons decision support opinions reading product use.	 Opinion mining is important and useful for three major reasons  First it can aid decision support it can help us optimize our decisions  We often look at other people’s opinions by reading their reviews in order to make a decision such as which product to buy or which service to use.
First look opinions reviews decision product decide	 First it can aid decision support it can help us optimize our decisions  We often look at other people’s opinions by reading their reviews in order to make a decision such as which product to buy or which service to use  We also would be interested in others’ opinions when we decide whom to vote for.
We opinions decide want constituents’	 We also would be interested in others’ opinions when we decide whom to vote for  Policymakers may also want to know their constituents’ opinions when designing a new policy.
want constituents’ designing policy.	 Policymakers may also want to know their constituents’ opinions when designing a new policy.
What winning products competitors’ Market research understanding opinions.	 What are the winning features of their products or competitors’ products Market research has to do with understanding consumer opinions.
If media networks general huge prediction text data 18.	 If we aggregate opinions from social media we can study the behavior of people on social networks  In general we can gain a huge advantage in any prediction task because we can leverage the text data as extra data to any problem  18.
In prediction extra	 In general we can gain a huge advantage in any prediction task because we can leverage the text data as extra data to any problem.
Sentiment known classification.	1 Sentiment Classification If we assume that most of the elements in an opinion representation are already known then our only task may be sentiment classification.
emotion analysis	 The other is emotion analysis that can go beyond polarity to characterize the precise feeling of the opinion holder.
task essentially classification before.	 Thus the task is essentially a classification task or categorization task as we’ve seen before.
happened document rare consider ngrams A followed	 In reality though the 7gram just happened to occur with the positive document and no others because it was so rare  We can consider ngrams of partofspeech tags  A bigram feature could be an adjective followed by a noun.
classes like POS tags semantic representing concepts Word Net University 2010 place	 These classes can be syntactic like POS tags or could be semantic by representing concepts in a thesaurus or ontology like Word Net Princeton University 2010  Or they can be recognized name entities like people or place and these categories can be used to enrich the representation as additional features.
entities categories enrich representation	 Or they can be recognized name entities like people or place and these categories can be used to enrich the representation as additional features.
Furthermore represents frequent set occur occur context.	 Furthermore we can have a frequent pattern syntax which represents a frequent word set these are words that do not necessarily occur next to each other but often occur in the same context.
words occur patterns discriminative words.	 We’ll also have locations where the words may occur more closely together and such patterns provide more discriminative features than words.
They generalize better data overfitting complex.	 They may generalize better than just regular ngrams because they are frequent meaning they are expected to occur in testing data although they might still face the problem of overfitting as the features become more complex.
It combine error knowledge main understanding With feature space machine learning program	 It would be most effective if you can combine machine learning error analysis and specific domain knowledge when designing features  First we want to use do main knowledge that is a specialized understanding of the problem  With this we can design a basic feature space with many possible features for the machine learning program to work on.
These tech analyze overfitting.	 These features can then be further analyzed by humans through error analysis using evaluation tech niques we discuss in this book  We can look at categorization errors and further analyze what features can help us recover from those errors or what features cause overfitting.
look features help errors features cause overfitting.	 We can look at categorization errors and further analyze what features can help us recover from those errors or what features cause overfitting.
section logistic regression sentiment	 18 2 Ordinal Regression In this section we will discuss ordinal logistic regression for sentiment analysis.
Regression In ordinal sentiment typical rating prediction try sentiment e g.	2 Ordinal Regression In this section we will discuss ordinal logistic regression for sentiment analysis  A typical sentiment classification problem is related to rating prediction because we often try to predict sentiment value on some scale e g.
A related rating try sentiment scale	 A typical sentiment classification problem is related to rating prediction because we often try to predict sentiment value on some scale e g.
negative	g  positive to negative with other labels in between.
opinionated document want rating k Since rating k categories.	 We have an opinionated text document d as input and we want to generate as output a rating in the range of 1 through k  Since it’s a discrete rating this could be treated as a categorization problem finding which is the correct of k categories.
One addresses ordinal logistic regression.	 One approach that addresses this issue is ordinal logistic regression.
usual representation	 As usual these features can be a representation of a text document.
response variable 1 means X means X course problem regression recall Chapter 10 features.	 X is a binary response variable 0 or 1 where 1 means X is positive and 0 means X is negative  Of course this is then a standard two category categorization problem and we can apply logistic regression  You may recall from Chapter 10 that in logistic regression we assume the log probability that Y  1 is a linear function of the features.
allow X transformed linear function The parameters.	 This would allow us to also write pY  1  X as a transformed form of the linear function of the features  The βi’s are parameters.
The βi’s parameters.	 The βi’s are parameters.
This direct binary	 This is a direct application of logistic regression for binary categorization.
binary case j 1 rating	5  The idea is that we can introduce multiple binary classifiers in each case we ask the classifier to predict whether the rating is j or above  So when Yj  1 it means the rating is j or above.
After separately trained k − logistic regression instance invoke look corresponds rating k.	 After we have separately trained these k − 1 logistic regression classifiers we can take a new instance and then invoke classifiers sequentially to make the decision  First we look at the classifier that corresponds to the rating level k.
look rating This classifier If probability	 First we look at the classifier that corresponds to the rating level k  This classifier will tell us whether this object should have a rating of k or not  If the probability according to this logistic regression classifier is larger than 0.
If 0 − 1.	 If it’s less than 0 5 we need to invoke the next classifier which tells us whether it’s at least k − 1.
β values.	 In effect we have more data to help us choose good β values.
Of course predict different rating αj parameters βi’s M k parameters.	 Of course this value is needed to predict the different rating levels  So αj is different since it depends on j  but the rest of the parameters the βi’s are the same  We now have M  k − 1 parameters.
Then range decide	 Then using the range we can decide which rating the object should receive.
In particular we’re going introduce Latent Aspect Analysis allows perform analysis overall	 In particular we’re going to introduce Latent Aspect Rating Analysis LARA which allows us to perform detailed analysis of reviews with overall ratings.
reviewers given stars.	 Both reviewers are given five stars.
If overall ratings different aspects opinions	 If we can decompose the overall ratings into ratings on these different aspects we can obtain a much more detailed understanding of the reviewers’ opinions about the hotel.
Such weight distribution placed value place priority different aspects.	 Such a case is what’s shown on the left for the weight distribution where you can see most weight is placed on value  Clearly different users place priority on different rating aspects.
example imagine stars value expensive.	 For example imagine a hotel with five stars for value  Despite this it might still be very expensive.
Thus reviews overall	 Thus the task is to take these reviews and their overall ratings as input and generate both the aspect ratings and aspect weights as output.
value	 Second is ratings on each aspect such as value and room service.
First stages.	 First we will talk about how to solve the problem in two stages.
mention	 Later we mention that we can do this in a unified model.
we’re pick words talking condition particular words segment particular segment like room price aspect segment.	 First we will segment the aspects we’re going to pick out what words are talking about location what words are talking about room condition and so on  In particular we will obtain the counts of all the words in each segment denoted by ciw d where i is a particular segment index  This can be done by using seed words like location room or price to retrieve the aspect label of each segment.
particular ciw segment index.	 In particular we will obtain the counts of all the words in each segment denoted by ciw d where i is a particular segment index.
overall rating aspect allows predict observable word left 18.	 This method assumes the overall rating is simply a weighted average of these aspect ratings which allows us to predict the overall rating based on the observable word frequencies  On the left side of Figure 18.
seen PLSA predict topics	 We have seen such cases before in other models such as PLSA where we predict topics in text data.
mean average rid course assumption way model allows interesting case aspect ratings	 rd is assumed to follow a normal distribution with a mean that is a weighted average of the aspect ratings rid and variance δ2  Of course this is just our assumption  As always when we make this assumption we have a formal way to model the problem and that allows us to compute the interesting quantities—in this case the aspect ratings and the aspect weights.
Of course model interesting case aspect ratings aspect weights.	 Of course this is just our assumption  As always when we make this assumption we have a formal way to model the problem and that allows us to compute the interesting quantities—in this case the aspect ratings and the aspect weights.
As allows compute case Each aspect rating assumed sum weights aspect The α aspects overall drawn multivariate Gaussian distribution ∼ N μ	 As always when we make this assumption we have a formal way to model the problem and that allows us to compute the interesting quantities—in this case the aspect ratings and the aspect weights  Each aspect rating rid is assumed to be a sum of sentiment weights of words in aspect i  The vector of weights α for the aspects in the overall rating is it self drawn from another multivariate Gaussian distribution αd ∼ N μ .
aspect weights aspect	 Each aspect rating rid is assumed to be a sum of sentiment weights of words in aspect i.
Note β indexed	 Note that β is indexed by both i and w.
	 18.
	e  topics.
	 2011 for additional reading.
breakdown opinions different reviews	 This breakdown can give us detailed opinions at the aspect level  Another application is that you can compare different reviews on the same hotel.
learn information This kind lexicon useful If battery positive.	 Thus we can also learn sentiment information directly from the data  This kind of lexicon is very useful because in general a word like long may have different sentiment polarities for different contexts  If we see “The battery life of this laptop is long” then that’s positive.
But “The rebooting time laptop long”	 But if we see “The rebooting time for the laptop is long” then that’s clearly not good.
model inferred challenge evaluation.	 Recall that the model can infer whether a reviewer cares more about service or the price  How do we know whether the inferred weights are correct This poses a very difficult challenge for evaluation.
18.	 Figure 18.
These ratios computed based inferred weights model prices hotels	 These ratios are computed based on the inferred weights from the model  We can see the average prices of hotels favored by the top ten reviewers are indeed much cheaper than those that are favored by the bottom ten.
This informative overall rating text 18 results rating	 This is more informative than the original review that just has an overall rating and review text  Figure 18 15 shows some interesting results on analyzing user rating behavior.
Figure 18 behavior.	 Figure 18 15 shows some interesting results on analyzing user rating behavior.
15 shows results analyzing user	15 shows some interesting results on analyzing user rating behavior.
validate model	 This is another way to validate the model by the inferred weights.
case mining humans patterns opinions different consumer different Of general applied overall	 This is a case where text mining algorithms can go beyond what humans can do to discover interesting patterns in the data  We can compare different hotels by comparing the opinions from different consumer groups in different locations  Of course the model is quite general so it can be applied to any reviews with an overall ratings.
compare different different different locations.	 We can compare different hotels by comparing the opinions from different consumer groups in different locations.
For example query shows value user getting hotel—an reviewers query user’s.	 For example we have a query here that shows 90 of the weight should be on value and 10 on others  That is this user just cares about getting a cheap hotel—an emphasis on the value dimension  With this model we can find the reviewers whose weights are similar to the query user’s.
For reviews holder target making	 For product reviews the opinion holder and the opinion target are clear making them easy to analyze.
presented 2008 comprehensive survey analysis 18.	 For future reading on topics presented in this chapter we suggest Pang and Lee 2008 a comprehensive survey on opinion mining and sentiment analysis  18.
Opinion Mining chapter opinion classification topic analysis discussed Chapter Chapter	4 Evaluation of Opinion Mining and Sentiment Analysis In this chapter we investigated opinion mining and sentiment analysis from the viewpoint of both classification or regression and topic analysis  Thus evaluation from these perspectives will be similar to those discussed in Chapter 15 catego rization and Chapter 17 topic analysis.
standard machine learning	 This then becomes the standard machine learning testing setup where we can use techniques such as crossfold validation to determine the effectiveness of our method.
topic analysis like ensure documents dataset.	 From a topic analysis viewpoint we would like to ensure that the topics are coherent and have a believable coverage and distribution over the documents in the dataset.
al	 2010 Wang et al  2011.
act values	 We are interested in predicting such variables because we might want to act on the inferred values or make decisions based on the inferred values.
analysis data predictors variables us.	 Through analysis of all the data we can generate multiple predictors of the interesting variables to us.
combined predictive interesting variable.	 We call these predictors features and they can further be combined and put into a predictive model to actually predict the value of any interesting variable.
The allow general process based	 The prediction results would then allow us to act and change the world  This is the general process for making a prediction based on any data.
help need text data data humans involved adjusting tive	 Thus machines must help and that’s why we need to do text data mining  Sometimes machines can even “see” patterns in data that humans may not see even if they have the time to read all the data  Next humans also must be involved in building adjusting and testing a predic tive model.
interesting involved controlling sensors collect data mining loop perturb collect additional data allowing improve prediction data	 Finally it’s interesting that a human could be involved in controlling the sensors to collect the most useful data for prediction  Thus this forms a data mining loop because as we perturb the sensors they will collect additional new and potentially more useful data allowing us to improve the prediction  In this loop humans will recognize what additional data will need to be collected.
identify data general collect data useful The study data points machine learning machine	 Machines can help humans identify what data should be collected next  In general we want to collect data that is most useful for learning  The study of how to identify data points that would be most helpful for machine learning is often referred to as active learning which is an important subarea in machine learning.
behavior	 Typically in such a case the prediction is about human behavior or human preferences or opinions.
text data	 In general though text data will be put together with nontext data.
This interaction topic	 This interaction is the topic of the chapter.
analysis Specifically data provide mining text enable ways companion nontext	 First nontext data can enrich text analysis  Specifically nontext data can often provide a context for mining text data and thus enable us to partition data in different ways based on the companion nontext data e.
technique pattern discussed et	 This technique is called pattern annotation and discussed in detail in Mei et al.
time When topics represent opinions opinions.	g  time or location  When topics represent opinions we may also reveal contextdependent opinions.
mining useful prediction effective sophisticated	 Contextual text mining can be very useful for text based prediction because it allows us to combine nontext data with text data to derive potentially very effective sophisticated predictors.
text mining useful rich contextual information.	 Contextual text mining is generally useful because text often has rich contextual information.
The di figure venues papers vertical unit ID text partition papers	 The horizontal di mension of the figure shows different conference venues where the papers are published and the vertical dimension shows the time of a publication  We can treat each paper as a separate unit in this case a paper ID serves as the con text and each paper has its own context  We can treat all the papers published in 1998 as one group and partition papers by the year due to the availability of time as a nontext variable.
We treat paper case paper ID papers partition year availability ics different years.	 We can treat each paper as a separate unit in this case a paper ID serves as the con text and each paper has its own context  We can treat all the papers published in 1998 as one group and partition papers by the year due to the availability of time as a nontext variable  Such a partitioning would allow us to compare top ics in different years.
treat published 1998 group partition year time compare ics different based published KDD ACL.	 We can treat all the papers published in 1998 as one group and partition papers by the year due to the availability of time as a nontext variable  Such a partitioning would allow us to compare top ics in different years  Similarly we can partition the data based on the venues we can group all the papers published in SIGIR and compare them with those published in KDD or ACL.
Such allow ics different partition group papers published ACL enabled conference venue.	 Such a partitioning would allow us to compare top ics in different years  Similarly we can partition the data based on the venues we can group all the papers published in SIGIR and compare them with those published in KDD or ACL  This comparison is enabled by the availability of the nontext variable of the conference venue.
S.	S.
additional authors Such contextual view data pare written American written authors countries Sometimes data.	 by using additional con text of the authors  Such a contextual view of the data would allow us to com pare papers written by American authors with those written by authors in other countries  Sometimes we can use topics to partition the data without involving nontext data.
different reveal differences interesting questions require text mining answer.	 Comparing topics in different contexts can also reveal differences about the two contexts  There are many interesting questions that require contextual text mining to answer.
example answer question “What topics getting increasing attention mining need	 For example to answer a question such as “What topics have been getting increasing attention recently in data mining research” we would need to analyze text in the context of time.
research researchers case context difference research topics S.	 What are the common research interests of two researchers In this case authors can be the context  Is there any difference in the research topics published by authors in the U S.
difference research U include location.	 Is there any difference in the research topics published by authors in the U S  and those outside Here the context would include the authors and their affiliation and location.
topics specific document In CPLSA generation process similar time location generation words conditioned specific context	 coverage of topics is specific to a document  In CPLSA the generation process is similar but since we assume that we have context information time or location about a document the generation of words in the document may be conditioned on the specific context of the document.
view topics imposed context period location multiple comparable represent different topics 19 use collection Katrina illustrate idea.	 For example we might have a particular view of all the topics imposed by a particular context such as a particular time period or a particular location so we may have multiple sets of comparable topics that represent different views of these topics associated with different contexts  In Figure 19 4 we use a collection of blog articles about Hurricane Katrina to illustrate this idea.
To document d choose view based For chosen location Texas Texas specific topics word.	4  To generate a word in document d we would first choose a particular view of all the topics based on the context of the document  For example the view being chosen may be the location Texas in such a case we would be using the Texas specific topics to generate the word.
different independent other.	 This is very different from the PLSA where each document has its own coverage preference which is independent of each other.
	e.
topic gen erate word In generation word potentially generated view topic coverage distribution contexts chosen direct	 chosen view of the topic to gen erate a word  In such a generation process each word can be potentially generated using a different view and a different topic coverage distribution depending on the contexts chosen to direct the generation process.
es mixture component models associated contexts views topic depend discovery variations topic different contexts view ages coverage context.	 This is illustrated in Figure 19 5 where we show that all the words in the document have now been generated by using CPLSA which is es sentially a mixture model with many component models associated with different contexts  In CPLSA both the views of topics and the topic coverage depend on context so it enables discovery of different variations of the same topic in different contexts due to the dependency of a view of topics on context and different topic cover ages in different contexts due to the dependency of topic coverage on context.
contextdependent patterns answering questions mentioned PLSA seen context context coverage As PLSA topics text reveal difference	 Such contextdependent topic patterns can be very useful for answering various questions as we mentioned earlier  The standard PLSA can easily be seen as a special case of CPLA when we have used just one single view of topics by using the whole collection as context and used each document ID as a context for deciding topic coverage  As a result what we can discover using PLSA is just one single set of topics characterizing the content in the text data with no way to reveal the difference of topics covered in different contexts.
result PLSA data topics The PLSA reveal associated particular context contrast CPLSA embed needed discovery multiple views contextspecific topics enriching patterns	 As a result what we can discover using PLSA is just one single set of topics characterizing the content in the text data with no way to reveal the difference of topics covered in different contexts  The standard PLSA can only reveal the coverage of topics in each document but cannot discover the topic coverage associated with a particular context  In contrast CPLSA would provide more flexibility to embed the context variables as needed to enable discovery of multiple views of topics and contextspecific coverage of topics thus enriching the topical patterns that can be discovered.
The PLSA coverage topics discover coverage particular In enable views topics contextspecific enriching patterns Since problem estimation number estimated larger PLSA.	 The standard PLSA can only reveal the coverage of topics in each document but cannot discover the topic coverage associated with a particular context  In contrast CPLSA would provide more flexibility to embed the context variables as needed to enable discovery of multiple views of topics and contextspecific coverage of topics thus enriching the topical patterns that can be discovered  Since CPLSA remains a mixture model we can still use the EM algorithm to solve the problem of parameter estimation although the number of parameters to be estimated would be significantly larger than PLSA.
In CPLSA flexibility context variables multiple contextspecific coverage remains mixture model problem parameter number parameters significantly larger PLSA Thus theoretically speaking CPLSA topics topic coverage distri butions.	 In contrast CPLSA would provide more flexibility to embed the context variables as needed to enable discovery of multiple views of topics and contextspecific coverage of topics thus enriching the topical patterns that can be discovered  Since CPLSA remains a mixture model we can still use the EM algorithm to solve the problem of parameter estimation although the number of parameters to be estimated would be significantly larger than PLSA  Thus theoretically speaking CPLSA can allow us to discover any contextspecific topics and topic coverage distri butions.
2004 26	 2004  We have 30 articles on the Iraq war and 26 articles on the Afghanistan war.
What’s shown second rows right Nations view topic topic respectively The results War involved Alliance.	 What’s more interesting however is the two cells of word distributions shown on the second and third rows in the first column right below the cell about the United Nations  These two cells show the Iraqspecific view of the topic about the United Nations and the Afghanistan view of the same topic respectively  The results show that in the Iraq War the United Nations was more involved in weapon inspections whereas in the Afghanistan War it was more involved in perhaps aid to the Northern Alliance.
results Nations Afghanistan views Nations variations detailed way useful specific text collection e.	 The results show that in the Iraq War the United Nations was more involved in weapon inspections whereas in the Afghanistan War it was more involved in perhaps aid to the Northern Alliance  These two contextspecific views of the topic of the United Nations show different variations of the topic in the two wars and reveal a more detailed understanding of topics in a contextspecific way  This table is not only immediately useful for understanding the major topics and their variations in these two sets of news articles but can also serve as entry points to facilitate browsing into very specific topics in the text collection e.
It CPLSA common As case collectionspecific column different contexts	 It also again confirms that CPLSA is able to extract meaningful common topics  As in the case of the first column the collectionspecific topics in the third column also further show the variations of the topic in these two different contexts  In Figure 19.
blog articles yaxis	7 we show the temporal trends of topics discovered from blog articles about Hurricane Katrina  The xaxis is the time and the yaxis is the coverage of a topic.
time enabled parameters CPLSA Figure	 The xaxis is the time and the yaxis is the coverage of a topic  The plots are enabled directly by the parameters of CPLSA where we used time and location as context  In Figure 19.
The plot shows trends	 The top plot shows the temporal trends of two topics.
period Rita triggered flooding price.	 This turns out to be the time period when another hurricane Hurricane Rita hit the region which apparently triggered more discussion about the flooding of the city but not the discussion of oil price.
	S.
Text Conference 1992 annual sponsored	 One is the launch of the Text and Retrieval Conference TREC around 1992 a major annual evaluation effort sponsored by the U S.
The TREC study retrieval models model TREC study retrieval connected tasks years.	 The results on the top show that before TREC the study of retrieval models was mostly on the vector space model and Boolean models but after TREC the study of retrieval models apparently explored a variety application tasks e g  XML retrieval email retrieval and subtopic retrieval which are connected to some tasks introduced in TREC over the years.
19 Topic Analysis Context network context.	 19 4 Topic Analysis with Social Networks as Context In this section we discuss how to mine text data with a social network as context.
guide analyzing text characterize content	 Such heuristics can be used to guide us in analyzing topics  Second text can also help characterize the content associated with each subnetwork.
For example difference opinions expressed revealed analysis We analysis network called network supervised The general idea illustrated	 For example the difference between the opinions expressed in two subnetworks can be revealed by doing this type of joint analysis  We now introduce a specific technique for analysis of text with network as context called a network supervised topic model  The general idea of such a model is illustrated in Figure 19.
technique analysis context called network	 We now introduce a specific technique for analysis of text with network as context called a network supervised topic model.
view model e.	 First we can view any generative model e.
mining text estimated values regarded algorithm model.	 From the perspective of mining text data the estimated parameter values ∗ can be regarded as the output of the mining algorithm based on the model.
performing joint analysis context use impose parameters text nodes assumed cover similar topics.	 Following this thinking the main idea of performing joint analysis of text and associated network context is to use the network to impose some constraints on the model parameters  For example the text at adjacent nodes of the network can be assumed to cover similar topics.
likelihood function	 We add a networkinduced regularizer to the likelihood objective function as shown in Figure 19.
Thus view network parameters view new optimization problem conceptually Bayesian defined prior parameters Note idea regularizing probabilistic model text guage network connects objects analyze regularizer heuristics particular cation combination multiple	 Thus we may also view the impact of the network context as imposing some prior on the model parameters if we view the new optimization problem conceptually as Bayesian inference of parameter values even though we do not have any explicitly defined prior distribution of parameters  Note that such an idea of regularizing likelihood function is quite general in deed the probabilistic model can be any generative model for text such as a lan guage model and the network can be also any network or graph that connects the text objects that we hope to analyze  The regularizer can also be any regularizer that we would like to use to capture different heuristics suitable for a particular appli cation it may even be a combination of multiple regularizers.
Note idea regularizing likelihood deed model generative model network graph connects text objects hope The like use heuristics suitable cation combination	 Note that such an idea of regularizing likelihood function is quite general in deed the probabilistic model can be any generative model for text such as a lan guage model and the network can be also any network or graph that connects the text objects that we hope to analyze  The regularizer can also be any regularizer that we would like to use to capture different heuristics suitable for a particular appli cation it may even be a combination of multiple regularizers.
specify separate based context making constrained optimization problem lies specific problem remain tractable.	 Another variation is to specify separate constraints that must be satisfied based on network context making a constrained optimization problem  Although the idea is quite general in practice the challenge often lies in how to instantiate such a general idea with specific regularizers so as to make the op timization problem remain tractable.
Below specific NetPLSA 11 network context heuristic network topic	 Below we introduce a specific instantiation called NetPLSA shown in Figure 19 11 which is an extension of PLSA to incor porate network context by implementing the heuristic that the neighbors on the network must have similar topic distributions  As shown in Figure 19.
Clearly regularizer main constraint difference selection probabilities strongly prefers pθj u values term v network edges	 Clearly if λ  0 the model reduces to the standard PLSA  In the regularizer we see that the main constraint is the square loss defined on the difference of the topic selection probabilities of the two neighboring nodes u and v ∑k j1pθj  u − pθj  v2 which strongly prefers to give pθj  u and pθj  v similar values  In front of this regularization term we see a weight wu v which is based on our network context where the edges may be weighted.
In weight edges weighted.	 In front of this regularization term we see a weight wu v which is based on our network context where the edges may be weighted.
As topics coherent represented collaboration results useful characterizing associated collaboration.	 As a result the topics would be more coherent and also better correspond to the communities represented by subnetworks of collaboration  These results are also useful for characterizing the content associated with each subnetwork of collaboration.
These results useful characterizing general text treat text living network associate structures	 These results are also useful for characterizing the content associated with each subnetwork of collaboration  Taking a more general view of text mining in the context of networks we can treat text data as living in a rich information network environment  That is we can connect all the related data together as a big network and associate text data with various structures in the network.
market time companion text news	 For example we might have observed a sudden drop in prices on the stock market in a particular time period and would like to see if the companion text data such as news might help explain what happened.
related time potentially illustrated	 Here we use the term causal in a non rigorous way to refer to any topic that might be related to the time series and thus can be potentially causal  This analysis task is illustrated in Figure 19.
potentially explain series better series.	 We call these topics causal topics since they can potentially explain the cause of fluctuation of the time series and offer insights for humans to further analyze the topics for better understanding of the time series.
topic topics explain text causal topics discovered meaning ful coherent regular To solve natural CPLSA text number topics coverage time.	 In regular topic modeling our goal is to discover topics that best explain the content in the text data but in our setup of discovering causal topics the topics to be discovered should not only be semantically meaning ful and coherent as in the case of regular topic modeling but also be correlated with the external time series  To solve this problem a natural idea is to apply a model such as CPLSA to our text stream so as to discover a number of topics along with their coverage over time.
Thus topics discovered time previous generation repeat analyze words correlated topics set prior.	 Thus we can expect the topics discovered by the topic model in the next iteration to be more correlated with the time series than the original topics discovered from the previous iteration  Once we discover a new generation of topics we can repeat the process to analyze the words in correlated topics and generate another set of seed topics which would then be fed into the topic model again as prior.
correlation common range −1 1 sign value orientation case causal measure significance	 Pearson correlation is one of the most common methods used to measure the correlation between two variables  It gives us a correlation value in the range of −1 1 and the sign of the output value indicates the orientation of the correlation which we will exploit to quantify the impact in the case of a causal relation  We can also measure the significance of the correlation value.
It correlation value range 1 output quantify relation correlation value.	 It gives us a correlation value in the range of −1 1 and the sign of the output value indicates the orientation of the correlation which we will exploit to quantify the impact in the case of a causal relation  We can also measure the significance of the correlation value.
	   .
.	 .
perform F evaluate x significant data Granger test causality.	1 We then perform an F test to evaluate if retaining or removing the lagged x terms would make a statistically significant difference in fitting the data  Because the Granger test is essentially an F test it naturally gives us a significance value of causality.
We impact y terms coefficients ∑p bi value.	 We can estimate the impact of x on y based on the coefficients of the xi terms for example we can take the average of the xi term coefficients ∑p i1 bi p use it as an “impact value.
topics discovered news data different stock time series 17.	 First we show a sample of causal topics discovered from a news data set when using two different stock time series as context in Figure 19 17.
17 Times articles time June December 2011.	17  The text data set here is the New York Times news articles in the time period of June 2000 through December 2011.
news set discover topics American like Amer Airlines corresponding	 If we are to use a topic model to mine the news data set to discover topics we would obtain topics that are neutral to both American Airlines and Apple  We would like to see whether we can discover biased topics toward either Amer ican Airlines or Apple when we use their corresponding time series as context.
topics points look causal topics series clearly role humans play discovery topics predictive	 These topics can serve as entry points for analysts to further look into the details for any potential causal relations between topics and time series  These results also clearly suggest the important role that humans must play in any real application of causal topic discovery but these topics can be immediately used as features in a predictive model for predicting stock prices.
These results humans real topic topics model stock topics features simple features ngrams collection	 These results also clearly suggest the important role that humans must play in any real application of causal topic discovery but these topics can be immediately used as features in a predictive model for predicting stock prices  It is reasonable to assume that some of these topics would make better features than simple features such as ngrams or ordinary topics discovered from the collection without considering the time series context.
The shown words significant campaign partly use context	 The results shown here are the top three words from the most significant causal topics from New York Times  Intuitively they are indeed quite related to the campaign  The high relevance of topics discovered is at least partly due to the use of the presidential candidate names as an additional context i.
e helped text data.	e  as filters which helped eliminate a lot of non relevant text data.
The Mei 2009 discussion text mining explorations notably modeling.	 Bibliographic Notes and Further Reading The dissertation Mei 2009 has an excellent discussion of contextual text mining with many specific explorations of using context for text analysis notably with topic modeling.
Specifically probabilistic Mei Zhai 2006 context	 Specifically both the contextual probabilistic latent semantic analysis model Mei and Zhai 2006 and topic modeling with network as context Mei et al.
companion structured information A algorithms Sun Han Interactive analysis data users “text hierarchical al.	 Text and the companion structured data can often be generally mod eled as an information network  A systematic discussion of algorithms for analyzing information networks can be found in Sun and Han 2012  Interactive joint analysis of text and structured data can also be supported by combining the traditional On line Analytical Processing OLAP techniques with topic modeling to enable users to drilldown and rollup in the “text dimension” using a hierarchical topic structure as described in Zhang et al.
A discussion information 2012.	 A systematic discussion of algorithms for analyzing information networks can be found in Sun and Han 2012.
20Toward A Unified previous chapters algorithms techniques managing analyzing	 20Toward A Unified System for Text Management and Analysis In the previous chapters we introduced many specific algorithms and techniques for managing and analyzing text data.
From high plication access text analysis data access useful text relevant appli support search current querying help users relevant documents.	 From a user’s perspective we distinguished at a very high level two related ap plication tasks text data access and text analysis  The goal of text data access is to enable users to identify and obtain the most useful text data relevant to an appli cation problem which is often achieved through the support of a search engine  The current search engines primarily support querying which is however only one way to help users find relevant documents.
achieved text sis In browsing search support recommendation multimode information access mode browsing recom	 The support of browsing is generally achieved by adding structures to text data by using clustering categorization or topic analy sis  In addition to supporting querying and browsing a search engine can also support recommendation of information thus providing multimode information access through both pull mode querying and browsing and push mode recom mendation.
In supporting support recommendation multimode pull querying mode text data meant consumed current techniques ers understanding necessary involve Analysis humans sense text management requiring analysts text data optimize	 In addition to supporting querying and browsing a search engine can also support recommendation of information thus providing multimode information access through both pull mode querying and browsing and push mode recom mendation  Since text data are created by humans and often meant to be consumed by humans—and the current NLP techniques are still not mature enough for comput ers to have accurate understanding of text data—it is generally necessary to involve 446 Chapter 20 Toward A Unified System for Text Management and Analysis humans in any text data application  In this sense a text data management and analysis system should serve as an intelligent assistant for users requiring informa tion or the analysts that would like to leverage text data for intelligence to optimize decision making.
Thus optimize ma chines This means computer’s handle exploiting understanding assessing knowledge inter active process “dialogue” text information generally beneficial assist goal	 Thus it is quite important to optimize the collaboration of humans and ma chines  This means that we should take advantage of a computer’s ability to handle large amounts of text data while exploiting humans’ expertise in detailed language understanding and assessing knowledge for decision making  Supporting an inter active process to engage users in a “dialogue” with the intelligent text information system is generally beneficial as it enables the users and system to have more com munications between each other and assist each other to work together toward accomplishing the common goal of solving a user’s problem by analyzing text data.
search perform 1.	 This is the current scenario when people use a search engine to perform mostly manual text mining as shown in Figure 20 1.
However	 However the system i.
Yet possibility relatively text identify particular predic ensure text analysis provide text analysis operators different prob algorithms introduced book general operators inevitably different requirements opera flexibly combined potentially different workflows.	 Yet we can envision the possibility of developing a relatively general text analysis system to help users identify and extract effective features for a particular predic tion task  In order to ensure generality such a text analysis system must provide relatively general text analysis operators that can be applied to many different prob lems most algorithms we introduced in this book are of this kind of nature and can thus be implemented in a system as general operators  However each specific text analysis application would inevitably have different requirements thus the opera tors must also be standardized and compatible with each other so that they can be flexibly combined to support potentially many different workflows.
text provide relatively general operators prob lems algorithms book nature general operators However specific text different opera tors combined different workflows.	 In order to ensure generality such a text analysis system must provide relatively general text analysis operators that can be applied to many different prob lems most algorithms we introduced in this book are of this kind of nature and can thus be implemented in a system as general operators  However each specific text analysis application would inevitably have different requirements thus the opera tors must also be standardized and compatible with each other so that they can be flexibly combined to support potentially many different workflows.
Most algorithms potentially serve analysis tools unified enable perform analysis way Although work	 Most algorithms we introduced in this book can potentially serve as such analysis tools thus providing a basis for developing a unified system to enable analysts to perform text analysis in such a way  Although there are still many challenges in designing and building such a system it is an important goal to work on.
discuss defining analysis	 In this section we discuss some possibilities of defining text analysis operators.
define OBJECTSET case distribution represented ranked list search results	 Based on these types we may also further define a topic as a WEIGHTEDTEXT OBJECTSET where each text object is associated with a numerical weight  As a spe cial case we can have words as text objects and thus have a word distribution represented as a WEIGHTEDTEXTOBJECTSET  WEIGHTEDTEXTOBJECTSEQUENCE can cover a ranked list of search results with scores.
example	 For example in Figure 20.
Union Intersection set operators applied Ranking.	 Union and Intersection  these are standard set operators that can be applied to any sets  Ranking.
Ranking input weighted set specifies spective objects produces order output.	 The Ranking operator takes as input a weighted set of text objects that specifies the per spective of ranking and a set of text objects and it produces a sequence of text objects sorted in order as output.
TEXTOBJECTSET .	 TEXTOBJECTSET .
TEXTOBJECTSET TOPICSET .	      TEXTOBJECTSET → TOPICSET .
→ TOPICSET	    TEXTOBJECTSET → TOPICSET   .
The operators 4 θ denotes vector.	 The formalization of some of the operators is illustrated in Figure 20 4 where θ denotes a weighted word vector.
availability data need manage serve analysis nontext context analyzing data general mining predictive modeling analysts consider mining application.	 In this figure we also show the availability of the nontextual data which would generally need a database system to manage it and serve some other mod ules such as text analysis in this case nontext data can be used as context for analyzing text data  Finally we see that we can often further apply general data mining algorithms such as EM and predictive modeling to process the results that our analysts have obtained  As an example consider a news mining application.
If analysis user finds merged searched keywords.	 If we run a topic analysis on the set of returned articles we can enable browsing the results to allow the user to more efficiently sift through the data  If the user finds two promising clusters they can be merged together and searched again with different keywords.
This user’s information	 This process may be repeated until the user’s information need is satisfied.
For example TEXTOBJECT	 For example   TEXTOBJECT is represented in META as a metacorpusdocument .
run e g.	 For this we can take the output from searching an inverted index and run e g.
algorithms run algorithm.	 The algorithms that run for Select could be a filter or search engine and Split could be metatopicsldamodel or some other clustering algorithm.
powers 1 − powers D θ θa1 − θbθH1 − θT θaH1 θbT	 binomial distribution it would make sense to have the prior also be proportional to the powers of θ and 1 − θ  Thus the posterior will also be proportional to those powers pθ  D ∝ pθpD  θ θa1 − θbθH1 − θT θaH1 − θbT .
distribution ∝ − θb Luckily	 So we need to find some distribution of the form Pθ ∝ θa1 − θb  Luckily there is something called the Beta distribution.
1x	 That is x  x − 1x − 1.
explain − xβ−1 458 Appendix Statistics In constant α As recall probability	 That still doesn’t explain the purpose of that constant in front of xα−11 − xβ−1  458 Appendix A Bayesian Statistics In fact this constant just ensures that given the α and β parameters the Beta distribution still integrates to one over its support  As you probably recall this is a necessity for a probability distribution.
A.	 A.
The T .	 The data we have observed is H  T .
Unlike estimation MLE parameter integrate expected given D “data” hyperparameters α β ∫ 1 θpθ α H α .	 Unlike maximum likeli hood estimation MLE where we have the parameter that maximizes our data we integrate over all possible θ  and find its expected value given the data Eθ  D  In this case our “data” is the flip results and our hyperparameters α and β Eθ  D ∫ 1 0 px  H  θpθ  Ddθ  H  α H  T  α  β .
In β ∫ 0 H Ddθ T .	 In this case our “data” is the flip results and our hyperparameters α and β Eθ  D ∫ 1 0 px  H  θpθ  Ddθ  H  α H  T  α  β .
8 solving integral isn’t focus.	8 We won’t go into detail with solving the integral since that isn’t our focus.
The given likelihood X picking it’s posterior coin case likelihood	 The relationship is such given a likelihood from an X distribution picking the conju gate prior distribution of X say it’s Y  will ensure that the posterior distribution is also a Y distribution  For our coin flipping case the likelihood was a binomial distribution.
reason look estimate prediction.	 If for some reason we believe that the coin may be biased we can incorporate that knowledge as well  If we look at the estimate for θ  we can imagine how setting our hyperparameters can influence our prediction.
single value 0 probability Perhaps Beta	 Even though it’s a single value on the range 0 1 we are still using the prior to produce a probability distribution  Perhaps we’d like to choose a unimodal Beta prior with a mean 0 8.
	 The higher the hyperparameters are the more pseudo counts we have which means our prior is “stronger.
At able rationalize Dirichlet smoothing retrieval	3 Generalizing to a Multinomial Distribution At this point you may be able to rationalize how Dirichlet prior smoothing for information retrieval language models or topic models works.
tails Before figure represent vocabulary For use	 However our proba bilities are over words now not just a binary heads or tails outcome  Before we talk about the Dirichlet distribution let’s figure out how to represent the probability of observing a word from a vocabulary  For this we can use a categorical distribution.
4 The 461 The multinomial distribution	4 The Dirichlet Distribution 461 The categorical distribution is to the multinomial distribution as the Bernoulli is to the binomial.
The function determined distribution comes The conjugate prior multinomial Dirichlet use prior posterior Dirichlet.	4 The Dirichlet Distribution We now have the likelihood function determined for a distribution with k out comes  The conjugate prior to the multinomial is the Dirichlet  That is if we use a Dirichlet prior the posterior will also be a Dirichlet.
positive vectors “simplex” live.	 That is if we use a Dirichlet prior the posterior will also be a Dirichlet  Like the multinomial the Dirichlet is a distribution over positive vectors that sum to one  The “simplex” is the name of the space where these vectors live.
1 2006 When don’t information individual indices value.	1 1 10  From Bishop 2006 When used as a prior we usually don’t have any specific information about the individual indices in the Dirichlet  Because of this we set them all to the same value.
specific information Dirichlet Because set value	 From Bishop 2006 When used as a prior we usually don’t have any specific information about the individual indices in the Dirichlet  Because of this we set them all to the same value  So instead of writing pθ  α where α is e.
0 .	1 0 1 .
1 α alpha θ Dir0 Betaα θ Beta0.	1 we simply say pθ  α where alpha is a scalar representing a vector of identical values  θ ∼ Dirα and θ ∼ Dir0 1 are also commonplace as is θ ∼ Betaα  β or θ ∼ Beta0.
	1.
For θ 33 0.	 For example θ  0 33 0.
34 With it’s likely we’ll like middle unsure kind θ we’ll	34  With α  10 it’s very likely that we’ll get a θ like this  In the middle picture we’re unsure what kind of θ we’ll draw.
middle picture we’re draw equally uneven between.	 In the middle picture we’re unsure what kind of θ we’ll draw  It is equally likely to get an even mixture uneven mixture or anywhere in between.
This prior—it information prior distribution sparse prior means it’s	 This is called a uniform prior—it can represent that we have no explicit information about the prior distribution  Finally the plot on the left is a sparse prior like a Beta where α  β  1  Note a uniform prior does not mean that we get an even mixture of components it means it’s equally likely to get any mixture.
Note prior mixture components means it’s equally likely uniform	 Note a uniform prior does not mean that we get an even mixture of components it means it’s equally likely to get any mixture  This could be confusing since the distribution we draw may actually be a uniform distribution.
A sparse actually relevant application dimensions high probability low sound Zipf’s	 A sparse prior is actually quite relevant in a textual application if we have a few dimensions with very high probability and the rest with relatively low occurrences this should sound just like Zipf’s law.
9	9 0.
	 A.
A 14 Using posterior product prior We constant proportionality multinomial distributions Gammas conjugacy.	 A 14 Using Bayes’ rule we represent the posterior as the product of the likelihood multinomial and prior Dirichlet We say these are proportional because we left out the constant of proportionality in the multinomial and Dirichlet distributions the ratio with Gammas  We can now observe that the posterior is also a Dirichlet as expected due to the conjugacy.
In smoothing Looking imagine case smoothing drew uniform distribution cw 1 V	17 In other words the Dirichlet prior for this smoothing method is proportional to the background collection language model  Looking back at Add1 smoothing we can imagine this as a special case of Dirichlet prior smoothing  If we drew the uniform distribution from our Dirichlet we’d get pw  d  cw d  1 d  V  A.
Once multidimensional dis tributions capable representing individual saw model Dirichlet prior likelihood	 Once we had this foundation we moved onto multidimensional dis tributions capable of representing individual words  We saw how the Betabinomial model is related to the Dirichletmultinomial model and inspected it in the context of Dirichlet prior smoothing for query likelihood in IR.
steps expectation i.	 It then iteratively alternates between two steps called the expectation step i.
” Appendix Expectation Maximization solving initial solution	” There are two commonly used strategies 466 Appendix B Expectation Maximization to solving this problem  The first is that we try many different initial values and choose the solution that has the highest converged likelihood value.
dk	      dk are “generated” from a mixture model with two multinomial component models.
C unknown language model θF	 One component model is the background model pw  C and the other is an unknown topic language model pw  θF  to be estimated.
method likelihood choose estimated given	2 Maximum Likelihood Estimation A common method for estimating θF is the maximum likelihood ML estimator in which we choose a θF that maximizes the likelihood of F   That is the estimated topic model denoted by θ̂F  is given by B 3 Incomplete vs.
3	3 Incomplete vs.
The “incomplete.	 The original data are thus treated as “incomplete.
maximize incomplete data original goal expected complete data likelihood expectation possible incomplete data likelihood variables.	” As we will see we will maximize the incomplete data likelihood our original goal through maximizing the expected complete data likelihood since it is much easier to maximize where expectation is taken over all possible values of the hidden variables since the complete data likelihood unlike our original incomplete data likelihood would contain hidden variables.
general θ X augment variable H 468 B Expectation θpX θ.	 What is the relationship between LcθF  and LθF  In general if our parameter is θ  our original data is X and we augment it with a hidden variable H  then 468 Appendix B Expectation Maximization pX H  θ  pH  X θpX  θ.
estimate parameters Lθ.	 Assuming that the current estimate of the parameters is θn our goal is to find another θn1 that can improve the likelihood Lθ.
left variable KLdivergence pH θ nonnegative.	 Note that the left side of the equation remains the same as the variable H does not occur there  The last term can be recognized as the KLdivergence of pH  X θn and pH  X θ which is always nonnegative.
1.	 1.
This maximize likelihood exist latent introduce likelihood easy maximize.	 This is why the Qfunction which is an expectation of Lcθ is often much easier to maximize than the original likelihood function  In cases when there does not exist a natural latent variable we often introduce a hidden variable so that the complete likelihood function is easy to maximize.
corresponding dij The involves Qfunction complex	 In our model however it only depends on the corresponding word dij   The Mstep involves maximizing the Qfunction  This may sometimes be quite complex as well.
general αd Indeed psw individual smoothing methods choice psw collection language C version cw V estimated	 In general αd may depend on d  Indeed if psw  d is given we must have Thus individual smoothing methods essentially differ in their choice of psw  d  The collection language model pw  C is typically estimated smoothed version cw C1 where V is an estimated vocabulary size e.
given individual smoothing methods essentially differ d The collection pw typically estimated smoothed version C1 e.	 Indeed if psw  d is given we must have Thus individual smoothing methods essentially differ in their choice of psw  d  The collection language model pw  C is typically estimated smoothed version cw C1 where V is an estimated vocabulary size e.
The model estimated smoothed version cw vocabulary size	 The collection language model pw  C is typically estimated smoothed version cw C1 where V is an estimated vocabulary size e g.
One zero probability term terms retrieval performance significant versions shown scheme KLdivergence essentially sides Note sum terms pw occur e.	 One advantage of the smoothed version is that it would never give a zero probability to any term but in terms of retrieval performance there will not be any significant difference in these two versions since It can be shown that with such a smoothing scheme the KLdivergence scoring formula is essentially the two sides are equivalent for ranking documents Note that the scoring is now based on a sum over all the terms that both have a nonzero probability according to pw  θ̂Q and occur in the document i e.
wondering compute This exactly KLdivergence simple query ways The simplest estimate maximum text evidence estimated KLdivergence mula essentially formula presented 2004.	 You may be wondering how we can compute pw  θ̂Q  This is exactly where the KLdivergence retrieval method is better than the simple query likelihood method— we can have different ways of computing it The simplest way is to estimate this probability by the maximum likelihood estimator using the query text as evidence  Using this estimated value you should see easily that the KLdivergence scoring for mula is essentially the same as the query likelihood retrieval formula as presented in Zhai and Lafferty 2004.
mula likelihood formula Zhai 2004 way pw exploit feedback documents Specifically interpolate pmlw θF based	 Using this estimated value you should see easily that the KLdivergence scoring for mula is essentially the same as the query likelihood retrieval formula as presented in Zhai and Lafferty 2004  A more interesting way of computing pw  θ̂Q is to exploit feedback documents  Specifically we can interpolate the simple pmlw  θ̂Q with a feedback model pw θF  estimated based on feedback documents.
exploit	 A more interesting way of computing pw  θ̂Q is to exploit feedback documents.
