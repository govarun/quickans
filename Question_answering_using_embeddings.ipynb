{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0435cb",
   "metadata": {},
   "source": [
    "# Question answering using embeddings-based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3839a6-9146-4f60-b74b-19abbc24278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import openai  # for calling the OpenAI API\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "#from keys import *\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d50792",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"/home/ishikaa2/quickans/ans_generator/data/ir_txt/ir_book_embeddings.csv\"\n",
    "\n",
    "df = pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70307f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert embeddings from CSV str type back to list type\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424162c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChengXiang Zhai Sean Massung Text Data Managem...</td>\n",
       "      <td>[-0.017203252762556076, 0.021963899955153465, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has led to an increasing demand for power...</td>\n",
       "      <td>[-0.016908559948205948, 0.010532873682677746, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike data generated by a computer system or ...</td>\n",
       "      <td>[-0.018540382385253906, 0.014797425828874111, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As such text data are especially valuable for ...</td>\n",
       "      <td>[-0.020149044692516327, 0.02085673063993454, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In contrast to structured data which conform t...</td>\n",
       "      <td>[-0.02140810526907444, 0.02335185930132866, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8045</th>\n",
       "      <td>Please note that this α is different from αd i...</td>\n",
       "      <td>[0.0068044946528971195, 0.035624921321868896, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>Of course the next question is how to estimate...</td>\n",
       "      <td>[0.007543814368546009, 0.032577402889728546, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8047</th>\n",
       "      <td>where F  d1 dk is the set of feedback document...</td>\n",
       "      <td>[-0.016099639236927032, 0.021384460851550102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>Now given λ the feedback documents F  and the ...</td>\n",
       "      <td>[-0.014733465388417244, 0.021942878141999245, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>.</td>\n",
       "      <td>[-0.008761508390307426, -0.016880812123417854,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8050 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     ChengXiang Zhai Sean Massung Text Data Managem...   \n",
       "1     This has led to an increasing demand for power...   \n",
       "2     Unlike data generated by a computer system or ...   \n",
       "3     As such text data are especially valuable for ...   \n",
       "4     In contrast to structured data which conform t...   \n",
       "...                                                 ...   \n",
       "8045  Please note that this α is different from αd i...   \n",
       "8046  Of course the next question is how to estimate...   \n",
       "8047  where F  d1 dk is the set of feedback document...   \n",
       "8048  Now given λ the feedback documents F  and the ...   \n",
       "8049                                                  .   \n",
       "\n",
       "                                              embedding  \n",
       "0     [-0.017203252762556076, 0.021963899955153465, ...  \n",
       "1     [-0.016908559948205948, 0.010532873682677746, ...  \n",
       "2     [-0.018540382385253906, 0.014797425828874111, ...  \n",
       "3     [-0.020149044692516327, 0.02085673063993454, 0...  \n",
       "4     [-0.02140810526907444, 0.02335185930132866, 0....  \n",
       "...                                                 ...  \n",
       "8045  [0.0068044946528971195, 0.035624921321868896, ...  \n",
       "8046  [0.007543814368546009, 0.032577402889728546, -...  \n",
       "8047  [-0.016099639236927032, 0.021384460851550102, ...  \n",
       "8048  [-0.014733465388417244, 0.021942878141999245, ...  \n",
       "8049  [-0.008761508390307426, -0.016880812123417854,...  \n",
       "\n",
       "[8050 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataframe has two columns: \"text\" and \"embedding\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a8c713-c8a9-47dc-85a4-871ee1395566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    "):\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da034bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8846211552296851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.  We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied.  Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8745729164337919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We’ve already described all indexing steps except crawling in detail in Chapter 8.  After our crawling discussion we move onto the particular challenges of web indexing.  Then we discuss how we can take advantage of links between pages in link analysis.  The last technique we discuss is learning to rank which is a way to combine many different features for ranking.  10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8734615622245382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.  We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied.  Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with.  One issue is robustness What if the server doesn’t respond or returns unparseable garbage What if there’s a trap that generates dynamically generated pages that attract your crawler to keep crawling the same site in circles Yet another issue is that we don’t want to overload one particular server with too many crawling requests.  Those may cause the site to experience a denial of service some sites will also block IP addresses that they believe to be crawling them or creating too many requests.  In a similar vein a crawler should respect the robot exclusion protocol.  A file called robots.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8728198850509681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the next section we will discuss crawling.  We’ve already described all indexing steps except crawling in detail in Chapter 8.  After our crawling discussion we move onto the particular challenges of web indexing.  Then we discuss how we can take advantage of links between pages in link analysis.  The last technique we discuss is learning to rank which is a way to combine many different features for ranking.  10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8724750641382434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'One interesting variation is called focused crawling.  Here we’re going to crawl some pages about a particular topic eg all pages about automobiles.  This is typically going to start with a query that you use to get some results.  Then you gradually crawl more.  An even more extreme version of focused crawling is for example downloading and indexing all forum posts on a particular forum.  In this case we might have a URL such as httpwww. textdatabookforum. comboardsid3 which refers to the third post on the forum.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examples\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"Web Crawling\", df, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f45cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = \"Suppose you are a teaching assistant for the course Advanced Information Retrieval and a student has posed the following question.\\n\"\n",
    "    question = f\"\\n\\nQuestion: {query}\\n\\n\"\n",
    "    end = 'How will you answer the question? \\n Here are some snippets from the course textbook which may be useful.\\n\\n\"'\n",
    "    book_info = \"\"\n",
    "    \n",
    "    preface = introduction + question + end\n",
    "    for string in strings:\n",
    "        next_article = f'\\n{string}\\n'\n",
    "        if (\n",
    "            len(preface + book_info + next_article) > 2500\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            book_info += next_article\n",
    "    return preface + book_info\n",
    "\n",
    "def api_call(message, model: str = GPT_MODEL):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    \n",
    "    reply = api_call(message, model)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'When modeling queries, how is multi-bernoulli different from multinomial?'\n",
    "\n",
    "api_call(query)\n",
    "# ask('When modeling queries, how is multi-bernoulli different from multinomial?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3cdb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query):\n",
    "    emb_reply = ask(query)\n",
    "    #emb_reply = 'emb_reply'\n",
    "    base_reply = api_call(query)\n",
    "    #base_reply = 'base_reply'\n",
    "    \n",
    "\n",
    "    print(f'Query:{query}\\n\\n')\n",
    "    print(\"Embedding Method response\\n\")\n",
    "    print(emb_reply+\"\\n\\n\")\n",
    "    print('Base GPT response\\n')\n",
    "    print(base_reply)\n",
    "\n",
    "    return emb_reply, base_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do you define a reference language model?'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefa8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How can we use language models for part of speech tagging?'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What happens to the Dirichlet smoothing when the document langeth goes to infinity'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25f56ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['What order should you evaluate the kl divergence?'] #, \n",
    "'''queries = ['Why do we need a background language model?', \n",
    "           'How does a tokenizer work?', \n",
    "           'What are the components of an inverted index?', \n",
    "           'What is the document filtering problem?', \n",
    "           'What are the aspects of a search engine?', \n",
    "           'For computing the f1 score, why can\\'t we take the mean of precision and recall?',\n",
    "           'Why can’t we use IR models for ranking of webpages?',\n",
    "           'How do you evaluate a filtering system?',\n",
    "           'What are the components in context-based filtering?',\n",
    "           'What is the exploration-exploitation tradeoff?',\n",
    "           'What is beta-gamma threshold learning?',\n",
    "           'What type of word relations are there?',\n",
    "           'What is intrusion detection?',\n",
    "           'What are some cluster similarity measures?']'''\n",
    "\n",
    "response_split = 'RESPONSESPLIT'\n",
    "\n",
    "queries = ['How is Jelinek-Mercer smoothing different from interpolation? The formulae are the same... Is JM a particular type of interpolation? If yes, then what type is it?',\n",
    "           'Going back to the point that KL-divergence is not symmetric unlike Mutual Information, can we think of real-time applications that would prefer an unsymmetric measure over a symmetric measure? That is, are there cases where we intentionally prefer to use unsymmetric measures? It is always useful to improve our understanding of the data, but what are the application level use cases?',\n",
    "           'In the following equation what if the denominator is zero i.e there are no counts for all the various sequences that appear in denominator. How is the probability calculated in this case? Is it a zero as even the numerator will be zero or is it not defined?',\n",
    "           'Can blind feedback be applied repeatedly? Basically, use the original query model q0 to retrieve top k documents and estimate an updated model q1, then use q1 to again retrieve top k documents and update the model and so on. Can this lead to any improvement over applying the feedback just once?',\n",
    "           'How do you tell if a retrieval function has IDF weighting? Do we basically look for a component that penalizes the frequent use of a term? Or is it when we compare the term against the collection/background probability to balance out the 2 probabilities?',\n",
    "           'Hi, I was wondering about Divergence minimization in the lecture notes. On line 3, why lambda * H(theta ; theta_C) can be joined into the summation? Isn\\'t it originally outside of the summation?',\n",
    "           'Is there any difference between two quantities being proportional and being rank equivalent? I thought two quantities have to be proportional in order for them to be rank equivalent. But, I vaugely remember the Prof. mentioning there was some difference. I may be wrong. Pls advise/share your thoughts.',\n",
    "           'Dirichlet prior smoothing. I can write the equation of score after multiply doucument k times. But how can we infer from this equation whether the score will increase or decrease? Since we don\\'t know the influence of k to each part of the equation.',\n",
    "           'In this picture, if w occurs very frequently in background model U, the p(w|U) will be large, right? I thought it should be if w occurs frequently then p(w|U) will be smaller so we can do the penalty. But here seems the opposite. Can you explain how we penalize the frequent word in detail?',\n",
    "           'Hello, this is a dumb question but I was wondering how the log-likelihood value is calculated here:',\n",
    "           'If we set a high value of  p ( θ B ) p(θ  B ​   )  (say, 0.9) in the above formula in Slide 32 of Mixture Language Models, we can get  p ( ‘ ‘ t e x t \" ∣ θ d ) = 0.4 1 − 0.9 + 0.1 = 0.4 0.1 + 0.1 = 4 + 0.1 = 4.1 > 1 ? p(‘‘text\"∣θ  d ​   )=  1−0.9 0.4 ​   +0.1=  0.1 0.4 ​   +0.1=4+0.1=4.1>1?  Can anyone please help me understand something I may be missing? Thanks']\n",
    "\n",
    "f = open('ground_truth_responses.txt', 'r')\n",
    "grouth_truth = f.read().split('\\n')\n",
    "\n",
    "import time\n",
    "if False:\n",
    "    for i in range(len(queries)):\n",
    "        q = queries[i]\n",
    "        print(q)\n",
    "        emb, base = compare_responses(q)\n",
    "        emb = ' '.join(emb.splitlines())\n",
    "        base = ' '.join(base.splitlines())\n",
    "        f = open('responses.csv', 'a')\n",
    "        f.write(f'{emb}{response_split}{base}{response_split}{grouth_truth[i+3]}\\n')\n",
    "        f.flush()\n",
    "        f.close()\n",
    "        time.sleep(60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b903bc91",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "533a0645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jelinek-Mercer smoothing is a specific type of linear interpolation with a fixed mixing coefficient. The formulae for both methods may look similar, but the key difference lies in how they estimate the probability of a term in a document. Jelinek-Mercer smoothing uses a fixed mixing coefficient to balance the contribution of the document language model and the collection language model, while interpolation uses a weighting scheme to combine the two models. Therefore, we can say that Jelinek-Mercer smoothing is a particular type of interpolation, but with a fixed mixing coefficient.RESPONSESPLITJelinek-Mercer smoothing and interpolation are both techniques used in language modeling to estimate the probability of a word given its context. However, they differ in the way they combine the probabilities of different n-grams.  Interpolation involves combining the probabilities of different n-grams using a weighted sum, where the weights are determined by the frequency of the n-gram in the training data. In contrast, Jelinek-Mercer smoothing involves weighting the probabilities of different n-grams based on their similarity to the context in which they occur.  Therefore, Jelinek-Mercer smoothing is a type of smoothing technique that can be used in conjunction with interpolation to improve the accuracy of language models. It is not a type of interpolation itself.RESPONSESPLITThere is really no difference. Yes, JM is a particular type of interpolation where the two functions to be interpolated are the ML estimate and a reference (background) language model and the coefficient is a fixed constant. You may notice that the Dirichlet prior smoothing is also a kind of interpolation, but there the coefficient is dynamic since its value depends on the sample size (the number of observed words).\\nThank you for your question. Yes, there are cases where we intentionally prefer to use unsymmetric measures over symmetric measures. One such case is in machine learning, specifically in unsupervised learning. In unsupervised learning, we do not have labeled data, and we are trying to find patterns or structure in the data. In this case, we may use KL-divergence as an unsymmetric measure to compare the similarity between two probability distributions. For example, we may use KL-divergence to measure the difference between the distribution of words in two documents. This can help us identify which documents are more similar to each other based on their word usage. Another example is in anomaly detection, where we may use KL-divergence to compare the distribution of normal data to the distribution of anomalous data. In this case, we are interested in detecting deviations from the normal distribution, and KL-divergence can help us identify these deviations. Overall, unsymmetric measures like KL-divergence can be useful in cases where we are interested in measuring the difference or distance between two probability distributions, and where the direction of the difference matters.RESPONSESPLITYes, there are cases where unsymmetric measures are preferred over symmetric measures. Here are a few examples:  1. Anomaly detection: In anomaly detection, we are interested in identifying unusual or rare events in a dataset. In this case, we may want to use an unsymmetric measure like KL-divergence to compare the distribution of the data with a reference distribution. This is because we may be more interested in detecting deviations from the reference distribution than in measuring the similarity between the two distributions.  2. Classification: In classification tasks, we may want to use an unsymmetric measure like cross-entropy loss to train a model. This is because the goal of classification is to minimize the error rate, which is not symmetric. For example, misclassifying a positive example as negative may be more costly than misclassifying a negative example as positive.  3. Recommendation systems: In recommendation systems, we may want to use an unsymmetric measure like personalized KL-divergence to measure the similarity between a user\\'s preferences and the preferences of other users or items. This is because users may have different preferences and we may want to give more weight to the preferences of users who are more similar to the target user.  Overall, the choice of a symmetric or unsymmetric measure depends on the specific application and the goals of the analysis. In some cases, a symmetric measure like mutual information may be more appropriate, while in other cases, an unsymmetric measure like KL-divergence may be more useful.RESPONSESPLITThis is an excellent question! There are many application scenarios where one order makes more sense than the other order. You\\'ll see such an example later in the lectures on the KL-divergence retrieval model (using KL-divergence to score documents w.r.t. a query). There, you\\'ll see it makes more sense to compute D(q||d) than D(d||q) where q and d are the (unigram) language models for the query and document, respectively.\\nThank you for your question. In the equation you provided, if the denominator is zero, it means that there are no counts for all the various sequences that appear in the denominator. In this case, we cannot calculate the probability using the equation as is. However, we can use smoothing techniques to avoid getting a zero probability. As discussed in the textbook, we can add a small constant to the counts so that we don\\'t get zero probability in any case. This is called smoothing. Smoothing for this application is displayed in Figure 13.15 of the textbook. We pretend to observe pseudosegments that would contribute additional counts of these words so that no event will have zero probability. In particular for this example, we introduce four pseudosegments. Each is weighted at 1/4. Once we have these counts, we can just normalize these counts by N, which is the total number of segments, giving us the probabilities that we need to compute mutual information. I hope this helps! Let me know if you have any further questions.RESPONSESPLITIf the denominator is zero, then the probability is undefined. This is because the probability is calculated by dividing the numerator by the denominator, and division by zero is undefined. In other words, if there are no counts for all the various sequences that appear in the denominator, then the probability cannot be calculated.RESPONSESPLITThat\\'s a good question. One solution is to do smoothing. Essentially, we can try to drop some words in the condition. In the extreme case, we\\'d drop all the words in the condition, i.e., we\\'ll have a unigram LM p(wm).\\nBased on the information provided in the course textbook, it is possible to apply blind feedback repeatedly by using the top k documents from the previous iteration to update the query model and retrieve a new set of top k documents. This process can be repeated multiple times to further refine the query model and improve the search results. However, it is important to note that there may be diminishing returns with each iteration, and it is possible to overfit the query model to the feedback data. Therefore, it is important to carefully evaluate the effectiveness of blind feedback and determine the optimal number of iterations to use.RESPONSESPLITYes, blind feedback can be applied repeatedly. This process is known as iterative feedback. In iterative feedback, the original query model is used to retrieve the top k documents, and then the user provides feedback on these documents. The feedback is then used to update the query model, and the process is repeated until the user is satisfied with the results.  Iterative feedback can lead to improvements over applying feedback just once. By using the feedback to update the query model and then retrieving the top k documents again, the system can learn from the user\\'s feedback and provide more relevant results. This process can be repeated multiple times, leading to further improvements in the results. However, it is important to note that iterative feedback can also lead to overfitting if the feedback is too specific to a particular set of documents. Therefore, it is important to balance the amount of feedback provided with the need for generalizability in the results.RESPONSESPLITGreat question & great idea! The short answer to your question is that it depends on the specific query and the accuracy of the initial retrieval algorithm. If the initial retrieval results are reasonably accurate (e.g., precision at 5 documents is at least 0.5), then we would expect the blind/pseudo feedback to help (if we use, say, top 5 documents for feedback). In such a case, it\\'s quite possible that the new results from the blind feedback would be even more accurate than the initial results, and thus we can expect a second round of blind feedback would further improve performance. However, if the initial results are not so accurate, doing it a second time might hurt the performance (causing the query concept to drift too much from the original query). Indeed, even doing it once may hurt the performance if the initial results aren\\'t accurate. How to intelligently determine for which query blind feedback should be done and how many iterations is mostly an open research question. It\\'s an excellent topic for a course project!\\nTo determine if a retrieval function has IDF weighting, we need to look for a component that balances out the frequency of a term in a document with its frequency in the collection or background. This is achieved through the use of IDF (Inverse Document Frequency) weighting, which penalizes popular terms that occur frequently in the collection. The IDF function is designed to favor terms that are fairly frequent but not too frequent, and it drops off at a point where the term is essentially not very useful. The specific function used for IDF weighting is not as important as the heuristic it captures, which is penalizing popular terms. It is an open research question whether there is a better form of the IDF function, but different instantiations can be tested using evaluation skills learned in Chapter 9 of the textbook.RESPONSESPLITTo determine if a retrieval function has IDF weighting, you can look for a component that penalizes the frequent use of a term. IDF (Inverse Document Frequency) is a measure of how rare a term is in a collection of documents. It is calculated by taking the logarithm of the ratio of the total number of documents in the collection to the number of documents that contain the term. A retrieval function that uses IDF weighting will give more weight to terms that are rare in the collection and less weight to terms that are common. This is because rare terms are more informative and can help to distinguish between documents, while common terms are less informative and may not be as useful for retrieval. Another way to identify IDF weighting is to look for a component that compares the term against the collection/background probability to balance out the two probabilities.RESPONSESPLITSince IDF is primarily to formulate against the boost in score caused by matching a frequent term, the first statement should be correct. Let\\'s say, a term is more frequent in most of the documents. Then, the value/impact of matching that term is not as much as matching another term that is rare. The match does not necessarily prove the relevance of any doc to the query. Therefore, any model that attempts to decrease the impact of matching frequent terms can be regarded as having an IDF weighting.\\nDear student,  Thank you for your question regarding the lecture notes on Divergence minimization. I understand your confusion about why lambda * H(theta ; theta_C) can be joined into the summation.   To answer your question, let me provide some context. In the lecture notes, we are discussing the Expectation-Maximization (EM) algorithm for estimating the parameters of a probabilistic model. The algorithm involves maximizing a lower bound on the likelihood function, which is derived using the Kullback-Leibler (KL) divergence.  In the derivation of the lower bound, we have the following equation:  L(θ,θ_C) = ∑_i ∑_z p(z|d_i,θ_C) log p(d_i,z|θ) - λ D(p(θ)||p(θ_C))  where L(θ,θ_C) is the lower bound, θ is the parameter vector we want to estimate, θ_C is a fixed reference parameter vector, p(z|d_i,θ_C) is the posterior distribution over latent variables z given document d_i and θ_C, and D(p(θ)||p(θ_C)) is the KL divergence between the prior distribution p(θ) and the reference distribution p(θ_C).  Now, to answer your question, lambda * H(theta ; theta_C) is actually the negative of the KL divergence term, i.e., lambda * H(theta ; theta_C) = -λ D(p(θ)||p(θ_C)). Therefore, we can rewrite the lower bound as:  L(θ,θ_C) = ∑_i ∑_z p(z|d_i,θ_C) log p(d_i,z|θ) + lambda * H(theta ; theta_C)  And since lambda * H(theta ; theta_C) is a constant with respect to the summation over documents and latent variables, we can join it into the summation and treat it as part of the lower bound.  I hope this explanation helps clarify your question. Please let me know if you have any further doubts or concerns.  Best regards, [Your Name]RESPONSESPLITAs an AI language model, I do not have access to the specific lecture notes you are referring to. However, in general, if we have a summation of terms and a constant factor outside the summation, we can distribute the constant factor across all the terms in the summation. This is a basic property of summation. Therefore, lambda * H(theta ; theta_C) can be joined into the summation.RESPONSESPLITIt\\'s because in line 3, -lambda*H(theta;theta_C) is first summed from i=1 to n, then multiplied by 1/n, thus canceled out.\\nProportional and rank equivalent are two different concepts. Two quantities are proportional if they have a constant ratio between them. For example, if the price of a product is proportional to its weight, then the price of a 2-pound product is twice the price of a 1-pound product. On the other hand, two quantities are rank equivalent if they have the same order or rank when compared to other quantities. For example, if two students have the same rank in a class, then they have the same position in the class when ordered by their grades.  In information retrieval, we use ranking functions to order documents based on their relevance to a query. The vector space model and probabilistic retrieval models are two popular approaches to designing ranking functions. In the vector space model, the ranking function is based on the similarity between the query and the document. In probabilistic models, the ranking function is based on the probability that a document is relevant to a query.  In both models, the ranking function produces a score for each document, which can be used to rank the documents. The scores are not necessarily proportional to each other, but they are rank equivalent. That is, the order of the documents based on their scores is the same as the order of the documents based on their relevance to the query.  In summary, two quantities can be rank equivalent without being proportional. In information retrieval, the ranking function produces scores that are rank equivalent but not necessarily proportional.RESPONSESPLITThere is a difference between two quantities being proportional and being rank equivalent.   When two quantities are proportional, it means that they have a constant ratio. For example, if the speed of a car is proportional to the distance it travels, then the ratio of speed to distance is constant. This means that if the distance is doubled, the speed will also double.  On the other hand, when two quantities are rank equivalent, it means that they have the same relative position in a set of data. For example, if two students have the same rank in a class based on their test scores, then they are rank equivalent. This means that they have the same level of performance relative to the rest of the class.  While proportional quantities can be rank equivalent, not all rank equivalent quantities are proportional. For example, the heights of two people may be rank equivalent in a group of people, but they may not be proportional to each other.RESPONSESPLITPerhaps some operations may not hold the proportionality rule but it may ensure rank equivalence. For example, ranking by a probability value and its logarithm are rank equivalent since if one increases, the other does too, but they are not proportional. So if two quantities are proportional, perhaps they are indeed rank equivalent, but the other way around is not necessarily true.\\nThank you for your question.   You are correct that it is difficult to infer whether the score will increase or decrease based solely on the equation of score after multiplying document k times. However, we can make some general observations about the influence of k on each part of the equation.   In the Dirichlet prior smoothing method, we use a prior distribution to estimate the probability of a term occurring in the collection. This prior distribution is then combined with the observed frequency of the term in the document to estimate the probability of the term occurring in the document.   When we multiply the document k times, we are essentially increasing the observed frequency of the term in the document. This will have different effects on the different parts of the equation.   For example, increasing the observed frequency of the term in the document will increase the numerator of the equation, which represents the likelihood of the query given the document. This is because the query is more likely to be relevant to the document if the term occurs more frequently in the document.   On the other hand, increasing the observed frequency of the term in the document will also increase the denominator of the equation, which represents the probability of the query under the prior distribution. This is because the prior distribution assumes that the term occurs less frequently in the collection than it actually does in the document.   Overall, it is difficult to predict whether the score will increase or decrease without knowing the specific values of the parameters in the equation. However, we can make some general observations about the influence of k on each part of the equation.   I hope this helps! Let me know if you have any further questions.RESPONSESPLITThe influence of k on each part of the equation can be determined by analyzing the individual terms and their relationship with k. For example, in the case of Dirichlet prior smoothing, the score after multiplying document k times can be written as:  score = (n_k + alpha) / (N + alpha * K) * prod((n_{k,w} + beta_w) / (n_k + V * beta_w))  Here, n_k represents the number of times topic k appears in the corpus, alpha is the hyperparameter for the topic distribution, N is the total number of words in the corpus, K is the total number of topics, n_{k,w} represents the number of times topic k appears with word w, beta_w is the hyperparameter for the word distribution, and V is the total number of unique words in the corpus.  By analyzing each term in this equation, we can determine how k affects the score. For example, the term (n_k + alpha) / (N + alpha * K) represents the probability of choosing topic k from the topic distribution, and we can see that increasing n_k (by multiplying the document k times) will increase the numerator of this term, leading to a higher score. Similarly, the term (n_{k,w} + beta_w) / (n_k + V * beta_w) represents the probability of choosing word w given topic k, and we can see that increasing n_{k,w} (by adding more occurrences of word w in document k) will increase the numerator of this term, leading to a higher score.  Overall, by analyzing the individual terms in the equation, we can infer how increasing k (by multiplying the document k times) will affect the score.RESPONSESPLITThis is an open-ended question and it would involve some algebraic transformation of formulas. Thus, you may not necessarily be able to derive a definitive answer. However, going through this analysis as an exercise can help you analytically assess the quality of a retrieval function (e.g., you might gain insights about when such a function might behave unreasonably). The math part is beyond the scope of this course, but if you are willing to work on it a bit, some simple transformation of the formula would likely provide some insights. In order to check whether the score would be increased or decreased, you may realize that you would like to compare the two scores (the score of the original document and the score of the generated long document by duplicating the document multiple times) in some way, e.g., taking their ratio or difference. In this case, subtracting one from the other would help simplify the analysis. Another way to simplify the problem is to consider special cases that are easier to analyze. For example, you could consider a simple case where there is only one query term or just two terms. Or you can consider a special document where you have only a few words. The conclusion might also depend on other factors such as whether c(w,d)/|d| is larger than p(w|C), etc. In general, just try to do some exploration by looking into special cases and write about whatever conclusion(s) you may have. It\\'s fine to only be able to say in a particular special case, the conclusion is ...., or under condition A, the conclusion would be ..., while under condition B, the conclusion would be different. If you want to know more about this kind of analysis, you can take a look at the following resource page about the axiomatic retrieval framework: https://www.eecis.udel.edu/~hfang/AX.html\\nThank you for your question. Let me explain how we penalize frequent words in detail.  In the context of information retrieval, we use a mixture model to estimate the probability distribution of words in a document. The mixture model consists of two components: the background model and the document model. The background model represents the probability distribution of words in a large corpus of documents, while the document model represents the probability distribution of words in the document we are interested in.  When a word occurs frequently in the background model, its probability of occurrence in the document model, denoted as p(w|θd), will also be high. However, we want to penalize frequent words in the document model to avoid overfitting. This is because frequent words may not be informative for distinguishing between different documents.  To penalize frequent words, we introduce a regularization term, denoted as p(θd), which represents the probability of choosing the document model over the background model. The smaller p(θd) is, the less important it is for the document model to respond to the increase of counts of a word in the data. In other words, the degree of increase of probability due to the increased counts of a word observed in the document is regularized by p(θd) or equivalently p(θB), where θB represents the background model.  To summarize, we penalize frequent words in the document model by introducing a regularization term that controls the degree of increase of probability due to the increased counts of a word observed in the document. This helps to avoid overfitting and improve the performance of information retrieval.RESPONSESPLITAs an AI language model, I do not have access to the picture you are referring to. However, I can explain the concept of penalizing frequent words in language modeling.  In language modeling, the goal is to predict the probability of a sequence of words occurring in a given context. This is typically done using a statistical model that estimates the probability of each word given the previous words in the sequence. One common approach is to use n-gram models, where the probability of a word is estimated based on the frequency of its occurrence in the training data.  However, a problem with this approach is that frequent words tend to dominate the probability estimates, even if they are not very informative in the context. For example, in the sentence \"The cat sat on the mat\", the word \"the\" occurs three times, but it does not provide much information about the meaning of the sentence.  To address this issue, language models often apply a penalty to frequent words, known as \"smoothing\" or \"discounting\". This involves reducing the probability estimate for each word based on its frequency in the training data, so that less frequent words are given more weight in the model.  There are many different smoothing techniques, but one common approach is to use a modified version of the maximum likelihood estimate, such as the Kneser-Ney smoothing or Good-Turing smoothing. These methods adjust the probability estimates based on the frequency of each word and the frequency of its context, so that the model assigns higher probabilities to more informative words.  In summary, penalizing frequent words in language modeling is a way to reduce the impact of uninformative words on the probability estimates, and give more weight to less frequent but more informative words.RESPONSESPLITHere, if w is frequent in the background model, then p(w|C) will be large. Thus, by having high value of lambda, the reason for high probability of w, will be explained by the background model. Thus, the p(w|d) will be almost same for two different documents, while the document that has important keywords which is essentially what we are looking for will be ranked high. For example, if one document has more occurrences of the word \"the\" than the other document, the P(\"the\"|d) for both the documents will almost be same. Hence, this gives out the effect of penalizing the occurrence of frequent word in the document.\\nDear student,  Thank you for your question. The log-likelihood value is calculated by taking the logarithm of the likelihood function. This is done for mathematical convenience, as taking the logarithm transforms the product in the likelihood function into a sum, which makes it easier to take the derivative and find the optimal solution. The sum is taken over all the unique data points, which in this case are the words in the vocabulary.  I hope this helps clarify the concept for you. Please let me know if you have any further questions.  Best regards, [Your Name]RESPONSESPLITTo calculate the log-likelihood value, you first need to have a statistical model that describes the probability distribution of your data. Let\\'s say you have a set of data points {x1, x2, ..., xn} and you assume that they are independent and identically distributed according to a normal distribution with mean μ and variance σ^2. Then, the probability density function (PDF) of each data point xi is:  f(xi | μ, σ^2) = (1 / sqrt(2πσ^2)) * exp(-(xi - μ)^2 / (2σ^2))  The likelihood function is the product of the PDFs of all the data points:  L(μ, σ^2) = f(x1 | μ, σ^2) * f(x2 | μ, σ^2) * ... * f(xn | μ, σ^2)  Taking the logarithm of the likelihood function, we get:  log L(μ, σ^2) = log f(x1 | μ, σ^2) + log f(x2 | μ, σ^2) + ... + log f(xn | μ, σ^2)  Substituting the PDF of the normal distribution, we get:  log L(μ, σ^2) = -n/2 * log(2π) - n/2 * log(σ^2) - 1/(2σ^2) * Σ(xi - μ)^2  The last term in the equation is the negative of the sum of squared errors (SSE) between the data points and the mean of the normal distribution. Maximizing the log-likelihood function is equivalent to minimizing the SSE, which is a common objective in regression analysis.RESPONSESPLITFirst of all, you need to understand that likelihood here refers to the probability of all the observed data. In this toy example, it\\'s the probability of observing a toy document with just four words. Log-likelihood simply refers to logarithm of the likelihood. Please review lectures 5.1 and 5.2\\nThe formula in Slide 32 of Mixture Language Models is used to estimate the probability of a term (text) given a document (θd) and a background model (θB). The question asks about the effect of setting a high value of p(θB) (0.9) in the formula.   Setting a high value of p(θB) means that we are giving more weight to the background model and less weight to the document model. This can be seen in the formula where the probability of text given the document model (θd) is divided by a sum of two terms, one representing the document model and the other representing the background model.   When p(θB) is high, the denominator of the formula becomes smaller, which means that the probability of text given the document model becomes larger. However, this does not mean that the probability becomes greater than 1. The probability of a term given a document model and a background model should always be between 0 and 1.   In summary, setting a high value of p(θB) in the formula can increase the probability of a term given a document model, but it should not result in a probability greater than 1.RESPONSESPLITIt seems that there is a mistake in the formula provided in the slide. The correct formula for calculating the probability of a word given a document in a mixture language model is:  p(\\'text\\' | θd) = ∑B p(\\'text\\' | θB) p(θB | θd)  where θB represents the language model for the background corpus B, and p(θB | θd) is the probability of selecting the background corpus B given the document θd.  If we set a high value of p(θB) (say, 0.9), then we are assuming that the background corpus is very likely to be selected for this document. This means that the contribution of the document-specific language model p(\\'text\\' | θd) will be relatively small compared to the contribution of the background language model p(\\'text\\' | θB).  In this case, we can calculate the probability of \\'text\\' given θd as:  p(\\'text\\' | θd) = p(\\'text\\' | θB) p(θB | θd) + p(\\'text\\' | θd) p(θd | θd)  = 0.1 * 0.9 + 0.4 * 0.1  = 0.13  This value is less than 1, which is the correct result. Therefore, it seems that there was an error in the formula provided in the slide.RESPONSESPLITThat is a great observation. I think the answer lies in the derivation where we said that for  m a x ( x y ) max(xy)  given,  x + y = k x+y=k  occurs at  x = y x=y . The maximum would occur at x = y, but the value of x at which it happens might be unattainable, such as in this case.  Refer to slide# 32 for the terms used below. If we assume, two terms of  p ( d ∣ λ ) , ( 1 − P ( θ B ) ) ∗ p ( “ t e x t ” ∣ θ d ) + P ( θ B ) ∗ 0.1 = x p(d∣λ),(1−P(θ  B \\u200b   ))∗p(“text”∣θ  d \\u200b   )+P(θ  B \\u200b   )∗0.1=x  and  ( 1 − P ( θ B ) ) ∗ p ( “ t h e ” ∣ θ d ) + P ( θ B ) ∗ 0.9 = y (1−P(θ  B \\u200b   ))∗p(“the”∣θ  d \\u200b   )+P(θ  B \\u200b   )∗0.9=y . We know that,  x + y = 1 x+y=1 , and we want to maximize  x y xy . This will happen at  x = y = 1 / 2 x=y=1/2 . And  x y xy  is increasing for  x < 1 / 2 x<1/2 . So, we can take the largest possible value of  x x  less than  1 / 2 1/2 , and the corresponding value of  P ( d ∣ λ ) P(d∣λ)  would be the maximum. Or we could take the smallest possible value of  x x  larger than 1/2 as  x y xy  is decreasing in that region. The range of values of  x x  given  p ( θ B ) = 0.9 p(θ  B \\u200b   )=0.9  is  [ 0.09 , 0.19 ] [0.09,0.19]  for  p ( “ t e x t ” ∣ θ d ) = 0 p(“text”∣θ  d \\u200b   )=0  and  1 1  respectively. Hence, the maximum value of  P ( d ∣ λ ) P(d∣λ)  in this case would be  x ( 1 − x ) = 0.19 ∗ 0.81 = 0.1539 x(1−x)=0.19∗0.81=0.1539 . This value is lesser than what  x ( 1 − x ) x(1−x)  could have reached given no constraints on  x x , i.e.  0.5 ∗ 0.5 = 0.25 0.5∗0.5=0.25'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('responses.csv', 'r')\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "509e7253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jelinek-Mercer smoothing is a specific type of linear interpolation with a fixed mixing coefficient. The formulae for both methods may look similar, but the key difference lies in how they estimate the probability of a term in a document. Jelinek-Mercer smoothing uses a fixed mixing coefficient to balance the contribution of the document language model and the collection language model, while interpolation uses a weighting scheme to combine the two models. Therefore, we can say that Jelinek-Mercer smoothing is a particular type of interpolation, but with a fixed mixing coefficient.RESPONSESPLITJelinek-Mercer smoothing and interpolation are both techniques used in language modeling to estimate the probability of a word given its context. However, they differ in the way they combine the probabilities of different n-grams.  Interpolation involves combining the probabilities of different n-grams using a weighted sum, where the weights are determined by the frequency of the n-gram in the training data. In contrast, Jelinek-Mercer smoothing involves weighting the probabilities of different n-grams based on their similarity to the context in which they occur.  Therefore, Jelinek-Mercer smoothing is a type of smoothing technique that can be used in conjunction with interpolation to improve the accuracy of language models. It is not a type of interpolation itself.RESPONSESPLITThere is really no difference. Yes, JM is a particular type of interpolation where the two functions to be interpolated are the ML estimate and a reference (background) language model and the coefficient is a fixed constant. You may notice that the Dirichlet prior smoothing is also a kind of interpolation, but there the coefficient is dynamic since its value depends on the sample size (the number of observed words).',\n",
       " \"Thank you for your question. Yes, there are cases where we intentionally prefer to use unsymmetric measures over symmetric measures. One such case is in machine learning, specifically in unsupervised learning. In unsupervised learning, we do not have labeled data, and we are trying to find patterns or structure in the data. In this case, we may use KL-divergence as an unsymmetric measure to compare the similarity between two probability distributions. For example, we may use KL-divergence to measure the difference between the distribution of words in two documents. This can help us identify which documents are more similar to each other based on their word usage. Another example is in anomaly detection, where we may use KL-divergence to compare the distribution of normal data to the distribution of anomalous data. In this case, we are interested in detecting deviations from the normal distribution, and KL-divergence can help us identify these deviations. Overall, unsymmetric measures like KL-divergence can be useful in cases where we are interested in measuring the difference or distance between two probability distributions, and where the direction of the difference matters.RESPONSESPLITYes, there are cases where unsymmetric measures are preferred over symmetric measures. Here are a few examples:  1. Anomaly detection: In anomaly detection, we are interested in identifying unusual or rare events in a dataset. In this case, we may want to use an unsymmetric measure like KL-divergence to compare the distribution of the data with a reference distribution. This is because we may be more interested in detecting deviations from the reference distribution than in measuring the similarity between the two distributions.  2. Classification: In classification tasks, we may want to use an unsymmetric measure like cross-entropy loss to train a model. This is because the goal of classification is to minimize the error rate, which is not symmetric. For example, misclassifying a positive example as negative may be more costly than misclassifying a negative example as positive.  3. Recommendation systems: In recommendation systems, we may want to use an unsymmetric measure like personalized KL-divergence to measure the similarity between a user's preferences and the preferences of other users or items. This is because users may have different preferences and we may want to give more weight to the preferences of users who are more similar to the target user.  Overall, the choice of a symmetric or unsymmetric measure depends on the specific application and the goals of the analysis. In some cases, a symmetric measure like mutual information may be more appropriate, while in other cases, an unsymmetric measure like KL-divergence may be more useful.RESPONSESPLITThis is an excellent question! There are many application scenarios where one order makes more sense than the other order. You'll see such an example later in the lectures on the KL-divergence retrieval model (using KL-divergence to score documents w.r.t. a query). There, you'll see it makes more sense to compute D(q||d) than D(d||q) where q and d are the (unigram) language models for the query and document, respectively.\",\n",
       " \"Thank you for your question. In the equation you provided, if the denominator is zero, it means that there are no counts for all the various sequences that appear in the denominator. In this case, we cannot calculate the probability using the equation as is. However, we can use smoothing techniques to avoid getting a zero probability. As discussed in the textbook, we can add a small constant to the counts so that we don't get zero probability in any case. This is called smoothing. Smoothing for this application is displayed in Figure 13.15 of the textbook. We pretend to observe pseudosegments that would contribute additional counts of these words so that no event will have zero probability. In particular for this example, we introduce four pseudosegments. Each is weighted at 1/4. Once we have these counts, we can just normalize these counts by N, which is the total number of segments, giving us the probabilities that we need to compute mutual information. I hope this helps! Let me know if you have any further questions.RESPONSESPLITIf the denominator is zero, then the probability is undefined. This is because the probability is calculated by dividing the numerator by the denominator, and division by zero is undefined. In other words, if there are no counts for all the various sequences that appear in the denominator, then the probability cannot be calculated.RESPONSESPLITThat's a good question. One solution is to do smoothing. Essentially, we can try to drop some words in the condition. In the extreme case, we'd drop all the words in the condition, i.e., we'll have a unigram LM p(wm).\",\n",
       " \"Based on the information provided in the course textbook, it is possible to apply blind feedback repeatedly by using the top k documents from the previous iteration to update the query model and retrieve a new set of top k documents. This process can be repeated multiple times to further refine the query model and improve the search results. However, it is important to note that there may be diminishing returns with each iteration, and it is possible to overfit the query model to the feedback data. Therefore, it is important to carefully evaluate the effectiveness of blind feedback and determine the optimal number of iterations to use.RESPONSESPLITYes, blind feedback can be applied repeatedly. This process is known as iterative feedback. In iterative feedback, the original query model is used to retrieve the top k documents, and then the user provides feedback on these documents. The feedback is then used to update the query model, and the process is repeated until the user is satisfied with the results.  Iterative feedback can lead to improvements over applying feedback just once. By using the feedback to update the query model and then retrieving the top k documents again, the system can learn from the user's feedback and provide more relevant results. This process can be repeated multiple times, leading to further improvements in the results. However, it is important to note that iterative feedback can also lead to overfitting if the feedback is too specific to a particular set of documents. Therefore, it is important to balance the amount of feedback provided with the need for generalizability in the results.RESPONSESPLITGreat question & great idea! The short answer to your question is that it depends on the specific query and the accuracy of the initial retrieval algorithm. If the initial retrieval results are reasonably accurate (e.g., precision at 5 documents is at least 0.5), then we would expect the blind/pseudo feedback to help (if we use, say, top 5 documents for feedback). In such a case, it's quite possible that the new results from the blind feedback would be even more accurate than the initial results, and thus we can expect a second round of blind feedback would further improve performance. However, if the initial results are not so accurate, doing it a second time might hurt the performance (causing the query concept to drift too much from the original query). Indeed, even doing it once may hurt the performance if the initial results aren't accurate. How to intelligently determine for which query blind feedback should be done and how many iterations is mostly an open research question. It's an excellent topic for a course project!\",\n",
       " \"To determine if a retrieval function has IDF weighting, we need to look for a component that balances out the frequency of a term in a document with its frequency in the collection or background. This is achieved through the use of IDF (Inverse Document Frequency) weighting, which penalizes popular terms that occur frequently in the collection. The IDF function is designed to favor terms that are fairly frequent but not too frequent, and it drops off at a point where the term is essentially not very useful. The specific function used for IDF weighting is not as important as the heuristic it captures, which is penalizing popular terms. It is an open research question whether there is a better form of the IDF function, but different instantiations can be tested using evaluation skills learned in Chapter 9 of the textbook.RESPONSESPLITTo determine if a retrieval function has IDF weighting, you can look for a component that penalizes the frequent use of a term. IDF (Inverse Document Frequency) is a measure of how rare a term is in a collection of documents. It is calculated by taking the logarithm of the ratio of the total number of documents in the collection to the number of documents that contain the term. A retrieval function that uses IDF weighting will give more weight to terms that are rare in the collection and less weight to terms that are common. This is because rare terms are more informative and can help to distinguish between documents, while common terms are less informative and may not be as useful for retrieval. Another way to identify IDF weighting is to look for a component that compares the term against the collection/background probability to balance out the two probabilities.RESPONSESPLITSince IDF is primarily to formulate against the boost in score caused by matching a frequent term, the first statement should be correct. Let's say, a term is more frequent in most of the documents. Then, the value/impact of matching that term is not as much as matching another term that is rare. The match does not necessarily prove the relevance of any doc to the query. Therefore, any model that attempts to decrease the impact of matching frequent terms can be regarded as having an IDF weighting.\",\n",
       " \"Dear student,  Thank you for your question regarding the lecture notes on Divergence minimization. I understand your confusion about why lambda * H(theta ; theta_C) can be joined into the summation.   To answer your question, let me provide some context. In the lecture notes, we are discussing the Expectation-Maximization (EM) algorithm for estimating the parameters of a probabilistic model. The algorithm involves maximizing a lower bound on the likelihood function, which is derived using the Kullback-Leibler (KL) divergence.  In the derivation of the lower bound, we have the following equation:  L(θ,θ_C) = ∑_i ∑_z p(z|d_i,θ_C) log p(d_i,z|θ) - λ D(p(θ)||p(θ_C))  where L(θ,θ_C) is the lower bound, θ is the parameter vector we want to estimate, θ_C is a fixed reference parameter vector, p(z|d_i,θ_C) is the posterior distribution over latent variables z given document d_i and θ_C, and D(p(θ)||p(θ_C)) is the KL divergence between the prior distribution p(θ) and the reference distribution p(θ_C).  Now, to answer your question, lambda * H(theta ; theta_C) is actually the negative of the KL divergence term, i.e., lambda * H(theta ; theta_C) = -λ D(p(θ)||p(θ_C)). Therefore, we can rewrite the lower bound as:  L(θ,θ_C) = ∑_i ∑_z p(z|d_i,θ_C) log p(d_i,z|θ) + lambda * H(theta ; theta_C)  And since lambda * H(theta ; theta_C) is a constant with respect to the summation over documents and latent variables, we can join it into the summation and treat it as part of the lower bound.  I hope this explanation helps clarify your question. Please let me know if you have any further doubts or concerns.  Best regards, [Your Name]RESPONSESPLITAs an AI language model, I do not have access to the specific lecture notes you are referring to. However, in general, if we have a summation of terms and a constant factor outside the summation, we can distribute the constant factor across all the terms in the summation. This is a basic property of summation. Therefore, lambda * H(theta ; theta_C) can be joined into the summation.RESPONSESPLITIt's because in line 3, -lambda*H(theta;theta_C) is first summed from i=1 to n, then multiplied by 1/n, thus canceled out.\",\n",
       " 'Proportional and rank equivalent are two different concepts. Two quantities are proportional if they have a constant ratio between them. For example, if the price of a product is proportional to its weight, then the price of a 2-pound product is twice the price of a 1-pound product. On the other hand, two quantities are rank equivalent if they have the same order or rank when compared to other quantities. For example, if two students have the same rank in a class, then they have the same position in the class when ordered by their grades.  In information retrieval, we use ranking functions to order documents based on their relevance to a query. The vector space model and probabilistic retrieval models are two popular approaches to designing ranking functions. In the vector space model, the ranking function is based on the similarity between the query and the document. In probabilistic models, the ranking function is based on the probability that a document is relevant to a query.  In both models, the ranking function produces a score for each document, which can be used to rank the documents. The scores are not necessarily proportional to each other, but they are rank equivalent. That is, the order of the documents based on their scores is the same as the order of the documents based on their relevance to the query.  In summary, two quantities can be rank equivalent without being proportional. In information retrieval, the ranking function produces scores that are rank equivalent but not necessarily proportional.RESPONSESPLITThere is a difference between two quantities being proportional and being rank equivalent.   When two quantities are proportional, it means that they have a constant ratio. For example, if the speed of a car is proportional to the distance it travels, then the ratio of speed to distance is constant. This means that if the distance is doubled, the speed will also double.  On the other hand, when two quantities are rank equivalent, it means that they have the same relative position in a set of data. For example, if two students have the same rank in a class based on their test scores, then they are rank equivalent. This means that they have the same level of performance relative to the rest of the class.  While proportional quantities can be rank equivalent, not all rank equivalent quantities are proportional. For example, the heights of two people may be rank equivalent in a group of people, but they may not be proportional to each other.RESPONSESPLITPerhaps some operations may not hold the proportionality rule but it may ensure rank equivalence. For example, ranking by a probability value and its logarithm are rank equivalent since if one increases, the other does too, but they are not proportional. So if two quantities are proportional, perhaps they are indeed rank equivalent, but the other way around is not necessarily true.',\n",
       " \"Thank you for your question.   You are correct that it is difficult to infer whether the score will increase or decrease based solely on the equation of score after multiplying document k times. However, we can make some general observations about the influence of k on each part of the equation.   In the Dirichlet prior smoothing method, we use a prior distribution to estimate the probability of a term occurring in the collection. This prior distribution is then combined with the observed frequency of the term in the document to estimate the probability of the term occurring in the document.   When we multiply the document k times, we are essentially increasing the observed frequency of the term in the document. This will have different effects on the different parts of the equation.   For example, increasing the observed frequency of the term in the document will increase the numerator of the equation, which represents the likelihood of the query given the document. This is because the query is more likely to be relevant to the document if the term occurs more frequently in the document.   On the other hand, increasing the observed frequency of the term in the document will also increase the denominator of the equation, which represents the probability of the query under the prior distribution. This is because the prior distribution assumes that the term occurs less frequently in the collection than it actually does in the document.   Overall, it is difficult to predict whether the score will increase or decrease without knowing the specific values of the parameters in the equation. However, we can make some general observations about the influence of k on each part of the equation.   I hope this helps! Let me know if you have any further questions.RESPONSESPLITThe influence of k on each part of the equation can be determined by analyzing the individual terms and their relationship with k. For example, in the case of Dirichlet prior smoothing, the score after multiplying document k times can be written as:  score = (n_k + alpha) / (N + alpha * K) * prod((n_{k,w} + beta_w) / (n_k + V * beta_w))  Here, n_k represents the number of times topic k appears in the corpus, alpha is the hyperparameter for the topic distribution, N is the total number of words in the corpus, K is the total number of topics, n_{k,w} represents the number of times topic k appears with word w, beta_w is the hyperparameter for the word distribution, and V is the total number of unique words in the corpus.  By analyzing each term in this equation, we can determine how k affects the score. For example, the term (n_k + alpha) / (N + alpha * K) represents the probability of choosing topic k from the topic distribution, and we can see that increasing n_k (by multiplying the document k times) will increase the numerator of this term, leading to a higher score. Similarly, the term (n_{k,w} + beta_w) / (n_k + V * beta_w) represents the probability of choosing word w given topic k, and we can see that increasing n_{k,w} (by adding more occurrences of word w in document k) will increase the numerator of this term, leading to a higher score.  Overall, by analyzing the individual terms in the equation, we can infer how increasing k (by multiplying the document k times) will affect the score.RESPONSESPLITThis is an open-ended question and it would involve some algebraic transformation of formulas. Thus, you may not necessarily be able to derive a definitive answer. However, going through this analysis as an exercise can help you analytically assess the quality of a retrieval function (e.g., you might gain insights about when such a function might behave unreasonably). The math part is beyond the scope of this course, but if you are willing to work on it a bit, some simple transformation of the formula would likely provide some insights. In order to check whether the score would be increased or decreased, you may realize that you would like to compare the two scores (the score of the original document and the score of the generated long document by duplicating the document multiple times) in some way, e.g., taking their ratio or difference. In this case, subtracting one from the other would help simplify the analysis. Another way to simplify the problem is to consider special cases that are easier to analyze. For example, you could consider a simple case where there is only one query term or just two terms. Or you can consider a special document where you have only a few words. The conclusion might also depend on other factors such as whether c(w,d)/|d| is larger than p(w|C), etc. In general, just try to do some exploration by looking into special cases and write about whatever conclusion(s) you may have. It's fine to only be able to say in a particular special case, the conclusion is ...., or under condition A, the conclusion would be ..., while under condition B, the conclusion would be different. If you want to know more about this kind of analysis, you can take a look at the following resource page about the axiomatic retrieval framework: https://www.eecis.udel.edu/~hfang/AX.html\",\n",
       " 'Thank you for your question. Let me explain how we penalize frequent words in detail.  In the context of information retrieval, we use a mixture model to estimate the probability distribution of words in a document. The mixture model consists of two components: the background model and the document model. The background model represents the probability distribution of words in a large corpus of documents, while the document model represents the probability distribution of words in the document we are interested in.  When a word occurs frequently in the background model, its probability of occurrence in the document model, denoted as p(w|θd), will also be high. However, we want to penalize frequent words in the document model to avoid overfitting. This is because frequent words may not be informative for distinguishing between different documents.  To penalize frequent words, we introduce a regularization term, denoted as p(θd), which represents the probability of choosing the document model over the background model. The smaller p(θd) is, the less important it is for the document model to respond to the increase of counts of a word in the data. In other words, the degree of increase of probability due to the increased counts of a word observed in the document is regularized by p(θd) or equivalently p(θB), where θB represents the background model.  To summarize, we penalize frequent words in the document model by introducing a regularization term that controls the degree of increase of probability due to the increased counts of a word observed in the document. This helps to avoid overfitting and improve the performance of information retrieval.RESPONSESPLITAs an AI language model, I do not have access to the picture you are referring to. However, I can explain the concept of penalizing frequent words in language modeling.  In language modeling, the goal is to predict the probability of a sequence of words occurring in a given context. This is typically done using a statistical model that estimates the probability of each word given the previous words in the sequence. One common approach is to use n-gram models, where the probability of a word is estimated based on the frequency of its occurrence in the training data.  However, a problem with this approach is that frequent words tend to dominate the probability estimates, even if they are not very informative in the context. For example, in the sentence \"The cat sat on the mat\", the word \"the\" occurs three times, but it does not provide much information about the meaning of the sentence.  To address this issue, language models often apply a penalty to frequent words, known as \"smoothing\" or \"discounting\". This involves reducing the probability estimate for each word based on its frequency in the training data, so that less frequent words are given more weight in the model.  There are many different smoothing techniques, but one common approach is to use a modified version of the maximum likelihood estimate, such as the Kneser-Ney smoothing or Good-Turing smoothing. These methods adjust the probability estimates based on the frequency of each word and the frequency of its context, so that the model assigns higher probabilities to more informative words.  In summary, penalizing frequent words in language modeling is a way to reduce the impact of uninformative words on the probability estimates, and give more weight to less frequent but more informative words.RESPONSESPLITHere, if w is frequent in the background model, then p(w|C) will be large. Thus, by having high value of lambda, the reason for high probability of w, will be explained by the background model. Thus, the p(w|d) will be almost same for two different documents, while the document that has important keywords which is essentially what we are looking for will be ranked high. For example, if one document has more occurrences of the word \"the\" than the other document, the P(\"the\"|d) for both the documents will almost be same. Hence, this gives out the effect of penalizing the occurrence of frequent word in the document.',\n",
       " \"Dear student,  Thank you for your question. The log-likelihood value is calculated by taking the logarithm of the likelihood function. This is done for mathematical convenience, as taking the logarithm transforms the product in the likelihood function into a sum, which makes it easier to take the derivative and find the optimal solution. The sum is taken over all the unique data points, which in this case are the words in the vocabulary.  I hope this helps clarify the concept for you. Please let me know if you have any further questions.  Best regards, [Your Name]RESPONSESPLITTo calculate the log-likelihood value, you first need to have a statistical model that describes the probability distribution of your data. Let's say you have a set of data points {x1, x2, ..., xn} and you assume that they are independent and identically distributed according to a normal distribution with mean μ and variance σ^2. Then, the probability density function (PDF) of each data point xi is:  f(xi | μ, σ^2) = (1 / sqrt(2πσ^2)) * exp(-(xi - μ)^2 / (2σ^2))  The likelihood function is the product of the PDFs of all the data points:  L(μ, σ^2) = f(x1 | μ, σ^2) * f(x2 | μ, σ^2) * ... * f(xn | μ, σ^2)  Taking the logarithm of the likelihood function, we get:  log L(μ, σ^2) = log f(x1 | μ, σ^2) + log f(x2 | μ, σ^2) + ... + log f(xn | μ, σ^2)  Substituting the PDF of the normal distribution, we get:  log L(μ, σ^2) = -n/2 * log(2π) - n/2 * log(σ^2) - 1/(2σ^2) * Σ(xi - μ)^2  The last term in the equation is the negative of the sum of squared errors (SSE) between the data points and the mean of the normal distribution. Maximizing the log-likelihood function is equivalent to minimizing the SSE, which is a common objective in regression analysis.RESPONSESPLITFirst of all, you need to understand that likelihood here refers to the probability of all the observed data. In this toy example, it's the probability of observing a toy document with just four words. Log-likelihood simply refers to logarithm of the likelihood. Please review lectures 5.1 and 5.2\",\n",
       " \"The formula in Slide 32 of Mixture Language Models is used to estimate the probability of a term (text) given a document (θd) and a background model (θB). The question asks about the effect of setting a high value of p(θB) (0.9) in the formula.   Setting a high value of p(θB) means that we are giving more weight to the background model and less weight to the document model. This can be seen in the formula where the probability of text given the document model (θd) is divided by a sum of two terms, one representing the document model and the other representing the background model.   When p(θB) is high, the denominator of the formula becomes smaller, which means that the probability of text given the document model becomes larger. However, this does not mean that the probability becomes greater than 1. The probability of a term given a document model and a background model should always be between 0 and 1.   In summary, setting a high value of p(θB) in the formula can increase the probability of a term given a document model, but it should not result in a probability greater than 1.RESPONSESPLITIt seems that there is a mistake in the formula provided in the slide. The correct formula for calculating the probability of a word given a document in a mixture language model is:  p('text' | θd) = ∑B p('text' | θB) p(θB | θd)  where θB represents the language model for the background corpus B, and p(θB | θd) is the probability of selecting the background corpus B given the document θd.  If we set a high value of p(θB) (say, 0.9), then we are assuming that the background corpus is very likely to be selected for this document. This means that the contribution of the document-specific language model p('text' | θd) will be relatively small compared to the contribution of the background language model p('text' | θB).  In this case, we can calculate the probability of 'text' given θd as:  p('text' | θd) = p('text' | θB) p(θB | θd) + p('text' | θd) p(θd | θd)  = 0.1 * 0.9 + 0.4 * 0.1  = 0.13  This value is less than 1, which is the correct result. Therefore, it seems that there was an error in the formula provided in the slide.RESPONSESPLITThat is a great observation. I think the answer lies in the derivation where we said that for  m a x ( x y ) max(xy)  given,  x + y = k x+y=k  occurs at  x = y x=y . The maximum would occur at x = y, but the value of x at which it happens might be unattainable, such as in this case.  Refer to slide# 32 for the terms used below. If we assume, two terms of  p ( d ∣ λ ) , ( 1 − P ( θ B ) ) ∗ p ( “ t e x t ” ∣ θ d ) + P ( θ B ) ∗ 0.1 = x p(d∣λ),(1−P(θ  B \\u200b   ))∗p(“text”∣θ  d \\u200b   )+P(θ  B \\u200b   )∗0.1=x  and  ( 1 − P ( θ B ) ) ∗ p ( “ t h e ” ∣ θ d ) + P ( θ B ) ∗ 0.9 = y (1−P(θ  B \\u200b   ))∗p(“the”∣θ  d \\u200b   )+P(θ  B \\u200b   )∗0.9=y . We know that,  x + y = 1 x+y=1 , and we want to maximize  x y xy . This will happen at  x = y = 1 / 2 x=y=1/2 . And  x y xy  is increasing for  x < 1 / 2 x<1/2 . So, we can take the largest possible value of  x x  less than  1 / 2 1/2 , and the corresponding value of  P ( d ∣ λ ) P(d∣λ)  would be the maximum. Or we could take the smallest possible value of  x x  larger than 1/2 as  x y xy  is decreasing in that region. The range of values of  x x  given  p ( θ B ) = 0.9 p(θ  B \\u200b   )=0.9  is  [ 0.09 , 0.19 ] [0.09,0.19]  for  p ( “ t e x t ” ∣ θ d ) = 0 p(“text”∣θ  d \\u200b   )=0  and  1 1  respectively. Hence, the maximum value of  P ( d ∣ λ ) P(d∣λ)  in this case would be  x ( 1 − x ) = 0.19 ∗ 0.81 = 0.1539 x(1−x)=0.19∗0.81=0.1539 . This value is lesser than what  x ( 1 − x ) x(1−x)  could have reached given no constraints on  x x , i.e.  0.5 ∗ 0.5 = 0.25 0.5∗0.5=0.25\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = data.split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba9bf404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How is Jelinek-Mercer smoothing different from interpolation? The formulae are the same... Is JM a particular type of interpolation? If yes, then what type is it?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_lines = [line.split(f'{response_split}') for line in lines]\n",
    "len(split_lines)\n",
    "ft = [sl[0] for sl in split_lines]\n",
    "gpt = [sl[1] for sl in split_lines]\n",
    "book = [sl[2] for sl in split_lines]\n",
    "output = pd.DataFrame({'fine_tuned':ft, 'gpt':gpt, 'textbook':book})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4db0bcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How is Jelinek-Mercer smoothing different from interpolation? The formulae are the same... Is JM a particular type of interpolation? If yes, then what type is it?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 0\n",
    "\n",
    "queries[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "daffc0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jelinek-Mercer smoothing and interpolation are both techniques used in language modeling to estimate the probability of a word given its context. However, they differ in the way they combine the probabilities of different n-grams.  Interpolation involves combining the probabilities of different n-grams using a weighted sum, where the weights are determined by the frequency of the n-gram in the training data. In contrast, Jelinek-Mercer smoothing involves weighting the probabilities of different n-grams based on their similarity to the context in which they occur.  Therefore, Jelinek-Mercer smoothing is a type of smoothing technique that can be used in conjunction with interpolation to improve the accuracy of language models. It is not a type of interpolation itself.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loc[ind]['gpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4e8d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebb64590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------How is Jelinek-Mercer smoothing different from interpolation? The formulae are the same... Is JM a particular type of interpolation? If yes, then what type is it?------\n",
      "textbook with fine_tuned: 0.14285714285714285\n",
      "textbook with gpt: 0.1\n",
      "------Going back to the point that KL-divergence is not symmetric unlike Mutual Information, can we think of real-time applications that would prefer an unsymmetric measure over a symmetric measure? That is, are there cases where we intentionally prefer to use unsymmetric measures? It is always useful to improve our understanding of the data, but what are the application level use cases?------\n",
      "textbook with fine_tuned: 0.07203389830508475\n",
      "textbook with gpt: 0.056105610561056105\n",
      "------In the following equation what if the denominator is zero i.e there are no counts for all the various sequences that appear in denominator. How is the probability calculated in this case? Is it a zero as even the numerator will be zero or is it not defined?------\n",
      "textbook with fine_tuned: 0.06930693069306931\n",
      "textbook with gpt: 0.056179775280898875\n",
      "------Can blind feedback be applied repeatedly? Basically, use the original query model q0 to retrieve top k documents and estimate an updated model q1, then use q1 to again retrieve top k documents and update the model and so on. Can this lead to any improvement over applying the feedback just once?------\n",
      "textbook with fine_tuned: 0.10077519379844961\n",
      "textbook with gpt: 0.09032258064516129\n",
      "------How do you tell if a retrieval function has IDF weighting? Do we basically look for a component that penalizes the frequent use of a term? Or is it when we compare the term against the collection/background probability to balance out the 2 probabilities?------\n",
      "textbook with fine_tuned: 0.08018867924528301\n",
      "textbook with gpt: 0.08636363636363636\n",
      "------Hi, I was wondering about Divergence minimization in the lecture notes. On line 3, why lambda * H(theta ; theta_C) can be joined into the summation? Isn't it originally outside of the summation?------\n",
      "textbook with fine_tuned: 0.006896551724137931\n",
      "textbook with gpt: 0.03529411764705882\n",
      "------Is there any difference between two quantities being proportional and being rank equivalent? I thought two quantities have to be proportional in order for them to be rank equivalent. But, I vaugely remember the Prof. mentioning there was some difference. I may be wrong. Pls advise/share your thoughts.------\n",
      "textbook with fine_tuned: 0.0738255033557047\n",
      "textbook with gpt: 0.1\n",
      "------Dirichlet prior smoothing. I can write the equation of score after multiply doucument k times. But how can we infer from this equation whether the score will increase or decrease? Since we don't know the influence of k to each part of the equation.------\n",
      "textbook with fine_tuned: 0.06412478336221837\n",
      "textbook with gpt: 0.044802867383512544\n",
      "------In this picture, if w occurs very frequently in background model U, the p(w|U) will be large, right? I thought it should be if w occurs frequently then p(w|U) will be smaller so we can do the penalty. But here seems the opposite. Can you explain how we penalize the frequent word in detail?------\n",
      "textbook with fine_tuned: 0.08022922636103152\n",
      "textbook with gpt: 0.057291666666666664\n",
      "------Hello, this is a dumb question but I was wondering how the log-likelihood value is calculated here:------\n",
      "textbook with fine_tuned: 0.08029197080291971\n",
      "textbook with gpt: 0.05384615384615385\n",
      "------If we set a high value of  p ( θ B ) p(θ  B ​   )  (say, 0.9) in the above formula in Slide 32 of Mixture Language Models, we can get  p ( ‘ ‘ t e x t \" ∣ θ d ) = 0.4 1 − 0.9 + 0.1 = 0.4 0.1 + 0.1 = 4 + 0.1 = 4.1 > 1 ? p(‘‘text\"∣θ  d ​   )=  1−0.9 0.4 ​   +0.1=  0.1 0.4 ​   +0.1=4+0.1=4.1>1?  Can anyone please help me understand something I may be missing? Thanks------\n",
      "textbook with fine_tuned: 0.04326923076923077\n",
      "textbook with gpt: 0.04823151125401929\n"
     ]
    }
   ],
   "source": [
    "# not super efficient, but because we are dealing with a small dataframe, we can get away with it\n",
    "for i, row in output.iterrows():\n",
    "    print(f'------{queries[i]}------')\n",
    "    list1 = row['textbook'].split(' ')\n",
    "\n",
    "    #fine_tuned\n",
    "    list2 = row['fine_tuned'].split(' ')\n",
    "    sim = jaccard(list1, list2)\n",
    "    print(f'textbook with fine_tuned: {sim}')\n",
    "\n",
    "    #plain gpt\n",
    "    list2 = row['gpt'].split(' ')\n",
    "    sim = jaccard(list1, list2)\n",
    "    print(f'textbook with gpt: {sim}')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
