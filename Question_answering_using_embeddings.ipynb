{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright 2023 QuickAns\n",
    " *\n",
    " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " * you may not use this file except in compliance with the License.\n",
    " * You may obtain a copy of the License at\n",
    " *\n",
    " *    http://www.apache.org/licenses/LICENSE-2.0\n",
    " *\n",
    " * Unless required by applicable law or agreed to in writing, software\n",
    " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " * See the License for the specific language governing permissions and\n",
    " * limitations under the License.\n",
    " '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0435cb",
   "metadata": {},
   "source": [
    "# Question answering using embeddings-based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3839a6-9146-4f60-b74b-19abbc24278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import openai  # for calling the OpenAI API\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "#from keys import *\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d50792",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"/home/ishikaa2/quickans/ans_generator/data/ir_txt/ir_book_embeddings.csv\"\n",
    "\n",
    "df = pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70307f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert embeddings from CSV str type back to list type\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424162c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe has two columns: \"text\" and \"embedding\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8c713-c8a9-47dc-85a4-871ee1395566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    "):\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da034bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"Web Crawling\", df, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f45cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = \"Suppose you are a teaching assistant for the course Advanced Information Retrieval and a student has posed the following question.\\n\"\n",
    "    question = f\"\\n\\nQuestion: {query}\\n\\n\"\n",
    "    end = 'How will you answer the question? \\n Here are some snippets from the course textbook which may be useful.\\n\\n\"'\n",
    "    book_info = \"\"\n",
    "    \n",
    "    preface = introduction + question + end\n",
    "    for string in strings:\n",
    "        next_article = f'\\n{string}\\n'\n",
    "        if (\n",
    "            len(preface + book_info + next_article) > 2500\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            book_info += next_article\n",
    "    return preface + book_info\n",
    "\n",
    "def api_call(message, model: str = GPT_MODEL):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    \n",
    "    reply = api_call(message, model)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'When modeling queries, how is multi-bernoulli different from multinomial?'\n",
    "\n",
    "api_call(query)\n",
    "# ask('When modeling queries, how is multi-bernoulli different from multinomial?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query):\n",
    "    emb_reply = ask(query)\n",
    "    #emb_reply = 'emb_reply'\n",
    "    base_reply = api_call(query)\n",
    "    #base_reply = 'base_reply'\n",
    "    \n",
    "\n",
    "    print(f'Query:{query}\\n\\n')\n",
    "    print(\"Embedding Method response\\n\")\n",
    "    print(emb_reply+\"\\n\\n\")\n",
    "    print('Base GPT response\\n')\n",
    "    print(base_reply)\n",
    "\n",
    "    return emb_reply, base_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How do you define a reference language model?'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefa8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How can we use language models for part of speech tagging?'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What happens to the Dirichlet smoothing when the document langeth goes to infinity'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f56ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['What order should you evaluate the kl divergence?'] #, \n",
    "'''queries = ['Why do we need a background language model?', \n",
    "           'How does a tokenizer work?', \n",
    "           'What are the components of an inverted index?', \n",
    "           'What is the document filtering problem?', \n",
    "           'What are the aspects of a search engine?', \n",
    "           'For computing the f1 score, why can\\'t we take the mean of precision and recall?',\n",
    "           'Why can’t we use IR models for ranking of webpages?',\n",
    "           'How do you evaluate a filtering system?',\n",
    "           'What are the components in context-based filtering?',\n",
    "           'What is the exploration-exploitation tradeoff?',\n",
    "           'What is beta-gamma threshold learning?',\n",
    "           'What type of word relations are there?',\n",
    "           'What is intrusion detection?',\n",
    "           'What are some cluster similarity measures?']'''\n",
    "\n",
    "response_split = 'RESPONSESPLIT'\n",
    "\n",
    "queries = ['How is Jelinek-Mercer smoothing different from interpolation? The formulae are the same... Is JM a particular type of interpolation? If yes, then what type is it?',\n",
    "           'Going back to the point that KL-divergence is not symmetric unlike Mutual Information, can we think of real-time applications that would prefer an unsymmetric measure over a symmetric measure? That is, are there cases where we intentionally prefer to use unsymmetric measures? It is always useful to improve our understanding of the data, but what are the application level use cases?',\n",
    "           'In the following equation what if the denominator is zero i.e there are no counts for all the various sequences that appear in denominator. How is the probability calculated in this case? Is it a zero as even the numerator will be zero or is it not defined?',\n",
    "           'Can blind feedback be applied repeatedly? Basically, use the original query model q0 to retrieve top k documents and estimate an updated model q1, then use q1 to again retrieve top k documents and update the model and so on. Can this lead to any improvement over applying the feedback just once?',\n",
    "           'How do you tell if a retrieval function has IDF weighting? Do we basically look for a component that penalizes the frequent use of a term? Or is it when we compare the term against the collection/background probability to balance out the 2 probabilities?',\n",
    "           'Hi, I was wondering about Divergence minimization in the lecture notes. On line 3, why lambda * H(theta ; theta_C) can be joined into the summation? Isn\\'t it originally outside of the summation?',\n",
    "           'Is there any difference between two quantities being proportional and being rank equivalent? I thought two quantities have to be proportional in order for them to be rank equivalent. But, I vaugely remember the Prof. mentioning there was some difference. I may be wrong. Pls advise/share your thoughts.',\n",
    "           'Dirichlet prior smoothing. I can write the equation of score after multiply doucument k times. But how can we infer from this equation whether the score will increase or decrease? Since we don\\'t know the influence of k to each part of the equation.',\n",
    "           'In this picture, if w occurs very frequently in background model U, the p(w|U) will be large, right? I thought it should be if w occurs frequently then p(w|U) will be smaller so we can do the penalty. But here seems the opposite. Can you explain how we penalize the frequent word in detail?',\n",
    "           'Hello, this is a dumb question but I was wondering how the log-likelihood value is calculated here:',\n",
    "           'If we set a high value of  p ( θ B ) p(θ  B ​   )  (say, 0.9) in the above formula in Slide 32 of Mixture Language Models, we can get  p ( ‘ ‘ t e x t \" ∣ θ d ) = 0.4 1 − 0.9 + 0.1 = 0.4 0.1 + 0.1 = 4 + 0.1 = 4.1 > 1 ? p(‘‘text\"∣θ  d ​   )=  1−0.9 0.4 ​   +0.1=  0.1 0.4 ​   +0.1=4+0.1=4.1>1?  Can anyone please help me understand something I may be missing? Thanks']\n",
    "\n",
    "f = open('ground_truth_responses.txt', 'r')\n",
    "grouth_truth = f.read().split('\\n')\n",
    "\n",
    "import time\n",
    "if False:\n",
    "    for i in range(len(queries)):\n",
    "        q = queries[i]\n",
    "        print(q)\n",
    "        emb, base = compare_responses(q)\n",
    "        emb = ' '.join(emb.splitlines())\n",
    "        base = ' '.join(base.splitlines())\n",
    "        f = open('responses.csv', 'a')\n",
    "        f.write(f'{emb}{response_split}{base}{response_split}{grouth_truth[i+3]}\\n')\n",
    "        f.flush()\n",
    "        f.close()\n",
    "        time.sleep(60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b903bc91",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('responses.csv', 'r')\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = data.split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9bf404",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lines = [line.split(f'{response_split}') for line in lines]\n",
    "len(split_lines)\n",
    "ft = [sl[0] for sl in split_lines]\n",
    "gpt = [sl[1] for sl in split_lines]\n",
    "book = [sl[2] for sl in split_lines]\n",
    "output = pd.DataFrame({'fine_tuned':ft, 'gpt':gpt, 'textbook':book})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "\n",
    "queries[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daffc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.loc[ind]['gpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e8d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb64590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not super efficient, but because we are dealing with a small dataframe, we can get away with it\n",
    "for i, row in output.iterrows():\n",
    "    print(f'------{queries[i]}------')\n",
    "    list1 = row['textbook'].split(' ')\n",
    "\n",
    "    #fine_tuned\n",
    "    list2 = row['fine_tuned'].split(' ')\n",
    "    sim = jaccard(list1, list2)\n",
    "    print(f'textbook with fine_tuned: {sim}')\n",
    "\n",
    "    #plain gpt\n",
    "    list2 = row['gpt'].split(' ')\n",
    "    sim = jaccard(list1, list2)\n",
    "    print(f'textbook with gpt: {sim}')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
