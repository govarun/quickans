{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0435cb",
   "metadata": {},
   "source": [
    "# Question answering using embeddings-based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3839a6-9146-4f60-b74b-19abbc24278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import openai  # for calling the OpenAI API\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "#from keys import *\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d50792",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"/home/ishikaa2/quickans/ans_generator/data/ir_txt/ir_book_embeddings.csv\"\n",
    "\n",
    "df = pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70307f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert embeddings from CSV str type back to list type\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424162c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChengXiang Zhai Sean Massung Text Data Managem...</td>\n",
       "      <td>[-0.017203252762556076, 0.021963899955153465, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has led to an increasing demand for power...</td>\n",
       "      <td>[-0.016908559948205948, 0.010532873682677746, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike data generated by a computer system or ...</td>\n",
       "      <td>[-0.018540382385253906, 0.014797425828874111, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As such text data are especially valuable for ...</td>\n",
       "      <td>[-0.020149044692516327, 0.02085673063993454, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In contrast to structured data which conform t...</td>\n",
       "      <td>[-0.02140810526907444, 0.02335185930132866, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8045</th>\n",
       "      <td>Please note that this α is different from αd i...</td>\n",
       "      <td>[0.0068044946528971195, 0.035624921321868896, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>Of course the next question is how to estimate...</td>\n",
       "      <td>[0.007543814368546009, 0.032577402889728546, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8047</th>\n",
       "      <td>where F  d1 dk is the set of feedback document...</td>\n",
       "      <td>[-0.016099639236927032, 0.021384460851550102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>Now given λ the feedback documents F  and the ...</td>\n",
       "      <td>[-0.014733465388417244, 0.021942878141999245, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>.</td>\n",
       "      <td>[-0.008761508390307426, -0.016880812123417854,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8050 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     ChengXiang Zhai Sean Massung Text Data Managem...   \n",
       "1     This has led to an increasing demand for power...   \n",
       "2     Unlike data generated by a computer system or ...   \n",
       "3     As such text data are especially valuable for ...   \n",
       "4     In contrast to structured data which conform t...   \n",
       "...                                                 ...   \n",
       "8045  Please note that this α is different from αd i...   \n",
       "8046  Of course the next question is how to estimate...   \n",
       "8047  where F  d1 dk is the set of feedback document...   \n",
       "8048  Now given λ the feedback documents F  and the ...   \n",
       "8049                                                  .   \n",
       "\n",
       "                                              embedding  \n",
       "0     [-0.017203252762556076, 0.021963899955153465, ...  \n",
       "1     [-0.016908559948205948, 0.010532873682677746, ...  \n",
       "2     [-0.018540382385253906, 0.014797425828874111, ...  \n",
       "3     [-0.020149044692516327, 0.02085673063993454, 0...  \n",
       "4     [-0.02140810526907444, 0.02335185930132866, 0....  \n",
       "...                                                 ...  \n",
       "8045  [0.0068044946528971195, 0.035624921321868896, ...  \n",
       "8046  [0.007543814368546009, 0.032577402889728546, -...  \n",
       "8047  [-0.016099639236927032, 0.021384460851550102, ...  \n",
       "8048  [-0.014733465388417244, 0.021942878141999245, ...  \n",
       "8049  [-0.008761508390307426, -0.016880812123417854,...  \n",
       "\n",
       "[8050 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataframe has two columns: \"text\" and \"embedding\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a8c713-c8a9-47dc-85a4-871ee1395566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    "):\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da034bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8846888413640148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.  We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied.  Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8746059769842044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We’ve already described all indexing steps except crawling in detail in Chapter 8.  After our crawling discussion we move onto the particular challenges of web indexing.  Then we discuss how we can take advantage of links between pages in link analysis.  The last technique we discuss is learning to rank which is a way to combine many different features for ranking.  10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8735251396541552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.  We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied.  Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with.  One issue is robustness What if the server doesn’t respond or returns unparseable garbage What if there’s a trap that generates dynamically generated pages that attract your crawler to keep crawling the same site in circles Yet another issue is that we don’t want to overload one particular server with too many crawling requests.  Those may cause the site to experience a denial of service some sites will also block IP addresses that they believe to be crawling them or creating too many requests.  In a similar vein a crawler should respect the robot exclusion protocol.  A file called robots.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8728600235603673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the next section we will discuss crawling.  We’ve already described all indexing steps except crawling in detail in Chapter 8.  After our crawling discussion we move onto the particular challenges of web indexing.  Then we discuss how we can take advantage of links between pages in link analysis.  The last technique we discuss is learning to rank which is a way to combine many different features for ranking.  10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8725280804082206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'One interesting variation is called focused crawling.  Here we’re going to crawl some pages about a particular topic eg all pages about automobiles.  This is typically going to start with a query that you use to get some results.  Then you gradually crawl more.  An even more extreme version of focused crawling is for example downloading and indexing all forum posts on a particular forum.  In this case we might have a URL such as httpwww. textdatabookforum. comboardsid3 which refers to the third post on the forum.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examples\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"Web Crawling\", df, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f45cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = \"Suppose you are a teaching assistant for the course Advanced Information Retrieval and a student has posed the following question.\\n\"\n",
    "    question = f\"\\n\\nQuestion: {query}\\n\\n\"\n",
    "    end = 'How will you answer the question? \\n Here are some snippets from the course textbook which may be useful.\\n\\n\"'\n",
    "    book_info = \"\"\n",
    "    \n",
    "    preface = introduction + question + end\n",
    "    for string in strings:\n",
    "        next_article = f'\\n{string}\\n'\n",
    "        if (\n",
    "            len(preface + book_info + next_article) > 2500\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            book_info += next_article\n",
    "    return preface + book_info\n",
    "\n",
    "def api_call(message, model: str = GPT_MODEL):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    \n",
    "    reply = api_call(message, model)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d61f413f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multi-Bernoulli and Multinomial are two different models used in probability theory and statistics to model queries. \\n\\nMulti-Bernoulli is a model used to represent a sequence of binary events, where each event can have one of two outcomes (success or failure). It is used to model queries where the data is binary, such as in object tracking or sensor fusion. In Multi-Bernoulli, the probability of success or failure is assumed to be constant across all events.\\n\\nOn the other hand, Multinomial is a model used to represent a sequence of events where each event can have one of several outcomes. It is used to model queries where the data is categorical, such as in natural language processing or image classification. In Multinomial, the probability of each outcome is assumed to be constant across all events.\\n\\nIn summary, Multi-Bernoulli is used for binary data, while Multinomial is used for categorical data.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'When modeling queries, how is multi-bernoulli different from multinomial?'\n",
    "\n",
    "api_call(query)\n",
    "# ask('When modeling queries, how is multi-bernoulli different from multinomial?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3cdb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query):\n",
    "    emb_reply = ask(query)\n",
    "    base_reply = api_call(query)\n",
    "    \n",
    "\n",
    "    print(f'Query:{query}\\n\\n')\n",
    "    print(\"Embedding Method response\\n\")\n",
    "    print(emb_reply+\"\\n\\n\")\n",
    "    print('Base GPT response\\n')\n",
    "    print(base_reply)\n",
    "\n",
    "    return emb_reply, base_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d277e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:How do you define a reference language model?\n",
      "\n",
      "\n",
      "Embedding Method response\n",
      "\n",
      "A reference language model is a probability distribution over word sequences that is used to estimate the probability of an unseen word in a corpus. If a word is not observed in the corpus, its probability is assumed to be governed by the reference language model. In the case of retrieval, a natural choice for the reference language model would be the collection language model. The collection language model is used to adjust the probability of the maximum likelihood estimate of seen terms. The general English language model can be used as a background language model to filter out common words and obtain a probability ratio for each word. Language models are useful for quantifying the uncertainties associated with the use of natural language and can help answer many interesting questions related to text analysis and information retrieval. Neural language models can be represented as a neural network and can be used to learn vector representations for all the words in a specific dataset.\n",
      "\n",
      "\n",
      "Base GPT response\n",
      "\n",
      "A reference language model is a pre-trained language model that serves as a benchmark for evaluating the performance of other language models. It is typically trained on a large corpus of text data and is used to generate high-quality text samples. The reference language model is often used to compare the performance of other language models, such as those trained on specific domains or with different architectures. The reference language model is also used to evaluate the quality of text generated by other models, by comparing it to the text generated by the reference model.\n"
     ]
    }
   ],
   "source": [
    "query = 'How do you define a reference language model?'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebefa8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:How can we use language models for part of speech tagging?\n",
      "\n",
      "\n",
      "Embedding Method response\n",
      "\n",
      "To use language models for part of speech tagging, we can assign a part of speech tag to each word based on its surrounding words. This allows us to count the most frequent nouns or determine what kind of nouns are associated with what kind of verbs. We can also consider n-grams of part of speech tags and mix n-grams of words and part of speech tags to create hybrid features that can be useful for sentiment analysis. Additionally, we can use language models to perform semantic analysis of word relations by finding what words are semantically associated with a given word. However, it is important to note that while part of speech tagging is a relatively easy task, full structure parsing and semantic analysis remain difficult and are only successful for some aspects of analysis. Overall, using language models for part of speech tagging can enrich the representation of text data and enable deeper analysis.\n",
      "\n",
      "\n",
      "Base GPT response\n",
      "\n",
      "Language models can be used for part of speech tagging by training them on a large corpus of text data that has been manually annotated with part of speech tags. The language model learns to predict the most likely part of speech tag for each word in a sentence based on the context of the surrounding words. This is done by using a probabilistic model that assigns a probability to each possible part of speech tag for each word in the sentence. The model then selects the most likely tag for each word based on these probabilities. Once the language model has been trained, it can be used to automatically tag the parts of speech in new text data. This can be useful for a variety of natural language processing tasks, such as text classification, sentiment analysis, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "query = 'How can we use language models for part of speech tagging?'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2589a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:What happens to the Dirichlet smoothing when the document langeth goes to infinity\n",
      "\n",
      "\n",
      "Embedding Method response\n",
      "\n",
      "When the document length goes to infinity, the effect on Dirichlet smoothing is that the coefficient αd tends to zero. This is because αd penalizes long documents, and as the document length increases, the number of observed words also increases, making it less necessary to do smoothing. Therefore, the Dirichlet smoothing becomes less effective as the document length increases towards infinity. However, it is important to note that this effect is not unique to Dirichlet smoothing and is a general property of smoothing methods that penalize long documents.\n",
      "\n",
      "\n",
      "Base GPT response\n",
      "\n",
      "When the document length goes to infinity, the effect of Dirichlet smoothing decreases. This is because as the document length increases, the probability estimates become more accurate and the need for smoothing decreases. In other words, as the amount of data increases, the impact of the prior distribution (used in Dirichlet smoothing) becomes less significant. Therefore, in practice, Dirichlet smoothing is typically used for smaller datasets where the probability estimates may be unreliable due to limited data.\n"
     ]
    }
   ],
   "source": [
    "query = 'What happens to the Dirichlet smoothing when the document langeth goes to infinity'\n",
    "\n",
    "compare_responses(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25f56ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do we need a background language model?\n",
      "How does a tokenizer work?\n",
      "What are the components of an inverted index?\n",
      "What is the document filtering problem?\n",
      "What are the aspects of a search engine?\n",
      "For computing the f1 score, why can't we take the mean of precision and recall?\n",
      "Why can’t we use IR models for ranking of webpages?\n",
      "How do you evaluate a filtering system?\n",
      "What are the components in context-based filtering?\n",
      "What is the exploration-exploitation tradeoff?\n",
      "What is beta-gamma threshold learning?\n",
      "What type of word relations are there?\n",
      "What is intrusion detection?\n",
      "What are some cluster similarity measures?\n"
     ]
    }
   ],
   "source": [
    "#['What order should you evaluate the kl divergence?'] #, \n",
    "queries = ['Why do we need a background language model?', \n",
    "           'How does a tokenizer work?', \n",
    "           'What are the components of an inverted index?', \n",
    "           'What is the document filtering problem?', \n",
    "           'What are the aspects of a search engine?', \n",
    "           'For computing the f1 score, why can\\'t we take the mean of precision and recall?',\n",
    "           'Why can’t we use IR models for ranking of webpages?',\n",
    "           'How do you evaluate a filtering system?',\n",
    "           'What are the components in context-based filtering?',\n",
    "           'What is the exploration-exploitation tradeoff?',\n",
    "           'What is beta-gamma threshold learning?',\n",
    "           'What type of word relations are there?',\n",
    "           'What is intrusion detection?',\n",
    "           'What are some cluster similarity measures?']\n",
    "import time\n",
    "for q in queries:\n",
    "    print(q)\n",
    "    #emb, base = compare_responses(q)\n",
    "    #print('\\n\\n-----------------------NEXT QUESTION----------------------')\n",
    "    #time.sleep(60)\n",
    "\n",
    "    ''' f = open('responses.csv', 'w')\n",
    "    f.write(f'{q}, {emb}, {base}')\n",
    "    f.close()'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b903bc91",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "533a0645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A background language model is necessary in information retrieval because it helps to normalize the language model and obtain a probability ratio for each word. This allows us to identify words that are semantically associated with a particular topic or query. Additionally, using a single model with a maximum likelihood estimate would force high probabilities for common words like \"the,\" which is not ideal. By using a background language model, we can assign high probabilities to these common words and reduce their probability in the main model. Essentially, the background language model helps to explain and account for the occurrence of common words in a given context.ISHIKAA background language model is needed to improve the accuracy of natural language processing tasks such as speech recognition, machine translation, and text generation. It helps in predicting the probability of a sequence of words in a given language. This model is trained on a large corpus of text data and learns the patterns and relationships between words in a language. By using a background language model, the system can better understand the context and meaning of the words being used, which leads to more accurate and natural language processing.ISHIKAClearly, we need to get rid of these stop words. In fact, we have already seen one way to do that, by using a background language model while learning word associations in Chapter 2.',\n",
       " 'A tokenizer is a tool used in information retrieval to segment a document into countable features or tokens. The document is then represented by how many and what kind of tokens appear in it. The raw counts of these tokens are used by the scorer to formulate the retrieval scoring functions. The most basic tokenizer is a whitespace tokenizer, which simply delimits words by their whitespace. An analyzer, on the other hand, takes a \"filter chain\" that is used to generate the final tokens for its tokenization process. The filter chains are always defined as a specific tokenizer class followed by a sequence of zero or more filter classes, each of which reads from the previous class\\'s output. For example, a simple filter chain that lowercases all tokens and only keeps tokens with a certain length range could be icutokenizer → lowercasefilter → lengthfilter. Tokenizers always come first and define how to split a document\\'s string content into tokens. Some examples are icutokenizer, which converts documents into streams of tokens by following the Unicode standards for sentence and word segmentation, and charactertokenizer, which converts documents into streams of single characters. Filters come next and can be chained together to define ways that text can be modified or transformed. ISHIKATokenization is a fundamental step in many natural language processing tasks, such as text classification, sentiment analysis, and machine translation. By breaking down a text into smaller units, a tokenizer makes it easier for a computer to analyze and understand the meaning of the text.ISHIKAThus, in tokenization we will simply use the raw count of features (words), since the raw count can be used by the scorer to calculate some weighted term representation. Additionally, calculating something like TF-IDF is more involved than a simple scanning of a single document (since we need to calculate IDF). ',\n",
       " 'The components of an inverted index include the lexicon and the postings file. The lexicon is a lookup table that contains term-specific information such as document frequency and where in the postings file to access the per-document term counts. The postings file maps from any term integer ID to a list of document IDs and frequency information of the term in those documents. Additionally, it is common to store the position of each term occurrence in order to support \"proximity heuristics\" and check whether all the query terms are matched within a certain window of text. This information is stored in the postings file since it is document-specific. Overall, an inverted index allows for quick lookup of documents that contain any given term and is the main data structure used in a search engine.ISHIKA1. Terms: The terms or words that are indexed. 2. Document IDs: The unique identifier for each document that contains the term. 3. Positions: The position of the term within the document. 4. Term frequency: The number of times the term appears in the document. 5. Inverse document frequency: A measure of how rare or common the term is across all documents in the collection. 6. Metadata: Additional information about the document, such as title, author, date, etc.ISHIKAAn inverted index is the main data structure used in a search engine. It allows for quick lookup of documents that contain any given term. The relevant data structures include (1) the lexicon (a lookup table of term-specific information, such as document frequency and where in the postings file to access the per-document term counts) and (2) the postings file (mapping from any term integer ID to a list of document IDs and frequency information of the term in those documents). ',\n",
       " 'The document filtering problem refers to the task of only returning documents that meet a certain criteria. For example, if we want to only return documents that were written within the past year, we need to filter out documents that do not meet this criteria. This is a common problem in information retrieval, and there are different strategies to address it. One approach is to use term-at-a-time ranking, where we can ignore documents that are not in the correct date range by not updating their scores in the score accumulator. Another approach is to use document-at-a-time ranking, where we can simply skip the document if it doesn’t pass the filter when creating the context for that particular document. The goal of document filtering is to deliver relevant documents to the user as soon as they become available, making it more difficult than search where we can provide a ranked list and rely on the user to set the cutoff. However, by collecting feedback information, we can expect to get more information about what the user likes, making it easier to distinguish relevant documents from non-relevant ones.ISHIKAThe document filtering problem is the task of automatically categorizing or classifying documents based on their content. This involves analyzing the text of a document and determining its topic or subject matter, as well as identifying any relevant keywords or phrases. The goal of document filtering is to sort large collections of documents into categories or topics, making it easier to search and retrieve relevant information. This problem is often addressed using machine learning algorithms, such as Naive Bayes or Support Vector Machines, which are trained on a set of labeled documents to learn how to classify new, unseen documents.ISHIKAAnother common task is only returning documents that meet a certain criteria. For example, our index may store newspaper articles with dates as metadata. In our top-k search, suppose we want to only return documents that were written within the past year. This is a common document filtering problem',\n",
       " \"There are three major aspects of a search engine that can be measured: effectiveness or accuracy, efficiency, and usability. Effectiveness or accuracy refers to how accurate the search results are, and how well the system can rank relevant documents on top of non-relevant ones. Efficiency refers to how quickly a user can get the results, and how much computing resources are needed to answer a query. Usability is also an important aspect, as it refers to how easy it is for users to interact with the search engine and find the information they need. These three aspects are often used to compare different search engine algorithms and improve search engine systems in general.ISHIKA1. Crawling: The process of discovering and indexing web pages by following links from one page to another. 2. Indexing: The process of storing and organizing the information gathered during crawling in a searchable database. 3. Ranking: The process of determining the relevance and importance of a web page based on various factors such as keywords, content quality, and backlinks. 4. Retrieval: The process of returning relevant search results to the user based on their search query. 5. User interface: The design and functionality of the search engine's website or app, including features such as autocomplete, filters, and advanced search options. 6. Advertising: The display of paid search results or sponsored content alongside organic search results. 7. Analytics: The collection and analysis of data on user behavior and search patterns to improve the search engine's performance and user experience.ISHIKAEffectiveness or accuracy. How accurate are the search results? In this case, we’re measuring a system’s capability of ranking relevant documents on top of non-relevant ones. Efficiency. How quickly can a user get the results? How large are the computing resources that are needed to answer a query? In this case, we need to measure the space and time overhead of the system. Usability. How useful is the system for real user tasks? Here, interfaces and many other things are also important and we typically have to do user studies.\",\n",
       " \"The reason we cannot take the mean of precision and recall to compute the F1 score is because the arithmetic mean tends to be dominated by large values. This means that if we have a very high precision or a very high recall, we don't care about whether the other value is low since the whole sum would still be high. This is not desirable because we can easily have a perfect recall by returning all the documents, which would give us a perfect recall but a low precision. The F1 score takes into account both precision and recall and balances them out, giving us a more accurate measure of the performance of our information retrieval system. Additionally, if we use a value of β that is not equal to one, we can control the emphasis between precision and recall.ISHIKAWe cannot take the mean of precision and recall to compute the F1 score because precision and recall have different denominators. Precision is calculated as the ratio of true positives to the sum of true positives and false positives, while recall is calculated as the ratio of true positives to the sum of true positives and false negatives. Therefore, the mean of precision and recall would not give an accurate representation of the balance between precision and recall. The F1 score is designed to take into account both precision and recall, and it is calculated as the harmonic mean of precision and recall, which gives more weight to low values.ISHIKAWhy is this not as good as F1, i.e., what’s the problem with an arithmetic mean? The arithmetic mean tends to be dominated by large values. That means if you have a very high P or a very high R, then you really don’t care about whether the other value is low since the whole sum would be high. This is not the desirable effect because one can easily have a perfect recall by returning all the documents!\",\n",
       " \"We can use IR models for ranking of webpages, but they are not sufficient on their own. The reason for this is that the web has different information needs compared to traditional library search. For example, people may search for a specific webpage or entry page, which is known as a navigational query. In such cases, link information can be useful for ranking webpages. Therefore, while IR models are important building blocks for supporting web search, they need to be supplemented with other techniques such as link analysis to improve the ranking of webpages.ISHIKAAs an AI language model, I don't have personal opinions, but I can provide you with the following information: IR (Information Retrieval) models are used for ranking webpages based on their relevance to a user's query. However, these models have certain limitations when it comes to ranking webpages. Firstly, IR models rely heavily on keyword matching, which can lead to irrelevant results if the query is ambiguous or the keywords used are not specific enough. Secondly, IR models do not take into account the context of the query or the user's intent, which can result in irrelevant or inaccurate results. Thirdly, IR models do not consider the quality or authority of the webpage, which can lead to low-quality or unreliable results being ranked higher than more trustworthy sources.  Therefore, while IR models can be useful for certain types of queries, they are not always the most effective or accurate way to rank webpages. Other factors such as machine learning algorithms, user behavior, and content quality are also important considerations for webpage ranking.ISHIKAFirst, on the web we tend to have very different information needs. For example, people might search for a web page or entry page—this is different from the traditional library search where people are primarily interested in collecting literature information. [...] Secondly, web documents have much more information than pure text; there is hierarchical organization and annotations such as the page layout, title, or hyperlinks to other pages. These features provide an opportunity to use extra context information of the document to improve scoring. Finally, information quality greatly varies\",\n",
       " \"To evaluate a filtering system, we need to measure its performance in terms of how well it is able to filter out irrelevant items and recommend relevant ones to the user. One common strategy is to use a utility function, such as the linear utility function U3, which assigns a score to each item based on its relevance to the user's information need. The system then makes a decision in real-time whether the item is above the acceptance threshold θ or not. To evaluate the performance of the filtering system, we need to collect feedback from the user over time and use this feedback to adjust the system's parameters. This feedback can be long-term, allowing the system to collect much information about the user's interests and improve the classifier. However, we cannot use ranking evaluation measures such as MAP or NDCG because we cannot afford to wait for a significant number of documents to rank them to make a decision for the user. Instead, we need to focus on absolute relevance and measure the system's ability to correctly filter out irrelevant items and recommend relevant ones to the user in real-time.ISHIKAAs an AI language model, I do not have personal preferences or opinions. However, here are some general ways to evaluate a filtering system: 1. Accuracy: The filtering system should accurately identify and block unwanted content while allowing legitimate content to pass through. 2. Efficiency: The filtering system should not slow down the system or cause delays in accessing content. 3. Flexibility: The filtering system should be flexible enough to allow customization and adjustments to meet the specific needs of the user or organization. 4. User-friendliness: The filtering system should be easy to use and understand, with clear instructions and user-friendly interfaces. 5. Security: The filtering system should be secure and protect against potential threats such as malware, viruses, and phishing attacks. 6. Cost-effectiveness: The filtering system should provide value for money and be cost-effective for the user or organization. 7. Compliance: The filtering system should comply with relevant laws and regulations, such as data protection and privacy laws.ISHIKAOne common strategy is to use a utility function, and below is an example of a linear utility function: U = 3 . |R| − 2 . |R |, (11.1) where R is the set of relevant documents delivered to the user and R is the set of non-relevant documents delivered to the user (that the user rejected).\",\n",
       " \"Context-based filtering is a type of content-based filtering that takes into account the context in which a user is searching for information. The three basic components in context-based filtering are the initialization module, decision module, and utility function. The initialization module gets the system started based on a limited text description or a few examples from the user. The decision module decides whether a document should be delivered or not based on a text document and a profile description of the user. The utility function has to be designed based on a specific application preference, which may be different for different users. Content-based filtering can only be applied to a case where we know how to measure similarity of items. In any specific application, we will want to combine the two approaches to optimize the filtering performance. Content-based filtering can usually be done by extending a retrieval system to add a thresholding component. There are two challenges associated with threshold setting. First, at the beginning, we must set an initial threshold without requiring much information from a user. Second, over time, we need to learn from feedback to optimize the threshold. Many threshold learning methods have been proposed. In practice, a simple thresholding strategy such as the betagamma threshold setting method is often good enough.ISHIKAContext-based filtering typically involves the following components: 1. User profile: This includes information about the user's preferences, interests, and behavior. 2. Item profile: This includes information about the items being recommended, such as their attributes, features, and characteristics. 3. Contextual information: This includes information about the user's current situation, such as their location, time of day, and device they are using. 4. Recommendation algorithm: This is the algorithm that uses the user profile, item profile, and contextual information to generate personalized recommendations. 5. Feedback mechanism: This allows users to provide feedback on the recommendations they receive, which can be used to improve the recommendation algorithm over time.ISHIKAThe three basic components in content-based filtering are the following. Initialization module. Gets the system started based only on a very limited text description, or very few examples, from the user. Decision module. Given a text document and a profile description of the user, decide whether the document should be delivered or not. Learning module. Learn from limited user relevance judgments on the delivered documents. (If we don’t deliver a document to the user, we’d never know whether the user likes it or not.)\",\n",
       " 'The exploration-exploitation tradeoff refers to the dilemma of balancing the desire to explore new information with the need to exploit existing knowledge. In the context of information retrieval, this tradeoff arises when trying to deliver relevant information to a user. On one hand, we want to explore the document space to see if the user might be interested in the documents that we have not yet labeled. On the other hand, we don’t want to show the user too many non-relevant documents or they will be unsatisfied with the system. Exploitation means taking advantage of the information learned about the user. For example, if we know the user is interested in a particular topic, we don’t want to deviate too much. However, if we don’t deviate at all, we might miss the opportunity to learn another interest of the user. This is a difficult problem to solve, and there is no one-size-fits-all solution. One approach is to lower the threshold a little bit and deliver some near misses to the user to see their response to this extra document. Ultimately, the goal is to strike a balance between exploration and exploitation to deliver the most relevant information to the user.ISHIKAThe exploration-exploitation tradeoff is a decision-making dilemma that arises when an individual or organization must choose between exploring new options and exploiting known options. Exploration involves seeking out new information, experiences, or opportunities, while exploitation involves maximizing the benefits of existing knowledge, resources, or strategies. The tradeoff arises because investing too much in exploration may lead to missed opportunities for exploitation, while investing too much in exploitation may lead to stagnation and missed opportunities for growth. Finding the right balance between exploration and exploitation is critical for long-term success and innovation.ISHIKAThis issue is called the exploration-exploitation tradeoff. This means we want to explore the document space to see if the user might be interested in the documents that we have not yet labeled, but we don’t want to show the user too many nonrelevant documents or they will be unsatisfied with the system.',\n",
       " 'Beta-gamma threshold learning is an algorithm used in information retrieval to determine the optimal score threshold for a given utility function. The algorithm works by plotting the utility value at each different cutoff position and adjusting the threshold dynamically based on the number of judged examples in the training database. The parameters alpha, beta, and gamma are used to determine the optimal cutoff point between the optimal and zero utility points. The goal of the algorithm is to achieve the maximum utility by choosing the optimal threshold.ISHIKABeta-gamma threshold learning is a type of machine learning algorithm that is used for classification tasks. It is based on the idea of setting a threshold value for the output of a neuron, which determines whether the neuron fires or not. The threshold value is adjusted during the learning process based on the error rate of the network. The beta-gamma threshold learning algorithm is particularly useful for problems where the input data is noisy or incomplete, as it can adapt to these conditions and still produce accurate results.ISHIKAGiven a ranked list of all the documents in the training database sorted by their scores on the x-axis, their relevance, and a specific utility U, we can plot the utility value at each different cutoff position θ. Each cutoff position corresponds to a score threshold. Figure 11.5 shows this configuration and how a choice of α determines a cutoff point between the optimal and the zero utility points, and how β and γ help us to adjust α dynamically according to the number of judged examples in the training database. The optimal point θopt is the point when we would achieve the maximum utility if we had chosen this threshold. The θzero threshold is the zero utility threshold. Between these two θ values give us a safe point to explore the potential cutoff values.',\n",
       " 'There are two types of word relations: paradigmatic and syntagmatic. Words have a paradigmatic relation if they can be substituted for each other without affecting the understanding of the sentence. This means that the two words are in the same semantic or syntactic class. For example, the words \"cat\" and \"dog\" have a paradigmatic relation because they are both in the same word class, \"animal.\" On the other hand, words have a syntagmatic relation if they are used together in a sentence. This means that the words are related in terms of their position in a sentence and the meaning they convey when used together. Chapter 14 of the course textbook provides additional ideas on how to group similar terms together.ISHIKAThere are several types of word relations, including: 1. Synonyms: words that have similar meanings, such as \"happy\" and \"joyful.\" 2. Antonyms: words that have opposite meanings, such as \"hot\" and \"cold.\" 3. Homonyms: words that sound the same but have different meanings, such as \"flower\" and \"flour.\" 4. Homophones: words that sound the same but are spelled differently and have different meanings, such as \"there\" and \"their.\" 5. Hyponyms: words that are more specific than a general term, such as \"apple\" being a hyponym of \"fruit.\" 6. Hypernyms: words that are more general than a specific term, such as \"vehicle\" being a hypernym of \"car.\" 7. Meronyms: words that are part of a larger whole, such as \"wheel\" being a meronym of \"car.\" 8. Holonyms: words that are the larger whole of a part, such as \"car\" being a holonym of \"wheel.\"ISHIKAIn general, there are two types of word relations; one is called a paradigmatic relation and the other is a syntagmatic relation. Word a and b have a paradigmatic relation if they can be substituted for each other. That means the two words that have paradigmatic relation would be in the same semantic class, or syntactic class. [...] The second kind of relation is called a syntagmatic relation. In this case, the two words that have this relation can be combined with each other. Thus, words a and b have a syntagmatic relation if they can be combined with each other in a grammatical sentence—meaning that these two words are semantically related',\n",
       " 'Intrusion detection is a measure used in the evaluation of topic models, as described in the course textbook. It involves adding an additional random word to a list of top k similar words for a given vocabulary word, and seeing if it is easy to spot the intruder. If the top k words form a coherent group, then they are considered a good word association group. Human judges can respond to intrusion detection scenarios to measure the coherency of topic-word distributions. Another test for coherency is the document-topic distribution evaluation, which involves selecting the top three topics for a given document and adding an additional low probability topic to see if it is easy to spot.ISHIKAIntrusion detection is the process of monitoring a network or system for unauthorized access or malicious activity. It involves analyzing network traffic, system logs, and other data sources to identify potential security threats and alert security personnel to take action. Intrusion detection systems can be either host-based or network-based, and they use a variety of techniques such as signature-based detection, anomaly detection, and behavioral analysis to identify potential threats. The goal of intrusion detection is to detect and respond to security incidents in a timely manner, before they can cause significant damage to the network or system.ISHIKAA human-based evaluation metric would be intrusion detection. [...] If the word associations are good, it should be fairly easy to find an “intruder” that has been added into the top k similar words. [...] L1 = {dog , cat, horse, apple, pig , cow}, L2 = {car, teacher, platypus, agile, blue, Zaire} [...] In L1, it’s quite obvious that apple is the intruder since it doesn’t fit in with the other words in the list. Thus, the remaining words form a good word association list. In L2, we can’t really tell which word is the intruder, meaning the word association algorithm used to create the k candidates in L2 is not as good as the one used to generate L1.',\n",
       " 'There are several cluster similarity measures that can be used in information retrieval. Three common measures are single-link, complete-link, and average-link. Single-link merges the two clusters with the smallest minimum distance, resulting in looser clusters. Complete-link merges the two clusters with the smallest maximum distance between elements, resulting in very tight and compact clusters. Average-link is a compromise between single-link and complete-link. In addition to these measures, there are also similarity functions such as the Pearson Correlation Coefficient and the cosine measure. These measures are based on user preferences on items and do not use any content information of these items.When evaluating clusters, there are three criteria to consider: coherence, separation, and utility. Coherence and separation can be measured automatically with measures such as vector similarity, purity, or mutual information. However, evaluating term clustering may be more challenging since word-to-word similarity algorithms may not be as obvious as document-to-document similarities.ISHIKA1. Euclidean distance 2. Manhattan distance 3. Cosine similarity 4. Jaccard similarity 5. Pearson correlation coefficient 6. Spearman correlation coefficient 7. Hamming distance 8. Mahalanobis distance 9. Tanimoto coefficient 10. Bray-Curtis dissimilarity.ISHIKASingle-link merges the two clusters with the smallest minimum distance. This results in “looser” clusters, since we only need to find two individually close elements in each cluster in order to perform the merge. Complete-link merges the two clusters with the smallest maximum distance between elements. This results in very “tight” and “compact” clusters since the cluster diameter is kept small (i.e., the distance between all elements low). Average-link is a compromise between the two previous measures. As its name implies, it takes the smallest average distance between two clusters']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('texts', 'r')\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "509e7253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A background language model is necessary in information retrieval because it helps to normalize the language model and obtain a probability ratio for each word. This allows us to identify words that are semantically associated with a particular topic or query. Additionally, using a single model with a maximum likelihood estimate would force high probabilities for common words like \"the,\" which is not ideal. By using a background language model, we can assign high probabilities to these common words and reduce their probability in the main model. Essentially, the background language model helps to explain and account for the occurrence of common words in a given context.ISHIKAA background language model is needed to improve the accuracy of natural language processing tasks such as speech recognition, machine translation, and text generation. It helps in predicting the probability of a sequence of words in a given language. This model is trained on a large corpus of text data and learns the patterns and relationships between words in a language. By using a background language model, the system can better understand the context and meaning of the words being used, which leads to more accurate and natural language processing.ISHIKAClearly, we need to get rid of these stop words. In fact, we have already seen one way to do that, by using a background language model while learning word associations in Chapter 2.',\n",
       " 'A tokenizer is a tool used in information retrieval to segment a document into countable features or tokens. The document is then represented by how many and what kind of tokens appear in it. The raw counts of these tokens are used by the scorer to formulate the retrieval scoring functions. The most basic tokenizer is a whitespace tokenizer, which simply delimits words by their whitespace. An analyzer, on the other hand, takes a \"filter chain\" that is used to generate the final tokens for its tokenization process. The filter chains are always defined as a specific tokenizer class followed by a sequence of zero or more filter classes, each of which reads from the previous class\\'s output. For example, a simple filter chain that lowercases all tokens and only keeps tokens with a certain length range could be icutokenizer → lowercasefilter → lengthfilter. Tokenizers always come first and define how to split a document\\'s string content into tokens. Some examples are icutokenizer, which converts documents into streams of tokens by following the Unicode standards for sentence and word segmentation, and charactertokenizer, which converts documents into streams of single characters. Filters come next and can be chained together to define ways that text can be modified or transformed. ISHIKATokenization is a fundamental step in many natural language processing tasks, such as text classification, sentiment analysis, and machine translation. By breaking down a text into smaller units, a tokenizer makes it easier for a computer to analyze and understand the meaning of the text.ISHIKAThus, in tokenization we will simply use the raw count of features (words), since the raw count can be used by the scorer to calculate some weighted term representation. Additionally, calculating something like TF-IDF is more involved than a simple scanning of a single document (since we need to calculate IDF). ',\n",
       " 'The components of an inverted index include the lexicon and the postings file. The lexicon is a lookup table that contains term-specific information such as document frequency and where in the postings file to access the per-document term counts. The postings file maps from any term integer ID to a list of document IDs and frequency information of the term in those documents. Additionally, it is common to store the position of each term occurrence in order to support \"proximity heuristics\" and check whether all the query terms are matched within a certain window of text. This information is stored in the postings file since it is document-specific. Overall, an inverted index allows for quick lookup of documents that contain any given term and is the main data structure used in a search engine.ISHIKA1. Terms: The terms or words that are indexed. 2. Document IDs: The unique identifier for each document that contains the term. 3. Positions: The position of the term within the document. 4. Term frequency: The number of times the term appears in the document. 5. Inverse document frequency: A measure of how rare or common the term is across all documents in the collection. 6. Metadata: Additional information about the document, such as title, author, date, etc.ISHIKAAn inverted index is the main data structure used in a search engine. It allows for quick lookup of documents that contain any given term. The relevant data structures include (1) the lexicon (a lookup table of term-specific information, such as document frequency and where in the postings file to access the per-document term counts) and (2) the postings file (mapping from any term integer ID to a list of document IDs and frequency information of the term in those documents). ',\n",
       " 'The document filtering problem refers to the task of only returning documents that meet a certain criteria. For example, if we want to only return documents that were written within the past year, we need to filter out documents that do not meet this criteria. This is a common problem in information retrieval, and there are different strategies to address it. One approach is to use term-at-a-time ranking, where we can ignore documents that are not in the correct date range by not updating their scores in the score accumulator. Another approach is to use document-at-a-time ranking, where we can simply skip the document if it doesn’t pass the filter when creating the context for that particular document. The goal of document filtering is to deliver relevant documents to the user as soon as they become available, making it more difficult than search where we can provide a ranked list and rely on the user to set the cutoff. However, by collecting feedback information, we can expect to get more information about what the user likes, making it easier to distinguish relevant documents from non-relevant ones.ISHIKAThe document filtering problem is the task of automatically categorizing or classifying documents based on their content. This involves analyzing the text of a document and determining its topic or subject matter, as well as identifying any relevant keywords or phrases. The goal of document filtering is to sort large collections of documents into categories or topics, making it easier to search and retrieve relevant information. This problem is often addressed using machine learning algorithms, such as Naive Bayes or Support Vector Machines, which are trained on a set of labeled documents to learn how to classify new, unseen documents.ISHIKAAnother common task is only returning documents that meet a certain criteria. For example, our index may store newspaper articles with dates as metadata. In our top-k search, suppose we want to only return documents that were written within the past year. This is a common document filtering problem',\n",
       " \"There are three major aspects of a search engine that can be measured: effectiveness or accuracy, efficiency, and usability. Effectiveness or accuracy refers to how accurate the search results are, and how well the system can rank relevant documents on top of non-relevant ones. Efficiency refers to how quickly a user can get the results, and how much computing resources are needed to answer a query. Usability is also an important aspect, as it refers to how easy it is for users to interact with the search engine and find the information they need. These three aspects are often used to compare different search engine algorithms and improve search engine systems in general.ISHIKA1. Crawling: The process of discovering and indexing web pages by following links from one page to another. 2. Indexing: The process of storing and organizing the information gathered during crawling in a searchable database. 3. Ranking: The process of determining the relevance and importance of a web page based on various factors such as keywords, content quality, and backlinks. 4. Retrieval: The process of returning relevant search results to the user based on their search query. 5. User interface: The design and functionality of the search engine's website or app, including features such as autocomplete, filters, and advanced search options. 6. Advertising: The display of paid search results or sponsored content alongside organic search results. 7. Analytics: The collection and analysis of data on user behavior and search patterns to improve the search engine's performance and user experience.ISHIKAEffectiveness or accuracy. How accurate are the search results? In this case, we’re measuring a system’s capability of ranking relevant documents on top of non-relevant ones. Efficiency. How quickly can a user get the results? How large are the computing resources that are needed to answer a query? In this case, we need to measure the space and time overhead of the system. Usability. How useful is the system for real user tasks? Here, interfaces and many other things are also important and we typically have to do user studies.\",\n",
       " \"The reason we cannot take the mean of precision and recall to compute the F1 score is because the arithmetic mean tends to be dominated by large values. This means that if we have a very high precision or a very high recall, we don't care about whether the other value is low since the whole sum would still be high. This is not desirable because we can easily have a perfect recall by returning all the documents, which would give us a perfect recall but a low precision. The F1 score takes into account both precision and recall and balances them out, giving us a more accurate measure of the performance of our information retrieval system. Additionally, if we use a value of β that is not equal to one, we can control the emphasis between precision and recall.ISHIKAWe cannot take the mean of precision and recall to compute the F1 score because precision and recall have different denominators. Precision is calculated as the ratio of true positives to the sum of true positives and false positives, while recall is calculated as the ratio of true positives to the sum of true positives and false negatives. Therefore, the mean of precision and recall would not give an accurate representation of the balance between precision and recall. The F1 score is designed to take into account both precision and recall, and it is calculated as the harmonic mean of precision and recall, which gives more weight to low values.ISHIKAWhy is this not as good as F1, i.e., what’s the problem with an arithmetic mean? The arithmetic mean tends to be dominated by large values. That means if you have a very high P or a very high R, then you really don’t care about whether the other value is low since the whole sum would be high. This is not the desirable effect because one can easily have a perfect recall by returning all the documents!\",\n",
       " \"We can use IR models for ranking of webpages, but they are not sufficient on their own. The reason for this is that the web has different information needs compared to traditional library search. For example, people may search for a specific webpage or entry page, which is known as a navigational query. In such cases, link information can be useful for ranking webpages. Therefore, while IR models are important building blocks for supporting web search, they need to be supplemented with other techniques such as link analysis to improve the ranking of webpages.ISHIKAAs an AI language model, I don't have personal opinions, but I can provide you with the following information: IR (Information Retrieval) models are used for ranking webpages based on their relevance to a user's query. However, these models have certain limitations when it comes to ranking webpages. Firstly, IR models rely heavily on keyword matching, which can lead to irrelevant results if the query is ambiguous or the keywords used are not specific enough. Secondly, IR models do not take into account the context of the query or the user's intent, which can result in irrelevant or inaccurate results. Thirdly, IR models do not consider the quality or authority of the webpage, which can lead to low-quality or unreliable results being ranked higher than more trustworthy sources.  Therefore, while IR models can be useful for certain types of queries, they are not always the most effective or accurate way to rank webpages. Other factors such as machine learning algorithms, user behavior, and content quality are also important considerations for webpage ranking.ISHIKAFirst, on the web we tend to have very different information needs. For example, people might search for a web page or entry page—this is different from the traditional library search where people are primarily interested in collecting literature information. [...] Secondly, web documents have much more information than pure text; there is hierarchical organization and annotations such as the page layout, title, or hyperlinks to other pages. These features provide an opportunity to use extra context information of the document to improve scoring. Finally, information quality greatly varies\",\n",
       " \"To evaluate a filtering system, we need to measure its performance in terms of how well it is able to filter out irrelevant items and recommend relevant ones to the user. One common strategy is to use a utility function, such as the linear utility function U3, which assigns a score to each item based on its relevance to the user's information need. The system then makes a decision in real-time whether the item is above the acceptance threshold θ or not. To evaluate the performance of the filtering system, we need to collect feedback from the user over time and use this feedback to adjust the system's parameters. This feedback can be long-term, allowing the system to collect much information about the user's interests and improve the classifier. However, we cannot use ranking evaluation measures such as MAP or NDCG because we cannot afford to wait for a significant number of documents to rank them to make a decision for the user. Instead, we need to focus on absolute relevance and measure the system's ability to correctly filter out irrelevant items and recommend relevant ones to the user in real-time.ISHIKAAs an AI language model, I do not have personal preferences or opinions. However, here are some general ways to evaluate a filtering system: 1. Accuracy: The filtering system should accurately identify and block unwanted content while allowing legitimate content to pass through. 2. Efficiency: The filtering system should not slow down the system or cause delays in accessing content. 3. Flexibility: The filtering system should be flexible enough to allow customization and adjustments to meet the specific needs of the user or organization. 4. User-friendliness: The filtering system should be easy to use and understand, with clear instructions and user-friendly interfaces. 5. Security: The filtering system should be secure and protect against potential threats such as malware, viruses, and phishing attacks. 6. Cost-effectiveness: The filtering system should provide value for money and be cost-effective for the user or organization. 7. Compliance: The filtering system should comply with relevant laws and regulations, such as data protection and privacy laws.ISHIKAOne common strategy is to use a utility function, and below is an example of a linear utility function: U = 3 . |R| − 2 . |R |, (11.1) where R is the set of relevant documents delivered to the user and R is the set of non-relevant documents delivered to the user (that the user rejected).\",\n",
       " \"Context-based filtering is a type of content-based filtering that takes into account the context in which a user is searching for information. The three basic components in context-based filtering are the initialization module, decision module, and utility function. The initialization module gets the system started based on a limited text description or a few examples from the user. The decision module decides whether a document should be delivered or not based on a text document and a profile description of the user. The utility function has to be designed based on a specific application preference, which may be different for different users. Content-based filtering can only be applied to a case where we know how to measure similarity of items. In any specific application, we will want to combine the two approaches to optimize the filtering performance. Content-based filtering can usually be done by extending a retrieval system to add a thresholding component. There are two challenges associated with threshold setting. First, at the beginning, we must set an initial threshold without requiring much information from a user. Second, over time, we need to learn from feedback to optimize the threshold. Many threshold learning methods have been proposed. In practice, a simple thresholding strategy such as the betagamma threshold setting method is often good enough.ISHIKAContext-based filtering typically involves the following components: 1. User profile: This includes information about the user's preferences, interests, and behavior. 2. Item profile: This includes information about the items being recommended, such as their attributes, features, and characteristics. 3. Contextual information: This includes information about the user's current situation, such as their location, time of day, and device they are using. 4. Recommendation algorithm: This is the algorithm that uses the user profile, item profile, and contextual information to generate personalized recommendations. 5. Feedback mechanism: This allows users to provide feedback on the recommendations they receive, which can be used to improve the recommendation algorithm over time.ISHIKAThe three basic components in content-based filtering are the following. Initialization module. Gets the system started based only on a very limited text description, or very few examples, from the user. Decision module. Given a text document and a profile description of the user, decide whether the document should be delivered or not. Learning module. Learn from limited user relevance judgments on the delivered documents. (If we don’t deliver a document to the user, we’d never know whether the user likes it or not.)\",\n",
       " 'The exploration-exploitation tradeoff refers to the dilemma of balancing the desire to explore new information with the need to exploit existing knowledge. In the context of information retrieval, this tradeoff arises when trying to deliver relevant information to a user. On one hand, we want to explore the document space to see if the user might be interested in the documents that we have not yet labeled. On the other hand, we don’t want to show the user too many non-relevant documents or they will be unsatisfied with the system. Exploitation means taking advantage of the information learned about the user. For example, if we know the user is interested in a particular topic, we don’t want to deviate too much. However, if we don’t deviate at all, we might miss the opportunity to learn another interest of the user. This is a difficult problem to solve, and there is no one-size-fits-all solution. One approach is to lower the threshold a little bit and deliver some near misses to the user to see their response to this extra document. Ultimately, the goal is to strike a balance between exploration and exploitation to deliver the most relevant information to the user.ISHIKAThe exploration-exploitation tradeoff is a decision-making dilemma that arises when an individual or organization must choose between exploring new options and exploiting known options. Exploration involves seeking out new information, experiences, or opportunities, while exploitation involves maximizing the benefits of existing knowledge, resources, or strategies. The tradeoff arises because investing too much in exploration may lead to missed opportunities for exploitation, while investing too much in exploitation may lead to stagnation and missed opportunities for growth. Finding the right balance between exploration and exploitation is critical for long-term success and innovation.ISHIKAThis issue is called the exploration-exploitation tradeoff. This means we want to explore the document space to see if the user might be interested in the documents that we have not yet labeled, but we don’t want to show the user too many nonrelevant documents or they will be unsatisfied with the system.',\n",
       " 'Beta-gamma threshold learning is an algorithm used in information retrieval to determine the optimal score threshold for a given utility function. The algorithm works by plotting the utility value at each different cutoff position and adjusting the threshold dynamically based on the number of judged examples in the training database. The parameters alpha, beta, and gamma are used to determine the optimal cutoff point between the optimal and zero utility points. The goal of the algorithm is to achieve the maximum utility by choosing the optimal threshold.ISHIKABeta-gamma threshold learning is a type of machine learning algorithm that is used for classification tasks. It is based on the idea of setting a threshold value for the output of a neuron, which determines whether the neuron fires or not. The threshold value is adjusted during the learning process based on the error rate of the network. The beta-gamma threshold learning algorithm is particularly useful for problems where the input data is noisy or incomplete, as it can adapt to these conditions and still produce accurate results.ISHIKAGiven a ranked list of all the documents in the training database sorted by their scores on the x-axis, their relevance, and a specific utility U, we can plot the utility value at each different cutoff position θ. Each cutoff position corresponds to a score threshold. Figure 11.5 shows this configuration and how a choice of α determines a cutoff point between the optimal and the zero utility points, and how β and γ help us to adjust α dynamically according to the number of judged examples in the training database. The optimal point θopt is the point when we would achieve the maximum utility if we had chosen this threshold. The θzero threshold is the zero utility threshold. Between these two θ values give us a safe point to explore the potential cutoff values.',\n",
       " 'There are two types of word relations: paradigmatic and syntagmatic. Words have a paradigmatic relation if they can be substituted for each other without affecting the understanding of the sentence. This means that the two words are in the same semantic or syntactic class. For example, the words \"cat\" and \"dog\" have a paradigmatic relation because they are both in the same word class, \"animal.\" On the other hand, words have a syntagmatic relation if they are used together in a sentence. This means that the words are related in terms of their position in a sentence and the meaning they convey when used together. Chapter 14 of the course textbook provides additional ideas on how to group similar terms together.ISHIKAThere are several types of word relations, including: 1. Synonyms: words that have similar meanings, such as \"happy\" and \"joyful.\" 2. Antonyms: words that have opposite meanings, such as \"hot\" and \"cold.\" 3. Homonyms: words that sound the same but have different meanings, such as \"flower\" and \"flour.\" 4. Homophones: words that sound the same but are spelled differently and have different meanings, such as \"there\" and \"their.\" 5. Hyponyms: words that are more specific than a general term, such as \"apple\" being a hyponym of \"fruit.\" 6. Hypernyms: words that are more general than a specific term, such as \"vehicle\" being a hypernym of \"car.\" 7. Meronyms: words that are part of a larger whole, such as \"wheel\" being a meronym of \"car.\" 8. Holonyms: words that are the larger whole of a part, such as \"car\" being a holonym of \"wheel.\"ISHIKAIn general, there are two types of word relations; one is called a paradigmatic relation and the other is a syntagmatic relation. Word a and b have a paradigmatic relation if they can be substituted for each other. That means the two words that have paradigmatic relation would be in the same semantic class, or syntactic class. [...] The second kind of relation is called a syntagmatic relation. In this case, the two words that have this relation can be combined with each other. Thus, words a and b have a syntagmatic relation if they can be combined with each other in a grammatical sentence—meaning that these two words are semantically related',\n",
       " 'Intrusion detection is a measure used in the evaluation of topic models, as described in the course textbook. It involves adding an additional random word to a list of top k similar words for a given vocabulary word, and seeing if it is easy to spot the intruder. If the top k words form a coherent group, then they are considered a good word association group. Human judges can respond to intrusion detection scenarios to measure the coherency of topic-word distributions. Another test for coherency is the document-topic distribution evaluation, which involves selecting the top three topics for a given document and adding an additional low probability topic to see if it is easy to spot.ISHIKAIntrusion detection is the process of monitoring a network or system for unauthorized access or malicious activity. It involves analyzing network traffic, system logs, and other data sources to identify potential security threats and alert security personnel to take action. Intrusion detection systems can be either host-based or network-based, and they use a variety of techniques such as signature-based detection, anomaly detection, and behavioral analysis to identify potential threats. The goal of intrusion detection is to detect and respond to security incidents in a timely manner, before they can cause significant damage to the network or system.ISHIKAA human-based evaluation metric would be intrusion detection. [...] If the word associations are good, it should be fairly easy to find an “intruder” that has been added into the top k similar words. [...] L1 = {dog , cat, horse, apple, pig , cow}, L2 = {car, teacher, platypus, agile, blue, Zaire} [...] In L1, it’s quite obvious that apple is the intruder since it doesn’t fit in with the other words in the list. Thus, the remaining words form a good word association list. In L2, we can’t really tell which word is the intruder, meaning the word association algorithm used to create the k candidates in L2 is not as good as the one used to generate L1.',\n",
       " 'There are several cluster similarity measures that can be used in information retrieval. Three common measures are single-link, complete-link, and average-link. Single-link merges the two clusters with the smallest minimum distance, resulting in looser clusters. Complete-link merges the two clusters with the smallest maximum distance between elements, resulting in very tight and compact clusters. Average-link is a compromise between single-link and complete-link. In addition to these measures, there are also similarity functions such as the Pearson Correlation Coefficient and the cosine measure. These measures are based on user preferences on items and do not use any content information of these items.When evaluating clusters, there are three criteria to consider: coherence, separation, and utility. Coherence and separation can be measured automatically with measures such as vector similarity, purity, or mutual information. However, evaluating term clustering may be more challenging since word-to-word similarity algorithms may not be as obvious as document-to-document similarities.ISHIKA1. Euclidean distance 2. Manhattan distance 3. Cosine similarity 4. Jaccard similarity 5. Pearson correlation coefficient 6. Spearman correlation coefficient 7. Hamming distance 8. Mahalanobis distance 9. Tanimoto coefficient 10. Bray-Curtis dissimilarity.ISHIKASingle-link merges the two clusters with the smallest minimum distance. This results in “looser” clusters, since we only need to find two individually close elements in each cluster in order to perform the merge. Complete-link merges the two clusters with the smallest maximum distance between elements. This results in very “tight” and “compact” clusters since the cluster diameter is kept small (i.e., the distance between all elements low). Average-link is a compromise between the two previous measures. As its name implies, it takes the smallest average distance between two clusters']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = data.split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba9bf404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fine_tuned</th>\n",
       "      <th>gpt</th>\n",
       "      <th>textbook</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A background language model is necessary in in...</td>\n",
       "      <td>A background language model is needed to impro...</td>\n",
       "      <td>Clearly, we need to get rid of these stop word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A tokenizer is a tool used in information retr...</td>\n",
       "      <td>Tokenization is a fundamental step in many nat...</td>\n",
       "      <td>Thus, in tokenization we will simply use the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The components of an inverted index include th...</td>\n",
       "      <td>1. Terms: The terms or words that are indexed....</td>\n",
       "      <td>An inverted index is the main data structure u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The document filtering problem refers to the t...</td>\n",
       "      <td>The document filtering problem is the task of ...</td>\n",
       "      <td>Another common task is only returning document...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There are three major aspects of a search engi...</td>\n",
       "      <td>1. Crawling: The process of discovering and in...</td>\n",
       "      <td>Effectiveness or accuracy. How accurate are th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The reason we cannot take the mean of precisio...</td>\n",
       "      <td>We cannot take the mean of precision and recal...</td>\n",
       "      <td>Why is this not as good as F1, i.e., what’s th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We can use IR models for ranking of webpages, ...</td>\n",
       "      <td>As an AI language model, I don't have personal...</td>\n",
       "      <td>First, on the web we tend to have very differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>To evaluate a filtering system, we need to mea...</td>\n",
       "      <td>As an AI language model, I do not have persona...</td>\n",
       "      <td>One common strategy is to use a utility functi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Context-based filtering is a type of content-b...</td>\n",
       "      <td>Context-based filtering typically involves the...</td>\n",
       "      <td>The three basic components in content-based fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The exploration-exploitation tradeoff refers t...</td>\n",
       "      <td>The exploration-exploitation tradeoff is a dec...</td>\n",
       "      <td>This issue is called the exploration-exploitat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beta-gamma threshold learning is an algorithm ...</td>\n",
       "      <td>Beta-gamma threshold learning is a type of mac...</td>\n",
       "      <td>Given a ranked list of all the documents in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>There are two types of word relations: paradig...</td>\n",
       "      <td>There are several types of word relations, inc...</td>\n",
       "      <td>In general, there are two types of word relati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Intrusion detection is a measure used in the e...</td>\n",
       "      <td>Intrusion detection is the process of monitori...</td>\n",
       "      <td>A human-based evaluation metric would be intru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>There are several cluster similarity measures ...</td>\n",
       "      <td>1. Euclidean distance 2. Manhattan distance 3....</td>\n",
       "      <td>Single-link merges the two clusters with the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           fine_tuned  \\\n",
       "0   A background language model is necessary in in...   \n",
       "1   A tokenizer is a tool used in information retr...   \n",
       "2   The components of an inverted index include th...   \n",
       "3   The document filtering problem refers to the t...   \n",
       "4   There are three major aspects of a search engi...   \n",
       "5   The reason we cannot take the mean of precisio...   \n",
       "6   We can use IR models for ranking of webpages, ...   \n",
       "7   To evaluate a filtering system, we need to mea...   \n",
       "8   Context-based filtering is a type of content-b...   \n",
       "9   The exploration-exploitation tradeoff refers t...   \n",
       "10  Beta-gamma threshold learning is an algorithm ...   \n",
       "11  There are two types of word relations: paradig...   \n",
       "12  Intrusion detection is a measure used in the e...   \n",
       "13  There are several cluster similarity measures ...   \n",
       "\n",
       "                                                  gpt  \\\n",
       "0   A background language model is needed to impro...   \n",
       "1   Tokenization is a fundamental step in many nat...   \n",
       "2   1. Terms: The terms or words that are indexed....   \n",
       "3   The document filtering problem is the task of ...   \n",
       "4   1. Crawling: The process of discovering and in...   \n",
       "5   We cannot take the mean of precision and recal...   \n",
       "6   As an AI language model, I don't have personal...   \n",
       "7   As an AI language model, I do not have persona...   \n",
       "8   Context-based filtering typically involves the...   \n",
       "9   The exploration-exploitation tradeoff is a dec...   \n",
       "10  Beta-gamma threshold learning is a type of mac...   \n",
       "11  There are several types of word relations, inc...   \n",
       "12  Intrusion detection is the process of monitori...   \n",
       "13  1. Euclidean distance 2. Manhattan distance 3....   \n",
       "\n",
       "                                             textbook  \n",
       "0   Clearly, we need to get rid of these stop word...  \n",
       "1   Thus, in tokenization we will simply use the r...  \n",
       "2   An inverted index is the main data structure u...  \n",
       "3   Another common task is only returning document...  \n",
       "4   Effectiveness or accuracy. How accurate are th...  \n",
       "5   Why is this not as good as F1, i.e., what’s th...  \n",
       "6   First, on the web we tend to have very differe...  \n",
       "7   One common strategy is to use a utility functi...  \n",
       "8   The three basic components in content-based fi...  \n",
       "9   This issue is called the exploration-exploitat...  \n",
       "10  Given a ranked list of all the documents in th...  \n",
       "11  In general, there are two types of word relati...  \n",
       "12  A human-based evaluation metric would be intru...  \n",
       "13  Single-link merges the two clusters with the s...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_lines = [line.split('ISHIKA') for line in lines]\n",
    "ft = [sl[0] for sl in split_lines]\n",
    "gpt = [sl[1] for sl in split_lines]\n",
    "book = [sl[2] for sl in split_lines]\n",
    "output = pd.DataFrame({'fine_tuned':ft, 'gpt':gpt, 'textbook':book})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e8d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebb64590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Why do we need a background language model?------\n",
      "textbook with fine_tuned: 0.07633587786259542\n",
      "textbook with gpt: 0.06896551724137931\n",
      "------How does a tokenizer work?------\n",
      "textbook with fine_tuned: 0.08298755186721991\n",
      "textbook with gpt: 0.06593406593406594\n",
      "------What are the components of an inverted index?------\n",
      "textbook with fine_tuned: 0.27058823529411763\n",
      "textbook with gpt: 0.0958904109589041\n",
      "------What is the document filtering problem?------\n",
      "textbook with fine_tuned: 0.13526570048309178\n",
      "textbook with gpt: 0.07971014492753623\n",
      "------What are the aspects of a search engine?------\n",
      "textbook with fine_tuned: 0.17341040462427745\n",
      "textbook with gpt: 0.045662100456621\n",
      "------For computing the f1 score, why can't we take the mean of precision and recall?------\n",
      "textbook with fine_tuned: 0.2247191011235955\n",
      "textbook with gpt: 0.08670520231213873\n",
      "------Why can’t we use IR models for ranking of webpages?------\n",
      "textbook with fine_tuned: 0.1509433962264151\n",
      "textbook with gpt: 0.08713692946058091\n",
      "------How do you evaluate a filtering system?------\n",
      "textbook with fine_tuned: 0.06866952789699571\n",
      "textbook with gpt: 0.04326923076923077\n",
      "------What are the components in context-based filtering?------\n",
      "textbook with fine_tuned: 0.12830188679245283\n",
      "textbook with gpt: 0.04945054945054945\n",
      "------What is the exploration-exploitation tradeoff?------\n",
      "textbook with fine_tuned: 0.15\n",
      "textbook with gpt: 0.058823529411764705\n",
      "------What is beta-gamma threshold learning?------\n",
      "textbook with fine_tuned: 0.16842105263157894\n",
      "textbook with gpt: 0.06190476190476191\n",
      "------What type of word relations are there?------\n",
      "textbook with fine_tuned: 0.15346534653465346\n",
      "textbook with gpt: 0.045081967213114756\n",
      "------What is intrusion detection?------\n",
      "textbook with fine_tuned: 0.10747663551401869\n",
      "textbook with gpt: 0.0430622009569378\n",
      "------What are some cluster similarity measures?------\n",
      "textbook with fine_tuned: 0.10599078341013825\n",
      "textbook with gpt: 0.008264462809917356\n"
     ]
    }
   ],
   "source": [
    "# not super efficient, but because we are dealing with a small dataframe, we can get away with it\n",
    "for i, row in output.iterrows():\n",
    "    print(f'------{queries[i]}------')\n",
    "    list1 = row['textbook'].split(' ')\n",
    "\n",
    "    #fine_tuned\n",
    "    list2 = row['fine_tuned'].split(' ')\n",
    "    sim = jaccard(list1, list2)\n",
    "    print(f'textbook with fine_tuned: {sim}')\n",
    "\n",
    "    #plain gpt\n",
    "    list2 = row['gpt'].split(' ')\n",
    "    sim = jaccard(list1, list2)\n",
    "    print(f'textbook with gpt: {sim}')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
