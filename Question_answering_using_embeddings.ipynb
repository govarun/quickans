{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0435cb",
   "metadata": {},
   "source": [
    "# Question answering using embeddings-based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3839a6-9146-4f60-b74b-19abbc24278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import openai  # for calling the OpenAI API\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "from keys import *\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d50792",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"ans_generator/data/ir_book_embeddings.csv\"\n",
    "\n",
    "df = pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70307f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert embeddings from CSV str type back to list type\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424162c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChengXiang Zhai Sean Massung Text Data Managem...</td>\n",
       "      <td>[-0.017203252762556076, 0.021963899955153465, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has led to an increasing demand for power...</td>\n",
       "      <td>[-0.016908559948205948, 0.010532873682677746, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike data generated by a computer system or ...</td>\n",
       "      <td>[-0.018540382385253906, 0.014797425828874111, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As such text data are especially valuable for ...</td>\n",
       "      <td>[-0.020149044692516327, 0.02085673063993454, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In contrast to structured data which conform t...</td>\n",
       "      <td>[-0.02140810526907444, 0.02335185930132866, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8045</th>\n",
       "      <td>Please note that this α is different from αd i...</td>\n",
       "      <td>[0.0068044946528971195, 0.035624921321868896, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>Of course the next question is how to estimate...</td>\n",
       "      <td>[0.007543814368546009, 0.032577402889728546, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8047</th>\n",
       "      <td>where F  d1 dk is the set of feedback document...</td>\n",
       "      <td>[-0.016099639236927032, 0.021384460851550102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>Now given λ the feedback documents F  and the ...</td>\n",
       "      <td>[-0.014733465388417244, 0.021942878141999245, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>.</td>\n",
       "      <td>[-0.008761508390307426, -0.016880812123417854,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8050 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   \n",
       "0     ChengXiang Zhai Sean Massung Text Data Managem...  \\\n",
       "1     This has led to an increasing demand for power...   \n",
       "2     Unlike data generated by a computer system or ...   \n",
       "3     As such text data are especially valuable for ...   \n",
       "4     In contrast to structured data which conform t...   \n",
       "...                                                 ...   \n",
       "8045  Please note that this α is different from αd i...   \n",
       "8046  Of course the next question is how to estimate...   \n",
       "8047  where F  d1 dk is the set of feedback document...   \n",
       "8048  Now given λ the feedback documents F  and the ...   \n",
       "8049                                                  .   \n",
       "\n",
       "                                              embedding  \n",
       "0     [-0.017203252762556076, 0.021963899955153465, ...  \n",
       "1     [-0.016908559948205948, 0.010532873682677746, ...  \n",
       "2     [-0.018540382385253906, 0.014797425828874111, ...  \n",
       "3     [-0.020149044692516327, 0.02085673063993454, 0...  \n",
       "4     [-0.02140810526907444, 0.02335185930132866, 0....  \n",
       "...                                                 ...  \n",
       "8045  [0.0068044946528971195, 0.035624921321868896, ...  \n",
       "8046  [0.007543814368546009, 0.032577402889728546, -...  \n",
       "8047  [-0.016099639236927032, 0.021384460851550102, ...  \n",
       "8048  [-0.014733465388417244, 0.021942878141999245, ...  \n",
       "8049  [-0.008761508390307426, -0.016880812123417854,...  \n",
       "\n",
       "[8050 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataframe has two columns: \"text\" and \"embedding\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a8c713-c8a9-47dc-85a4-871ee1395566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da034bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.  We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied.  Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We’ve already described all indexing steps except crawling in detail in Chapter 8.  After our crawling discussion we move onto the particular challenges of web indexing.  Then we discuss how we can take advantage of links between pages in link analysis.  The last technique we discuss is learning to rank which is a way to combine many different features for ranking.  10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.  We then add them to a queue and then explore those page’s links in a breadthfirst search until we are satisfied.  Building a real crawler is quite tricky and there are some complicated issues that we inevitably deal with.  One issue is robustness What if the server doesn’t respond or returns unparseable garbage What if there’s a trap that generates dynamically generated pages that attract your crawler to keep crawling the same site in circles Yet another issue is that we don’t want to overload one particular server with too many crawling requests.  Those may cause the site to experience a denial of service some sites will also block IP addresses that they believe to be crawling them or creating too many requests.  In a similar vein a crawler should respect the robot exclusion protocol.  A file called robots.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the next section we will discuss crawling.  We’ve already described all indexing steps except crawling in detail in Chapter 8.  After our crawling discussion we move onto the particular challenges of web indexing.  Then we discuss how we can take advantage of links between pages in link analysis.  The last technique we discuss is learning to rank which is a way to combine many different features for ranking.  10. 1 Web Crawling The crawler is also called a spider or a software robot that crawls traverses parses and downloads pages on the web.  Building a toy crawler is relatively easy because you just need to start with a set of seed pages fetch pages from the web and parse these pages’ new links.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'One interesting variation is called focused crawling.  Here we’re going to crawl some pages about a particular topic eg all pages about automobiles.  This is typically going to start with a query that you use to get some results.  Then you gradually crawl more.  An even more extreme version of focused crawling is for example downloading and indexing all forum posts on a particular forum.  In this case we might have a URL such as httpwww. textdatabookforum. comboardsid3 which refers to the third post on the forum.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examples\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"Web Crawling\", df, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f45cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = \"Suppose you are a teaching assistant for the course Advanced Information Retrieval and a student has posed the following question.\\n\"\n",
    "    question = f\"\\n\\nQuestion: {query}\\n\\n\"\n",
    "    end = 'How will you answer the question? \\n Here are some snippets from the course textbook which may be useful.\\n\\n\"'\n",
    "    book_info = \"\"\n",
    "    \n",
    "    preface = introduction + question + end\n",
    "    for string in strings:\n",
    "        next_article = f'\\n{string}\\n'\n",
    "        if (\n",
    "            num_tokens(preface + book_info + next_article, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            book_info += next_article\n",
    "    return preface + book_info\n",
    "\n",
    "def api_call(message, model: str = GPT_MODEL):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    \n",
    "    reply = api_call(message, model)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d61f413f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multi-Bernoulli and Multinomial are two different models used in probability theory and statistics to model queries. \\n\\nMulti-Bernoulli is a model used to represent a sequence of binary events, where each event can have one of two outcomes (success or failure). It is used to model queries where the data is binary, such as in object tracking or sensor fusion. In Multi-Bernoulli, the probability of success or failure is assumed to be constant across all events.\\n\\nOn the other hand, Multinomial is a model used to represent a sequence of events where each event can have one of several outcomes. It is used to model queries where the data is categorical, such as in natural language processing or image classification. In Multinomial, the probability of each outcome is assumed to be constant across all events.\\n\\nIn summary, Multi-Bernoulli is used for binary data, while Multinomial is used for categorical data.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'When modeling queries, how is multi-bernoulli different from multinomial?'\n",
    "\n",
    "api_call(query)\n",
    "# ask('When modeling queries, how is multi-bernoulli different from multinomial?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3cdb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query):\n",
    "    emb_reply = ask(query)\n",
    "    base_reply = api_call(query)\n",
    "\n",
    "    print(f'Query:{query}\\n\\n')\n",
    "    print(\"Embedding Method response\\n\")\n",
    "    print(emb_reply+\"\\n\\n\")\n",
    "    print('Base GPT response\\n')\n",
    "    print(base_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d277e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:How do you define a reference language model?\n",
      "\n",
      "\n",
      "Embedding Method response\n",
      "\n",
      "A reference language model is a probability distribution over word sequences that is used to estimate the probability of an unseen word in a corpus. If a word is not observed in the corpus, its probability is assumed to be governed by the reference language model. In the case of retrieval, a natural choice for the reference language model would be the collection language model. The collection language model is used to adjust the probability of the maximum likelihood estimate of seen terms. The general English language model can be used as a background language model to filter out common words and obtain a probability ratio for each word. Language models are useful for quantifying the uncertainties associated with the use of natural language and can help answer many interesting questions related to text analysis and information retrieval. Neural language models can be represented as a neural network and can be used to learn vector representations for all the words in a specific dataset.\n",
      "\n",
      "\n",
      "Base GPT response\n",
      "\n",
      "A reference language model is a pre-trained language model that serves as a benchmark for evaluating the performance of other language models. It is typically trained on a large corpus of text data and is used to generate high-quality text samples. The reference language model is often used to compare the performance of other language models, such as those trained on specific domains or with different architectures. The reference language model is also used to evaluate the quality of text generated by other models, by comparing it to the text generated by the reference model.\n"
     ]
    }
   ],
   "source": [
    "query = 'How do you define a reference language model?'\n",
    "\n",
    "compare_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebefa8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:How can we use language models for part of speech tagging?\n",
      "\n",
      "\n",
      "Embedding Method response\n",
      "\n",
      "To use language models for part of speech tagging, we can assign a part of speech tag to each word based on its surrounding words. This allows us to count the most frequent nouns or determine what kind of nouns are associated with what kind of verbs. We can also consider n-grams of part of speech tags and mix n-grams of words and part of speech tags to create hybrid features that can be useful for sentiment analysis. Additionally, we can use language models to perform semantic analysis of word relations by finding what words are semantically associated with a given word. However, it is important to note that while part of speech tagging is a relatively easy task, full structure parsing and semantic analysis remain difficult and are only successful for some aspects of analysis. Overall, using language models for part of speech tagging can enrich the representation of text data and enable deeper analysis.\n",
      "\n",
      "\n",
      "Base GPT response\n",
      "\n",
      "Language models can be used for part of speech tagging by training them on a large corpus of text data that has been manually annotated with part of speech tags. The language model learns to predict the most likely part of speech tag for each word in a sentence based on the context of the surrounding words. This is done by using a probabilistic model that assigns a probability to each possible part of speech tag for each word in the sentence. The model then selects the most likely tag for each word based on these probabilities. Once the language model has been trained, it can be used to automatically tag the parts of speech in new text data. This can be useful for a variety of natural language processing tasks, such as text classification, sentiment analysis, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "query = 'How can we use language models for part of speech tagging?'\n",
    "\n",
    "compare_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2589a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:What happens to the Dirichlet smoothing when the document langeth goes to infinity\n",
      "\n",
      "\n",
      "Embedding Method response\n",
      "\n",
      "When the document length goes to infinity, the effect on Dirichlet smoothing is that the coefficient αd tends to zero. This is because αd penalizes long documents, and as the document length increases, the number of observed words also increases, making it less necessary to do smoothing. Therefore, the Dirichlet smoothing becomes less effective as the document length increases towards infinity. However, it is important to note that this effect is not unique to Dirichlet smoothing and is a general property of smoothing methods that penalize long documents.\n",
      "\n",
      "\n",
      "Base GPT response\n",
      "\n",
      "When the document length goes to infinity, the effect of Dirichlet smoothing decreases. This is because as the document length increases, the probability estimates become more accurate and the need for smoothing decreases. In other words, as the amount of data increases, the impact of the prior distribution (used in Dirichlet smoothing) becomes less significant. Therefore, in practice, Dirichlet smoothing is typically used for smaller datasets where the probability estimates may be unreliable due to limited data.\n"
     ]
    }
   ],
   "source": [
    "query = 'What happens to the Dirichlet smoothing when the document langeth goes to infinity'\n",
    "\n",
    "compare_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f56ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
