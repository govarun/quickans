{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright 2023 QuickAns\n",
    " *\n",
    " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " * you may not use this file except in compliance with the License.\n",
    " * You may obtain a copy of the License at\n",
    " *\n",
    " *    http://www.apache.org/licenses/LICENSE-2.0\n",
    " *\n",
    " * Unless required by applicable law or agreed to in writing, software\n",
    " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " * See the License for the specific language governing permissions and\n",
    " * limitations under the License.\n",
    " '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key\n",
    "1. Collect: We download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import mwclient  # for downloading example Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "import openai  # for generating embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API key (if needed)\n",
    "\n",
    "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keys import *\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed document chunks\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"ans_generator/data/ir_txt/paragraphs.txt\"\n",
    "with open(in_file, 'r') as f:\n",
    "    data_book = f.read().split('\\n')\n",
    "    data_book.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tokens(data[0])\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(data_book), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = data_book[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": data_book, \"embedding\": embeddings})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store document chunks and embeddings\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "\n",
    "SAVE_PATH = \"ans_generator/data/ir_book_embeddings.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "#     \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "#     chunks = string.split(delimiter)\n",
    "#     if len(chunks) == 1:\n",
    "#         return [string, \"\"]  # no delimiter found\n",
    "#     elif len(chunks) == 2:\n",
    "#         return chunks  # no need to search for halfway point\n",
    "#     else:\n",
    "#         total_tokens = num_tokens(string)\n",
    "#         halfway = total_tokens // 2\n",
    "#         best_diff = halfway\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             left = delimiter.join(chunks[: i + 1])\n",
    "#             left_tokens = num_tokens(left)\n",
    "#             diff = abs(halfway - left_tokens)\n",
    "#             if diff >= best_diff:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 best_diff = diff\n",
    "#         left = delimiter.join(chunks[:i])\n",
    "#         right = delimiter.join(chunks[i:])\n",
    "#         return [left, right]\n",
    "\n",
    "\n",
    "# def truncated_string(\n",
    "#     string: str,\n",
    "#     model: str,\n",
    "#     max_tokens: int,\n",
    "#     print_warning: bool = True,\n",
    "# ) -> str:\n",
    "#     \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "#     encoding = tiktoken.encoding_for_model(model)\n",
    "#     encoded_string = encoding.encode(string)\n",
    "#     truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "#     if print_warning and len(encoded_string) > max_tokens:\n",
    "#         print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "#     return truncated_string\n",
    "\n",
    "\n",
    "# def split_strings_from_subsection(\n",
    "#     subsection: tuple[list[str], str],\n",
    "#     max_tokens: int = 1000,\n",
    "#     model: str = GPT_MODEL,\n",
    "#     max_recursion: int = 5,\n",
    "# ) -> list[str]:\n",
    "#     \"\"\"\n",
    "#     Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "#     Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "#     \"\"\"\n",
    "#     titles, text = subsection\n",
    "#     string = \"\\n\\n\".join(titles + [text])\n",
    "#     num_tokens_in_string = num_tokens(string)\n",
    "#     # if length is fine, return string\n",
    "#     if num_tokens_in_string <= max_tokens:\n",
    "#         return [string]\n",
    "#     # if recursion hasn't found a split after X iterations, just truncate\n",
    "#     elif max_recursion == 0:\n",
    "#         return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "#     # otherwise, split in half and recurse\n",
    "#     else:\n",
    "#         titles, text = subsection\n",
    "#         for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "#             left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "#             if left == \"\" or right == \"\":\n",
    "#                 # if either half is empty, retry with a more fine-grained delimiter\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 # recurse on each half\n",
    "#                 results = []\n",
    "#                 for half in [left, right]:\n",
    "#                     half_subsection = (titles, half)\n",
    "#                     half_strings = split_strings_from_subsection(\n",
    "#                         half_subsection,\n",
    "#                         max_tokens=max_tokens,\n",
    "#                         model=model,\n",
    "#                         max_recursion=max_recursion - 1,\n",
    "#                     )\n",
    "#                     results.extend(half_strings)\n",
    "#                 return results\n",
    "#     # otherwise no split was found, so just truncate (should be very rare)\n",
    "#     return [truncated_string(string, model=model, max_tokens=max_tokens)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
