There is really no difference. Yes, JM is a particular type of interpolation where the two functions to be interpolated are the ML estimate and a reference (background) language model and the coefficient is a fixed constant. You may notice that the Dirichlet prior smoothing is also a kind of interpolation, but there the coefficient is dynamic since its value depends on the sample size (the number of observed words).
This is an excellent question! There are many application scenarios where one order makes more sense than the other order. You'll see such an example later in the lectures on the KL-divergence retrieval model (using KL-divergence to score documents w.r.t. a query). There, you'll see it makes more sense to compute D(q||d) than D(d||q) where q and d are the (unigram) language models for the query and document, respectively.
That's a good question. One solution is to do smoothing. Essentially, we can try to drop some words in the condition. In the extreme case, we'd drop all the words in the condition, i.e., we'll have a unigram LM p(wm).
Great question & great idea! The short answer to your question is that it depends on the specific query and the accuracy of the initial retrieval algorithm. If the initial retrieval results are reasonably accurate (e.g., precision at 5 documents is at least 0.5), then we would expect the blind/pseudo feedback to help (if we use, say, top 5 documents for feedback). In such a case, it's quite possible that the new results from the blind feedback would be even more accurate than the initial results, and thus we can expect a second round of blind feedback would further improve performance. However, if the initial results are not so accurate, doing it a second time might hurt the performance (causing the query concept to drift too much from the original query). Indeed, even doing it once may hurt the performance if the initial results aren't accurate. How to intelligently determine for which query blind feedback should be done and how many iterations is mostly an open research question. It's an excellent topic for a course project!
Since IDF is primarily to formulate against the boost in score caused by matching a frequent term, the first statement should be correct. Let's say, a term is more frequent in most of the documents. Then, the value/impact of matching that term is not as much as matching another term that is rare. The match does not necessarily prove the relevance of any doc to the query. Therefore, any model that attempts to decrease the impact of matching frequent terms can be regarded as having an IDF weighting.
It's because in line 3, -lambda*H(theta;theta_C) is first summed from i=1 to n, then multiplied by 1/n, thus canceled out.
Perhaps some operations may not hold the proportionality rule but it may ensure rank equivalence. For example, ranking by a probability value and its logarithm are rank equivalent since if one increases, the other does too, but they are not proportional. So if two quantities are proportional, perhaps they are indeed rank equivalent, but the other way around is not necessarily true.
This is an open-ended question and it would involve some algebraic transformation of formulas. Thus, you may not necessarily be able to derive a definitive answer. However, going through this analysis as an exercise can help you analytically assess the quality of a retrieval function (e.g., you might gain insights about when such a function might behave unreasonably). The math part is beyond the scope of this course, but if you are willing to work on it a bit, some simple transformation of the formula would likely provide some insights. In order to check whether the score would be increased or decreased, you may realize that you would like to compare the two scores (the score of the original document and the score of the generated long document by duplicating the document multiple times) in some way, e.g., taking their ratio or difference. In this case, subtracting one from the other would help simplify the analysis. Another way to simplify the problem is to consider special cases that are easier to analyze. For example, you could consider a simple case where there is only one query term or just two terms. Or you can consider a special document where you have only a few words. The conclusion might also depend on other factors such as whether c(w,d)/|d| is larger than p(w|C), etc. In general, just try to do some exploration by looking into special cases and write about whatever conclusion(s) you may have. It's fine to only be able to say in a particular special case, the conclusion is ...., or under condition A, the conclusion would be ..., while under condition B, the conclusion would be different. If you want to know more about this kind of analysis, you can take a look at the following resource page about the axiomatic retrieval framework: https://www.eecis.udel.edu/~hfang/AX.html
Here, if w is frequent in the background model, then p(w|C) will be large. Thus, by having high value of lambda, the reason for high probability of w, will be explained by the background model. Thus, the p(w|d) will be almost same for two different documents, while the document that has important keywords which is essentially what we are looking for will be ranked high. For example, if one document has more occurrences of the word "the" than the other document, the P("the"|d) for both the documents will almost be same. Hence, this gives out the effect of penalizing the occurrence of frequent word in the document.
First of all, you need to understand that likelihood here refers to the probability of all the observed data. In this toy example, it's the probability of observing a toy document with just four words. Log-likelihood simply refers to logarithm of the likelihood. Please review lectures 5.1 and 5.2
That is a great observation. I think the answer lies in the derivation where we said that for  m a x ( x y ) max(xy)  given,  x + y = k x+y=k  occurs at  x = y x=y . The maximum would occur at x = y, but the value of x at which it happens might be unattainable, such as in this case.  Refer to slide# 32 for the terms used below. If we assume, two terms of  p ( d ∣ λ ) , ( 1 − P ( θ B ) ) ∗ p ( “ t e x t ” ∣ θ d ) + P ( θ B ) ∗ 0.1 = x p(d∣λ),(1−P(θ  B ​   ))∗p(“text”∣θ  d ​   )+P(θ  B ​   )∗0.1=x  and  ( 1 − P ( θ B ) ) ∗ p ( “ t h e ” ∣ θ d ) + P ( θ B ) ∗ 0.9 = y (1−P(θ  B ​   ))∗p(“the”∣θ  d ​   )+P(θ  B ​   )∗0.9=y . We know that,  x + y = 1 x+y=1 , and we want to maximize  x y xy . This will happen at  x = y = 1 / 2 x=y=1/2 . And  x y xy  is increasing for  x < 1 / 2 x<1/2 . So, we can take the largest possible value of  x x  less than  1 / 2 1/2 , and the corresponding value of  P ( d ∣ λ ) P(d∣λ)  would be the maximum. Or we could take the smallest possible value of  x x  larger than 1/2 as  x y xy  is decreasing in that region. The range of values of  x x  given  p ( θ B ) = 0.9 p(θ  B ​   )=0.9  is  [ 0.09 , 0.19 ] [0.09,0.19]  for  p ( “ t e x t ” ∣ θ d ) = 0 p(“text”∣θ  d ​   )=0  and  1 1  respectively. Hence, the maximum value of  P ( d ∣ λ ) P(d∣λ)  in this case would be  x ( 1 − x ) = 0.19 ∗ 0.81 = 0.1539 x(1−x)=0.19∗0.81=0.1539 . This value is lesser than what  x ( 1 − x ) x(1−x)  could have reached given no constraints on  x x , i.e.  0.5 ∗ 0.5 = 0.25 0.5∗0.5=0.25